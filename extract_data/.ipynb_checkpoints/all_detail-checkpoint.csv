hackathon_name,project_name,project_link,winner,inspiration,what_it_does,how_we_built_it,challenges,accomplishments,what_we_learned,next_steps,built_with,external_links
Auburn Hacks 2024,Resumate,https://devpost.com/software/resumate-bwk81m,True,"Resumate was inspired by the challenge job seekers face in tailoring their resumes to specific job descriptions. With the job market becoming increasingly competitive, it's essential for candidates to present their experience and skills in a way that aligns closely with the requirements and language of a job posting. Recognizing this need, Resumate aims to streamline the resume customization process using the power of AI, ensuring that each application sent out is as relevant and compelling as possible.","Resumate is a web application that utilizes advanced AI to help users rewrite and optimize their resumes in real-time. Users can upload their existing resume, input the job description they're targeting, and receive suggestions on how to rephrase and structure their resume content to better match the job's requirements. The system focuses on making minimal yet impactful changes, maintaining the original intent while enhancing relevance.","We built Resumate using Flask for the backend, which handles form submissions, file uploads, and interacts with our AI model. The frontend was developed with HTML, CSS, and JavaScript, providing a user-friendly interface for uploading resumes and entering job descriptions. The AI rewriting functionality is powered by OpenAI's GPT-4, which processes the input and generates tailored content. All of this is supported by a MongoDB database that stores submission data for further analysis and refinement of the service.","One of the main challenges was ensuring the AI's suggestions remained accurate and true to the user's original experiences and skills. We also faced technical difficulties with file handling and maintaining state across the web application. Ensuring a seamless integration between the frontend, backend, and the AI model required meticulous debugging and testing.","We're proud of creating a tool that can genuinely help job seekers improve their chances of landing an interview. Implementing the AI to understand and process complex job descriptions and resume data was a significant achievement. Additionally, creating an intuitive user interface that simplifies a typically daunting task is something we're particularly pleased with.","Throughout the development of Resumate, we learned about the nuances of natural language processing and its application in real-world scenarios. We also gained experience in developing a full-stack application, from database management to user interface design, and the importance of user experience in the success of a tool like this.","Looking ahead, we plan to incorporate more personalized feedback mechanisms, allowing users to receive more granular suggestions. We also aim to expand our database to include a diverse range of job descriptions and resume samples, which will help refine the AI's accuracy. In the long term, we're exploring partnerships with job boards and recruitment agencies to integrate Resumate directly into the job application process.","css3, flask, html5, javascript, openai",https://github.com/Michaelgathara/resumate
Auburn Hacks 2024,Courage The Not-so Cowardly Rover,https://devpost.com/software/courage-the-not-so-cowardly-rover,True,"Our inspiration came from the resources NASA provides through their APIs, specifically some interesting data on Mars. We utilized their APIs heavily in the creation of our game.","In this Mars Rover educational game, players embark on a thrilling exploration of the Martian surface built using real terrain data, interacting with non-player characters (NPCs) powered by large language models (LLMs) who guide them through various scientific missions and challenges. These LLM-based NPCs assist, offer educational content, and simulate interactions, enhancing the learning experience with engaging and interactive dialogues about Mars exploration, geology, and the technology behind rovers.",,,,,,"blendr, c#, mixtral, nasa, nginx, ollama, unity",https://github.com/abearinatrap/auburnhack2024
Auburn Hacks 2024,Planetarium,https://devpost.com/software/planetarium-nzc1a7,True,Animated Earth Animated sprite fl studio Clip studio paint unity block design,,,,,,,"clipstudiopaint, flstudio, unity",
Auburn Hacks 2024,Orbital Sandbox,https://devpost.com/software/orbital-sandbox,True,Interplanetary space exploration is becoming all-to prevalent in the modern day. We wanted to design a simulator that allows the general public and kids to explore and gain understanding of some of the basic orbital mechanics and challenges facing space travel.,"The simulator allows you to fly a spacecraft around Jupiter and its moons, accurately simulating gravitational dynamics and controls. For example, any change to your angle is added as thrust, using fuel and battery, and giving you angular velocity, meaning you have to reverse your thrust when you are pointing in the right direction. The batteries charge based on whether the solar panels can see the sun (including effects from Jupiter's shadow), and you can both change your orientation and fire thrusters to change your trajectory (with real-time n-body orbital prediction generated through an RK4 numeric propagator).","The app was built in the Unity game-engine, which allowed for rapid development and quick iteration of 3D designs. 3D Models were produced in OnShape and Blender, and the textures for Jupiter were acquired from NASA. GitHub was used to manage commits and changes.",The primary challenge for the design was building something that was a reasonable compromise between being realistic and not painful to play. This meant the addition of some unrealistic features (which could be toggled on/off depending on user's skill-level) like adding angular drag to slow down rotation speed for easier control.,"We are very proud of the real-time simulation for spacecraft dynamics, and the detailed control system and UI we implemented. A lot of testing went into designing controls for the most elegant use and optimizing the orbital propagator for the highest frame-rate possible.",,"Hopefully, after the completion of AuburnHACKS, we will continue to work on the simulator, adding more intuitive visual feedback, improved texturing and models, and adding additional gameplay elements like objectives or a tutorial mode.","c#, unity",https://github.com/EirikMulder/HackEpicUnityProject
Auburn Hacks 2024,OrbitQuest,https://devpost.com/software/orbitquest,True,The theme this year is space travel. So why not create a project around those actually traveling through Space?,"OrbitQuesttracks the ISS as it travels around the Earth, completing an orbit in around 90 minutes.
You can also view NASA's Astronomy picture of the day.","We usedthree.jsfor the 3-D graphics portion of the website. For real-time ISS positioning, thethree.jsscript communicates with Django to grab position data.three.jsis a powerful library that allows us to easily load the ISS model from NASA and texture a sphere to appear as the Earth. We also used various APIs for real-time ISS positioning and for a NASA picture of the day.
The back-end was created in Django and is served over HTTPS with Nginx.","One significant challenge we faced was maintaining a multi-user workflow using Git. There were more than a few instances in which our individual code conflicted with other team members.
Implementing HTTPS was also complicated, but we figured it out after reading too much Nginx documentation.",We are extremely proud of the appearance of our ISS tracker. We are very new to JavaScript and learned a lot very quickly. The 3D graphics also appear very well on the website and it is compatible with a mobile viewport.,,"In the future, we want to implement real-time moon and sun tracking.
We also want to add user functionality where you can log in and set your address to receive notifications when the ISS is visible from your location.
Additionally, we want to integrate hardware such as the esp32s3 module.
Ideally, we would utilize RFID scanners with NFC Tags to send and pull data from the website.
This is done with serial communication between an Arduino Uno and the esp32s3.
Currently, the only part missing from this next step is the refinement of code, circuitry, and API requests","css, debian, digitalocean, django, git, github, html, https, javascript, nginx, opengl, python, three.js, tls","https://orbitquest.co, https://github.com/TheSlabby/OrbitQuest"
Auburn Hacks 2024,Pseduo,https://devpost.com/software/pseduo,True,The lack of ability to ask questions to trustworthy developers who can answer more complex questions about different programming projects. We were looking for a good platform that would allow developers to start their journey in a more applicable way.,"It allows people with minimal coding experience or a lot of coding experience to sign up and ask questions or post blogs about different topics within programming. The caveat is that the more professional experience you have, the more your answers are prioritized in forums.","We used T3 stack (React, TypeScript, NextJs, TailwindCSS) mixed with Auth0, MongoDB, and ExpressJs.",Auth0 struggled to work with TypeScript and we had a tough time connecting our database to it as well.,"Fully functional API, some pretty good UI for the time we had, functional login system using Auth0.",,,"auth0, express.js, javascript, mongodb, nextjs, node.js, react, typescript",https://github.com/TrevorAupperle/pseudo
Auburn Hacks 2024,Auburn Quest,https://devpost.com/software/auburn-quest,True,"We, the project developers, are international students coming all the way from India! Our transition to Auburn was quite difficult due to several reasons including temporary accommodation, lack of transportation, unavailability of minimum necessities that could otherwise be solved by a trusted marketplace, and a friend to rely upon. So we decided to do a thing! During Auburn Hacks 2024, we wanted to develop a solution that simplifies the transition of new students coming from a different place, or even a different country.",Auburn Quest is a web-based application that hosts the following:,Our tech stack consists of the following:,"Identifying the most important features to be built during the hackathon was a challenge as we had to identify the most pressing challenges. 
Technology wise, integrating MongoDB, creating collections, and accessing them was slightly difficult as there was some confusion on how to create collections and access them programmatically.",We are extremely proud of building the following features and being able to do a full system-integration and testing:,"This is the first time we were able to use MongoDB effectively for all common Create, Read, Update, Delete operations.
We have also spent considerable time integrating auth0 into our application for enhanced security and offset the authentication process.
Further, it was a great experience learning about and integrating cloudflare's AI application to enable prompt-response based communication to answer questions related to Auburn University.","We intend to demo this application to the Auburn University Graduate School and get their feedback, address it, and work with them to integrate the application into their website.
We also plan on talking to local businesses to advertise their business on our website to get them more customers with customized referral links for exclusive discounts.","auth0, bootstrap, cloudflare, express.js, mongodb, react","https://github.com/lmedury/auburn-hacks-backend, https://github.com/lmedury/auburn-hacks-ui, https://drive.google.com/file/d/1hpjYuxnsaXlR_WvR9F5-2C83tVxYdnSO/view?usp=sharing"
Auburn Hacks 2024,Starfall,https://devpost.com/software/starfall,False,"There's a lot of good productivity apps out there, like Notion, Google Calendar or Apple Reminders, but there's no one-stop shop for getting all the features you want in one place.","Our app helps you focus on tasks, take notes, create (and finish!) to-do lists, set your calendar and track your health--all in one app.
Each time you complete a task, you grow a star in the sky! And every time you finish a focus session, you grow a flower in the clouds. By the end of the week, you'll have a full garden! Then, it resets, and you'll be able to grow another garden the next week too!","We coded our app entirely in Swift. We used Apple's SwiftUI and Core Data as the primary infrastructure for our project.
We started with the basics of the MVC Architecture, with each page of the app getting its own View, and then expanded from there.
All data is persistent and stored through Apple's CoreData, which is an easy and effective way to keep all of your data in one place.
In order to make the interacting parts of the app, we created custom entities with many interconnected relationships. This is how we are able to do things like attaching completing a task to a specific star.","Three out of four of us had never used Swift before, so we had to learn a new tech stack extremely quickly to finish our project on time.
The haptic feedback on the circular timer was particularly rough to work with, since we had to to translate an x and y position from touch to its corresponding time on a circle. There was a lot of math involved!
Additionally, our app is super interconnected, so we often found ourselves at a standstill because portions of our app were blocked by someone else's part.","We were really happy that even though we essentially had zero experience, we were able to make a great looking and functional app that we actually want to use in our own lives.
We also were able to fully implement a working star farm that successfully restarts each week and has fallbacks in case there is not one for the previous week. I think that using Swift's canvas feature to draw the garden, along with using entities to store persistent data such as star locations, was particularly inspired.
Also, having directories in the Notes part of our app was really exciting. It's rare to be able to sort notes efficiently on a mobile app!",One of the things we learned is that Apple has built a fantastic IOS developer experience. We only used library that was not natively built into Swift.,"We are going to work on this project and add all the extra features we wanted that we didn't have time to build (hint: AI). 
Other features include: finishing the health page, which will include sections for mental and physical health.
We will improve the way we work with CoreData and how we organize models and relationships.
Most importantly, we want to continue working on this app until it truly does become our everyday, all-in-one, one-of-a-kind mobile app!","core-data, swift, uikit, xcode",https://github.com/eris-he/starfall
Auburn Hacks 2024,planet simulation,https://devpost.com/software/planet-simulation,False,The program I made was based on the theme of this event.,,,"I faced many difficulties while building the project, but I am grateful to my classmates and friends for their help in getting me through the problems.",,I learned about cooperation and teamwork.,,c#,https://github.com/TiantangWangAUBURN/planet-simulation
Auburn Hacks 2024,Constellation Achievements,https://devpost.com/software/constellation-achievements,False,,"in the front end, it generates a list of your assignments due within the week accompanied by a nice rocket waiting to be blasted off into space, the only way to get the rocket to the coveted planet is to finish your assignments, further incentivizing and motivating you to stay productive, and in theory, will give you nice and motivating quotes during your journey.","Front end: HTML/CSS and Javascript
Back end: Javascript","Limited coding experience, new languages (Javascript and CSS).",Coding a product with no past coding experience.,Much more CSS and Javascript.,An interactive week visualizer and the flames of the rocket ship's fire and the rocket ship moving closer and closer to the planet the more you complete your assignments. And API implementations.,"css, html, javascript",https://github.com/westond44/Aubhacks24.git
Auburn Hacks 2024,SpacePal,https://devpost.com/software/spacepal,False,Our main inspiration was myFitnessPal and its calorie tracking features along with the large food nutritional database it gives its users access to.,"This app calculates a user's BMR and adjusts it according to how their bodies respond to microgravity. It also allows them to input their calories and track them, as well as their exercise. It features a small database of nutritional info on a hypothetical menu on the ISS.",We built the project mostly on HTML with a little bit of Javascript for percentage bars. We used HTML documentation from W3 schools and used Notepadd++ and VS Code as our main coding environments.,Some of the challenges we ran into were that the majority of the group was learning HTML and Javascript for the first time and running into problems with formatting.,We got the website to work the way we wanted it to work for the most part and everyone got to contribute to the best of their ability.,We learned that designing any project takes time and taking a database and implementing it into a website takes a bit more knowledge with SQL or other database technologies.,Implement the project into an app rather than a website so that actual mobile users will have access,"html, javascript",https://replit.com/@henryplaysgames/SpacePal#BMRcal.html
Auburn Hacks 2024,HAL 9000 Desktop Buddy,https://devpost.com/software/hal-9000-desktop-buddy,False,We heard that it was space themed and immediately thought why not make a HAL chatbot.,"It listens to the user and allows for a spoken conversation with HAL. When run, it will open HAL and start talking.",We used a variety of libraries in order to get this to work correctly. First we used google speech recognition to listen and transcribe audio inputs. Then the audio text is passed to chatgpt to start a conversation. Finally the output is passed through a google text to speech layer and output as an audio file.,"We used tkinter to create the HAL output image, however in order to have audio processing and the HAL image we had to use threading. Tkinter does not support threading though so we were having issues with exiting the program on the window close. These have since been resolved.
Another issue was that initially we tried to use theplaysoundlibrary in python. However this will not actually stop playing a sound and let you reuse the function so for HAL to continuously talk we had to find another option. We settled on used the sound option in pygame.","We were able to get a fully fledged program that will actually do the desired affect! We also added some special keywords to start the conversation, end the conversation, and also play a the song ""Daisy"" sung by HAL from the original movie.","We learned a lot of different things, from multithreading, to race conditions, to git etiquette within in a team, signal processing, and how to interface with multiple functions running at once.","The next thing for the HAL 9000 Desktop Buddy is to add more scifi and other character profiles, add some more movie quotes from various films depicting said characters, and make it so the chatgpt API key will not be revoked.","chatgpt-api, google-speech-recognition, google-text-to-speech, pygame, python",https://github.com/CampS0up/HAL_hackathon
Auburn Hacks 2024,Andromeda,https://devpost.com/software/andromeda-y1pmjk,False,"PCPartPicker
NASA Aesthetics",Allows you to configure rocket from set parts,Angular Material,Developing with Angular Material for first time and getting it looking nice,Some of what we pulled off with the software doing things like rxjs subscriptions and css tricks,"Typescript, git, GitHub, NodeJS, Angular, CSS, HTML",Maybe make a spaceship building game in Unity,"angular.js, css, html5, node.js, typescript",https://github.com/JPedroPV/AH24Andromeda/tree/master
Auburn Hacks 2024,Sound Safari,https://devpost.com/software/sound-safari,False,Spotify Wrapped was our inspiration for this web application. We wanted to display interesting statistics from the Spotify API through a web application.,Sound Safari is a web application that takes an Artist's name and returns the top 5 most related artists on Spotify. There is also another function to get more stats where we used dashboards and the Spotify API to display more interesting statistics.,,"None of us had ever used Spotify API and the front end development started out very slow because of problems with Vue and Node. After a few hours, those problems were fixed and we started to develop an actual project.","As a team, we are proud of creating and finishing a front and back end for our web application and integrating them together.",We learned how to start from scratch and develop a full front and back end system using the Spotify API.,"In the future, if we were to continue to develop this web application and data analysis tool, we would add more pages with more stats, make it look more professional, and try to improve the overall User Experience.","css, fastapi, python, react, spotifyapi",https://github.com/cbn0006/AuburnHacks-2024
Auburn Hacks 2024,Demo,https://devpost.com/software/quizzify-q1lvrc,False,We wanted to make a powerful quiz generation app that would automatically make questions just by going over the file or text submitted by the user with the help of AI.,"On the web app, a user can upload notes or a block of text and generate quizzes from it. Then, the user can host a quiz that others can join, either for personal or classroom use. The user can also look at quiz records for his/her account. On the mobile app, users can connect to quizzes hosted on the web app—perfect for taking classroom quizzes.","We used React to make the frontend of the  web application, and Golang for the backend, and the OpenAI API. We used Kotlin in Android Studio to create the mobile app. We used firebaseDB to store the data and connect the mobile app and website.",Hosting the backend online was both difficult. We lacked extensive experience with UI which made it hard to polish.,We're proud that we put everything together in time. Making a web app and a mobile app together was also a big accomplishment.,We greatly increased our skillsets in Kotlin and Android Studio. We also learned the importance of patience.,"We plan to finish the Notes feature so that the quiz material can be modified and/or corrected. Then, we aim to make the service web-hosted for convenience and ease of use. Next, we will fully implement the mobile app, giving it the capabilities of the web version. Finally, we plan to publish the app on the app store for the public.","android-studio, api, firebase, go, kotlin, react, tailwindcss","https://github.com/pranayjoshi/quizify, https://github.com/Jayyk09/Quizzify.git, https://github.com/Jayyk09/Quizzify/tree/UIDev"
Auburn Hacks 2024,Solar Flare Prediction,https://devpost.com/software/solar-flare-prediction,False,"We were inspired to create a project on solar flares because we thought the space theme was neat, and also because solar flares are a powerful and interesting phenomenon. Once we realized there was a very important application to our project, integrating the model into the software of satellites (and saving lots of money), we were more excited. We also wanted to in","Our project analyzes historical data in order to predict if a solar flare event ofclass M or Xis likely to occur using machine learning. It then outputs the result of the prediction onto a website. A variety of ML techniques were used to find the best model, and it was determined that SVC ML is the most accurate.","We broke the problem up into three categories:data manipulation, machine learning, and visualization.Data manipulationprimarily used pandas to import the DSFP dataset and adjust the features in order to prepare for the prediction steps.Machine learningprimarily used scikit-learn and the various models within to evaluate which classifier was most effective at predicting solar flares, and then using that model with a specifically tuned set of hyperparameters to predict whether a solar event will or will not occur.Visualizationprimarily used HTML, CSS, and JS in order to create a website that would be used to display the predictions, as well as serve as documentation for our project.","Having no experience in web development, learning how different UI elements worked together and making sure they did not override each other was a challenge.
Tuning hyperparameters was also a challenge at first, as not having a strong math background made understanding which parameters were important to the model's success was very difficult. However, diving into","The functionality of the website and its sleek interface were proud accomplishments. Especially, we are proud of the accuracy generated with SVC using our fine-tuned parameters.","We learned how to use different machine learning functions in Python, how to use UI/UX languages and tools, how to access a variety of databases and integrate those into a project, how to manage and reverse-engineer datasets, how to run a website, some photo editing tools, and more.","What we hope to do isbroaden and modernize the dataset we use. Currently, the DSFP dataset only goes from 2010-2017. Broadening the range of the dataset would allow us to both increase the accuracy of the model, and make more real-time predictions.
We attempted this during the hackathon, however, the DSFP dataset used aggregate data from numerous sources, andthe way to access some of the data has changed drastically since DSFP's creation in 2019. The process of rewriting the dataset generation code was too involved due to the time constraint of the hackathon, but the potential is still present and something we hope to achieve by continuing this project into the future.","css3, google, html, javascript, pandas, python, sk-learn, sunpy",https://github.com/LoganMiller5505/auburnhacks24
Auburn Hacks 2024,Spaceify,https://devpost.com/software/spaceify-gu2p1z,False,"We saw a project that procedurally generated a topographical map based on bit data from Spotify. Instead, we thought a song representing each celestial body would be more theme-driven.","It allows the user to input a Spotify playlist URL to dynamically create a galaxy based on the songs' metadata. Based on the song, each planet varies in speed, color, size, texture, and rotational speed. Songs that are more popular produce larger planets for example.",We have a React frontend and a Python FastAPI backend. The frontend sends the playlist URL to the backend via fetch. Then the backend makes a call to Spotify API. The data is returned to the backend where the data is processed and organized into a planet data set with characteristics unique to the Spotify metadata. We use ReactThree.js to render 3d models in the react app. We hosted the microservices for our web app on AWS in an Ubuntu runner via Docker containers. NGINX was used to reverse proxy to route traffic from port 80 into our webapp container. We acquired a domain from GoDaddy to route to our public IP for the EC2 container.,"React rendering issues, it can get very complicated with states not doing what you want. We had issues with merge conflicts trying to divide and conquer tasks. Obviously its a marathon, coding when tired leads to easy mistakes. I spent 45 minutes debugging only to realize I wrote ""emtpy"" instead of ""empty"".","Josh did an amazing job implementing a very math heavy AI generation of planet textures from the spotify album art. We learned a ton about react and were super happy with the visual. Overally, it was amazing to see everything come together and learn so much in such a short period of time.","Communication is key, task delegation is extremely important and no single one of us could have accomplished this on our own.",I would really like the AWS containers to be built with Terraform in the future.,"amazon-web-services, docker, ec2, multi-threading, nginx, python, react, sklearn, spotifyapi","http://space.spaceify.co/, https://github.com/uabhacks-at-auhacks24"
Auburn Hacks 2024,Space smarts,https://devpost.com/software/space-smarts,False,"The inspiration for this game was creating a fun educational game, and to distribute chatgpt's seemingly limitless knowledge.","This is a game provide Inspiration: To merge education with fun by tapping into ChatGPT's vast knowledge in a game format.
What it does: A space-themed educational game where players answer trivia questions to fight against aliens.
How we built it: Python games and engaging graphics, and ChatGPT for dynamic content generation.
Challenges we ran into: tailoring trivia to player levels and integrating ChatGPT seamlessly for real-time interactions.
Accomplishments we're proud of: creating a fun, educational game that effectively utilizes ChatGPT for endless trivia questions.
What we learned: The importance of balancing educational content with gameplay and the potential of AI in learning.
What's next for Space Smarts: Expanding subjects, adding interactive elements, and adding more animations.",,,,,,"chatgpt, chatgptapi, dalle, github, pygame, python",https://github.com/lorewar2/space_based_smarts
Auburn Hacks 2024,OnePress,https://devpost.com/software/onepress,False,"In today's fast paced and unpredictable world, individuals find themselves in situations where they urgently need assistance but  the sitation might me more miserable and the individual cannot reach out the right person for help.  General methods of seeking help, such as making phone calls or sending text messages, can be time-consuming and impractical during emergencies. This is particularly true in scenarios where every second counts, such as medical emergencies, personal safety threats, or when one is lost in an unknown location. ​","Our project, OnePress, addresses this critical need by offering a efficient and user-friendly solution.OnePress enables users to send an emergency alert to all members of predefined groups with just a single click. Upon activation, the app sends a notification to the user's friends, family, or any designated group member, along with the user’s precise location, facilitating immediate understanding of the situation. ​",We built a Flutter based app for cross-platform compatibility and leveraging Firebase for robust backend storage and real-time data handling,We tried to integrate MongoDB database but we are not able to connect it due to lack of time. So we overcomed this challenge by using FireBase and successfully done it. While implementing FireBase we ran into dependency errors and were able to debug it.,Building the application inside 24 hour period.,We learned using flutter with firebase and building a real time project.,Our future Plans is to improve a well developed mobile app for this and increasing the efficiency of database. Adding more elements. We also need to improve the notifications section.,"dart, firebase, flutter, visual-studio","https://github.com/udaykiran1809/Onepress, https://tigermailauburn-my.sharepoint.com/:p:/r/personal/azg0192_auburn_edu/_layouts/15/doc.aspx?sourcedoc=%7B5842c7a1-2971-4bf4-a431-e802c82fe791%7D&action=edit"
Auburn Hacks 2024,Mars Meteo,https://devpost.com/software/mars-meteo,False,We wanted to provide users with a way to see a daily forecast for Mars.,Scrapes current weather data off of two NASA Websites updated daily by the curiosity and perseverance rovers and displays it in an easy to read format.,"We used React, HTML, and CSS",We originally planned on using a NASA API that used to update daily with new weather information but discovered that it had not been updated in three years. We then took the web scraping approach on an actively updated website.,"This was our team's first exposure to not only react, but all the languages we used as well as web scraping. Being able to make a complete project with our lack of experience is something that we are really proud of.","Learned how to develop a React app, scrape data off of websites, and collaborate using Git.",We'd love to work more on the UI and add for features for users. We'd also love to deploy it on our private domain.,"html/css, javascript, react, vscode","https://github.com/eli-standard/Mars-Meteo, https://eli-standard.github.io/Mars-Meteo/"
Auburn Hacks 2024,AstroPedia,https://devpost.com/software/astropedia,False,"AstroPedia  As we witness the increasing adoption of Artificial Intelligence (AI) and Virtual Reality (VR) and Augmented Reality (AR) technologies, we are eager to explore the possibilities of fusing these technologies to elevate the experience of space exploration. Our goal is to create a more immersive and captivating experience for space enthusiasts, by leveraging the power of AI and AR technologies in unprecedented ways. AstroPedia offers chat and AR experiences to explore planets in the Solar System. The chat feature allows users to ask specific planets questions, while the AR feature enables 3D exploration. We used React, Material UI, and React Three Fiber for the front-end, and Chatbase.co for the chat systems. We faced challenges when working with React Three Fiber for the first time. Additionally, we had a difficult time deciding how to implement the chat system feature. Despite the challenges, we're thrilled to present AstroPedia as a groundbreaking platform that redefines how people learn about space. We're proud of its immersive atmosphere, engaging storytelling, and accurate information. The process of creating AstroPedia has been a constant learning experience. Throughout this journey, we have come to understand the value of collaboration, pushing creative boundaries, and remaining true to our mission of making space exploration accessible and interactive. We have also been introduced to new technologies such as React Three Fiber, Three.js, and integrating AI chat systems into our applications. We are excited to continue developing our features after this weekend's hackathon. Our goal is to constantly expand the AstroPedia universe. We have plans to add new planets, moons, and missions in the near future, along with exciting features like multiplayer AR experiences and educational games. Our team is dedicated to making AstroPedia a valuable resource for educators, students, and space enthusiasts of all ages. We hope to inspire them to explore the wonders of our solar system and beyond.",,,,,,,"express.js, node.js, python, react, react-three-fiber, three.js","https://github.com/SirArnoldB/AstroPedia, https://astro-pedia.vercel.app/, http://astropediacosmic.com"
Auburn Hacks 2024,Event Horizon,https://devpost.com/software/event-horizon-ce43fz,False,"We wanted to make something to help students, specifically with informed decision making when creating their class schedule. To input class data, Tiger Scheduler was used.","Event Horizon is a cosmos-themed scheduling application that aides in the registration of class. The idea was that we could expand upon Tiger Scheduler and have some algorithm that considers travel distance by walking for in between classes and also GPT4's suggestion on where to place ""todo"" events. For example, if we want a time to exercise, we would give it a priority level and hopefully, GPT would make the informed decision to place the exercise event before the meal event using careful prompt engineering.","The project was built using Python/Flask, Mongo Cloud DB with Atlas, and Auth0 for user sessions.",Learning from scratch,We came from absolutely zero project experience to a functional web application in 24 hours with the assistance of Google and each other.,"We learned how to create a web app with Python, how to set up a VPS to host the web app, how to create a local database and also a cloud database, and how user authentication works with JWT tokens in Auth0.","Since we came for no background in programming, our next goal is to implement the graphical features to display on the Week-View calendar.","html, javascript, mongodb, python","http://sharewide.net:5000, http://sharewide.net:5000/login, https://github.com/Srob0203/Event-Horizon"
Auburn Hacks 2024,Spaced Out,https://devpost.com/software/spaced-out-38btga,False,"Two of our team members thought of a choose-your-own-adventure immediately when they heard the theme was space. We wanted to make a fun spacy adventure, with some space cowboys thrown in as well. The two members of our team that are learning Java had just learned about if-else statements and string formatting in class, so this felt like a manageable project.","This project is a huge branch of if-else statements, where each input the user gives leads the storyline down a different path.","We used constructors to make the basic code for two inputs, three inputs, and four inputs, and we called that constructor in our main code. Each decision the user makes prints a statement determined by the constructor and returns a variable number corresponding to their decision, which the code then uses to decide what to ask them next.","We weren't really sure how to use classes before this, so we did a lot of research and found out what constructors were and how to use them. We also weren't sure if we could actually use a constructor, since we have so much code within code with our branches, so one of our team members found out that returning a variable would solve this issue. Formatting was getting challenging with all the strings involved in our program, so we defined each one at the beginning to make it easier.","We're very proud of learning how to use classes, and simply just making a program! This is our first coding project, and we are so glad that we learned so many things!","We learned the difference between a class and a method, how to use constructors and return variables, and how to format strings. We gained confidence using Java in general.","We had four main storylines: space cowboy, food critic, desk job, and drafted. We only got space cowboy finished (though we're hoping to get a good chunk of food critic done before the submission deadline), so what's next is to finish out those other three storylines.",java,https://github.com/jen-ham/Spaced-Out.git
Auburn Hacks 2024,AlienAtlas,https://devpost.com/software/alienatlas,False,"We drew inspiration from our personal experiences as immigrants navigating the complex U.S. immigration system. Having undergone the legal process ourselves, we empathize with the challenges of obtaining the right information and guidance at each step. Recognizing the need for a centralized resource, we were inspired to create a platform that simplifies the immigration journey for foreign nationals, eliminating the reliance on lawyers for even basic queries.","Our platform serves as a comprehensive solution for individuals undergoing or about to undergo the U.S. legalization process. Users authenticate with the system to access their own personalized dashboard which they can use to seek guidance on their immigration status with the built-in conversational chatbot. The platform covers various aspects, including immigrant visas, student visas, and H-1B registrations. It aims to empower users by providing a one-stop destination for information, assistance, and tracking their progress through the intricate immigration system.","To realize our vision, we utilized a combination of powerful tools and technologies. The platform was crafted using Adobe Suite, CSS5, HTML5, Botpress (Chat bot, database of relevant data scraped fromwww.USCIS.gov), Python, GitHub, PiCharm, Auth0 User Database and Authentication platforms, and with a unique URL obtained through GoDaddy. This blend of technologies ensures a seamless and user-friendly experience for individuals seeking clarity and support throughout their immigration journey. Our team's firsthand understanding of the immigration process fueled our commitment to building a platform that simplifies, informs, and guides users at every stage.","One notable challenge we faced revolved around the integration of Cloudflare into our project. Given its integral role in various aspects of our platform, any disruptions in Cloudflare's services had a widespread impact on our project's functionality. Unfortunately, this issue surfaced at a crucial juncture, causing disruptions just before the final stages. The technical complexities of Cloudflare's integration posed obstacles that required a swift and strategic response from our team. Despite the setback, we navigated through the challenges, making adjustments to ensure the smooth functioning of our platform. This experience highlighted the importance of anticipating and addressing potential disruptions in third-party services for future projects.","Our proudest achievement lies in the dedicated effort and collective determination that fueled the creation of our project within the tight time-frame of 24 hours. The realization of a comprehensive platform designed specifically for countless immigrants navigating the labyrinthine paths of the U.S. legalization system is a testament to our team's commitment and shared experiences. Building a project tailored to address the challenges faced by immigrants, or as commonly referred to as ""Aliens,"" resonates deeply with our own journeys. We take immense pride in contributing a solution that can potentially alleviate the struggles we and many others faced during the legal immigration process. This accomplishment not only showcases our technical skills but also reflects our empathy and commitment to making a positive impact on the lives of those navigating the complexities of the U.S. immigration system.","Navigating the challenges we encountered, particularly with Cloudflare, provided invaluable lessons for our team. Firstly, we gained a deeper understanding of the critical role that third-party integrations play in the overall functionality of our platform. The experience underscored the importance of anticipating potential disruptions and implementing contingency plans to ensure project resilience. Additionally, we learned the significance of adaptability and quick problem-solving in the face of unexpected setbacks. The Cloudflare incident prompted us to enhance our troubleshooting skills and refine our processes for seamless integration with external services. Moving forward, these lessons will undoubtedly contribute to our team's growth, enabling us to build more robust and resilient projects in the ever-evolving landscape of technology and service dependencies.","Looking ahead, our roadmap for AlienAtlas involves a continuous commitment to refinement and user-centric enhancements. We aim to streamline and simplify our product further, ensuring an even more intuitive and accessible experience for end-users navigating the U.S. immigration system. One significant step in this evolution is the incorporation of text-to-speech functionality, enhancing accessibility and catering to diverse user preferences. Recognizing the global nature of immigration, we plan to broaden our platform's reach by introducing support for multiple languages. This expansion aligns with our vision to be a truly inclusive resource for individuals worldwide, offering guidance and support tailored to their specific linguistic and cultural needs. As we move forward, our focus remains on user satisfaction and creating a platform that not only meets but exceeds the expectations of those undertaking the intricate journey of achieving legal status in the United States.","adobe, adobe-illustrator, auth0, botpress, css3, github, godaddy, html5, photoshop, picharm, porkbun, python, render, xd","https://github.com/cullen-s/alien_atlas, https://mediafiles.botpress.cloud/42557aab-846c-4bce-89c2-b6c3481bba0e/webchat/bot.html, https://youtu.be/Q_Aeeo1VxEQ"
Auburn Hacks 2024,Nebula Navigators,https://devpost.com/software/nebula-navigators,False,"The Intergalactic theme of the AuburnHacks 2024 competition led us to want to create a space based sandbox game where the player can explore the unique features of the planets around them and manage a finite amount of resources such as food, water, oxygen and fuel.",Nebula Navigators is a two-dimensional space exploration game with realistic rocket movement and planetary physics.,,,,,,"pygame, python",https://github.com/ahconnors/Nebula-Navigators
Auburn Hacks 2024,Starbite,https://devpost.com/software/starbite,False,One of our members remembers struggling to get into good habits and learn how best to adapt to having diabetes as a young teen. We believed that creating an application that makes the interaction more game like will make teaching good habits easier than the current sterile application.,"This app shows the current, real-time glucose level by connecting to the report server that a paired device attached to the user sends values to. It has interesting designs to show active trends in the data with options to include graphs and other themes to personalize the interactions.",We developed using Android Studio and Dexcom devices to get real time information.,"We have never used Android Studio before so learning to use that on the fly while also working in different elements to create an interesting project.
Additionally, the Dexcom database that is open to the public sends data on a 3 hours delay which is not good for real-time occurrences, so we had to figure out how to receive the data directly from the device.","We were able to get a working UI with Android Applications, and the data is real-time now which means we were able to get the data from the device with a minimal delay for sending and processing time.","There are multiple different ways to accomplish creating the interface, and starting a new IDE and build process before a short time frame like this is probably not best for overall productivity.",Including a history of past readings to allow for easier visualization of trends and summaries of what the user has had as their normal glucose level.,"android, dexcom, java, kotlin, xml",https://github.com/sigelski/StarBite.git
Auburn Hacks 2024,Playlist Planet,https://devpost.com/software/playlist-planet,False,Space.,Turns Music into a Planet,"React, OpenAI, Dalle-3",dependencies,Yeat Planet iykyk,I learned a lot about myself.,Playlist Galaxy NFT Smart Contract Blockchain AI,"dall-e, gpt, node.js, openai, react, spotify",https://github.com/owenstrength/SpaceHackathon
Auburn Hacks 2024,Planet Patrol,https://devpost.com/software/planet-patrol,False,The cosmic theme of this Hackathon,"It is a game, the objective is to rescue all 5 aliens and collect all 30 crystals.",We used Unity and Blender,"-we managed lighting challenges in Unity. 
-we experienced texture problems in Blender.",We are proud of getting the game to run on the steam deck.,We learned a lot about texturing and lighting.,We are done with it after this hackathon.,"blender, c#, photoshop, rider, unity","https://github.com/WarEagleBiggs/AuHacks2024, https://drive.google.com/file/d/1iQ4sUzIQ2CjWqtMbfvyDGpBiGt1fzHgz/view, https://biggsgames.itch.io/planet-patrol"
Auburn Hacks 2024,Cosmic Colonies,https://devpost.com/software/cosmic-colonies,False,Cosmic Colonies was inspired by Settlers of Catan.,"It is a real-time online board game where 4 players battle over the cosmos (a 5x5 grid of planets).
Players create an account that lets them keep track of how many wins they have.
Whoever gets to 5 colonies first wins!","Cosmic Colonies is built entirely in C# using ASP.NET Core Blazor Server.
Auth0 is used for user authentication, and EFCore is used for storing extra user data.",The biggest challenge was properly syncing the data between the client and server. Building on the web did not make this any easier.,"We are very fond of our art style for the game. It's simple, mostly made of simple lines and shapes, but it conveys the message it needs to.",,"In the future, we may add some features that other online games have: rating systems, ranked gameplay, skill-based matchmaking, and leaderboards.","aspnetcore, blazor, c#, entity-framework",https://github.com/Techcraft7/AU-Hacks-24
Auburn Hacks 2024,WeAuburn,https://devpost.com/software/weauburn,False,,,,,,,I will keep develop my project,"android, firebase, kotlin",https://github.com/JJproefssor/AuburnHack24
Auburn Hacks 2024,Marshall's Space Return,https://devpost.com/software/shmore-o-s-space-return,False,"Space theme and the team's interest in games of the platformer genre, as well as the old arcade game Duck Hunt.",A game to entertain the consumers,Unity and Piskelapp.com for sprite generation,"Collision issues, sprite animation events, jetpack mechanics, game mode transitions",Amostlyworking,We learned how to use Unity and several features needed of a platformer and sprite animations.,"Bug fix optimizations (not just working, but efficient). More levels with more mechanics.","c#, piskel, unity",https://github.com/M0-17/RocketCity_____
HackDearborn 2024,Second Chapter,https://devpost.com/software/second-chapter,True,"An estimated 320 million books are thrown away in landfills every year. This adds to the amount of paper that accounts for 25% of landfill waste! Books are loved by many all over the world and they deserve their own Second Chapter in live. We aim to create a digital version of local ""take a book, leave a book"" pop up libraries. Our site adds in the personal aspects to second-hand giveaways hoping to not only contribute to a more sustainable Earth, but to make some like-minded friends along the way :)","Second Chapter promotes a community that values both the planet and reading. We encourage people to share and claim used books, reducing waste, conserving environmental resources, and promoting a positive message about conscious consumption. We hope to inspire a culture of sustainability, where knowledge is shared, and environmental impact is minimized... one book at a time. Users can browse for all the books that are up for grabs in the main section. You can filter by genre, and even see how far away you are from the book (distances are filtered and sorted by closest to farthest for ease of use). Not sure where that is? Check the interactive map to your left! All you have to do is click the claim button, and an email will be sent to the current owner of the book where all further communication can be handled. Users can also add a book of their own (if they're willing to part with it of course) by clicking on the Add Book button. Search for the title of your book, and the API will grab the rest!","Front End: React
Database: MongoDB
APIs: Google Books API, Resend API, Leaflet, NextAuth
Server: Next.js","We ran into a lot of challenges with the DNS as it wasn't verifying for us. We also ran into difficulties managing all the different APIs, but we were able to pull through!","If 1,000 books are traded, 200-300 trees are saved, reducing the demand for thousands of gallons of water, and preventing significant CO2 emissions. If 10,000 books are traded, the impact multiplies.",We learned A LOT! About full stack development and what it means to work as a team!,,"css, google-book-api, html, leaflet.js, mongodb, nextjs, react, tailwind, typescript",https://github.com/joemmalatesta/second-chapter
HackDearborn 2024,SwarmAid,https://devpost.com/software/swarmaid,True,"Mission Control Mission Control User Dashboard The staggering fact that one-third of all food produced globally is wasted while millions go hungry is heartbreaking. This project was born out of the urgent need to bridge that gap—connecting those with surplus to those in need. We were inspired by Swarm's multi-agent orchestration, envisioning a future where intelligent collaboration could turn food waste into sustenance for communities. 🌍🍽️ SwarmAid connects food suppliers with surplus—like restaurants, grocery stores, and farms—to food banks and shelters in real-time. It uses intelligent agents that automatically detect surplus food, match it with demand, optimize logistics for transportation, and ensure compliance with health standards. By facilitating fast, efficient redistribution, SwarmAid not only reduces food waste but helps feed those in need. ❤️🍲 We leveraged Swarm's multi-agent orchestration framework, building out a modular system where agents collaborate seamlessly: Using Python for the backend, Leaflet.js for the interactive map, and APIs for real-time communication, we created a proof-of-concept that simulates these interactions and highlights the power of decentralized coordination. 💻🗺️ One of the biggest challenges was ensuring seamless communication between agents without creating bottlenecks. Coordinating real-time logistics and compliance while keeping the system lightweight and scalable also proved tricky. We had to experiment with different architectures and APIs to get it just right. 🚧🤔 We’re incredibly proud of building a working proof-of-concept that not only shows how Swarm’s multi-agent system can reduce food waste but also has the potential to impact millions of lives. The real-time coordination and smooth agent handoff are highlights, and seeing our demo work as we envisioned was a thrilling moment! 🏆💫 This journey taught us so much about the power of intelligent agents and decentralized coordination. We learned how to implement and fine-tune Swarm’s multi-agent system and discovered new ways to approach logistics and real-time data handling. Most importantly, we realized how technology can be used to solve real-world issues like hunger and food waste. 📚🔍 We aim to take SwarmAid beyond a demo! Next steps include integrating with real food inventory systems for actual surplus data, enhancing logistics with traffic and weather data, and expanding the system to handle larger geographic areas. We also envision building a mobile app for greater accessibility and using machine learning to predict future supply and demand patterns. 🌎📲",,,,,,,"nextjs, openai, python, swarm",https://github.com/IdkwhatImD0ing/SwarmAid
HackDearborn 2024,2NDs,https://devpost.com/software/2nds,True,"We noticed that many campus organizations end up having to dispose of perfectly good food after events, which contributes to both food waste and missed opportunities for students to enjoy free meals. This inspired us to create a solution that addresses both issues. By developing an app that connects students to leftover food from events, we aim to reduce food waste and ensure that students can benefit from these surplus resources.","Our app notifies students and campus members in real time when there is leftover food from events happening on campus. Users receive alerts about available food, the location, and a limited time window to grab the food before it's gone. The app helps reduce food waste and provides students with free meals, benefiting both the environment and the campus community.","We used a web scraping API to extract event data from the campus website, which we integrated into the app's backend. The app scans event descriptions for keywords related to food and leftovers, then sends notifications to users when relevant events are found. The front-end was developed using React Native, allowing us to create a mobile application that works on both iOS and Android. We used Firebase for real-time notifications and database management, storing event data and user preferences.","One major challenge was ensuring that our web scraping API consistently detected the right events with leftover food. Since event details are not always standardized on the campus website, we had to develop a flexible keyword filtering system. Another challenge was integrating real-time notifications efficiently, ensuring that users are alerted promptly without draining battery life or overwhelming them with alerts.","We’re proud that our app is able to make a tangible impact by reducing food waste and helping students. Additionally, successfully integrating both web scraping and real-time notifications into a cross-platform mobile app was a big accomplishment for our team. We're especially excited that our app not only solves a problem but also strengthens the campus community by making resources like food more accessible.","Throughout this project, we learned how to integrate web scraping technologies with mobile app development, which was new for most of our team. We also gained a deeper understanding of notification systems and how to optimize them for mobile users. Perhaps most importantly, we learned how tech can be leveraged to address sustainability challenges, like reducing food waste, in a simple yet effective way.","Looking ahead, we plan to expand the app to allow people in different locations of the world post in the app in real time.","android-studio, beautiful-soup, figma, git, github, python, react-native",https://github.com/Chineme1/2NDs
HackDearborn 2024,Green Gale,https://devpost.com/software/green-gale,True,"As inflation rises, it has become increasingly difficult to save money, and energy consumption is a major contributor to daily expenses. At the same time, the impact of energy usage on the environment is undeniable. Our team was inspired to take on both challenges by developingGreen Gale. The goal was to create a project that not only helps people save money by monitoring their energy usage but also promotes sustainable living by harnessing clean energy from wind.","Green Galeis a model windmill system that generates energy using an Arduino/Genuino 101. The energy data collected from the windmill is uploaded to a web-based dashboard, where users can easily see how much energy they've generated and how much money they've saved. The project aims to raise awareness about energy consumption and provide users with financial insights to promote smarter energy use.","We builtGreen Galeusing an Arduino/Genuino 101 to control the windmill model, which simulates wind energy generation. The windmill’s voltage output was monitored, and this data was uploaded to a custom-built website using a Flask-based backend. We used SQLite as our database to store energy data, and the React frontend allows users to see their energy production and financial savings in real-time. The entire system was designed to make complex energy data accessible and understandable to users.","One of the biggest challenges was integrating the hardware components with the software. Setting up communication between the Arduino and the web application was tricky, and there were many technical difficulties in ensuring that the energy data was correctly captured and displayed. We also faced some hurdles while learning how to use SQLite for storing and managing the energy data.","We are proud of successfully building a functional model that demonstrates how wind energy can be tracked and monetized. From constructing the windmill model to developing a user-friendly interface, each piece came together to makeGreen Galea complete solution. We’re especially proud of learning new tools and technologies such as SQLite and managing to integrate them into the project.","Throughout this project, we learned a lot about how to combine hardware and software into a cohesive system. Working with Arduino was a valuable experience, and we deepened our knowledge of web development by using Flask for the backend and React for the frontend. We also gained a better understanding of database management by working with SQLite.","Our next step forGreen Galeis to build a mobile application that allows users to easily track their energy generation and financial savings on the go. We believe that making this data accessible on mobile devices will further empower users to monitor and reduce their energy consumption, contributing to both financial literacy and environmental sustainability.","arduino, css, flask, html, javascript, mathlab, operationsystem, python, react, sqlite, virtualbox",
HackDearborn 2024,SwarmSense: Connected Intelligence for Connected Vehicles,https://devpost.com/software/swarmsense-connected-intelligence-for-connected-vehicles,True,"The inspiration for SwarmSense came from observing how swarms in nature, like bees or ants, work together to achieve complex tasks. We wondered: what if we could apply this collective intelligence to vehicles on our roads? With the increasing number of sensors in modern vehicles and advancements in AI and communication technologies, we saw an opportunity to create a system that could dramatically improve road safety and efficiency. SwarmSense is not just a harmonic alliteration It is:",,,,,,,"c++, edgeimpulse, lidar, opencv, python, realsense, ros2",
HackDearborn 2024,SaveCircle,https://devpost.com/software/savecircle,True,"We were inspired by the challenges that people face when trying to save money for group activities, like vacations, events, or shared goals. Often, it’s hard to keep track of who has saved what, and motivation can drop when there’s no clear view of the collective progress. We wanted to create a solution that makes saving together easy, transparent, and fun. SaveCircle was born out of the need to foster accountability and collaboration when managing group finances.","SaveCircle allows groups to create shared savings goals (e.g., vacations, events, or any collective financial objective) and track contributions in real time. Each member of the group can update their progress, see how much others have contributed, and visualize the overall savings goal with progress bars and charts. The app sends notifications to remind group members of upcoming deadlines or when someone makes a contribution, helping everyone stay on track.",,Some challenges we faced included coming up with an idea we were all interested in as well as managing time efficiently as beginners,"We’re proud of creating a functional app within the hackathon timeframe that solves a real problem. Successfully implementing real-time updates and building a clean, user-friendly interface are accomplishments we’re particularly excited about. We are also proud of the fact that we kept pushing in order to have something to show for our first hackathon. As we are all beginners who only met yesterday, that is an accomplishment.",,"In the future, we plan to add features such as integrating with banking platforms so users can directly transfer their contributions. We’re also considering introducing savings tips, milestone celebrations (e.g., when a group reaches 50% of the goal), and personalized recommendations to help groups meet their financial objectives more efficiently. We hope to expand SaveCircle to make group savings even more engaging and practical.","figma, java, sqlite",https://github.com/Ludidas/Hack-Dearborn-3
HackDearborn 2024,DevLingo,https://devpost.com/software/devlingo,True,"DevLingo was inspired by the challenges teens face when trying to learn coding. Traditional platforms can be intimidating, especially for beginners, and many schools, especially in underserved areas, do not offer adequate coding classes. This gap limits opportunities for young people to pursue careers in technology and STEM. Our goal is to make coding education accessible, engaging, and fun for all teens, regardless of their background!","DevLingo is an interactive web platform designed to teach programming to teens through gamified lessons and quizzes. It allows users to progress at their own pace, covering a wide range of programming languages through a mix of theory and practice. The platform also includes AI-powered assistance to help students when they get stuck, and it fosters a community that encourages collaboration and peer learning.","We built DevLingo using NextJS for the frontend to handle content rendering and navigation. For the backend, we used Python Flask to manage routing, user login, and data storage, which is handled via SQLite. To enhance the learning experience, we integrated uAgent and Google Gemini AI for real-time assistance, helping students by answering their coding questions and providing explanations. The platform also uses a speech-to-text feature for more interactive support.","One of the biggest challenges we encountered was integrating the AI assistance in a way that was both responsive and useful for beginners.
Another major challenge we faced was implementing an interactive quiz system that dynamically loads the correct set of questions based on the user's progress through different chapters. The complexity arose from structuring the quiz data to align with the various sections and chapters while ensuring the quiz logic handled different states like loading questions, tracking answers, and providing feedback. We struggled with accessing the correct quiz data, as our data structure was nested across multiple chapters and sections, and mapping the correct quiz to the user's input required careful handling of indices and state. Initially, we ran into issues with incorrectly rendering quizzes, and in some cases, no quiz would appear at all due to mismatches in how we were referencing data.",--,--,"In the future, we plan to expand DevLingo by adding more advanced programming languages and topics. We aim to introduce a leaderboard and a coin-based rewards system to increase motivation. Additionally, we plan to offer memberships for schools and parents, enabling broader access and engagement. We also envision incorporating more community features, such as coding challenges and peer-to-peer tutoring, to further enhance collaborative learning.","flask, gemini, javascript, python, react, sql, uagents",https://github.com/Zahraa222/DevLingo
HackDearborn 2024,Hocus Focus,https://devpost.com/software/hocus-focus-1c9vxr,True,"An article fromThe Atlanticrevealed that many Cornell students struggled to finish reading an entire book, highlighting a broader issue of declining focus in today's digital age. Motivated by this challenge, we createdHocus Focus, a mobile app designed to help people rediscover the joy of reading and improve their focus.","Hocus Focusencourages users to increase their focus while reading through a system of positive and negative reinforcement. The app features a diverse library covering multiple genres and tracks users' screen time, comparing the time spent reading onHocus Focuswith time spent on other apps. Users earn points for reading and completing quizzes at the end of each chapter, while time spent on social media incurs penalties. Points can be redeemed for exclusive books or used to make charitable donations.","We built the app using Figma for design and React-Native, HTML, CSS, JavaScript for the front-end, Python Flask, and Crew.ai for the simulation. Android Studio was used for testing and deployment.","Given a limited amount of time, it's difficult for us to deploy all the desired backend functionalities or integrate AI features like Crew.ai.","We’re proud of the sleek and intuitive UI we designed. The app performs smoothly on IOS simulator, web app using Crew.ai, with seamless transitions and interactive features that elevate the user experience.","Throughout this project, we learned how to better navigate documentation (Crew.ai) and streamline the app deployment process. It also strengthened our understanding of user interface design and cross-platform development.","Looking ahead, we aim to introduce audiobooks and an offline mode, allowing users to download books and quizzes for uninterrupted reading. We also envision a ""Community"" feature, where users can join virtual book clubs, fostering motivation and discussion around shared reading goals.","android-studio, crew.ai, css3, html5, javascript, python, react-native","https://github.com/aryanmiriyala/HocusFocus, https://github.com/GunnarK77/HocusFocus, https://github.com/GunnarK77/crew.ai_simulation/tree/main"
HackDearborn 2024,CrashCam,https://devpost.com/software/crashcam,True,,,"Front End:StreamLit, GoDaddyData Input:Arduino Vibration Module + Camera ModuleCrash Recognition:Fetch.ai's agentverse and uagentsData Storage:MongoDBEmail Automation:Twilio SendGrid",,,,We're looking to have the CrashCam more integrated to an actual dashcam that way we can put it into the commercial market and we would integrate it with more specialized hardware such as PCBs instead of using arduino kits.,"ai-agents, arduino, fetch.ai, mongodb, python, sendgrid, streamlit, twilio","https://github.com/CrashCamMichigan/Base, https://crashcam.hackdearbornwinner.vip"
HackDearborn 2024,WattX,https://devpost.com/software/wattx,True,"   Over a billion people still don't have access to energy — of any kind. The world’s energy markets are highly centralized, inefficient, and often leavemarginalizedcommunities behind. These systems are often inefficient, with transmission losses accounting for nearly8%of the total electricity produced globally. Our inspiration for WattX was to disrupt the status quo by decentralizing energy markets through peer-to-peer trading within microgrids. With sustainability and equity in mind, WattX aims to givepower to the people, literally and figuratively. 🌱💡 We want to incentivizesustainability,minimize transmission losses, and ensureenergy securityby enabling communities to trade energy directly, without intermediaries. This approach alsopioneers energy equityby allowing underdeveloped areas to benefit from decentralized microgrids, fostering self-sustaining communities. WattXcreates adecentralized energy trading platformwithin microgrids, where participants can seamlessly trade energy using autonomous agents and dynamic pricing models. By utilizingmock token transactions, users can exchange energy peer-to-peer, ensuring that those who produce excess energy (like solar) can sell it directly to others in the community. This decentralized approach allows users to trade energy efficiently, transparently, and without the need for large centralized energy companies. 🌞🔋 We combined a range of cutting-edge technologies to bring WattX to life: We built the frontend of WattX using Next.js and TypeScript to make it fast and scalable, with React driving the interactive UI components. To create a clean and modern look, we used Material UI and Aeternity UI. One of the coolest parts of the project was integrating 3D elements from Spline for the first time, which really added a new level of interactivity to the platform. It was a challenging but rewarding experience, and we’re proud of how it turned out! The backend of the project was built by using multiple technologies such as python,flask server, Openai and next.js. The blockchain ledger and the handling of the bank account is handled by supabase and python. The supabase keeps the account of the bank and the python uses the distributed ledger to create the blockchain Wattcoin. Then there are two autonomous fetch AI agents which are stimulating the real life situation of the negation of the energy exchange with the use of Wattcoin. Adding to that the flask server acts as the middleman between the Next.js website and the python files running the agents and the blockchain. The Next.js handles the functionality of the website and how all the things are displayed. Building a project of this scale in just 24 hours was no small feat! Here are some key challenges we tackled: The hackathon experience taught us an incredible amount about: Our hackathon prototype is just the beginning! Here’s our roadmap: Together, we are theBuilders of Tomorrowand we’re ready to bring decentralized energy trading to the world, starting with WattX! ⚡🌍",,,,,,,"flask, javascript, next.js-(frontend), python","http://wattx.us, https://github.com/Codingbysid/EnerTrade-.git"
HackDearborn 2024,PayUp,https://devpost.com/software/n-a-rbk57i,True,"As we sat down for this hackathon, we thought to ourselves what we should eat. Then suddenly it hit us! What if we can make a mobile app that forces us to pay up our split of the bill to remove the obnoxious part about splitting the bill, paying the one who's card we put it on.",This mobile app is going to scan a receipt and with ML/AI it's going to check for the all the different items ordered and the total. You can then decide to have people pay for what they ordered or just split the bill evenly. From it'll send an Apple Pay request via iMessage and its smooth sailing from there!,"We took advantage of SwiftUI and a bunch of different Swift frameworks such as Vision, VisionKit",Our biggest challenge was working with iMessage and Apple Pay.... TBC,,How to create with Adobe Express!,,"coreml, swift, swiftui, vision, visionkit",https://github.com/Christopher723/HackDearborn3
HackDearborn 2024,Interactive Media Challenge: Deepfake Detection for Images,https://devpost.com/software/interactive-media-challenge-deepfake-detection-for-images,True,"Inspiration for this project stemmed from the increasing sophistication of deepfake technology, which poses a serious threat to the authenticity of digital media. As a group, we were driven by the need to protect the integrity of images in interactive environments, where the rapid sharing of content often leads to misinformation. We realized that manual oversight wouldn't be enough in the face of this growing challenge, so we turned to Fetch.AI autonomous agents for a scalable, decentralized solution. The idea was to empower AI agents to operate independently, monitoring image authenticity in real-time without the need for centralized control. This approach not only provides a higher degree of trust but also ensures that the system can adapt and function in a variety of interactive media environments. By merging cutting-edge autonomous agent technology with the pressing issue of deepfake detection, we aimed to contribute to a safer, more reliable digital landscape.","Our project utilizes Fetch.AI agents to autonomously detect image content and identify deepfake manipulations in real-time. The solution integrates deep learning models for thorough analysis, while decentralized nodes collaborate to validate the results. This approach ensures that platforms are protected from the growing threat of harmful deepfake content. We designed a multi-agent system where four distinct agents handle different parts of the workflow: Together, these agents work in a decentralized environment to ensure that the entire deepfake detection process—from data preparation to model inference and explanation—is both robust and scalable. This multi-agent system offers a comprehensive solution for real-time monitoring and detection of manipulated content.",,,,,"What's Next for the Interactive Media Challenge: Deepfake Detection for Images? As we move forward with our deepfake detection solution, our focus will be on refining and scaling the system to meet real-world demands. Here's what comes next: By enhancing, scaling, and exploring new use cases, we aim to provide a comprehensive solution to combat the rising threat of deepfakes in media.","fetch.ai-agents-(-agents-), keras, opencv, pandas, pillow, python, tensorflow",
HackDearborn 2024,YOFY^T,https://devpost.com/software/yofy-t,False,Mental Health Accessibility,Helps analyze your mood and gives solutions based on Cognitive Behavior Therapy,,,,,na,"java, swiftui",https://github.com/Oquba/Hack-Dearborn-3
HackDearborn 2024,IntelliSchedule,https://devpost.com/software/intellischedule-zprjbw,False,"Daily, we all force ourselves to use calendar apps that have quite a poor UI and take up too much of our time. We knew we could craft a solution to eliminate time wasted scheduling events and automate task creation with AI, so IntelliSchedule was born.","IntelliSchedule allows users to input their to-do list with various tasks/events as if they were writing in the notes app. Our fine-tuned OpenAI model then evaluates the task and automatically blocks off a certain amount of time in your Google Calendar, intelligently predicting the time needed. Forget wasting time adding dozens of functions and events to your calendar every day, IntelliSchedule automates and imports your to-do list directly into your Google Calendar while providing an AI ChatBot to answer any questions you may have about your calendar, why our AI added a task for the specific time, and things like ""What do I need to do tomorrow?"".","IntelliSchedule is a duo iOS & Web App built primarily on Swift and Typescript. This allows IntelliSchedule to utilize the reliability of Apple's technologies, incredible User Experience with SwiftUI, and an appealing application accessible from your computer. IntelliSchedule uses Supabase to store and connect data through our iOS and Web Apps in combination with OpenAI API for a fine-tuned model trained on over 100 custom calendar prompts and the Google Calendar API. Finally, we built our brand and logo using Adobe Express Add-Ons and utilizing a creative domain from GoDaddy.","Establishing a secure connection between Supabase, the iOS App, and the Web App was difficult, but we worked as a team to push through and provide IntelliSchedule.","We are most proud of deploying TWO apps, 2x more than most hackers, and connecting them to a secure server to pass data as well as automating tasks in real-time to the user's Google Calendar.",IntelliSchedule taught us how much more we need to learn and the endless possibilities of hackathons. We also learned how to work as a team and how to use our tech skills to aid the lives of others around us.,,"api, google-calendar, ios, openai, server, supabase, swift, swiftui, typescript, vercel, web","http://intellischedule.co, https://github.com/ajeddin/hackDearborn3iOS, https://github.com/vanshb03/hack-dearborn"
HackDearborn 2024,Finance Buddy,https://devpost.com/software/finanace-buddy,False,"As an international student in the USA, it is very easy to run off the money when you are balancing your day-to-day life. From groceries to books and from Spotify to car rentals, with no family around, most students struggle with allocating money wisely. It turns out that budget planning helps most adults, regardless of their profession, to manage their expenses up to a mark. With advancements in AI, this task can be left in the hands of software to automatically plan your budget based on your expenditures.","Finance Buddy assesses your last month's expenses, and based on that, it will create money-saving challenges for you. These challenges will be highly engaging, and the fun part? They will always be a surprise for us, too! Why? Because it is powered by AI which will automatically create challenges for you, two challenges at a time. You will earn points (XP) if you complete each of the challenges. The more XPs you earn, the more levels you will open.","Finance Buddy is technically a Streamlit app that runs on a web browser. It relies on various APIs, among which Splitwise Expense API and OpenAI API are prominent. Upon authorization, Finance Buddy fetches your last month's records from Splitwise's servers and processes them with advanced analytics in Python. The resulting JSON file contains analytics results that will be used by an underlying Large Language Model (LLM) to generate two challenges along with hints to save money. The model is prompt-engineered in such a way that it will make use of your budget plan and analytics on the statement to create new challenges every time you click ""Advise me"".","The first and foremost challenge was to collect bank statements or expense data. We had limited access to APIs available, which were not compatible with our idea or at least had no sufficient input parameters. We got an idea to use Splitwise API, but the integration is non-trivial. In addition to that, it is highly difficult to prompt-engineered powerful models like GPT-4. The additional challenge is there is no or limited discipline to achieve a perfectly working model with prompt engineering.","It was our first attempt to integrate multiple APIs and programs related to expense data, AI, and data analysis to solve one single problem. From the top, this problem looks extremely simple to solve. The extra challenge is introduced by the integration of AI models and steering its outcomes to develop the proof-of-concept for Finance Buddy. We are proud of this achievement.","On the way, we learned a lot of things. As we aspire to become software engineers in the future, we learned a bunch of useful tools that we can use during our full-time roles to solve many similar problems. Since this hackathon is highly time-sensitive, we as a team planned things and managed sleep schedules accordingly taking our teamwork skills to the next level.","We see Finance Buddy as a mobile app in the future with live communication with Splitwise to track the user's expenditure behavior silently and suggest new fun challenges on demand. Due to its gamified nature for such an important topic of financial literacy, we aim to make this app more fun and have levels to unlock each time a user earns XPs for their success in a challenge. In the fast-forward future, every college student will be on Finance Buddy's social network to take on challenges with friends to save money for, for example, a week-long trip to Mackinaw Islands in the up-north!","html5, json, openai, python, splitwise, streamlit",https://github.com/punarvas/financebuddy
HackDearborn 2024,CUT THE CARBON,https://devpost.com/software/cut-the-carbon,False,We originally planned on creating a marketplace for used textiles like ripped pants to help reduce waste in landfills and also allow artists/fashionistas to get some free material,Our website links you to footprintcalculator.org an already existing website that calculates both ecological and carbon footprints,We used python for backend and html/figma for the front end,Tried adding AI for a good few hours never got far with it,We got ourselves out of a big rut,NEVER GIVE UP,Gonna try adding a new AI agent that can help with more alternatives and maybe create our own calculator in website.,"figma, html, python","https://www.figma.com/proto/UyoDpIQB1sJkklAH6VFVrB/Cut-The-Carbon?content-scaling=fixed&kind=proto&node-id=41-37&page-id=1%3A1670&scaling=scale-down, https://github.com/Ilyashhussein/AL-THE-PAL"
HackDearborn 2024,Tic Tac Toe,https://devpost.com/software/tic-tac-toe-360iye,False,,,,,,,,"c++, visual-studio",https://github.com/Ruby9813/HackAThon-Dearborn
HackDearborn 2024,VeriVote,https://devpost.com/software/homomorphic-zkp-a-crypgrapher-s-wet-dream,False,"The end goal and inspiration was a decentralized, secure blockchain-like election API utilizing Partial homomorphic encryption for tallying votes with a unique method of authentication/authorization using Zero Knowledge Proof's to secure transaction's anonymously. The idea was to use properties of blockchain technology for more of a utility based approach as a public ledger of transaction's; with each transaction being verified by the previous transaction and transaction following said transaction. Ideally, the end goal was anonymous, secure, verifiable transactions (votes in this case rather than monetary transactions) in such a manner that doesn't divulge confidential personal information regarding the vote itself or who it came from and the PII needed for typical voter registration/identification.",,,,,,,"flask, python, sagemath, sql",https://github.com/supasuge/VeriVote
HackDearborn 2024,lightWEB,https://devpost.com/software/lightweb,False,Helping parents managing their kids devices.,Control social media.,Google Chrome Extension,Keeping track of the data,Extension working in Google and social media like Facebook and Instagram are actually blocked,How to create your own extension,Working on the security and keeping the information of the users safe.,"css, html5, javascript, json",https://github.com/Rensildi/lightWEB
HackDearborn 2024,Recognition,https://devpost.com/software/recognition,False,"We were inspired by the need to improve road safety by addressing driver emotions like stress. We used problems we faced while driving, such as feeling sleepy or drowsy especially when driving at night, or when I'm angry and I wish my favorite song played",It detects the driver’s emotional state through sensors and adapts the car’s environment to reduce stress and increase comfort.,,,,,,python,https://github.com/AkramMoustafa/Hackathon-Project-2024-UVM.git
HackDearborn 2024,Sigmund's Quest,https://devpost.com/software/sigmund-s-quest,False,"We were inspired by the card game known as spot it. The games rules are simple. There is a set of two cards and only 1 symbol will match on each card, and you have call out the symbols name. However, we grew bored with the same old symbols used on the cards, so we decided to create our own on a locally run html file where a user can match symbols by typing in their names.","It allows children to play a the card game ""spot it"" on 3 preset levels, and a custom mode where they can choose through 5 different sets of symbols, amount of cards till they win, and the amount of symbols per card.","We built our program using css, javascript, and html. Our program is divided between 5 different html files with various style sheets and javascript files to make it more responsive and give our project its function as an educational tool for children.","We ran into problems working with GitHub, and working with repositories as we were not familiar working on such big coding projects. We were also having a hard time saving variables locally when we had to go between multiple html files.",We are proud we were able to refine the mechanics and presentation of our core game. We are also proud that we checked our ambitions in this project to allow us the time to polish the core experience.,"We learned that developers have to make a lot of compromise especially when it comes to having a time crunch. Also, this was an unforgettable learning experience as for most of us this was our first project using these programing languages.","In the future we would like to implement the ability for users to upload their own set of images they can play with which can streamline the process for teachers to create a fun flashcard game for students. Also, we would like to include a local multiplayer mode where two users can compete to find the matching symbol first to get points.","css, github, html, javascript, visual-studio-code",https://github.com/Razanshams/Sigmund-s-Quest-Practice/tree/main/images
HackDearborn 2024,Lucros,https://devpost.com/software/lucros,False,"The frustration I experienced as a teenage stock market enthusiast served as the inspiration for Lucros. I was eager to contribute, but I had a terrible time knowing where to begin. Online investing is a world full of conflicting advice, poisonous gurus, and an abundance of material that frequently causes more confusion than clarity. Many young people are losing money and spending time because they don't know where to start or who to believe. My creation of a platform that facilitates investment and is secure, easy to use, and accessible was inspired by this experience. Lucros seeks to free people from the clutter and noise of traditional investing to enable them, particularly novices, to make wiser financial decisions.","A micro-investment platform called Lucros combines AI tools, automated roundups, and fractional shares. Every purchase you make is automatically rounded up to the next dollar and invested using the bank account connection. Regardless of users' level of financial literacy, the app offers individualized stock recommendations, market sentiment research, and portfolio optimization to help them invest more wisely. Lucros facilitates accessible and data-driven investing for all levels of experience.","We built Lucros internally from the ground up usingDjangofor the backend and React.js for the front end. The backend connects toPlaidsecurely to obtain bank data, manage transactions, and arrange investment roundups.React.jspowers the user interface, offering a seamless, immediate experience. AI-driven tools provide tailored stock recommendations, sentiment analysis, and portfolio optimization. We intend to connect theWebull APIto speed up transaction execution and let users make investments straight through the website. We useAWSfor deployment, guaranteeing high availability, security, and scalability. We've created a completely customized, secure, and easy-to-use investing experience by developing everything in-house.",,,,,"amazon-web-services, bash, cloudflare, django, fedility, json, python, react, sqlite, ubuntu, webull-api","https://lucros.us, https://github.com/KarthikeyaThotak/lucros"
HackDearborn 2024,ProFormAI,https://devpost.com/software/proformai,False,"    In today’s fitness industry, there are countless apps designed to teach users the correct way to perform exercises. However, few apps provide real-time, personalized feedback that helps users improve their form by directly comparing it to professional standards. This gap inspired the creation of ProFormAI — a solution that not only instructs but also guides users throughout their fitness journey, ensuring they perform exercises with the correct form, improving both performance and reducing the risk of injury. ProFormAI focuses on making professional-level coaching accessible to everyone, offering actionable insights that adapt to each user’s specific needs and movements. ProFormAI empowers users by analyzing their workout videos and providing instant feedback on their form. Users can either upload pre-recorded videos or record their workouts in real-time using the app’s interface. The AI analyzes the user’s movements, compares them against professional standards, and offers detailed recommendations for improving posture, alignment, and overall technique. The app tracks progress over time, allowing users to see how their form improves. Additional features include the ability to input workout schedules, with the app providing motivational quotes tailored to each user’s journey, keeping them inspired and on track. Over time, users receive personalized insights into their performance and areas for improvement, ensuring they stay motivated and engaged with their fitness goals. We built ProFormAI using a combination of frontend and backend technologies to ensure a seamless user experience while leveraging advanced AI techniques for form analysis. Frontend: Built with React, the user interface is intuitive and responsive, enabling users to upload workout videos or record workouts in real-time. We ensured compatibility across multiple devices and browsers, maintaining consistent functionality regardless of the platform being used. Backend: We implemented a Flask backend that handles the video data processing and communication with the AI model. The backend is built in Python, enabling seamless integration with machine learning algorithms and ensuring that video data is processed efficiently. AI Analysis: The core of our form analysis is built using a Random Forest Classifier from scikit-learn. The model was trained on a dataset of professional workout videos to accurately classify movements and assess user form. By comparing user movements with professional standards, the AI provides tailored suggestions on how to improve posture, technique, and balance for various exercises. Additional Features: Users can input their workout schedules and receive daily motivational quotes designed to boost their confidence and commitment. The app also tracks user progress, providing a history of performance improvements and goals achieved. Developing ProFormAI presented several unique challenges: Video Recording & Analysis: Implementing a video recording feature that supports real-time analysis while maintaining accuracy and performance was a significant technical hurdle. Ensuring compatibility across devices, maintaining low-latency processing, and optimizing the AI analysis were key focus areas. Managing the performance trade-offs between real-time video processing and delivering meaningful feedback was particularly challenging. Real-time Feedback: Designing an AI system that provides accurate, actionable feedback in real-time required extensive fine-tuning of our models. We had to balance between speed and precision to ensure that users received feedback quickly while still maintaining a high level of accuracy. User Engagement: Keeping users motivated was another challenge. Providing detailed feedback on form correction is essential, but equally important is offering a psychologically engaging experience. We incorporated motivational elements, such as personalized quotes, to help users feel encouraged and inspired to continue their fitness journey. We’re proud of several key accomplishments that mark the success of ProFormAI: Successfully integrating AI-powered form analysis into a real-time video recording system, enabling users to get immediate, personalized feedback on their form. Building an intuitive and responsive user interface that not only allows users to record and upload workout videos seamlessly but also fosters an enjoyable user experience. Implementing a motivational component that complements the technical feedback by providing users with encouraging quotes and tracking their progress over time, creating a more holistic fitness coaching environment. Combining artificial intelligence and fitness coaching is a multifaceted challenge that requires a careful balance between technology and human engagement. We learned that adapting machine learning models, such as the Random Forest Classifier, to exercise movements requires a dataset rich in diverse workout forms to ensure high-quality predictions. Our AI model was trained with professional data, but we quickly realized the need for broader training sets to account for variations in body types, flexibility, and fitness levels. Additionally, we gained insight into the psychology of user engagement. While technical feedback is essential, it’s not enough to keep users engaged. Providing an experience that balances technical feedback with motivational support is key to retaining users and helping them achieve long-term success. Advanced AI Feedback: We are working on enhancing the AI’s ability to provide even more specific, granular feedback by targeting individual muscle groups and focusing on a wider range of exercises. This will allow users to receive tailored advice on how to engage specific muscles more effectively during their workouts. Interactive Workouts: In the future, users will be able to set personalized workout plans that adapt based on their progress. The app will offer AI-guided coaching during real-time workouts, offering corrections on form as the user performs each exercise. Community Engagement: We plan to introduce social features, such as workout challenges, leaderboards, and the ability to share progress with friends or a community of fitness enthusiasts. These features will help users stay connected and motivated, driving engagement and accountability.",,,,,,,"api, clustering, flask, javascript, python, react, sci-kit, sklearn","https://github.com/Parthiv12/ProFormAI, https://pro-form-ai-xpm2.vercel.app/"
HackDearborn 2024,E'm-pathy,https://devpost.com/software/i-m-pathy,False,Having no immediate solutions to sudden mental turmoil is no good place to be. Hence we have this solution cater to people's immediate need for therapeutic suggestions tailored according to individual needs and behaviours.,The product is backed with pretrained AI models that analyses user's current state of mind and the intensity of those emotions using NLP induced sentimental analysis and providing suggestive actions if the emotions are mildly negative and possible nearby help to reach out to if the emotions are extremely negative by any chance. The product also holds some extra features along with the MVP that cater a few specific immediate needs that aligns with the main functionality,"We have made use of a python backend that holds an api integration with a pipeline of AI models that goes in the following order NLTK, Sentimental analysis, Specifically catered NotebookLM.","We still are figuring out the integration between AI apis, moving data between two different AI models appears to not be a straightforward task","Getting the idea on paper and the initial shift from brainstorming to the implementation only sounded like a good drift, but we've mostly been lagging since then 😅",AI integration doesnt appear that easy as its said to be. And the insane number of possibilities you can wrap the functionality across any AI solution does make it a lot complicated because everybody offers a equally powered AI right out of the box and its becoming even harder to judge.,"Getting all the planned things into the code is still a hefty task to pull off. Having to deal with a lot of unknowns did create a lot of curiosity in us, so hoping that we still tend to work our way to complete it irrespective of the hackathon window","golang, googleai, notebooklm, python, tailwind",
HackDearborn 2024,TrunkMonkey,https://devpost.com/software/trunkmonkey,False,"Wiring the trunk Team Photo Checking the system 🌟 Inspiration
Every day, we interact with motors — from robotic arms to vehicles and even devices like your computer or mouse. Motors are everywhere! This project was inspired by the need for a safety solution that could prevent injuries, especially for children or adults who might accidentally get their hands stuck while closing something like a car trunk. We set out to create a smart, motorized system that could detect such dangers. 🚗👶 💡 What it does
We developed a model of a trunk operated by a DC motor. As the trunk closes, it can detect if a hand is nearby and automatically reverses the motor to reopen the trunk, preventing injury. 🛑👋 Additionally, we created a simulation in MATLAB to demonstrate how the PWM signal is converted into angular velocity using MATLAB's Simulink scope. ⚙️📊 🔧 How we built it
We used an Arduino, a DC motor, and Grove sensors to build the physical model, along with MATLAB for the simulations. For live PWM signal demonstrations, we used an FRDM board. 🚧 Challenges we ran into
We faced a steep learning curve with Simulink, and we even fried a motor shield along the way, which delayed us a bit. 😅 But hey, that’s all part of the journey, right? 🏆 Accomplishments that we're proud of
We successfully simulated stepper motors and implemented a real-world safety mechanism that could be used in automobiles! Overall, it was an amazing experience — learning control systems and implementing PID controllers. We couldn’t be prouder of what we achieved. 🚀🤖 📚 What we learned
We learned a lot about DC motors, Simulink, properly handling motor shields, and building outputs to enhance motor safety. It was a fantastic learning experience! 🎓⚡",,,,,,,"arduino, c++, matlab",https://github.com/akirumisu/HackDearborn3
HackDearborn 2024,Risky Business,https://devpost.com/software/risky-business,False,"What is the inspiration?
The inspiration behind this project was the increasing prevalence of API security vulnerabilities in modern applications. With APIs being central to many software systems, there is a growing need for tools that can automatically identify potential risks, misconfigurations, or security gaps. The team was motivated by the lack of accessible, automated tools that could provide both developers and security experts with real-time feedback on their API configurations, making security audits easier and faster. What is the product?
The product is Risky Business, an AI-powered cybersecurity audit tool designed to analyze API logs and configurations. It automatically detects security vulnerabilities, highlights risky practices, and provides actionable insights and recommendations to improve API security. The platform is accessible via a web-based dashboard where users can upload their API logs or connect their API directly for real-time monitoring and analysis. How does it work?
Risky Business works by utilizing AI/ML algorithms to analyze API logs and configurations in real-time. It scans for potential security risks such as weak authentication methods, exposed credentials, and misconfigurations based on industry standards like OWASP’s API Security Top 10. After processing the data, it generates an audit report that includes: A risk score for each API endpoint.
Detailed explanations of the risks detected.
Recommendations to mitigate or resolve the vulnerabilities. Users can access these insights through a dashboard that visualizes risk levels and provides step-by-step guides to improve API security. Data availability and quality: Collecting real-world security data to train the AI model was difficult due to privacy concerns.
Model accuracy: Balancing the AI’s sensitivity to avoid false positives and false negatives was a complex and iterative process.
User experience: Designing a dashboard that could effectively communicate technical security risks in a simple and actionable way required significant testing and feedback.
Real-time performance: Ensuring the tool could handle large API logs in real-time without slowing down the user experience was another technical challenge.
Integration with third-party tools: Seamless integration with popular services like AWS and maintaining compatibility with their API changes required ongoing adjustments. Successfully training an AI model that can identify and classify various API security risks with reasonable accuracy.
Building a user-friendly interface that simplifies complex security information into actionable steps for developers of all skill levels.
Creating an automated report generation feature that translates raw log data into insightful security audits with tailored recommendations.
Ensuring the platform is scalable to handle multiple users and real-time API requests without compromising performance. The importance of high-quality data: Developing effective AI models depends heavily on having a diverse and comprehensive dataset.
Balancing security and usability: While it’s essential to provide thorough security insights, presenting them in an accessible and understandable way for users is equally important.
Security is an evolving field: New vulnerabilities and attack vectors emerge constantly, so it’s critical to design the tool in a way that it can adapt to future security challenges.
The complexity of API security: API security goes beyond surface-level vulnerabilities and requires deep understanding and continuous monitoring. Expanding the AI model to cover more types of security risks and vulnerabilities as the threat landscape evolves.
Adding more third-party integrations, including popular cloud services and development tools, to make the audit process more seamless for users.
Introducing more customizable risk thresholds, allowing users to fine-tune the AI’s sensitivity based on their specific security needs.
Enhancing the chatbot assistant to provide more interactive and detailed guidance for users when interpreting their security reports.
Developing a mobile-friendly version of the dashboard to give users more flexibility in monitoring their API risks on the go.",,,,,,,"api, javascript, python",https://adamatieh55.wixsite.com/risky-business
HackDearborn 2024,TutorVerse,https://devpost.com/software/teach_llm,False,"Our group wholeheartedly believes that education can be changed through LLMs, currently, we see many of our classmates using ChatGPT to understand assignments, so, what if we thought, what if we built ChatGPT for education, on steroids?","The teacher is able to upload coursework, PDF's, quizzes, and youtube videos, the LLM analyzes it, and generates material, i.e deep fakes of the professor explaining and/or audio clips, more quizzes, practice problems, as well as a chatbot for the students to ask questions.","We used OpenAI API, langgraph, and a custom FlaskAPI model to create multiple agents interacting in the backend and a solid web front end.","We ran into challenges in trying to set the embeddings for the LLMs, as well as connecting the front and backends,","We have a working chatbot, that is able to use uploaded material to explain problems properly!","We all learned more about LLM implementations, custom APIs, as well as some refreshers on Javascript.",We will continue developing this solution and attempt to commercialize it in the near future.,"api, database, openai, python",https://github.com/alpaslan-exe/TutorVerse
HackDearborn 2024,Motive,https://devpost.com/software/motive-9axbqy,False,"Motive was inspired by the challenges of trip planning—whether it’s managing a budget, organizing for a group, or finding the right places. We wanted to create a solution that simplifies the process and helps users save time and effort.","Motive allows users to plan activities and trips by searching for places based on budget, group size, and location. The AI chatbot is available for those who need help with suggestions, while users can also input preferences themselves to find nearby places. The app optimizes results for each user’s needs, making planning quick and easy.","We built Motive using TypeScript and React for the front end, with Flask and Python on the back end. We integrated Google Maps API for location-based searches and added AI chatbot functionality for personalized recommendations. Additionally, we set up middleware for protected endpoints, implemented authentication, and included interceptors on the front end for smooth request handling.","There were no major challenges, but we focused on perfecting the AI chatbot integration and refining search filters. Setting up secure middleware and endpoints also required careful attention to detail.","We’re proud of how we seamlessly integrated the AI chatbot, implemented secure authentication, and provided an optimized user experience with middleware and interceptors.","This project helped us deepen our understanding of front-end development with TypeScript and React, as well as secure back-end development using Flask and Python. We also refined our approach to integrating AI and enhancing the overall user experience.","The next move for Motive is to allow users to book all services—such as activities and accommodations—in one place. This will make Motive a comprehensive tool for planning and booking trips, further streamlining the process for users.","daisy, flask, postgresql, python, react, supabase, tailwind, typescript","https://github.com/Wruqe/Motive, https://github.com/Wruqe/MotiveRest"
HackDearborn 2024,UMCU,https://devpost.com/software/extrack,False,"There was a variety of apps that inspired our project. Some examples of those include apps like Truebill / Rocket Money, Honeydue, etc.",This is a revamp of the UMCU REVAMP for ios app,We created Figma and SwiftUI wireframes as a mockup for a final version of the app.,Things haven't gone very smoothly. We started off overly-ambitious and tried to use frameworks / technologies that we weren't very familiar with. We didn't really know where to start. We ended up figuring out how to use SWIFTUI,"We're proud that we got something to show for our first hackathon. I don't think any of us were sure how this would pan out, but we'll be signifcantly more prepared for future types of events! Aswell as learning how to do SWIFTUI",We learned how to adapt to different issues that come up. and SWIFTUI,ExTrack might be dead in the water here... it was an ambitious attempt,"swift, swiftui",
HackDearborn 2024,RecruitMatic,https://devpost.com/software/recruitmetic,False,"The recruitment process can be tedious and time-consuming, especially when dealing with hundreds of resumes for a single job posting. Traditional methods and even most Applicant Tracking Systems (ATS) focus heavily on keyword matching, which often leads to talented candidates being overlooked. The need for a smarter and more precise solution inspired us to create RecruitMatic, a platform that leverages AI to help recruiters identify the best candidates faster by deeply analyzing resumes, job descriptions, and specific questions posed by recruiters.","RecruitMatic blends cutting-edge artificial intelligence with an intuitive chat interface, transforming the recruitment landscape into an interactive knowledge hub. By leveraging the OpenAI API, it effortlessly extracts insights from a diverse array of file types—such as PDFs, Word documents, and Excel sheets—empowering recruiters to ask specific questions and receive tailored answers. This seamless integration of language processing and document analysis makes navigating candidate data simple and engaging, unlocking new pathways for discovery. Ultimately, RecruitMatic offers a groundbreaking way to connect with talent, revolutionizing how we engage with information and make informed hiring decisions.","Building RecruitMatic was a collaborative journey that started with a brainstorming session where our team identified the challenges recruiters face in navigating a sea of applicant data. Driven by a shared vision, we decided to create an AI-powered solution that could streamline this process. Next, we carefully selected our tech stack based on our team’s collective knowledge and skills. We opted for Python for its versatility, along with the OpenAI API for natural language processing capabilities, ensuring we could effectively analyze resumes and generate meaningful insights. Once we had the foundational resources in place, we focused on developing the bare bones of the application. The core functionality involved uploading resumes, which our code then processed to extract key information, allowing recruiters to ask specific questions and receive ranked responses based on candidate qualifications. After successfully implementing this core logic, we shifted our attention to the user interface. We aimed for an intuitive design that would make it easy for users to interact with the app, presenting data in a clear and engaging way. This phase was crucial in enhancing the overall user experience, ensuring that RecruitMatic not only functioned well but also looked polished and professional.","During the development of RecruitMatic, we encountered several challenges that tested our problem-solving skills. One major hurdle was accurately parsing resumes, given their diverse formats and styles, which made consistent data extraction tricky. Additionally, fine-tuning the natural language processing algorithms to understand context beyond basic keyword matching required extensive experimentation. We also faced integration issues with the OpenAI API, particularly in optimizing response times. To address this, we iterated on our chunking strategy, breaking resumes into smaller segments for more efficient analysis, which significantly improved response times without sacrificing the quality of insights. These obstacles ultimately strengthened our coding practices and enhanced the overall performance of RecruitMatic.","We’re proud of several key accomplishments in developing RecruitMatic. We successfully integrated the OpenAI API, allowing for advanced natural language processing that delivers meaningful insights from resumes. Our intuitive user interface enhances the recruiter experience, making data navigation seamless. Additionally, by refining our chunking strategy in the code, we significantly improved response times, ensuring timely information delivery. We also created a logo for our team, adding a professional touch to our application. Overall, our collaborative effort in overcoming challenges has resulted in a powerful recruitment tool and strengthened our team's skills and cohesion.","During the development of RecruitMatic, we learned valuable lessons about teamwork, technical skills, and user experience. Collaborating closely allowed us to leverage individual strengths, enhancing our problem-solving abilities. We gained insights into natural language processing and the OpenAI API, improving our coding proficiency. Iterative testing became essential, particularly for optimizing our chunking strategy and overall performance. Most importantly, we recognized the significance of creating a user-friendly interface to ensure accessibility for recruiters. This project not only enhanced our technical skills but also fostered a strong sense of collaboration and innovation within our team.","Looking ahead, we have ambitious plans for RecruitMatic. Our focus will be on enhancing its capabilities by expanding the types and volume of data it can analyze for more in-depth candidate evaluations. We aim to transition to cloud deployment, integrating a secure vector store to manage data efficiently and securely. Leveraging the Retrieval-Augmented Generation (RAG) approach, we plan to evolve RecruitMatic into a fully developed web app, with the potential integration of Fetch.ai to further streamline the recruitment process as an AI Agent. Currently, it runs locally, but cloud deployment is on our roadmap. Additionally, we'll gather user feedback to improve the interface, explore partnerships with job boards and HR platforms, and stay at the cutting edge of recruitment technology to make hiring smarter and more efficient.","css, csv, html, io, langchain, openai, openaiapi, openpyxl, python, python-docx, python-pptx, sessionstate, streamlit",https://github.com/kritish07/RecuitMatic
HackDearborn 2024,Travel Agent,https://devpost.com/software/travel-agent-vts92n,False,"When exploring a new country for the first time, it can often feel very overwhelming. There's so much to do, but not enough time to do it. With Travel Agent, users can make the most of their vacation.","The Travel Agent allows a user to input where they are, 
and the program will generate a recommended list of tourist attractions in a user-set radius. The recommendations will depend on what other parameters the user includes (price range, high ratings, children friendly etc).","The front-end was developed with HTML and Javascript, while back-end used Fetch.ai and the Google Maps API to generate the desired vacation spots.",We were struggling for a good while with getting the agents to communicate with each other on the Fetch.ai blockchain.,We are proud of being able to integrate a Javascript programmed front-end with a Python programmed back-end using Flask.,"We learned what Fetch.ai, where it was used, and how to use it.","The next step would definitely be to include more parameters on our main website, to give better suggestions to the user. Also, a dedicated chatbot to bounce vacation ideas off of plus advice on which activities would be the most fun.","ascertanity, css, fetch.ai, google-cloud-console, google-geocoding, google-places, html, javascript, jquery, python, react","https://github.com/MinhazCodes-R/HackDearborn3/tree/main, https://docs.google.com/presentation/d/1_QIW2PQEQXpNR4nZQ3aJUn575AOh8fhbFOoEUCpk_aY/edit"
HackDearborn 2024,AI Nutrition Planner,https://devpost.com/software/ai-nutrition-planner,False,"The idea came from the everyday struggle of deciding what to cook with available ingredients. Many people, whether they're busy or health-conscious, find it hard to come up with meal ideas that match their ingredients, calorie preferences, or favorite cuisine. We wanted to create a tool that simplifies meal planning while promoting healthy eating habits.","AI Nutrition Planner helps users quickly create custom recipes based on the ingredients they have at home. By inputting ingredients, desired calorie count, and preferred cuisine, the app generates a tailored recipe, removing the guesswork from meal planning and promoting healthier eating with personalized meal suggestions.","We used a combination of Next.js for the frontend and FastAPI for the backend, both containerized using Docker. For the AI recipe generation, we leveraged natural language processing (NLP) to understand ingredient lists and match them with suitable recipes. The app is powered by an AI model trained on a large dataset of recipes, calories, and cuisines. We also integrated a user-friendly interface using Material-UI for an intuitive experience.",One major challenge was ensuring that the AI-generated recipes matched the users’ calorie preferences accurately. It was tricky to balance ingredient variations while keeping the recipes nutritious and customizable. Another challenge was integrating the recipe generation API in a way that remained fast and responsive without compromising accuracy.,We’re proud of successfully building a functional and user-friendly app that uses AI to personalize meal planning. Making the AI understand various cuisine preferences and being able to generate recipes that fit users' dietary and caloric requirements in real-time was a significant milestone.,"We learned a lot about AI model integration, the complexities of recipe generation, and handling dynamic inputs like ingredients and calorie counts. We also deepened our understanding of user experience (UX) design, ensuring that the app is intuitive and easy to navigate, even for people unfamiliar with technology.","We plan to improve the AI's capability to suggest alternate ingredients, expand the database with more cuisines and dietary restrictions (like vegan or keto), and implement features like meal planning for an entire week. We also hope to integrate voice assistance to make the app even more convenient for users.","fastapi, nextjs, openai-api, python, react","https://github.com/naligant/Recipe_Suggester, https://umich-my.sharepoint.com/:p:/g/personal/sambel_umich_edu/ETJSJcCGJgFFrikW2oJ95ksBbtc9pZjmZZSf_F9XC5rSfA?rtime=cUh6PJ_r3Eg"
HackDearborn 2024,Pick2Plan,https://devpost.com/software/pick2plan,False,"When I started my MS in AI program at UM Dearborn this Fall 2024, I noticed that the professors and university staff upload the schedules for semester in a table format such as at what dates and time what classes will be conducted and what topic will be discussed. So if a student uses a Google calendar and he wants the whole schedule on his calendar they will have to add event multiple times, so for them I wanted to create an agent that helps add all the events in one go. That's when we had the idea of Pick2Plan!!","Pick2Plan basically lets a user upload the image or screenshot of the table of schedule he wants in his calendar. Lets the user extra details in chat that he would like for additional context for the events to be added. After that it extracts the text data from the image, understand the individual event titles, dates, timings, etc and creates those events on the google calendar of the user.","We used Python, pytesseract, OpenCV, uagents, Fetch ai, in the backend, JavaScript for the frontend, Google Cloud Platform and Google Calendar API for event creation.",We struggled a bit for Google Calendar API usage and to create an agent which can properly understand the text in the image.,"We are proud of how instantly we were comfortable with each other when the team was formed on the spot. How effortlessly we understood our roles in the team and collaborated with each other and solved each other doubts easily. We are proud that the students, mentors and workshop speakers liked our project idea.","We learned the potential of Google cloud platform, Fetch AI, Gemini and Google's aistudio. We learned how ahead some companies are in the game of AI. We learned about many different and interesting project ideas.","We plan to increase its use cases for not only scheduled timetables but also, movie tickets, travel tickets, doctor visits prescriptions, appointments. It can have an additional feature where a user can create calendar events for multiple other users which can be useful in friend groups, office teams and study groups. Overall, Pick2Plan has the potential to not only be a useful extension for university and office platforms but also be a integrated solution across multiple apps where just a normal conversation of a meeting or hangout being confirmed for a user can create an alert for him if he wants to add it to the calendar or any other personal scheduler he wants.","css, fetch-ai, fetch.ai, google-calendar-api, google-cloud, html, in-the-backend, javascript, javascript-for-the-frontend, opencv, pytesseract, python, uagents",https://github.com/ShubhamDoshi126/Eventadder
HackDearborn 2024,Stock Feedback AI,https://devpost.com/software/stock-broker-1rnik9,False,"The motivation for building this program stems from the desire to empower everyday Americans to make informed financial decisions. Many investors, especially those new to the market, struggle to navigate the complexities of stock investments and often miss opportunities or take unnecessary risks due to a lack of expert guidance. By leveraging AI and real-time data, your program acts as a mobile, personalized financial advisor, providing immediate and actionable insights. It bridges the gap between professional financial consultancy and accessibility, making smart investing advice available anytime, anywhere.
This tool is designed to democratize access to sophisticated investment advice, ensuring that even those with modest portfolios can maximize their returns and minimize risks, helping build a stronger financial future for all users.","This program serves as an AI-powered financial consultant, providing real-time, personalized investment advice based on users' portfolios. It analyzes market data to offer recommendations on whether to buy, hold, or sell stocks, helping users make informed decisions and maximize returns. Designed for on-the-go use, it ensures that investors have immediate access to expert-level guidance anytime, without needing a professional advisor.","We built the program using a combination of modern web development and backend technologies to ensure a smooth user experience and powerful functionality: React JS: The frontend is built with React JS, allowing for a dynamic and interactive user interface. This framework provides flexibility and efficient state management for seamless user interaction. Tailwind CSS: Tailwind CSS is used for styling, giving the application a clean and responsive design. It allows for rapid UI development with utility-first CSS classes, ensuring the application looks good across all devices. Python with Flask: Flask powers the backend, handling the business logic and interacting with the AI algorithms that analyze stock market data. Python's flexibility makes it ideal for integrating AI-driven decision-making processes. Stripe: Stripe is integrated for managing payments, enabling users to subscribe to premium features. Stripe’s secure payment API ensures smooth and secure transactions. Firebase: Firebase is utilized for authentication, allowing users to securely log in and manage their portfolios. It also stores user data and integrates seamlessly with the app’s frontend. JavaScript and HTML: JavaScript, along with HTML, is used to handle various functionalities on the client side, ensuring the app responds quickly to user input and provides a smooth user experience. This tech stack was chosen to deliver a reliable, responsive, and user-friendly platform that investors can trust.","Building this program came with several challenges: Implementing APIs: Integrating various APIs, especially for stock market data and AI algorithms, was more complex than expected. Ensuring that the APIs worked seamlessly together and delivered real-time, accurate data was a major hurdle. Stripe Payment Integration: Setting up a fully functional payment system with Stripe took time and effort. We had to ensure that it was not only secure but also user-friendly. However, after extensive testing, we successfully implemented it, and now it works flawlessly. Tailwind CSS for UI Design: Adjusting to Tailwind CSS for building the user interface was a learning curve. While it offers powerful utility-first CSS, it required us to rethink our approach to styling, but we ultimately created a responsive and clean UI. Overcoming these challenges has made the project more robust, ensuring both functionality and a great user experience.","AI-Powered Feedback: One of our biggest accomplishments is training the AI to provide effective, personalized feedback to users based on their portfolios. The AI is able to analyze real-time market data and give actionable advice, mimicking the insights a financial consultant would provide. Real-Time Stock Analysis: We successfully integrated the AI with real-time stock charts, candles, and other financial indicators that professionals use to assess market conditions. This allows the program to offer in-depth, data-driven recommendations, helping users make informed investment decisions. These achievements are significant because they enable the program to function as a reliable, on-the-go financial advisor for investors.","Throughout the development of this project, we gained valuable insights in several areas: AI and Financial Markets: We deepened our understanding of how AI can be trained to interpret financial data, including stock charts, candles, and trends, to provide meaningful investment advice. This process helped us realize the potential for AI in automating complex financial decisions. API Integration: We learned the intricacies of integrating multiple APIs into a cohesive system, especially real-time data feeds. This taught us how to handle data synchronization, error handling, and API response times to ensure smooth user experiences. Payment Systems: Implementing Stripe was a learning experience in how to securely manage payments in a live environment, and we now have a much better understanding of handling subscription-based services and ensuring financial security. UI Development with Tailwind CSS: Adapting to Tailwind CSS taught us the importance of utility-first design for rapid and responsive UI development. We improved our ability to create sleek, mobile-friendly interfaces efficiently. Overall, this project enhanced our technical skills, improved our problem-solving abilities, and gave us a deeper understanding of the technologies used to build dynamic, data-driven applications.",,"firebase, html, javascript, react, stripe, tailwind",https://github.com/MerwinAamir1/stock-predicter-2.0
HackDearborn 2024,Scam Protect,https://devpost.com/software/scam-protect,False,Mark Rober's response to scammers with a glitter bomb.,Scam Protect injects a dynamic page into potentially scammy sites to give possible victims a warning before they continue.,"We used Javascript to create the injection and MongoDB, Google Cloud, and the Gemini API to create the  fine tuned support agent.",Integrating the Gemini API into the project.,How the UI and app turned out after we finished integrating everything.,We learning about fine tuning gemini and the architecture for a dynamic end to end web page application.,Dynamic URL scanning with a probability model to auto inject whenever a high probability is detected.,"express.js, gemini, google-cloud, javascript, mongodb, node.js","https://github.com/UmarRS/scam_prot, https://github.com/UmarRS/scam_prot_vote, https://github.com/UmarRS/scam_prot_backend"
HackDearborn 2024,Doctor Green – Your AI guide to recycling,https://devpost.com/software/ai-buddy-i3z8fj,False,"While many people want to recycle, we currently don't have user-friendly, accessible solutions to help them do it efficiently. Therefore, we're developing a simple, accessible tool that anyone can use regardless of their recycling knowledge. With Doctor Green, we aim to solve this problem by providing a platform to identify recyclable materials and locate nearby recycling centers.","Our AI-powered mobile app analyzes items via photo recognition, categorizes their recyclability, and directs users to the nearest recycling center in the USA. Our targeted users would be:Primary Users:Environmentally Conscious Millennials and Gen Z (aged 18-35): They are motivated to reduce their environmental impact. They are tech-savvy, prefer user-friendly apps, and actively seek ways to recycle and manage waste better. They value convenience, and quick access to information, and often use mobile technology for day-to-day tasks. They love intuitive apps that provide immediate results.Secondary Users:Busy Professionals:
These users care about recycling but are often pressed for time. They want fast, reliable information that can be accessed on the go. 
Urban and Suburban Families:
Parents might use the app as a tool to teach their kids about sustainability.","For the front end, we used Flutter to create a responsive mobile interface that allows users to capture images of items for recycling. On the backend, the app sends captured images to a GPT4 API for photo recognition, which categorizes the items based on their material. Then, the app determines the recyclability of the material and provides users with the nearest recycling locations through Google Places API. The app features a scoring system that tracks material type, recyclability category, item size, and the user's recycling streaks to enhance user engagement.",It took us a long time to make the app functional.,"Our app is fully functional with an intuitive UI/UX design. The app’s response time is fast, which allows users to get recyclability information. Additionally, we’ve implemented a points system that boosts user engagement by rewarding consistent recycling habits.",We learned how to create an AI agent to solve real-world challenges. We also gained insights into integrating the GPT4 Large Language Model to improve our app's functionality.,"We plan to improve Doctor Green with a modernized UI for an even smoother user experience. Future updates will introduce a social feature where users can add friends and compete on a local leaderboard to boost engagement. We will also implement automatic point updates triggered by chatbot responses, allowing users to see their recycling progress instantly after interacting with the app.","api, firebase, flutter, gpt",https://gitlab.com/ubernerd117/hackdearbornthree
HackDearborn 2024,Talk Tech,https://devpost.com/software/talk-tech,False,"          Inspiration Talk Tech was inspired by the need for a community-focused platform that empowers early-career individuals and students to discover local opportunities, promote small businesses, and foster collaboration. What it does The app connects users with local job and volunteer opportunities, supporting small businesses and initiatives while facilitating networking within the community. How we built it We built Talk Tech using Swift for the iOS app development, along with Xcode for coding and design. We utilized Visual Studio Code for web components, along with HTML, CSS, and JavaScript for the website. Design elements were created using Sketch and PowerPoint and databases. Challenges we ran into We faced challenges with uploading the app to the platform and unifying various files to ensure smooth functionality and deployment. Accomplishments that we're proud of We are proud of our strong teamwork, which enabled us to overcome obstacles and successfully bring our vision to life. We also take pride in creating a user-friendly app that benefits our community. What we learned Throughout the development process, we learned how to effectively use Xcode and code in Swift, enhancing our technical skills and collaborative abilities. What's next for Talk Tech Looking ahead, we plan to refine the app based on user feedback, expand our features, and increase outreach to more local businesses and organizations to further strengthen community connections.",,,,,,,"core-data, css, dabases, html, javascript, powerpoint, sketch, swift, ui, ux, vscode, xcode","https://youtube.com/shorts/yPCBirqc_z8, https://github.com/Lukas-2288/TalkTech"
HackDearborn 2024,G.U.A.R.D.,https://devpost.com/software/project-g58r61,False,"In manyremote areasandunder-resourced countries, limited access to emergency services and immediate care often leaves accidents unnoticed, putting lives at risk. Our goal is to bridge this gap by bringing safety directly to the driver’s phone. Using AI, our system is designed to detect and assess accidents in real-time, ensuring that any harm or sudden danger to the driver is quickly recognized, allowing for a rapid response when it’s needed most. This solution empowers drivers with enhanced safety, regardless of where they are.","Our app, named G.U.A.R.D. to reflect its role as a vigilant guardian, leverages current weather conditions and user driving habits to assess risks and provide simple, proactive safety recommendations tailored to situations like rain or snow. While this feature enhances the overall driving experience, the app's primary function is to ensure safety during accidents. It uses real-time location tracking to monitor speed and acceleration, recognizing patterns indicative of a potential crash. When the app detects an unusual or sudden halt not explained by rapid acceleration, it activates an AI chatbot to engage with the driver. If the driver doesn’t respond within a designated timeframe, the app automatically alerts first responders, sharing the driver’s precise location to expedite assistance and retrieval.","We used a combination of Flutter for mobile development and Fetch.ai to create a robust and responsive system for real-time emergency response. Google Gemini provided the natural language processing capabilities necessary to add an emotional component to the chatbot interactions, ensuring empathetic communication during stressful events. The app communicates with multiple agents to monitor critical metrics like speed, location, and heartbeat, seamlessly integrating Google Cloud's Speech-to-Text API to transcribe the driver’s voice and assess their well-being. By connecting these technologies, G.U.A.R.D. delivers a proactive safety solution that’s both adaptive and reliable.","We encountered several challenges throughout the development process. The most significant hurdle was managing real-time data processing and accurate accident detection without compromising the app’s speed and responsiveness. Additionally, integrating multiple AI components and ensuring seamless communication between the Fetch.ai and Google Gemini agents required meticulous troubleshooting. Another challenge was achieving consistent performance across varied network conditions, as our target audience includes drivers in remote or under-served areas with limited internet connectivity.","One of our proudest accomplishments is creating a solution that has the potential to save lives, particularly in areas with limited emergency resources. We’re thrilled with the accuracy and responsiveness of our crash detection system, which is capable of identifying even subtle signs of an accident. Additionally, we’re proud of the seamless integration between Fetch.ai and Google Gemini, enabling our app to interact with users in a supportive and empathetic manner. This project also pushed us to master new technologies and improve our problem-solving skills, resulting in a finished product that we are proud to present.","Developing G.U.A.R.D. taught us the importance of balancing multiple technologies to create a cohesive user experience. We gained hands-on experience with real-time data processing, and improved our understanding of how to design apps that can adapt to unpredictable situations. We also learned a great deal about AI-driven communication, honing our ability to build an empathetic user interface. The project provided insights into handling real-world challenges, such as low connectivity and variable GPS accuracy, which are crucial factors for a reliable safety app.","Moving forward, we aim to expand G.U.A.R.D.’s functionality by incorporating additional safety features, such as weather-based alerts and predictive analytics to warn users of potential hazards. We’re also planning to integrate health monitoring capabilities using wearable devices to capture vitals like heart rate and stress levels. Our goal is to improve the app’s ability to assist drivers in remote areas by exploring offline capabilities and increasing its adaptability across different mobile networks. Additionally, we hope to collaborate with emergency services to ensure that G.U.A.R.D. can directly notify responders in under-resourced regions, further strengthening its life-saving potential.","c++, fetch.ai, firebase, flutter, gcp, gemini, python, swift",https://github.com/obaidmm/HackDearborn/tree/main
HackDearborn 2024,NutriCML,https://devpost.com/software/nutricml,False,,,,,,,,react,
HackDearborn 2024,EnvoQuest,https://devpost.com/software/envoquest,False,There is often a lack of awareness about whether an object should be thrown in the trash or recycled.,"The application helps users sort objects into trash and recycling categories, promoting awareness about waste disposal practices.","We developed the application using Glitch.com, leveraging its user-friendly interface for rapid prototyping and collaboration.","We faced difficulties in two main areas: Scoreboard Implementation: Creating a functional scoreboard that accurately tracks user progress and performance.
Object Sorting Mechanism: Designing an effective method for sorting items into trash and recycling categories.","I am proud of the following achievements:
Concept Development: I conceived the initial idea and developed it into a working application.
Independence: As the sole member of my team, I took the initiative to build the project from the ground up after other teammates withdrew. This experience taught me the value of perseverance and self-reliance.","We learned important lessons about waste management, specifically regarding which items can be recycled and which should be discarded as trash.","Our future plans include adding more features to enhance user engagement and educational value, such as:
Interactive Learning Modules: Provide users with more information about recycling practices.
Expanded Item Database: Include a wider variety of items to help users make informed decisions.","css, html, javascript",https://regal-pentagonal-longship.glitch.me/
HackDearborn 2024,LiFi,https://devpost.com/software/lifi-4qzb8v,False,"There is a professor researching LiFi applications in V2V communications here at UM Dearborn. After learning about his work, we thought that this event would be a good opportunity to learn more about what goes into the technology.",,"The hardware we used were two Arduino 101 boards with Grove series shields, a Grove LED controller, and a Grove light sensor. Our, embedded software was built using the Arduino IDE in C++, and our UI/UX software was built using C#. We build the packets client-side and send them over a USB port to the transceiver, which is the first Arduino, with the attached LED socket. When the LED is off, the receiver interprets a logical 0, and when the LED is on, the receiver interprets a logical 1.","Our biggest challenge was probably hardware limitations. The LED simply doesn't have enough intensity for the photoresistor to detect a signal from a significant distance, and not in ambient lighting conditions. Additionally, while it is not a problem for this demo, since it was intentionally slowed down such that a human could visualize the packet as it is being sent, the photoresistor requires about 30 ms to  update, which for a real communications network would likely result in insufficient communication speeds.","We were very excited that we could implement this protocol, even in a small way in such a short period of time.","This demo taught us a lot about the physical layer of optical communication. Particular, we learned about hardware limitations, and the challenges faced by more serious researchers developing this technology.","In this demo we demonstrated a potential vehicle safety application that would allow an AEB to be applied if a human's reaction speed was insufficient to avoid a rear-end collision with another vehicle, but the infrastructure allows for any number of unique signals to be communicated across short-range networks. One stand out application far into the future, is that this signaling could enable self-driving vehicles to communicate data between each other to allow for better predictive decision making.","arduino, c, c#",https://github.com/lmahmed/HackDearborn-3-LiFi.git
HackDearborn 2024,RideWatch,https://devpost.com/software/ridewatch,False,Our project was inspired by wanting to ensure parents are aware of where their child is at and their safety,The app allows the bus to scan QR codes on the child's app to inform the parent that their child has got onto the bus.,"We built this app using React Native, Python with the Flask framework, and some other tools such as Git and a Raspberry Pi.","We had trouble setting up the database for the user login. At first we wanted to use MySQL then resorted to MongoDB, and then finally relied on built-in JSON files.","Getting the server set up. Using Flask for a React Native application is uncommon and hard to do, so having that set up was very cool to se..","Databases can be tricky. A lot of them require several other dependencies and a proper API to work with. Since they are usually incorporated after the frontend and backend are complete, they can completely change the direction of your project if you find out the database language isn't compatible.",Getting a full-fledged database in addition to more reliable QR code verification and Raspberry Pi compatibility.,"flask, javascript, python, raspberry-pi, react-native",
HackDearborn 2024,Drive-I,https://devpost.com/software/drivei,False,"As amateur drivers, we often face confusion regarding yielding right-of-way and using lane change signals. This inspired us to leverage Apple’s mobile intelligence to create an intelligent co-pilot and active passenger system. Our goal is to develop a fine-tuned language model (LLM) that focuses on traffic signals, driver safety, and vehicle specifications.","Our system interacts with users through zero-shot speech synthesis, guiding both drivers and passengers for a quality travel experience. It can anticipate human errors and detect vehicle defects, ultimately helping to prevent major accidents..","We utilised the Whisper Turbo model for its rapid speech-to-text conversion capabilities and it is integrated as uAgents using Fetch.ai starter kit. The core of our system is powered by LLAMA (META's LLM), which processes the driver’s spoken questions and generates text-based answers. These answers are then converted back into speech using XTTS.","1.) Model Installation: The PYLAMA model, trained with 30 billion parameters, posed installation challenges due to its high computational costs. As a solution, we opted for the 7 billion parameter version, which is more manageable for low-cost GPU models.
2.) Audio Capture Limitations: Initially, capturing 3-second audio clips for TTS was problematic. We adjusted our approach to capture audio within a duration of 6-10 seconds to improve functionality.","Speech Recognition Efficiency: We achieved speech recognition output within 5 seconds, considering our computational resources.
LLAMA Response Time: The output from the PYLAMA model is generated within 2 seconds.
TTS Output Speed: Text-to-speech (TTS) output is produced within 5 seconds.
With this setup, given an audio prompt, users can expect an audio response within 12 seconds. Our implementation of zero-shot speech synthesis using the powerful LLAMA model positions us competitively against systems like ChatGPT-4.0, which interacts with pre-trained voice models. Despite resource limitations, our system achieves comparable performance.","Through this project, we gained extensive insights into LLMs (specifically LLAMA) and their time efficiency, which is crucial for our system's performance.","We plan to scale up the system by incorporating a dedicated driver model and vehicle model that continuously monitor the driver's activities using multi-modal capabilities. This approach will ensure the system remains fully integrated with the vehicle, enhancing its ability to assist with safety and decision-making in real time.","audiomentation, fetch.ai, fetchai, gradio, librosa, openai, pydub, pyllama, python, tensorflow, torch, whisper, xtts","https://github.com/suryasubbu/DriveI, https://docs.google.com/spreadsheets/d/1A2Ko6T1g7o5jjHGz2XG4qZ0ppvWzFlKbDuT7AQOOmXg/edit?usp=sharing"
HackDearborn 2024,EyePi,https://devpost.com/software/lar,False,We're acreativeandintelligentgroup of individuals who have a strong desire to provide solutions through code.  Building EyePi allowed us to push ourselves beyond our known capabilities in order to provide a creative solution to urban and social sustainability.,"Combining powerful hardware with versatile software, EyePi provides a creative solution for individuals suffering from visual impairment disabilities, creating a more sure-footed and sustainable future. A VIRTUAL SOLUTION TO A REAL PROBLEM EyePi employs a Raspberry Pi 5 as the heart and brains our object-recognition system, capable of analyzing image frames every 3 milliseconds. EyePi will be available as a chest-mounted device, preventing restricted range of motion. The user turns on the device and through the speaker it lists 6 different language settings for the program. Once the user says their language of choice it turns the camera on and starts detecting objects in its surrounding. Based on the language chosen it will speak out the objects detected in that language.","Hardware Software We also created other different scripts for the language processing so that when the program starts it asks the user for the language setting. We used the speech recognition and Google translate python library to convert the audio into text, and text back to audio, as well as use google translate to do have our different language options.",The biggest challenge we ran into was remote booting and developing with the Raspberry Pi OS. We discovered the issue by reading the network IP Addresses and seeing that they were being scrambled due to the school WiFi's security protocols We overcame this by switching to hotspot and using micro-HDMI instead of remote SSH. Another challenge we faced was audibly sending and receiving the verbal messages between the Pi and the user. Our solution to this was communicating between the input and output devices (microphones and speakers) in order to prevent overlap.,"During the course of this project, we overcame a variety of hurdles. One of our proudest moments was fully transferring our codebase from our local machines to the Raspberry Pi OS, consistently encountering and resolving integration issues the whole way through.","We learned the importance of persistence. Without it, we wouldn't have made it through the first night. Persistence proved to be a common theme throughout the entire challenge. The countless hours researching new tech, testing existing code, troubleshooting new problems, and designing better algorithms strengthen us and gave us a challenge worth fighting for.","The modular nature of Raspberry Pi allows us to aim high and further develop the device to create a wearable product, capable of incredibly fast image display and classification. Additionally, an updated and enlarged list of classifiers would further fine-tune our precise algorithm. Increased processing power would further create a more realistic and professional experience for the user.","google-translate, linux, opencv, pygame, python, raspberry-pi, vscode",https://github.com/LucaIri/HackDearborn3/tree/main
HackDearborn 2024,AI assisted Telemedicine KIOSK,https://devpost.com/software/ai-assisted-telemedicine-kiosk,False,"Limited healthcare access to underprivileged  due to lack of financial support and medical infrastructure can be addressed through an AI-enabled robotic telemedicine kiosk. The kiosk provides user-friendly conversational interface to identify medical issues, enables remote consultations with doctors through telemedicine apps. This improves availability and accessibility of primary healthcare for underserved population of the country.",,"The kiosk provides user-friendly conversational interface to identify medical issues, enables remote consultations with doctors through telemedicine apps. We utilized Python and open source frameworks to generate a vector database of 5 different textbooks related to different sub branches of medicine. They cover fields like general medicine, ICU care, Critical treatment. The entire flow is deployed on a Flask application and is called using a website built using basic HTML, CSS and JS.",Currently in the stage where we are running into problems for integration of AI and HTML JS stuff.,"We are proud of the final model we achieved after 5 hours of finetuning. When asked with real doctors about the diagnosis by our AI-Doc, the feedback was positive which made us realize the potential of the project.","Apart from the technological knowledge gained in the fields of Generative AI and Web development, the skills necessary in daily life like perseverance to code till the finish of hackathon and the power we discovered in the word team.","We plan to train the model further more using many more textbooks and make the model more reliable and try to deploy it over a cloud server for faster response generation. Apart from that, we also plan to include priority queues for each doctor so that a patient in need can find the available doctor as soon as possible.","css, fetch.ai, flask, html, javascript, llm, pytorch, sql",https://github.com/vikram-p-velankar/HAI
HackDearborn 2024,BlurBeGone,https://devpost.com/software/blurbegone,False,"Foggy weather significantly affects driver experience and autonomous vehicle modelling, limiting visibility and safety while driving. Studies have even found that more than 16,000 people get injured in accidents caused by fog each year, while more than 600 pass away. This is due to the drastically reduced visibility caused by fog and adverse weather conditions (Julian Sanders J.D). That's why we wanted to create a low-cost solution that can be easily integrated into both non-advanced and modern vehicles alike to enhance safety and visibility on the road.","Blur Be Gone provides an integrated solution to reduce fogginess and increase visibility in real-time utilizing computer vision, image processing, and object recognition frameworks. Blur Be Safe prioritizes driver and civilian safety by serving as an embedded system that can be directly integrated into motor vehicles. Once fog is detected through the initial computer vision framework, the process then continues to the image processing and object-detection algorithms in real-time to provide visual enhancements that aid the driver's journey.","Our python-based project has three primary computer vision components that contribute to the main functionality of the system. First, we have our fog detection algorithm, followed by various image enhancement algorithms, and finally we have our object-detection algorithms. The fog detection algorithm utilized OpenCV to determine the presence of fog and then trigger the image enhancement processes based off of a foggy state. Once the system has entered a foggy state, the image processing algorithm begins and utilizes CLAHE, Dark Channel Prior, and additional sharpness/saturation filters to detect edges and increase image contrast in real time, providing visibility within adverse conditions. Following this, we enter into the final phase of our system, which is the object-detection framework that utilized YOLOv8 to detect and classify objects within the enhanced image frame in real-time. We tuned the confidence level of this pre-trained model in order to meet the specifications of our project. We ran the models using PyQT5 to create a GUI that accessed the webcam data live and applied all necessary filters for our desired visual outputs.","Originally, we wanted to create a hardware and software-based solution utilizing a Raspberry Pi4 and camera module to directly emulate our innovative system. This ambitious idea was put on hold due to several networking restrictions, such as firewall configurations that prevented SSH and Virtual Network Computing. After successfully overcoming the networking restrictions, we were able to flash our Pi4 with the relevant Python code and computer vision applications, however there were imaging latency and thermal issues due to the lack of processing capability on-board the Pi. We were unable to fully integrate this with our limited hardware and shifted directions to successfully implement our functioning software component. Fortunately, we were then able to implement the desired functionality of the original hardware assembly through the native webcam of one of our devices and a GUI made using PyQT5.","Three members of our team were new to hackathons as a whole, and two were electrical engineering students with limited software development exposure. Having a fully completed project that we were able to demo at the completion of the event was a significant milestone for us to meet considering our background. Additionally, we are proud of the various computer vision aspects that we were able to implement and layer into one integrated system in order to produce a high-visibility output with object recognition.","As previously mentioned, two of our members were electrical engineering students with little exposure to computer vision applications and frameworks. This project provided a well-rounded introduction to AI/ML applications within the automotive industry, and more-specifically allowed for thorough experimentation with computer vision and various libraries in Python.","As newer vehicle models advance technologically, older existing models have fallen behind. This solution will prove beneficial as an aftermarket addon for affordable assisted driving. Additionally, assisted driving systems employ multiple tools in addition to cameras, such as LiDAR and radar. An improvement in the capabilities of camera technologies uplifts modern systems as a whole. Utilizing LiDAR and radar will provide supplemental imaging support to assist in the image processing and capabilities of our current framework. Finally, cutting edge heads up displays provide an immersive way for the driver to receive information. The defogger can provide users with a live augmented view of their environment in real time and provide alerts in-vehicle that contribute to overall road safety and hazard management,","ai/ml, anaconda, clahe, computer-vision, dark-channel-prior, opencv, pyqt, python, raspberry-pi, yolov8",https://github.com/Abeal0/HackDearborn24
HackDearborn 2024,techRamsay,https://devpost.com/software/techramsay,False,"We were set on having a food-related project, but we crucially wanted it to be something novel. We got inspiration from one of team members who has a gluten allergy and thought we could create something to help folks with dietary restrictions indulge in meals they otherwise would avoid.",It's an application that generates alternate recipes for any dish the user can take a picture of.,"We utilized the GPT-4o API for the computer vision portion of the project, the rest of the application was coded in Dart with the help of Flutter. Much of the design was done using Figma and the logo was created using Adobe Photoshop.","Implementing the camera functionality, i.e., the ability to take pictures from within the application, was extremely challenging.","We're proud of the idea as it is novel and our UI is sleek, modern, and easy on the eyes.",We learned that no project is ever as simple as it initially seems and gremlins are bound to make themselves noticed.,"We are hoping to implement a ratings/reviews section where users can rate the recipes they've tried and post pictures of their results, the highest rated recipes would be recommended to other users.","dart, figma, flutter, gpt-4o, photoshop",https://github.com/fairuzguy/HackDearborn2024
HackDearborn 2024,PySurfers,https://devpost.com/software/temple-tilt,False,Kinect games on the Xbox 360 like Kinect Sports and Kinect Adventures,Allows you to play Subway Surfers through physical full-body movements.,Python opencv and augment reality. Utilized the webcam.,Adjusting the sensitivity of the movements,Making the game functional despite starting late and losing a teammate.,"Python skills
Teamwork","Running it on a a better system
Including more controls for boosters
Making it modular to support more games",python,
HackDearborn 2024,FinQuest,https://devpost.com/software/finquest-lj1i9d,False,"Despite the growing importance of financial literacy, most schools don’t adequately teach teens about money management, saving, or investing. Many young people are left unprepared to make informed financial decisions as they enter adulthood. This gap in education inspired us to create FinQuest—a fun, interactive platform designed to help teens gain essential financial knowledge. We saw an opportunity to fill this gap by gamifying the learning experience, making financial literacy engaging and accessible to a generation that thrives on interactive content.","FinQuest is an interactive quiz app that helps teens build critical financial literacy skills. Each topic includes educational resources—videos and articles—to introduce key concepts such as budgeting, saving, and investing. After exploring these resources, users take personalized quizzes to test their understanding. As users progress, the quiz difficulty increases, keeping teens engaged and continually learning. The quizzes offer multiple-choice questions with instant feedback, making financial education fun, dynamic, and challenging, while reinforcing the concepts learned from the resources.","We built FinQuest using Streamlit for the frontend and LangChain to dynamically generate quiz content. We integrated educational resources—videos and articles—into the app so teens could review the material before taking the quizzes. OpenAI powers the dynamic quiz generation, ensuring that each quiz adapts to the user's knowledge level and provides relevant, challenging questions. Our interface is teen-friendly, designed to be easy to navigate while offering a mix of learning resources and interactive testing to make financial literacy more engaging.","One of the biggest challenges was making financial concepts engaging and accessible for teens. Another challenge was designing visually appealing and interactive quiz cards within Streamlit, which is not traditionally used for card-style layouts. We also needed to ensure that the resources we provided before each quiz were informative and easy to understand while allowing teens to apply the knowledge during the quizzes. Technically, we had to overcome hurdles in dynamically generating accurate quiz content based on different difficulty levels, while still maintaining an engaging user experience. Lastly, we focused on balancing educational content with gamification to keep teens motivated.","We’re proud of creating a platform that addresses the lack of financial education in schools by providing both learning resources and quizzes. The seamless integration of videos and articles before each quiz allows teens to learn at their own pace, and the instant feedback from quizzes reinforces those concepts. We’re especially proud of overcoming the challenge of designing interactive and user-friendly quiz cards in Streamlit, ensuring a smooth and visually appealing experience for our users. Additionally, our successful integration of AI-driven content generation with LangChain ensures that the quizzes are personalized and adaptive to each user’s skill level. We’re also proud of the clean, intuitive design, making it easy and enjoyable for teens to learn about money.","In building FinQuest, we honed our skills in working with LangChain and OpenAI to dynamically generate accurate, contextually relevant quiz questions. Fine-tuning prompts to balance difficulty levels and ensuring real-time responsiveness were key challenges that deepened our understanding of AI content generation. We also learned to push the boundaries of Streamlit, particularly in creating interactive and visually appealing quiz cards. By leveraging custom HTML and CSS within Streamlit, we overcame layout limitations and designed an engaging user interface. Managing session state was another crucial area, allowing us to track user progress, scale quiz difficulty, and provide seamless feedback. Overall, we gained valuable insights into combining AI-driven content with interactive educational tools, optimizing both backend logic and frontend design to create a smooth, user-friendly experience.","Looking ahead, we plan to expand FinQuest with more advanced financial topics such as student loans, credit scores, and investing, all still targeted at teens preparing for adulthood. We will also introduce new gamification elements, such as rewards, badges, and leaderboards, to keep users engaged. Additionally, we hope to partner with schools and financial literacy organizations to bring FinQuest into classrooms. By combining educational resources and quizzes, we aim to fill the gap in financial education in today’s school system and empower teens to take control of their financial futures.","html/css, langchain, openai, python, streamlit",https://github.com/apatimar/hackdearborn-24
HackDearborn 2024,Major Match,https://devpost.com/software/major-match,False,,"Key Features:
    • Real-time cost breakdowns: See detailed costs of tuition, housing, and other expenses at universities worldwide.
    • Comparison tools: Compare schools based on financial affordability.
    • Budget planning: Create a budget to understand how much to save and what financial aid options you can explore.
Major Match is here to ensure students aren’t overwhelmed by the financial side of their dreams—because education is the investment of a lifetime.”",Built on XCode,Integrating AI to app,,,Adding,"figma, xcode",
HackDearborn 2024,AIdvising,https://devpost.com/software/aidvising,False,,"Replaces your advisor for help deciding majors, classes, and planning college life.",Gemini/Python/JS/Flask/HTML/CSS,Frontend development and inputting specific information like major course guides,That we were able to make a functional product,Frontend is overrated,Expanding to more schools and more majors,"css3, flask, gemini, git, github, html5, javascript, python",https://github.com/dave-grave/dearbornhacks-24
HackDearborn 2024,TalkTuahCar,https://devpost.com/software/talktuahcar,False,"We were bored in the car, so we thought it would be nice to talk to others on the road!","Think of it like the radio, you come into the proximity of a car, you connect to a voice channel, then talk! It automatically disconnects when you are too far away!",Our tech stack includes:,"Unfortunately, due to the constraints of the HTTP protocol, being used with sensitive data formats such as video and audio feeds, we were unable to show off the full capabilities given an HTTPS certificate with an off-site server.",We did not leave and actually completed our project (to the best of our ability) and did really well in CTF!,The Automotive industry is an interesting place to be frfr,Fully implement the web app and potentially put more funny/interesting things in!,"cors, css, express.js, javascript, node.js, socket.io",https://github.com/KNQuoc/talktuahcar.git
HackDearborn 2024,NaviGree,https://devpost.com/software/navigree,False,"Choosing the appropriate courses for college can be a daunting task. Students frequently struggle to successfully plan their four-year journey, from handling prerequisites to balancing workloads and keeping on pace to graduate on time. The need to expedite and simplify the course selection process gave rise toNaviGree, an intuitive application that lets students plot their academic trajectory.","With the aid ofNaviGree, students may create a customized four-year degree plan that takes into account their chosen major and university. The trained AI Agent gives a logical and well-structured course sequence that takes into consideration prerequisite relationships, electives, and core requirements by entering the name of the university and degree program. This guarantees that students can see their whole academic journey clearly and choose their classes wisely each semester.",,,,,,"dataflow, gemini, mui, node.js, react, vertex",https://github.com/Kaavya-Karnati/NaviGree
HackDearborn 2024,MementoAI,https://devpost.com/software/mementoai,False,This assistant has been initiated to function for patients with Alzheimer’s disease or Parkinson’s disease during stage 1 and stage 2 of their treatment. People with these diseases eventually develop a habit of forgetting their daily chores. We wanted to build an assistant that can help people during these times.,"There are quite a few features that we have planned to incorporate into this model. The assistant serves to remind people of who their family members are through storing their images. The patient can ask the AI regarding who a particular person is and the AI will display the picture of the person using the images in its database. The assistant will be able to record user's conversations and store the summaries so that they might be useful at some point in the future. The AI serves as a reminder and also will be able to manage the patient's daily schedule through medications, daily chores, exercise and planning hospital visits. They will be taken as an input and will be notified to the patient through vocal notifications when it is time. Hospital visits are planned depending on the level of information the patient can remember.",Python was the main language that we have used for this project. We've planned to make the assistant by integrating the different features of the product in individual environments.,"There were quite a few errors that we were facing when we were trying to classify the input text to the respective image of the person. It was mainly about the deprecation of the keras and tensorflow libraries. We've also faced a few issues while making reminders and vocalizing notifications. To be specific, we faced issues with Python's text to speech library ""pyttsx3"" as the engine to vocalize the notifications was not restarting on its own. We fixed it by adding engine.stop() after each reminder was done and we reinitialized it for the upcoming reminders. We were also stuck with the integration of Fetch.AI's agent into our text-to-speech recognition code.","We're proud of how well we were able to combine many features, such as the ability to store and show pictures of family members, set and speak reminders, and summarize talks. Even though it was hard, especially with the text-to-speech engine and making TensorFlow and Keras work together, we were really close to making a prototype that works and can help people with Alzheimer's or Parkinson's disease.","We felt really happy that we've learnt quite a lot during this hackathon. We learnt a lot about Large Language models and the way they actually summarize the entire voice calls that two persons have. We also came to know about building an interface that takes reminders as input and sends voice notifications to remind us regarding the same. We've got to experience a glimpse into Fetch.AI's contribution towards developing AI agents covering many fields. In addition to learning technical skills, we also learned how to debug problems that come up when we use deprecated libraries like TensorFlow and Keras. We learned how to make tools that are easy for people to use and how important it is to make systems that meet the needs of people who have cognitive impairments. Another important thing we learned was how to use ""pyttsx3"" to handle voice notifications effectively. We also learned more about big language models and how they can summarize conversations. This helped us make our assistant more useful.","We will be making a complete prototype soon enough. Then we are planning to potentially make it a hardware system that can help us analyze the daily chores of people through a camera so that we get to track their attention span when they're working on a chore. If they tend to deviate, the AI assistant can remind them what they were doing.","flask, html, javascript, python, visual-studio",https://github.com/HarshithK13/MementoAI
HackDearborn 2024,EcoTrip,https://devpost.com/software/ecotrip-r0daph,False,"We were inspired by the environmental track and the increasing need to address carbon footprints as modern transportation and technologies expand. Many of us use cars, planes, and other methods of travel daily without fully realizing the impact they have on the environment. Our goal was to create an app that helps users understand and minimize their carbon footprint, promoting more eco-friendly travel choices.","EcoTrip allows users to input their vehicle information and see the distance traveled on a map. Based on this information, the app calculates the carbon footprint of the trip, helping users become more conscious of the environmental impact of their travel.","We used Vue.js to build the frontend and integrated APIs to calculate the carbon footprint based on vehicle type and travel distance. The map feature allows users to visualize their travel routes and distances. Our team worked together to ensure that the app’s design is modern, eco-friendly, and user-friendly.","We encountered challenges in finding the right API to calculate carbon emissions accurately based on different vehicles. Designing a sleek interface that fits with our green theme while being simple and functional also took time and effort. Additionally, learning Vue.js for the first time presented a learning curve, but we overcame it by working closely as a team.","We are proud of successfully learning and implementing Vue.js in a short amount of time. We also managed to integrate APIs and map functionality, providing users with a clear visualization of their travel routes and associated carbon emissions. Despite the time constraints, we delivered a working prototype with clean design and functionality.","We learned how to effectively use Vue.js and integrate external APIs to handle carbon footprint calculations. We also improved our teamwork, communication, and time management skills during the project. The experience helped us better understand how to structure and execute a project from idea generation to completion.","In the future, we aim to expand EcoTrip by adding more transportation options and carbon footprint calculators for various travel modes. We also plan to include features that suggest eco-friendlier alternatives and potentially integrate carbon offset programs to help users take actionable steps towards sustainability.","css, javascript, rapidapi, react, vue",https://ecotrip-ruby.vercel.app/
HackDearborn 2024,EyeSign,https://devpost.com/software/eyesign-v9hg5j,False,"We wanted to allow all users to be able to easily use video conferencing tools no matter what disabilities they may have. Whether they're hard of hearing, hard of seeing, or anywhere on the spectrum, our goal is to use ASL and emotion recognition to ease the burden of communication barriers amongst users.","Using a Google Extension, a bot account sets up a Zoom with the user's consent to feed in audio and visual data. Users can sign and the bot account will interpret it and translate it into captions and phrases. There will also be capabilities to use a text-to-voice generator to accommodate for blind users.","Uses OpenCV in a Google Extension to allow users the ability to sign words and phrases in a Google Meet conference call. There will be abilities to use AI text-to-voice generation to sound out generated captions based on signing.  Additionally, the dummy account which sets up the meeting will gather ""input"" from participants in the form of audio and video, and, using a pre-trained model incorporated with the ___ repository trained with TensorFlow and Keras, increases the accuracy of predicting what the user is signing.","This was our first time working with Zoom SDK and OAuth frameworks. As a result, establishing the actual Zoom connection itself using access tokens was a struggle, however it further expanded our understanding of how video conferencing software interacts with data servers and integrating JavaScript. Additionally, we struggled with finding ASL translation models that were able to be easily trained and recognize what the user is signing. We ended up using a cloned dataset and having to have the user sign out words and phrases letter by letter.","We were able to successfully incorporate OpenCV frameworks to integrate the software with ASL and facial recognition. We developed an understanding of the Zoom SDK and how to integrate OAuth frameworks to generate and authorize users into Zoom meetings. Finally, we were able to incorporate TensorFlow and Keras to work with a cloned repository to yield a pre-trained model that assists in predicting what the user is signing.","This project taught us how to integrate various APIs and understand how to work with authentication frameworks to establish video conferencing meetings. We also developed a much better sense of source control with Git, as this was a skill that we were not as skilled in before working on this project.","As we currently are only having users sign words and phrases letter by letter, we want to improve this model in the future by using frameworks and libraries that allow us to sign entire words and phrases, not having to sign things letter by letter. We also want to expand the dataset we have and be able to train models on a larger dataset in order to yield more accurate results. Finally, we want to look more into using software such as Recall.ai to improve upon the readings from the Zoom user. We had to do workarounds because current methods require us to create our own domain and extract the raw video and audio data in I420 raw frames and PCM 16LE raw format respectively, requiring us to encode our own data to train in the model which, given our time constraints, wasn't entirely feasible.","deepface, html, javascript, json, keras, oauth, opencv, python, speechapi, tensorflow, wsasl, zoom",https://github.com/8Haneen/EyeSign
HackDearborn 2024,Amazon Co2 Product Calculator Extension,https://devpost.com/software/amazon-co2-product-calculator-extension,False,"Politicians, scientists, and the older generation constantly talk about being aware of the impact we have on the earth and minimizing our carbon footprints, but so little do they explain how without completely overwhelming us with suggestions to completely change our lifestyles. Suggestions like biking to work at 6 a.m. may seem like a big change. We hope to encourage self-education about one's carbon impact on the world by attacking an activity that people probably do more than drive to work... shopping, specifically online shopping. Amazon is the world's largest online retailer, with a worldwide revenue of $138.9 billion, and Amazon is the biggest online retailer in the United States. Over 50% of online purchases are made impulsively. The average American makes three impulse purchases each week, totaling about 156 per year. Our idea is to make a Google extension that can easily be downloaded through the Chrome extension store. As the user navigates to different products the pop-up will tell the user how much carbon is emitted into the atmosphere when shipping that product. By actively showing the user this they can compare multiple products on Amazon and actively try to minimize their harmful impact on our ozone layer. We hope to teach the average American not only just how much their everyday impulse purchases harm the environment, but also we wish for the user to feel good about their purchases. As the user gets used to searching for the most efficient shops on Amazon and learns to buy more locally sourced goods, we hope the user will feel like they are actively helping the state of the world while also hopefully not getting too overwhelmed with the logistics of how much they have to change their lifestyles to achieve these sustainability goals.",Our Amazon CO2 Product Calculator is a Chrome extension the user can easily download. The extension will pop up in the corner every time the user navigates to a different product on Amazon and tells the user how much CO2 emissions are released with the shipment of the product. This in turn educates the user not only about which goods are more efficient to ship but also teaches the user about their impact on the environment.,"Our Chrome extension was built using many different components. We used many different APIs, we used a scraper, and we used a flask that ties all the components together to properly get the Chrome extension to work. Our scraper was coded first via Python notebook and scraped all the essential data we would need to compute CO2 emission, such as the address of where the product was shipping from. We then found an API that would calculate the CO2 emission based on the data we scraped. We modified the API to fit the parameters of the data we collected. From there we had a solid way of collecting data and calculating the information we wanted to output to the user. Now we just needed to figure out a way to put this on a pop-up via chrome extension. Chrome uses javascript so we needed a flask to properly feed the Chrome extension our data that was in Python. Our final project is a pop-up that will calculate the carbon emission of a product with just a click of a button. Note some products on Amazon don't directly state where they are being shipped from as it just says ""shipping from an Amazon warehouse."" To work around this we have calculated the distance between the user and every warehouse in their state, and we take the average of those distances as we don't know which warehouse location will fulfill the request as Amazon does not disclose this information. We believe this is a sufficient workaround as Amazon will satisfy the product request from the closest warehouse and we don't know which one is so taking the average of all the nearest warehouses (a.k.a. warehouses in the same state), we should be able to a least get close to where the product is shipping from.","As our group is comprised of entirely new hackathoners, it was difficult to connect all of the parts each person coded individually just as we coded in different languages and used different kinds of parameters and outputs. There were a lot of conceptual problems we ran into such as lack of data access or calculations that were too hard to compute, so we needed to be quick on our feet to brainstorm new ideas and workarounds, but in the end, we made do with what we could and are proud of our final result.","Again as we are relatively new to the hackathon scene it was a bit overwhelming at times. We had to teach ourselves many new concepts and work with tools like APIs, Flask, and Chrome extensions which we had never done before. We are really proud of the perseverance we had as we ran through many roadblocks. We really liked the idea we came up with and really wanted to do it justice and have a final product we could show off. We learned so much in just the span of 24 hours and made some lifelong bonds not only with friends and competitors but also with our code. This project is my baby now, I raised it from the ground up ain't nobody bashing my child, I'm proud of it, even if it's a little rough around the edges. It took hard work for us to even get to a final product.","We learned a lot about APIs and Scrapers, but also about Google Chrome's extension creator. It was honestly so different from what we've previously dealt with in the Computer Science field, and it felt very dignifying teaching ourselves how to use these new tools and actively use them to build an idea we thought up. Seeing our idea come to life is in itself such a cool experience. As we created something from nothing.","As of now we only really have it so we tell the user how much carbon it emits if the product is shipped. In the future, we can implement a comparison feature that shows the user the most fuel-efficient product, as well as calculate how much CO2 was emitted into the atmosphere at the creation of the actual product. There are so many ways to take this project and it can become all the more informative to the user.","css, flask, html, javascript, python",https://github.com/maryhuss2023/hack_dearborn_3
HackDearborn 2024,Code Kids,https://devpost.com/software/app-986taq,False,"Coding is hard, and we seek to make it easier to learn for beginners and lower the barriers to entry in terms of learning it. We want to create an engaging and interactive platform that introduces the fundamentals of coding to kids in a fun and easy-to-understand manner.","Code Kids is an educational app designed to teach kids the basics of coding. It features: Interactive Lessons: Easy-to-understand lessons that cover fundamental programming concepts.
Challenging Quizzes: Quizzes to test knowledge and reinforce learning.
Kid-Friendly Interface: An intuitive interface designed specifically for children.","We built the Code Kids app using the following technologies: Frontend: React.js for creating a dynamic and responsive user interface.
HTML/CSS: For structuring and styling the landing page and app components.","Some of the challenges we encountered include: Designing an interface that is both engaging and easy for kids to navigate.
Simplifying complex coding concepts to be understandable at a basic level.
Ensuring that the quizzes and lessons are interactive and educational.","Successfully creating a user-friendly platform that makes learning to code fun for kids.
Developing interactive components that keep children engaged and make learning enjoyable.
Receiving positive feedback from our initial user tests indicating that kids find the app helpful and easy to use.","Through this project, we learned: The importance of designing educational content that is age-appropriate and engaging.
Techniques for simplifying complex concepts into digestible lessons.
How to create interactive quizzes that reinforce learning in an effective manner.","Future plans for Code Kids include: Adding more lessons covering advanced topics as kids progress.
Implementing more interactive features such as coding games and challenges.
Expanding the app to include a broader range of subjects within STEM (Science, Technology, Engineering, and Math).","css, flutter, html, javascript, python, react, swift",https://github.com/mzhu1/dbo24
HackDearborn 2024,Byte by Byte,https://devpost.com/software/byte-by-btye,False,"Our inspiration was being frustrated with having to have so many different apps when they all could be so easily combined, as well as noticing that a lot of them are not private and are unnecessarily uploaded to the cloud.","Keeps a local database on our app that allows you to set a fitness schedule that you can just check off, Track your mood for the day and add notes, Mark off what types of things you have eaten, And track your PMS Cycle all in one.","We used react native with pouch-DB inspired database made by hand. Using Android Studio, Expo and VS-code.",We have run into a lot of issues of finding a database that is nice and lightweight and works nicely with what we want to do. We initially planned on using SQLite but have since moved on. We also ran into a lot of issue of having pages display the correct info from the custom DB,How good it looks! It really is a simple and intuitive app that makes tracking all of this stuff more intentional and less time consuming. It also is the first time either of us have properly packaged an app so it's really cool to see!,That dependencies can be very challenging as well as how many different versions and things you installed along the way. We also learned a lot about how Json based Database's work and a lot of their drawbacks.,"Fixing some of the database communications, Cleaning up the app some more, letting you download your data, and implementing a routine mode to help you get through the morning","database, javascript, json, react-native","https://github.com/ElijahTyler/byte-by-byte, https://drive.google.com/file/d/1hD3skrPiHX4pX1W8k3J2nMkli6TLwrq9/view?usp=sharing"
HackDearborn 2024,Road Report,https://devpost.com/software/road-report,False,"Michigan is known for its harsh winters, which often lead to severe pothole problems on roads across the state. As residents ourselves, we’ve experienced the frustration and damage caused by these hazards. We wanted to create an easy-to-use platform where anyone can report potholes, ensuring they are quickly brought to the attention of local authorities. Our goal is to improve road safety and the driving experience by empowering citizens to contribute to the maintenance of their community’s infrastructure","Using RoadReport, the user is able to fill in the exact location of a pothole onto an interactive map, grade the severity on a scale from 1 to 5, and the rating system prioritizes repairs based on urgency and hazard of operation. The app dictates the transcripts of the reports that will then be forwarded directly to the city officials for a fast and effective procedure in repairing potholes. They can also see the real-time status of their reports and other pothole reports that may be in proximity. It is simple and intuitive, aimed at making it easy to let citizens be part of improving safety and smoothness on roads in the community.","We built RoadReport using Next.js for the front-end to create a fast and user-friendly interface, with integrated Google Maps API for location tracking and map functionality. For the back-end, we used Flask to handle report submissions and status updates, providing a lightweight and efficient solution. Our database is powered by SQL, allowing for structured and reliable storage of user reports and pothole data. We also implemented an auto-transcription feature that generates formal requests for the city based on user input, ensuring a streamlined communication process with local authorities.","One of the main challenges we faced was getting accurate, live user location data for reporting potholes. The issue stemmed from the API refusing to send back the necessary location data consistently, which required extensive debugging and alternative solutions. This problem impacted the app’s ability to automatically detect user locations in real time. Additionally, we encountered several syntax issues during development, which required careful troubleshooting and slowed progress at certain points. Despite these challenges, we successfully worked through them and delivered a functional, user-friendly app.","As beginners, we’re especially proud of our ability to learn and effectively utilize APIs, which was a key part of making RoadReport functional. We successfully integrated technologies like React, Next.js, and Flask into our project, despite the steep learning curve. Overcoming challenges like API issues and debugging complex syntax has been a significant accomplishment, and we’re excited about how much we’ve grown as developers throughout this project. Seeing the app come together from concept to working product is something we take great pride in.","As beginners, we’re especially proud of our ability to learn and effectively utilize APIs, which was crucial to making RoadReport functional. We successfully integrated technologies like React, Next.js, and Flask into our project, even though the learning curve was steep. Throughout the process, we learned how to tackle API issues, resolve complex syntax errors, and optimize the interaction between frontend and backend components. This project has significantly enhanced our understanding of full-stack development, and we’re excited about how much we’ve grown as developers, turning a concept into a working product.","We would like to scale Road Report to the cities, or possibly integrate with existing map applications like Google or Apple maps.","flask, google-maps, javascript, next.js, python","https://github.com/rywinp/pothole, https://umich-my.sharepoint.com/:p:/r/personal/irwinp_umich_edu/Documents/Presentation1.pptx?d=wce02626c017441e5ad791bd35f431a90&csf=1&web=1&e=iRyc28"
HackDearborn 2024,SmartTrader V4,https://devpost.com/software/smarttrader-v4,False,,"Our application is a real-time AI crypto currency trader driven by sentiment and technical analysis. Each day we track both market sentiment, technical indicators and use the overall trend to predict buy or sell indicators.","We use two different clever approaches to collect data. We have access to a large dataset from X, which contains information about Bitcoin public sentiment. We use this large dataset to populate 6 months of public sentiment for our SmartTrader to backtest. To collect real-time data, we take advantage of Reddit’s API, to take advantage of detailed, deep conversations to take swing trading positions (over days to weeks) .
By taking advantage of VADER, the lightweight lexicon based nature allows us to efficiency and accurately categorize data between -1 and 1, with -1 representing a negative sentiment, and 1 representing a positive sentiment.
By implementing Alpaca’s powerful API, we can get access to real-time market data for Bitcoin to properly assess moving averages and Relative Strength Index.  We cross check these two model’s to see if they point to a buying signal, then cross check with our sentiment analysis model to then come out to a final verdict.
Front-End: React, Daisy UI, Tailwind, Chart.js
Back-End: Python, Fast API, PRAW, VADER, Alpaca Trade API, PostgreSQL, Pandas
By combining a lot of these lightweight powerful tools, our application is able to efficiently judge, and enter positions.","We had difficulties with financial side of application, specifically understanding financial approach in regards to number of queries we had access to, targeted slightly volatile markets as a solution.","We are proud of our product being finished, and the fact that we battled through a completely new topic.","We learned more about how quantitative firms operate, algorithmic programming, low-latency programming, machine learning, and full-stack development with a python back-end.","Though our product only supports Bitcoin, we plan on applying this to most CEX cryptocurrencies, and plan on targeting smaller caps to test our product in more volatile markets.","fastapi, postgresql, python, react, tailwindcss, typescript, vader",https://github.com/avnoor-ludhar/BitcoinTrader
HackDearborn 2024,Hand Gesture Car Racing,https://devpost.com/software/hand-gesture-racing-game,False,,"Our game is a racing game that requires no additional driving equipment or keyboard input. Using just your hands and computer webcam, we created a fully functional driving vehicle that can race across a course. For the main driving mechanism, we split the driving into two parts: driving and steering. In order to control the driving speed, the user must move their hands closer (faster) and farther (slower). To make sure the driving is always comfortable, the user must drive in a ""thumbs up"" position, and can zero the velocity at any time by putting their thumbs down. We created a race track to accompany the car, with a fully functional lap timer and checkpoint system, where the entire lap won't be counted until the car reaches all of the checkpoints and the finish line. Additionally, if the car leaves the track, the speed will drastically slow down. Finally, we added a settings menu to fine tune any multiplier to the user's preference, including velocity and turning multipliers. We also created a stats menu in order for the user to visualize how their car is moving.","Our two tasks were essentially computer vision, and the game itself. For computer vision, we decided to use Google's open source mediapipe ""Gesture Recognizer"" model, ensuring that we'd have the opportunity to extract gesture data. With some somewhat straightforward math, we're able to calculate the centers of the hands and determine the angle that the hands make relative to the origin. Additionally, we used the proportion that the distance is directly proportional to 1 / Area ^ 2, allowing us to directly relate the velocity to the (relative) distance that the hands were from the screen For the game, we decided to go with PixiJS, an HTML rendering engine for 2D games. We decided to create a top down racing game due to simplicity, and made the background move relative to the car, rather than the car move around a background. This allowed for bigger maps and a more natural feel to the driving. We used node.js and express.js to host the website via Heroku.","Our biggest challenge was definitely getting the driving mechanism to completely run with the movement feeling natural.  The car would often fly out of the screen randomly, or the car would turn opposite to the direction we wanted to, and so on and so forth. Additionally, setting up the Gesture Recognizer model and allowing it to connect with multiple hands provided a little more of a challenge than we expected. However, we were able to eventually implement the model in the end.","I'm extremely proud of the fact that the driving in the game, once you get the hang of it, is surprisingly natural and fluid. It's impressive that we were able to capture such a solid driving experience using only a webcam and hands. Additionally, it was difficult to complete all the game logic, and completing it was a task that we are proud of. We were also able to clean up the code and make it neat and readable, which proved to be a huge timesaver during troubleshooting.","We learned how to integrate computer vision into a webapp, and learning how to seemlessly integrate two different applicatoins into one big app. Additionally, we learned to be diligent with Git, and make sure we stay commiting to keep a solid commit history, in the case we want to look back in previous iterations of our code.","In the future, it's possible to add a leaderboard and login system, providing an ability to log and see ""top times"" of a track. Additionally, adding a multi-track selector would be another goal, allowing the user to pick and choose their favorite maps. Maybe we could even implement a track creator with this system. We also can add more complex driving mechanics, including drifting and a more sophisticated acceleration/braking system. For our settings, we could implement a cache system to remember the previous settings.","css, html, javascript, mediapipe, pixijs","https://computer-vision-car-game-64b0183ceff3.herokuapp.com/, https://github.com/albertchenn/HD2024"
HackDearborn 2024,PetPulse,https://devpost.com/software/petpulse,False,"Many sources formed the inspiration for this project. As someone who has had a pet before, and frequently spoke to it. I decided to make something based on this idea. The main interface was meant to resemble something similar to the talking tom games. Another thing that made me decide to go with this was the fact that Mental Health Day was a few days ago and I feel its a pressing issue.","The Cat Therapist is a conversational chatbot that utilizes speech recognition and text-to-speech capabilities to provide users with therapeutic support in a fun and engaging way. Users can interact with a virtual cat therapist to express their feelings and receive thoughtful responses, mimicking a real therapy session. The bot helps users navigate their emotions through casual conversation, making therapy more accessible and less intimidating.",Open ai api for the actual output. TTS for audio and React for front end.,"Ran into problems trying to create a 3d model, was unfortunately unable to resolve. Also problems trying to use a face recognition ai in my project (for mood detection).",Able to create a working product.,3d modelling sucks,"I hope to be able to get a proper 3d model in like a talking tom type. Also need to get better voices because the tts sounds too robotic. Hopefully, I can also add in the facial recognition.","node.js, openai, react, tts",https://github.com/Praise-creator/PetPulse
HackDearborn 2024,JudgedByAI,https://devpost.com/software/judgedbyai,False,"In a world where artificial intelligence is increasingly influential, we wanted to create an engaging platform that merges AI with interactive learning. We were inspired by the idea of simulating courtroom debates to help people improve their critical thinking, persuasive communication, and legal reasoning skills. By incorporating AI as an impartial judge, we aimed to make the experience both educational and entertaining.","JudgedByAI is an interactive debate game where two players assume the roles of plaintiff and defendant in a legal case. Each player receives exclusive case files with unique information to build their arguments. They engage in a back-and-forth debate, presenting and defending their positions. At the end of the debate, an AI-powered judge, utilizing advanced language models, evaluates the arguments and delivers an impartial verdict, declaring the winner based on the strength and coherence of their case.","We developed JudgedByAI using a combination of modern web technologies and AI services. The frontend is built with React.js, providing a responsive and intuitive user interface. The backend is powered by Flask, handling game logic and server-side operations. We integrated OpenAI's GPT-4 API to serve as the AI judge, leveraging its natural language processing capabilities to analyze arguments and render verdicts. We also implemented secure authentication and real-time communication features to enhance the user experience.","One of the primary challenges was ensuring that the AI judge could fairly and accurately evaluate the players' arguments. We had to fine-tune the prompts and manage the AI's responses to maintain impartiality and relevance. Another challenge was synchronizing the game state between players, especially managing turns and real-time updates. Implementing a system that would display all relevant information to the case proved more difficult than we thought.","We're proud to have created an engaging platform that effectively combines AI with interactive learning. Successfully integrating the AI judge to provide meaningful and fair verdicts was a significant achievement. We're also proud of the smooth user interface and the overall user experience, which allows players to immerse themselves in the role of courtroom litigators. Additionally, working with new technologies under a time crunch and delivering a working product with them was a notable achievement and we couldn't be happier.","Throughout the development of JudgedByAI, we deepened our understanding of AI integration within interactive applications. We learned how to effectively make use of API documentation. We also gained valuable insights into managing state synchronization in multiplayer environments and the importance of robust error handling.","Looking ahead, we plan to expand JudgedByAI by introducing more diverse and complex cases to challenge players further. We'd like to implement a point system to foster some competition between players. Additionally, we're considering adding support for more languages to make the platform accessible to a global audience. We also plan to enhance the AI judge by incorporating machine learning techniques to adapt and improve its evaluations over time. Ultimately, we like to envision JudgedByAI becoming a go-to platform for learning and practicing legal reasoning and debate skills.","firebase, flask, gpt-api, python, react",https://github.com/Kasataya/HACKDEARBORNPROJECT
HackDearborn 2024,Mnemosyne,https://devpost.com/software/mnemosyne-wmrq70,False,"There are a lot of people who struggle with dementia & mental illnesses (such as schizophrenia, bipolar disorder, depression, & BPD) who can benefit from more interactive applications.","This is an AI app that provides interactive conversations, games, & stimuli for patients/people who suffer from neurological & psychological issues. It's designed to boost neurological health & function by providing users with the ability to partake in engaging tasks & conversations. Categories users can choose from include:
-Just talk to me (casual conversation/anything the user wants to talk about)
-Make me laugh (includes jokes/lighthearted conversations)
-I want to play a game (includes riddles, talking RPGs, math games, etc)
-I need health advice (includes nutritional, lifestyle, & natural ways to boost health especially neurological & psychological health)
-I want stimuli (visual stimuli like art/images + audio stimuli like music + videos, sorted by different categories/needs)",**Will add this section soon,**Will add this section soon,**Will add this section soon,**Will add this section soon,**Will add this section soon,"firebase, flutter",
HackDearborn 2024,Reef Guard,https://devpost.com/software/reef-guard,False,"With carbon emission increasing we wanted to create awareness for our aquatic life, specifically the coral reefs as they provide an important ecosystem for life underwater, protect coastal areas by reducing the power of waves hitting the coast, and provide a source of income for people living near coastal areas.","It recognizes coral reefs, and analyzes their health based on visual inputs. The project mainly focuses on the color and visual ques on a given coral reef to determine the health of it. The whiter they get the closer they are to dying away.","We utilized YOLOv5 model and trained the model on an per-existing dataset. 
We also created a small website to show other various information related to coral reefs.","Main issue was deciding on the final goal of this project, we didn't want the project to be over ambitious and could our perception of what this project should perform like.",The future awareness we will create through this project is what we will be proud of.,We had to explore further into the life conditions of coral reefs to properly implement what the website information should contain and also what the model should learn.,There will be further diving needed to make this a more rounded project.,"css, html, python",https://github.com/crown-16/dearborn-hackathon.git
HackDearborn 2024,Ojibwe Dictionary,https://devpost.com/software/anishinaabemo,False,"There are more than 6,000 languages in the world, but anywhere between 50-90% of them could go extinct within our lifetime. The 20 most common languages are spoken by the majority of the world's population, but most of the world's languages are spoken by only a few thousand people. It is vital that we preserve these languages; when a language dies, the heart of the cultural history it carries dies with it. This project was driven by the urgent need for language preservation and revitalization. Hack Dearborn takes place in Michigan, on the ancestral homeland of the Anishinaabe people. Of the three Anishinaabeg peoples who predominantly reside in Michigan (Ojibwe, Ottawa, Potawatomi), the Ojibwe population is the largest. The Ojibwe dialect of Anishinaabemowin (sometimes referred to as Ojibwemowin) was used to create this dictionary.","This is a simple web app implementation of an Ojibwe-English dictionary. Users are able to search in both Ojibwe and English, and the dictionary will return entries that match the query. An entry contains the word in Ojibwe, the English translation, example sentences (if available), and inflectional/morphological data* (if available). *Anishinaabemowin is a morphologically rich language, a characteristic shared by many indigenous North American languages. The morphological complexity of these languages is often significant when compared to high-resource languages such as English. The structure of a language is an important factor that affects the ease with which the languages can be digitized; models trained on languages like English often simply cannot handle morphologically rich languages like Anishinaabemowin without extensive alteration.","All of the dictionary data used in this project was collected from the Ojibwe People's Dictionary, published by the University of Minnesota. This data is available underCreative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Dictionary data was processed using Python in Jupyter Notebook. The dictionary web implementation was created using React, JavaScript, and CSS in Visual Studio. ChatGPT assisted in troubleshooting coding errors.","My initial goal for this project was actually to create an Ojibwe-English machine translator. I attempted to utilize several models (mBART, MarianMT, T5), with varying degrees of success. After almost 9 hours of trial and error, I ultimately decided building a fully functioning translation model was outside of the scope of what I'd be able to accomplish within a 24-hour hackathon. I instead decided to use the extensive dictionary data I had collected to make a dictionary app.",,"This was actually my first time using React, so I basically had to learn it on the fly. I learned an incredible amount about React, UI/UX, and JavaScript that I might not have learned before this project. I also learned about recognizing my limits and being able to acknowledge when I had bitten off more than I could chew. Overall, it was an exceptionally informative experience.","I absolutely plan on continuing this project after the conclusion of the hackathon. With more time, I'll be able to research more in-depth about utilizing pre-trained language models to process low-resource languages. My main area of focus will likely be transfer learning. With a quality trained model, I'll be able to implement a machine translation feature (ideally both word-to-word and full sentences).","css, javascript, node.js, python, react",https://github.com/cinosenc/hackdearborn3/tree/main
HackDearborn 2024,Snap&Serve,https://devpost.com/software/foodscanner,False,,,,,,"We gained experience in both React and React Native, learning about their limitations. We also realized the importance of researching APIs and technologies before starting a project.",,"flask, gemini, godaddy, mongodbatlas, openfoodfactsapi, react",https://github.com/Yug3737/HackDearborn2024
HackHarvard 2024,Sustain-ify,https://devpost.com/software/sustain-ify,True,"Our inspiration for Sustain-ify came from observing the current state of our world. Despite incredible advancements in technology, science, and industry, we've created a world that's becoming increasingly unsustainable. This has a domino effect, not just on the environment, but on our own health and well-being as well. With rising environmental issues and declining mental and physical health, we asked ourselves:How can we be part of the solution? We believe that the key to solving these problems lies within us—humans. If we have the power to push the world to its current state, we also have the potential to change it for the better. This belief, coupled with the idea thatsmall, meaningful steps taken together can lead to a big impact, became the core principle of Sustain-ify.",Sustain-ify is an app designed to empower people to make sustainable choices for the Earth and for themselves. It provides users with the tools to make sustainable choices in everyday life. The app focuses on dual sustainability—a future where both the Earth and its people thrive. Key features include:,"Sustain-ify was built with a range of technologies and frameworks to deliver a smooth, scalable, and user-friendly experience. Technical Architecture: Frontend Technologies: Backend & Web Services: Data Processing & AI Models: Additional Services: Authentication & Security:","Throughout the development process, we faced several challenges:",We're proud of creating an app that:,"Through this project, we learned that:","Moving forward, we aim to: Together, we believe we can create a sustainable future where both humans and the planet can thrive. That's the ongoing mission of Sustain-ify, and we're excited to continue bringing this vision to life!","agentic-systems, crewai, css, dart, diy, fastapi, firebase, flutter, gemini-1.5-pro, gpt-4o, groq-lpus, html, hugging-face, javascript, langchain, llama-index, mongodb, python, scrapegraph, serper, streamlit, tavily",https://github.com/SrikarVamsi/Sustain-ify
HackHarvard 2024,Garuda,https://devpost.com/software/garuda-pb7qtf,True,"Natural disasters do more than just destroy property—they disrupt lives, tear apart communities, and hinder our progress toward a sustainable future. One of our team members from Rice University experienced this firsthand during a recent hurricane in Houston. Trees were uprooted, infrastructure was destroyed, and delayed response times put countless lives at risk. Inspired by these challenges, we aim to leverage AI, computer vision, and peer networks to provide rapid, actionable damage assessments. Our AI assistant can detect people in distress and deliver crucial information swiftly, bridging the gap between disaster and recovery.",,,,,,,"firebase, flask, google-maps, llava, llm, next.js, opencv, python, tensorflow, vlm",https://github.com/TheSacredJump/garuda
HackHarvard 2024,Diabeteasy,https://devpost.com/software/diabeteasy,True,,,"We build an insole with 3 sensors on it (the sensors are a hackathon project on their own), that checks the plantar pressure exerted by the patient. We stream and process the data and feed it to another model sole that changes shape based on the gait analysis so it helps correct the patients walk in realtime. Concurrently we stream the data out to our dashboard to show recent activity, alerts and live data about a patient's behavior so that doctors can monitor them remotely- and step in if any early signs of neural-degradation .",,,"Throughout this project, we learned that flexibility and resourcefulness are key when working with hardware, especially under tight time constraints, as we had to get creative with available materials. As well as this - we learnt a lot about preventative measures that can be taken to reduce the symptoms of diabetes and we have optimistic prospects about how we will can continue to help people with diabetes.","Everyone in our team has close family affected by diabetes, meaning this is a problem very near and dear to all of us. We strive to continue developing and delivering a prototype to those around us who we can see, first hand, the impact and make improvements to refine the design and execution. We aim to build relations with remote patient monitoring firms to assist within elderly healthcare, since we can provide one value above all; health.","arduino, bubbletea, bursukh, c++, cardboard, coffee, elliot, esp32, graphite, hardware, next.js, node.js, react, redbull, servo, tailwind, tape, three.js, typescript, websockets",
HackHarvard 2024,Flexy (And I Know It),https://devpost.com/software/flexy-and-i-know-it,True,"With 528 million people worldwide affected by osteoarthritis and that number expected to surge by 2050, maintaining joint and muscle health is essential. From arthritis and sports injuries to daily joint pain, millions struggle with conditions that limit movement. While there may be no cure, research has shown that targeted stretching and mobility exercises can significantly improve quality of life, reduce pain, and support recovery. Our app is designed to make rehabilitation more engaging and accessible, helping individuals recover faster and live pain-free. By offering a gamified experience with accurate posture detection, we ensure users perform exercises correctly, promoting better outcomes and alleviating physical challenges in a fun, user-friendly way.",,"Our project begins by capturing real-time video feeds using mobile devices. We utilize MediaPipe and OpenCV to detect key body joints and landmarks within the video, allowing us to map out the user's posture and movements in real-time. We used these tools to create our own custom dataset of demonstration exercises for our users to follow. Computational geometry techniques such as cosine similarity were utilized to determine the similarity between our demonstration and the user's form and it was also used to measure the angles between the landmarks and the drawing. We decided to built a facial authentication system by using blockchain technology and have a front end which has a dashboard of activeness and number of workout session completed. The entire frontend was built on React.JS and the backend with the help of Flask. We also took advantage by fine tuning a LLM solely for the purpose of suggesting tailored exercises.","One major challenge we encountered was the complexity of implementing real-time posture similarity while maintaining low latency across different devices. To ensure the accuracy of our model we had  process live video which was required optimizing our algorithms, especially when detecting subtle movements. We also faced challenges with integrating computational geometry for our features, such as recognizing body movements to draw precise shapes like waves and circles. Handling variability in user movements while maintaining the integrity of these shapes added a layer of complexity to the project. We also faced problem to properly pre-process our dataset and make the fine tune the cosine similarity algorithm.",We are proud of successfully implementing real-time posture detection similarity algorithm that accurately tracks user movements with low latency across different devices. Our integration of computational geometry allowed us to create an engaging and innovative experience where users can draw shapes like waves and circles to improve mobility.,"We also increased our understanding of computational geometry and how it can be applied to interactive, gamified features. Another key takeaway was the experience of creating your own custom datasets and train the existing model on that.","We aim to integrate Flexy with VR allowing users to visualize their movements and corrections during exercises in a more immersive way. We're also exploring the use of wearable technology, like smartwatches or sensors, to provide even more precise tracking and feedback on mobility and joint health.","blockchain, cloudflare, cosine, flask, mediapipe, midnight, opencv, python, react, sqlalchemy",https://github.com/Shameed4/Flexy
HackHarvard 2024,U-Plan,https://devpost.com/software/u-plan,True,"We're from Arizona, and yes—it really is incredibly hot. Having lived here for 2.5 years, each year seems to get hotter than the last. During a casual conversation with an Uber driver in Boston, we chatted about the weather. She mentioned that even the snowfall has been decreasing there. This got us thinking deeply about what's really happening to our climate. It's clear that climate change isn't some far-off concern; it's unfolding right now with far-reaching consequences around the world. Take Hurricane Milton in Florida, for example—it was so severe that even scientists and predictive models couldn't foresee its full impact. This realization made us wonder how we could contribute to a solution. One significant way is by tackling the issue ofUrban Heat Islands (UHIs). These UHIs not only make cities hotter but also contribute to the larger problem of global warming. But what exactly are Urban Heat Islands?",,,,,,,"cloudflare, flask, folium, geopy, mapbox, next.js, numpy, python, sam, workers",https://github.com/dhruvb26/u.plan
HackHarvard 2024,OpenLegislation,https://devpost.com/software/openlegislation,True,"With elections right around the corner, many young adults are voting for the first time, and may not be equipped with knowledge of the law and current domestic events. We believe that this is a major problem with our nation, and we seek to use open source government data to provide day to day citizens with access to knowledge on legislative activities and current affairs in our nation.","OpenLegislation aims to bridge the knowledge gap by providing easy access to legislative information. By leveraging open-source government data, we empower citizens to make informed decisions about the issues that matter most to them. This approach not only enhances civic engagement but also promotes a more educated and participatory democracy Our platform allows users to input an issue they are interested in, and then uses cosine analysis to fetch the most relevant bills currently in Congress related to that issue.","We built this application with a tech stack of MongoDB, ExpressJS, ReactJS, and OpenAI. DataBricks' Llama Index was used to get embeddings for the title of our bill. We used a Vector Search using Atlas's Vector Search and Mongoose for accurate semantic results when searching for a bill. Additionally, Cloudflare's AI Gateway was used to track calls to GPT-4o for insightful analysis of each bill.","At first, we tried to use OpenAI's embeddings for each bill's title. However, this brought a lot of issues for our scraper as while the embeddings were really good, they took up a lot of storage and were heavily rate limited. This was not feasible at all. To solve this challenge, we pivoted to a smaller model that uses a pre trained transformer to provide embeddings processed locally instead of through an API call. Although the semantic search was slightly worse, we were able to get satisfactory results for our MVP and be able to expand on different, higher-quality models in the future.","We are proud that we have used open source software technology and data to empower the people with transparency and knowledge of what is going on in our government and our nation. We have used the most advanced technology that Cloudflare and Databricks provides and leveraged it for the good of the people. On top of that, we are proud of our technical acheivement of our semantic search, giving the people the bills they want to see.","During the development of this project, we learned more of how vector embeddings work and are used to provide the best search results. We learned more of Cloudflare and OpenAI's tools in this development and will definitely be using them on future projects. Most importantly, we learned the value of open source data and technology and the impact it can have on our society.","For future progress of OpenLegislation, we plan to expand to local states! Constituents can know directly what is going on in their state on top of their country with this addition and actually be able to receive updates on what officials they elected are actually proposing. In addition, we would expand our technology by using more advanced embeddings for more tailored searches. Finally, we would implore more data anlysis methods with help from Cloudflare and DataBricks' Open-Source technologies to help make this important data more available and transparant for the good of society.","cloudflare, express.js, javascript, llama-index, mongodb, openai, react, vector","https://github.com/ryankamiri/OpenLegislation, https://openlegislation.asahoo.dev/"
HackHarvard 2024,TheftWatch,https://devpost.com/software/theft-watch,True,"One of our team members has firsthand experience working in retail, where shoplifting was always a looming threat. We wanted to help small business owners take back control of their stores and feel more secure. TheftWatch aims to reduce the anxiety and losses that come from theft by providing better insights and actionable alerts. Smart cities often incorporate technologies such as: How can smart city technologies be effectively integrated into retail environments to deter shoplifting?",We decided to go much more further and make our own live representation of a store. We even tested on the MLH Stand! After running this experiment on our workspace we realized that the information is very valuable for security staff from the event,"We leveraged computer vision tools and machine learning models to classify customer behavior and identify suspicious actions. The system integrates with WhatsApp for real-time alerts and uses object recognition and facial recognition technologies to provide clear insights into incidents. The intricate dashboard was created to give shop owners actionable data in a simple, visual way.","Developing an accurate classification system for customer behavior was challenging. Defining clear metrics for what constitutes suspicious or dangerous activity, while minimizing false positives, required extensive tuning and testing. We also faced difficulties integrating multiple technologies into a seamless product.",We're proud to have built a functional system that addresses a real problem for small businesses. Creating a reliable classification model for customer behavior and successfully integrating different tech components—including real-time notifications and a detailed dashboard—was a huge achievement for our team.,We learned a lot about the complexities of building a computer vision solution that works in real-time and can make meaningful decisions based on nuanced data. We also gained insights into balancing the need for accurate detection with ensuring a positive customer experience.,"We're planning to refine our classification model to improve accuracy and reduce false alarms. We also want to add more features to the dashboard, such as predictive analytics to help shop owners anticipate theft risks before they happen. Additionally, expanding our notification system to integrate with other platforms, such as SMS or email, is on our roadmap.","cloudflare, mongodb, openai, opencv, roboflow, streamlit, tensorflow, tillio, yolov8","https://theft-watch-4j.streamlit.app/, https://github.com/JocelynVelarde/Hack-Harvard-2024, https://www.canva.com/design/DAGTbKLlJBw/ocLznU3XEdAeqFwgL9gB4Q/edit?utm_content=DAGTbKLlJBw&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
HackHarvard 2024,Invaders,https://devpost.com/software/invaders-uq02re,True,"nexus shot 1 nexus shot 2 an alien got exploded more aliens getting killed a view of the stage 1 high score overview screen waiting screen before the game starts hall of fame Using MATLAB, we built out a game that looks fun and sounds cool! Our main motivation is to make a somewhat polished game during the span of HackHarvard 2024 and also to have fun! While playing or watching the game, you may notice that the aliens have different strategic movements depending on how far into the stage you are, and also depending on how aggressive you are within the game (whether you you the type of person who likes to combo up a bunch of aliens and shoot them all at once, or you are the type of person who is just throwing ammos everywhere without care!) Because of this, the simulation logic of the aliens is very complicated, and with MATLAB, we are able to generate the curves for the aliens path as well as their attacking behaviours. This makes for a tremendously more organic random-generated game. We are quite happy with how the aliens behave. If you want to see it for yourself, I encourage you to play it! As of the time of writing this, we are proud to share that during this hackathon, we have successfully: On the technical level, we: As for the game, some of its features include: The game is quite good looking already, but we want to add some more features to it after the hackathon, which may not just be limited to: Right now, the environment we used to build the game is Ubuntu 24.04 LTS. Windows is supported but is un-tested. Thank you everyone at HackHarvard this year for sharing positive energy all around!",,,,,,,"c++, freetype, matlab, miniaudio, opengl","https://github.com/salaadas/invaders, https://vimeo.com/1019066017?share=copy#t=0"
HackHarvard 2024,Open Context,https://devpost.com/software/open-context,True,"Access to course resources is often fragmented and lacks personalization, making it difficult for students to optimize their learning experiences. When students use Large Language Models (LLMs) for academic insights, they often encounter limitations due to LLMs’ inability to interpret various data formats—like lecture audio or photos of notes. Additionally, context often gets lost between sessions, leading to fragmented study experiences. We created OpenContext to offer a comprehensive solution that enables students to manage, customize, and retain context across multiple learning resources.","OpenContext is an all-in-one platform designed to provide students with personalized, contextually-aware resources using LLMs. It aims to make course data open source by allowing students to share their course materials. Full User Flow: The user faces a landing page where they are prompted with three options: Create a new chat, Upload documents, Generate quizzes. As the user navigates to Upload documents, they have the option to record their current lecture real time from their browser. Or, they can upload documents ranging from audio, pdf and image. We are using Tesseract for Optical Character Recognition on image files, OpenAI’s SpeechtoText API on audio files, and our own PDF parser for other class documents. The user can also record the lecture in real time which will be transcribed and processed in real time. After the transcription of a lecture or another class document is finished, it is displayed to the user. They will be able to create a new chat and ask our AI assistant anything related to their course materials. The assistant will have the full context of uploaded documents and will be able to answer with references to those documents. The user will also have the option to generate a quiz based on the transcription of the lecture that just got recorded. They will also be able to merge multiple class documents and generate a custom quiz out of all. The quizzes will have the format of quizlet flashcards, where a question is asked, 4 answers are provided as options and after an option is chosen, the website will prompt with either the chosen answer is correct or incorrect. The score for each question is calculated and at the end of the quiz a summary of the performance is written for the student user to track their progress.",,"One significant challenge was efficiently extracting text from images. This required converting images to PDFs, running OCR to overlay text onto the original document, and accurately placing the text for quiz generation. Ensuring real-time transcription accuracy and managing the processing load on our servers were also challenging.",,,"We aim to develop a secure information-sharing system within the platform, enabling students to share their study materials safely and privately with their peers. Additionally, we plan to introduce collaborative study sessions where students can work together on quizzes and share real-time notes. This could involve shared document editing and group quiz sessions to enhance the sense of open source.","cloudflare, defang, fastapi, nginx, openai, pinecone, python, react, terraform, tesseract","https://github.com/SunfishTK1/open-context, https://open-context.vercel.app/, https://alpnix-pdf-to-searchable--8080.prod1b.defang.dev/docs, https://www.figma.com/design/urZDaJEPu8S2LxGE9qyRWm/OpenContext?node-id=0-1&node-type=canvas&t=s4pSuAWf5JBgHs1E-0, https://lucid.app/lucidchart/f29e7da2-c622-43cc-95f1-cadb64a4099b/edit?beaconFlowId=EF00FCD67E4D2788&invitationId=inv_76c2206a-c79e-46db-b839-fd4d1b650de0&page=0_0"
HackHarvard 2024,citewise,https://devpost.com/software/citewise,True,"Legal research is tedious and time-consuming, with lawyers spending hours finding relevant cases. For instance, if a client was punched, lawyers must manually try different keywords like “kick” or “slap” to find matches. Analyzing cases is also challenging. Even with case summaries, lawyers must still verify their accuracy by cross-referencing with the original text, sometimes hundreds of pages long. This is inefficient, given the sheer length of cases and the need to constantly toggle between tabs to find the relevant paragraphs.","Our tool transforms legal research by offering AI-powered legal case search. Lawyers can input meeting notes or queries in natural language, and our AI scans open-source databases to identify the most relevant cases, ranked by similarity score. Once the best matches are identified, users can quickly review our AI-generated case summaries and full case texts side by side in a split-screen view, minimizing context-switching and enhancing research efficiency.","Our tool was developed to create a seamless user experience for lawyers. The backend process began with transforming legal text into embeddings using the all-MiniLM-L6-v2 model for efficient memory usage. We sourced data from the CourtListener, which is backed by the Free Law Project non-profit, and stored the embeddings in LanceDB, allowing us to retrieve relevant cases quickly. To facilitate the search process, we integrated the CourtListener API, which enables keyword searches of court cases. A FastAPI backend server was established to connect LanceDB and CourtListener for effective data retrieval.","The primary challenge was bridging the gap between legal expertise and software development. Analyzing legal texts proved difficult due to their complex and nuanced language. Legal terminology can vary significantly across cases, necessitating a deep understanding of context for accurate interpretation. This complexity made it challenging to develop an AI system that could generate meaningful similarity scores while grasping the subtleties inherent in legal documents.","Even though we started out as strangers and were busy building our product, we took the time to get to know each other personally and have fun during the hackathon too! This was the first hackathon for two of our teammates, but they quickly adapted and contributed meaningfully. Most importantly, we supported each other throughout the process, making the experience both rewarding and memorable.","Throughout the process, we emphasized constant communication within the team. The law student offered insights into complex research workflows, while the developers shared their expertise on technical feasibility and implementation. Together, we balanced usability with scope within the limited time available, and all team members worked hard to train the AI to generate meaningful similarity scores, which was particularly demanding. One teammate delved deep into embeddings, learning about their applications in similarity search, chunking, prompt engineering, and adjacent concepts like named entity recognition, hybrid search, and retrieval-augmented generation (RAG) — all within the span of the hackathon. Additionally, two of our members had no front-end development experience and minimal familiarity with design tools like Figma. By leveraging resources like assistant-ui, we quickly learned the necessary skills.","We aim to provide support for the complete workflow of legal research. This includes enabling lawyers to download relevant cases easily and facilitating collaboration by allowing the sharing of client files with colleagues. Additionally, we plan to integrate with paid legal databases, making our product platform agnostic. This will enable lawyers to search across multiple databases simultaneously, streamlining their research process and eliminating the need to access each database individually.","cloudflare, courtlistener, fastapi, gpt-4o-mini, huggingface, javascript, lancedb, python, react, text-embedding-3-small","https://github.com/FelixNgFender/HackHarvard24, https://hackharvard24.pages.dev/"
HackHarvard 2024,Embeddit,https://devpost.com/software/embeddit,True,"homepage One issue with underdiagnosed and underrecognized diseases is the lack of information and treatment plans provided by doctors. For example, small intestinal bacterial overgrowth (SIBO) plagues people worldwide with symptoms such as abdominal pain, nausea, and bloating. Diseases like this force patients to experiment with different solutions, relying on other people’s personal experiences for guidance. Sometimes patients look to the internet for advice. Subreddits like r/SIBO provide valuable personal experiences that can help people overcome their challenges. However, the variety of topics on Reddit can make it a pain to navigate. Our solution is Embeddit, a web application that allows users to find relevant information about a topic easily! Whether it’s looking for opinions about certain medications or finding suggestions for a specific tech stack to use, Embeddit makes searching easy. Embeddit allows users to ask questions about a specific subreddit. Users input the subreddit's name and a number of posts to scrape. Then we call reddit's API to grab posts, embed them and store them in a Pinecone vector database. Our application focuses on personal experiences from Reddit posts! It connects people around the world facing niche struggles and tough challenges. We implemented Next.js as our full-stack solution. The frontend was built using React, Tailwind, and DaisyUI components. Our backend handled the retrieval-augmented generation (RAGing) using the Pinecone vector database. ChatGPT was also used for the brains behind our chatbot and useful information generation. The application was deployed on Vercel. Some challenges we ran into included augmenting an LLM with little prior experience. We read a lot of documentation to understand how to integrate data into an LLM to produce useful information. Processing the Reddit post data through the Reddit API was also a challenge as generating the authentication token required using multiple functions and another application. Embedding Reddit data to implement RAG using Pinecone was also something new we had to learn in a short period of time. We are proud of our ability to integrate crowdsourced knowledge with artificial intelligence to create a seamless application. For the majority of the team it was our first time working with implementing RAG in Pinecone. We are also proud of designing a web app that we’d use ourselves and for optimizing a fast RAG without paying a lot of money. We learned to use Next.js for both frontend and backend development using typescript. We found that teamwork can sometimes be a challenging component of a hackathon. A team is a critical part of what makes or breaks the project. We also learned how interesting and awesome LLMs capability and use cases can be. Additionally emphasizing UI design is crucial for ease of use of an application. Right now our application only shows the ten most relevant posts in a subreddit. In the future we would like to implement an option to include more posts. We’d implement more safety surrounding biased online posts, perhaps using more artificial intelligence and sentiment analysis. We would also convert this application to a mobile app for convenient use anywhere.",,,,,,,"chatgpt, css, daisyui, nextjs, openai, pinecone, react, redditapi, tailwind, typescript, vercel",https://github.com/sethmorton/embeddit
HackHarvard 2024,Clean Sweep,https://devpost.com/software/cleansweep-tjq36w,True,"We aimed to build smart cities by helping garbage truck drivers receive the most efficient route in near-real-time, minimizing waste buildup in prone areas. This approach directly addresses issues of inefficiency in traditional waste collection, which often relies on static daily or weekly routes. You might be wondering, why not just let drivers follow standard routes? In densely populated areas, waste accumulates faster than in less populated zones. This means that a one-size-fits-all route approach leads to inefficient pickups, resulting in garbage buildup that contributes to air and water pollution. By targeting areas where waste accumulates more quickly, we can reduce contamination, improve air quality, and create greener, healthier environments. Additionally, optimizing waste collection can lead to more sustainable use of resources and reduce the strain on landfills.","CleanSweep has sensors that collect real-time information about all the trash cans in the city, detecting different waste levels. This data is live-updated in the truck drivers' main portal while they drive, allowing them to receive the most efficient collection routes. These routes are optimized based on real-time data, including the percentage of trash bin capacity filled, traffic conditions between bins, and the number of days since the last pickup. As drivers collect trash, they can update their progress live to receive the next optimized route for the remaining bins.","For our hardware: We used two phone cameras in two similar scenarios that sent photos repeatedly to a computer. This image is then passed onto the Raspberry Pi where Python OpenCV was used to detect the trash level filled. Then a Node.JS script was used to display the levels (1 to 5) on a set of LEDs and then pass the information to a local server for further processing on the software side. Backend: We received the reported trash level data from the hardware, information about the traffic in the surrounding vicinity as well as distance to the bin from the Google Maps API and Google Distance Matrix API, and pre-modeled values for time since last garbage retrieval and fed this into a Random Forest Classifier model on Databricks. Our model makes predictions on how to prioritize the bin routes in order to get a short path and distance–resulting in fewer emissions. An adjacency matrix was then used to retrieve the highest priority paths based on traffic and waste levels. Frontend: We used React for the UI and TailwindCSS for styling to create the portal, deployed over Terraform. We brought all of this information together to display the recommended optimal routes based on how full the bins were (which can be changed by adding more trash to bin 1 and 2 in real life).","Measuring the level of the trash bin was actually a very challenging task for what seemed like a simple task. We first tried looking for pre-trained models that could do it for us, but other models only worked with object tracking. We went to OpenCV, but that only worked in certain conditions. Thus, one of the hardest challenges we experienced was making sure the lighting conditions, the setup for the cameras and hardware, and our OpenCV contour algorithm worked deterministically, as even a shadow could mess with our results.","None of us had ever worked with OpenCV or with using hardware at a hackathon. We wanted to take some of the skills we learned in our electrical classes and implement them with a cool solution, even if it was a bit tacky. We were super proud of learning how OpenCV worked and how we could use hardware as well as a network to create a complete interactive solution. Another major component of this hackathon was hardware as we’ve always wanted to use such a concept to visualize our project and make it easier to understand for everyone on what our goals are. We enjoyed our varied skills by stepping out of our comfort zone and using new technologies; each member used a new skill to implement in the project.","All of us learned about the parts of networks and hardware required to communicate including the security standards and communication frequency needed to keep all the data real time. We learned how to build a working website with a login system through the use of react and vite, as well as implementing the Google maps API for visualizing the route for the drivers. We also learned how GPIO works on a raspberry pi and how a simple JS script can control the power output for these pins to control LEDs. Another skill we found interesting was the use of OpenCV for image processing as it made computing the percentage a task of a second rather than a longer time of manual image processing algorithms. Finally, we were able to start using an ML model to train and help us find the best routes for drivers, this resulted in faster travel times as traffic was avoided and garbage collection optimized.",Clean Sweep’s next goals would be to implement real time sensors in bins spread out all over the city to collect data and help train the model by providing the large amounts of data. This would then help implement this real time UI to efficiently visit all the necessary stops for the day. Such a system would also allow for coverage over rare cases such as a garbage truck missing a stop or allowing other trucks to take over a broken truck’s schedule as the route would update in real time to collect all garbage fast and efficiently.,"databricks, google-cloud, google-maps, node.js, opencv, python, raspberry-pi, react, tailwindcss, terraform",https://github.com/DPulavarthy/HackHarvard
HackHarvard 2024,FootPrint Mayhem,https://devpost.com/software/footprint-mayhem,True,"      Climate change is daunting, and it often feels like individual efforts don’t matter. That’s why we createdFootPrint Mayhem– a fun, addictive way to turn eco-friendly actions into a game! Think Duolingo, but for saving the planet. FootPrint Mayhem makes sustainability part of your daily routine with these features: Every small step counts, and we make it engaging and rewarding! Our tech stack: We kept things modular with components like Dashboard, UserInputForm, and LevelQuiz for maintainability. 🌱FootPrint Mayhem: Making the world greener, one streak at a time!",,,,,,,"defan.io, express.js, firebase, javascript, node.js, react, tailwindcss","https://footprintmayhem.org/, https://github.com/Jshot117/HackHarvard"
HackHarvard 2024,TradeREC,https://devpost.com/software/energy-trading,True,"The global renewable energy market was valued atUSD 1.21 trillion in 2023and is projected to grow at acompound annual growth rate (CAGR) of 17.2% from 2024 to 2030. This growth is reflected in theRenewable Energy Certificate (REC) market, valued atUSD 12.72 billion in 2023, which is expected to reachUSD 136.35 billion by 2033with aCAGR of 26.71%. RECs are tradable certificates that certify electricity was generated from renewable energy sources and represent the associated carbon offsets. For businesses, REC trading presents several advantages. Governments are implementingRenewable Portfolio Standards (RPS)that mandate a portion of electricity must come from renewable sources. RECs allow companies to meet these requirements without the need for compliance payments or costly energy infrastructure changes. In addition, many states providetax incentivesfor renewable energy adoption, such as the upcoming Clean Electricity Investment Tax Credit, which businesses can leverage through REC purchases. By acquiring RECs, companies can also offset their remaining greenhouse gas (GHG) emissions, boosting theirsustainability credentials—a key factor for consumers, investors, and stakeholders. Households benefit from REC trading as well. A 6-kilowatt residential solar system can generate approximately seven RECs annually, providing anadditional revenue streamfrom renewable energy production. As more participants enter the REC market, it creates financial incentives for the development of new renewable energy projects, driving the market toward greener energy production. However, the REC market faces challenges, including thelack of standardized rules and practicesacross trading platforms. This fragmentation makes trading RECs across regions difficult and hinders market efficiency. 
Furthermore,limited transparencyaround REC pricing and availability reduces market confidence, making it harder for businesses to assess the value of their REC investments. Households also face difficulties accessing the market due to the need for brokers, which leads tohigh middleman feesthat reduce profitability. Overcoming these issues is crucial to unlocking the full potential of the REC market and advancing global renewable energy efforts.",TradeREC revolutionizes the REC trading marketplace with three essential features:,"The frontend of our platform is built usingStreamlit and Python, providing a user-friendly interface for seamless interaction. On the backend, we utilize a combination ofMongoDB, Hedera, and Midnightusing Hedera for data storage, Midnight for encryption, and MongoDB for development to leverage blockchain technology for secure and transparent data management. Additionally, our OCR and NLP (powered by theOpenAI API) processes are designed to automate accurate verification of Renewable Energy Certificates (RECs) for streamlined trading.","Throughout the development of this project, we encountered numerous technical and conceptual challenges, each providing valuable learning experiences. Early on, we dedicated significant time to researching carbon caps and RECs, working to understand how these components of energy trading fit together. Our goal was to design an ecosystem that balanced ease of use with robust functionality for the end user.
During backend development, we faced compatibility issues between various blockchain platforms and our development stack. As a result, we opted to integrate MongoDB as a backup to blockchain for our database needs. Ultimately, we successfully implemented the Hydera and Midnight blockchain to secure clean energy trades, while MongoDB proved invaluable for testing and debugging throughout the process.","We are proud to have achieved our goal of creating a secure, blockchain-powered platform specifically designed for the Renewable Energy Credit (REC) trading market. By integrating blockchain technology, we ensure transparency and trust in every transaction, allowing users to track REC ownership, pricing, and authenticity with confidence. Our platform connects diverse groups—businesses, households, and renewable energy producers—enabling seamless, peer-to-peer REC trading without the need for intermediaries, reducing costs and simplifying the process.
Additionally, our use of OCR and NLP automates the verification of certificates, ensuring accuracy and speed in validating critical REC data. This combination of blockchain and automated verification not only enhances security and reliability but also delivers a streamlined, customized experience that maximizes functionality while minimizing complexity for all users.","Throughout this project, we gained valuable insights into both the technical and regulatory aspects of the Renewable Energy Credit (REC) market. We deepened our understanding of government regulations surrounding carbon emissions and the incentives promoting clean energy adoption. Technically, we developed a robust blockchain framework to guarantee secure and transparent data storage and trading, while integrating OCR and NLP to automate the verification process of RECs. This project was both exciting and challenging, greatly expanding our technological expertise and enhancing our understanding of the complexities within the energy market.",,"hedera, midnight, mongodb, ocr, openai, python, streamlit",https://github.com/eshansinghal14/hack-harvard-24
HackHarvard 2024,CognitiveKeyboard,https://devpost.com/software/cognitivekeyboard,True,"Our team was inspired by the AllHealth track presented at the opening ceremony and the challenge of enhancing patient care in the least intrusive way possible. We considered the technologies people use daily and landed on typing, something everyone does across devices—whether sending texts, searching online, or writing documents. Research shows that analyzing typing patterns, like the rate of typos, can help predict mental health risk factors, cognitive decline, and other health conditions like Parkinsons. Links to some of the studies that inspired our project can be found at the end.","CognitiveKeyboard is an alternative Android keyboard that updates a user’s typing statistics every 30 seconds. It tracks details like backspaces, special characters, typing speed, and pauses between keystrokes — without storing actual text. This approach eliminates privacy concerns about tracking what is typed. Using this data, along with demographic information (e.g., age, gender, known health conditions), we create data visualizations for researchers to analyze. They can filter participants, cluster users by typing patterns, and apply labels based on their research. These labels are fed back into our machine learning model to improve prediction accuracy.","We developed the keyboard and typing tracker in Android Studio, using Java and JDK, built on top of an open-source keyboard library. For the database, we used MongoDB Atlas, creating collections for user demographics, user clusters, and daily typing samples. Python APIs manage interactions between our backend and database, handling tasks like adding users, submitting samples, and retrieving data. The web interface for data visualization and researcher access was created using Python, while the Android app’s frontend was built with Java. We hosted everything on Linode.","One of our biggest challenges was choosing how to track typing statistics efficiently and privately. While using an alternate keyboard is unconventional, it offers the most secure and efficient method. Privacy was a key concern — this approach allowed us to analyze keystroke types without storing sensitive data like passwords. Alternative solutions either consumed too much battery or required excessive permissions, such as accessibility access to screen content.
Another challenge was identifying which typing statistics were most relevant for our analysis. We eventually focused on typing speed, keystroke intervals, typing accuracy (measured by backspaces), character variability, and special character usage. Managing this six-dimensional data presented visualization challenges, which we addressed by offering flexible filters and visualizing data in 2D or 3D, depending on the researchers’ preferences.","We’re proud of our commitment to patient privacy. Instead of storing sensitive data and encrypting it, we found a way to collect meaningful statistics without recording typed text. Our data visualizations are another key achievement—they simplify clustering and offer customization, making the research process more intuitive. Finally, we’re proud of staying sane and focused through the 36-hour marathon, creating something that has real potential to enhance patient care and identify early health risks.","As a group coming from a wide range of experience and backgrounds, through our collaboration we also had a wide range of takeaways. Taha:Throughout this project, I explored effective project management, particularly in a fast-paced environment. I learned how to delegate tasks, prioritize my own milestones, and quickly adapt when things didn’t go as planned. Working as a team under pressure and sleep deprivation taught me the importance of clear communication, supporting each other’s strengths, and maintaining focus. I also realized that persistence in debugging always pays off—it was worth sticking with a problem, no matter how long it took, because solving it in the end was rewarding. On the technical side, I gained valuable experience writing code in Java for Android applications. Shaurya:I worked on creating a machine learning model, developing data visualizations, and building a Python frontend for researchers. One major takeaway from this process was the importance of seeing the debugging process through from start to finish. I also learned how to manage our time across different tasks and the value of taking breaks to prevent burnout. This balance was key to maintaining productivity over the course of the project. Maggie:As a first-time hacker, I learned a lot from my teammates, especially about deploying a real-world project, managing time, and dividing responsibilities within the team. I appreciated the support we gave each other, especially during moments of exhaustion. On the technical side, I gained hands-on experience creating APIs to connect our backend with the database, which was completely new to me. Observing how my teammates handled their tasks also provided me with valuable second-hand learning experiences. Andrew:This was my first hackathon and my first time working in a team-based development environment, so I focused on learning from my teammates. I gained a lot from watching them work in different areas, particularly around creating visualizations and learning how to communicate effectively through the API. I also learned how to make API calls and how tools like MongoDB and Flask communicate. I familiarized myself with basic python packages like pandas, streamlit, numpy, and matplot.","Our next steps involve expanding the platform support. Since most typing happens on mobile, especially messaging, we prioritized mobile statistics for this hackathon. An iOS implementation was impractical within the timeframe due to App Store delays and cost barriers, but it’s on our roadmap. We also plan to support browser and desktop statistical analysis. Another future goal is to detect users at urgent risk, possibly through sentiment analysis or flagging risk-indicating keywords. Finally, we aim to develop an interface for healthcare providers, like our interface for researchers, to monitor patients at risk, catching potential issues early. Beyond these plans, there are numerous other directions for future expansion.","adb, android-studio, github, java, linode, mongodb, python",https://github.com/tahabykl/CognitiveKeyboard
HackHarvard 2024,Althea,https://devpost.com/software/althea,True,"This project is inspired by our personal experiences of seeing elderly relatives struggle with keeping track of their prescriptions as well as with the complexity of modern tech. By creating simple, no-signup-required, and user-friendly apps, we aim to enhance their quality of life and improve their health as well.","Althea allows users to enter which prescriptions drugs they are taking so that they can have a checklist to remember what they have taken on a certain day. Althea then asks the user if they felt any symptoms the same day and notifies the user if the symptoms, they had could be side effects of a drug they use. The user can rate the severity of the symptoms they felt. Finally, users would be able to see past logs and even export them as PDFs if they would like to share the information easily with their primary care providers.","For the front-end, we used JavaScript with Tailwind CSS and HTML for a smooth and sleek user experience. For the back-end, we used Python with Django. We organized schemas and used SQLite for patient data because of its sensitive nature. We used PostgreSQL for medicine data since we wanted it to be shared among different people and used Gemini AI in conjunction with it. For the mobile app alternative, we used pure Flutter and emulated on our machine.","One significant challenge we ran into early on was deciding with what tech to build our app. We originally agreed on React Native since we all were familiar with React, but we learned that they're not that similar. This caused us to lose quite some time as we struggle with figuring it out. After that, we decided to do React+Django web app with a Flutter mobile app in parallel with huge ambition to interconnect them. However, time constraints and technical challenges didn't allow us to achieve our initial goals.
Additionally, our web team was challenged by the undertaking of linking the frontend and backend via API endpoints.","We're proud of few things. First, we have a large codebase with well-maintained structure, tech and features. Second, we have a neat, streamlined and user-friendly navigation system for both the web and mobile app. Third, we did a decent design considering we are all primarily backend developers with minor experience in React or any other frontend tech. Lastly, we worked on two projects in parallel, even though linking them together didn't work out.","Teamwork and communication are a must. Hackathons are the grindiest grind out there (lol). Gradual development and constant updates via Git is the path to success. We made sure to work on separate branches and then merge them to avoid conflicts as well as having structured file system that promotes collaboration. And lastly, simplicity is a key, trying too hard or aiming to high often won't end good.","Next thing is implementing all the features that stayed on the whiteboard as well as connecting web and mobile apps into one ecosystem, allowing seamless access for wide audience. Definitely making our UI/UX even better but also streamlined and simple so any user, even least tech-wise, can use it. A cool and useful feature would be allowing users to securely scan their prescription label with their phone camera.","amazon-web-services, django, flutter, gemini, javascript, postgresql, postman, python, react, sqlite, tailwind",https://github.com/Givikap/Althea
HackHarvard 2024,SimplySign,https://devpost.com/software/simplysign,True,"After spending some time in the hospital last year, I quickly became aware of just how isolating and scary the experience can be. And after hearing one of my good friend’s stories of her own experience being hospitalized, I was sure that I could use our experiences to help other patients, and other people all around the world. My friend found herself in an Italian hospital, oceans away from home in a country whose language she didn't speak. This is an unfortunate reality that many face, though applications such as Google Translate can help to bridge the gaps in communication that can occur during these tense moments. However, without any Google Translate for sign language, as well as a 2022Reuters Healtharticle which reported that nearly one-third of hospitals in the US don't offer any sign language interpreters at all, I knew that this was likely an experience which was all too familiar for those who rely on signing to communicate. From there, my project began, and just as quickly as it did, it grew and became something that can apply to many situations outside of healthcare as well.",,,,,,,"css, github, html, javascript, mediapipe, opencv, python, pytorch, signlanguagemnistdataset, tensorflow.js, ultralytics-yolo, visual-studio-code, visualstudiocode, yolov8","https://github.com/syd-stokes/simply-sign, https://docs.google.com/presentation/d/1GX3tOtfBQNFpx0LZrGz_cu1p1_tTjzKFJEdOCvqHrPs/edit?usp=sharing"
HackHarvard 2024,T-Error,https://devpost.com/software/terror,True,"The inspiration for T-Error came from the common frustration that tech leads and developers face when debugging problems. Errors can occur frequently, but understanding their patterns and seeing what is really holding your team up can be tough. We wanted to create something that captures these errors in real time, visualizes them, and lets you write and seamlessly integrate documentation making it easier for teams to build faster.","T-Error is a terminal error-monitoring tool that captures and logs errors as developers run commands. It aggregates error data in real-time from various client terminals and provides a frontend dashboard to visualize error frequencies and insights, as well as adding the option to seamlessly add documentation. A feature we are really excited about is the ability to automatically run the commands in the documentation without needing to leave the terminal.","We built T-Error using:
Custom shell: we implemented a custom shell in c++ to capture stderr and seamlessly interface with our backend.
Backend: Powered by Node.js, the server collects, processes, and stores error data in mongoDB.
Frontend: Developed with React.js, the dashboard visualizes error trends with interactive charts, graphs, and logs, as well as an embedded markdown editor:).","One of the main challenges was ensuring the terminal wrappers were lightweight and didn’t disrupt normal command execution while effectively capturing errors. We spent hours trying to get bash scripts to do what we wanted, until we gave up and tried implementing a shell which worked much better. Additionally, coming up with the UX for how to best deliver existing documentation was a challenge but after some attempts, we arrived at a solution we were happy with.","We’re proud of building a fully functional MVP that successfully captures and visualizes error data in real-time. Our terminal wrappers integrate seamlessly with existing workflows, and the error analysis and automatic documentation execution has the potential to significantly speed up development.","Throughout this project, we learned about the complexities of error logging across multiple environments and how to efficiently process large volumes of real-time data. We also gained experience with the integration of frontend and backend technologies, as well as diving into the lower layers of the tech stack and smoothly chaining everything together.","Going forward, there are a few features that we want to implement. First is error reproduction - we could potentially gain more context about the error from the file system and previous commands and use that context to help replicate errors automatically. We also wanted to automate the process of solving these errors - as helpful as it is to have engineers write documentation, there is a reason there are gaps. This could be done using an intelligent agent for simple tasks, and more complex systems for others. We also want to be able to accommodate better to teams, allowing them to have groups where internal errors are tracked.","c++, javascript, mongodb, nextjs, node.js, react.js, typescript","https://github.com/hellolol2016/hackharvard2024, https://t-err.vercel.app/"
HackHarvard 2024,MediGuide,https://devpost.com/software/mediguide-c9qn3t,True,"   Medication non-persistence is a critical issue affecting patient safety in the United States and beyond. According to studies, ""between 40% and 75% of adults fail to take their prescribed medications on time each day"" [1], leading to worsening health outcomes, increased hospitalizations, and preventable deaths. Non-adherence can lead to serious complications, including ""delirium, hypoglycemia, and acute kidney injury"" [2]. Existing digital reminder solutions often suffer from high onboarding friction, making them less accessible to those who need them most—particularly senior citizens and less privileged members of society who may have limited access to or knowledge of modern technology like smartphones or the internet. We were inspired to create MediGuide to address these gaps, providing an inclusive, user-friendly solution that enhances medication adherence and improves patient safety for everyone. MediGuide is an AI-powered reminder and monitoring system designed to enhance patient safety by ensuring adherence to medication regimens. Here's how it works:
1️⃣ Seamless Onboarding with Healthcare Professionals: Patients are onboarded through a consultation session with a healthcare professional, either in-person or via telehealth. The doctor understands the patient's needs, writes a prescription, and enters it into the MediGuide platform, specifying the patient's preferred method of reminder.
2️⃣ Personalized Reminder Delivery: MediGuide's intelligent AI system identifies the medication and dosage, setting up personalized reminders for the patient via their chosen method—automated phone calls or text messages. Each reminder includes clear, simple instructions tailored to the patient's specific medication regimen.
3️⃣ Monitoring and Feedback: After each reminder, the system prompts the patient to confirm medication intake. It periodically checks in about their experiences, detecting early signs of adverse reactions and prompting timely interventions.
4️⃣ AI-Powered Adaptation: The AI continuously learns from patient behavior, using predictive analytics to adapt and optimize reminders for each individual, improving adherence over time.
By integrating this data-driven approach, MediGuide aims to minimize medication-related risks, reduce hospital readmissions, and ultimately improve overall patient safety. We built MediGuide using Next.js and Flask for the backend, providing a robust and scalable infrastructure. The frontend was developed with React.js, TypeScript, and shadcn, ensuring a responsive and user-friendly interface. To understand doctors' prescriptions and generate smart, personalized reminders, we utilized top-performing Large Language Models (LLMs). For the phone call reminders, we leveraged advanced conversational voice AI technologies to deliver engaging and accessible interactions with patients who may not be familiar with modern digital devices. One of the main challenges we faced was integrating advanced AI technologies within a tight time frame. Developing an AI system that accurately understands and processes doctors' prescriptions required careful tuning of Large Language Models. Ensuring that our platform is accessible to patients with limited technological proficiency was another challenge, as we needed to design interfaces and interaction methods (like phone calls) that are user-friendly for all age groups and technological backgrounds. Additionally, we had to consider compliance with healthcare regulations and patient privacy laws, which added complexity to our development process. We are proud of being able to create MediGuide in under 36 hours for a noble cause. Developing a functional AI-powered system capable of understanding prescriptions, generating personalized reminders, and interacting with patients through conversational voice AI within such a short time frame was a significant achievement. We hope that this creation can eventually evolve into a product that helps more patients take their medications timely and recover faster, improving patient safety for millions across the world. Throughout this project, we learned about the complexities surrounding medication adherence and the significant impact it has on patient safety and healthcare costs. We gained valuable experience in integrating advanced AI technologies, such as Large Language Models and conversational voice AI, into a practical application. Additionally, we learned the importance of designing solutions that are accessible and user-friendly for all patients, regardless of their technological proficiency. This project reinforced the value of interdisciplinary collaboration between technology and healthcare to address real-world problems. We're excited to hear feedback from our peers and mentors and to speak with actual healthcare practitioners. Our next steps involve engaging with medical professionals to gather insights and validate our solution in real-world settings. We aim to explore avenues of collaboration with hospitals and clinics to integrate MediGuide into existing healthcare systems, ultimately improving patient safety on a larger scale. We're also considering further development to comply with healthcare regulations, enhance AI capabilities, and expand accessibility features to reach more patients.
Citations",,,,,,,"flask, nextjs, openai, prisma, python, react, shadcn, typescript, vapi",https://github.com/MediGuideApp/MediGuide
HackHarvard 2024,Harvard Burger,https://devpost.com/software/harvard-burger,True,"Partially inspired by the Smart Cities track, we wanted our app to have the direct utility of ordering food, while still being fun to interact with. We aimed to combine convenience with entertainment, making the experience more enjoyable than your typical drive-through order.","You interact using only your voice. The app automatically detects when you start and stop talking, uses AI to transcribe what you say, figures out the food items (with modifications) you want to order, and adds them to your current order. It even handles details like size and flavor preferences. The AI then generates text-to-speech audio, which is played back to confirm your order in a humorous, engaging way. There is absolutely zero set-up or management necessary, as the program will completely ignore all background noises and conversation. Even then, it will still take your order with staggering precision.","The frontend of the app is built with React and TypeScript, while the backend uses Flask and Python. We containerized the app using Docker and deployed it using Defang. The design of the menu is also done in Canva with a dash of Harvard colors.","One major challenge was getting the different parts of the app—frontend, backend, and AI—to communicate effectively. From media file conversions to AI prompt engineering, we worked through each of the problems together. We struggled particularly with maintaining smooth communication once the app was deployed. Additionally, fine-tuning the AI to accurately extract order information from voice inputs while keeping the interaction natural was a big hurdle.","We're proud of building a fully functioning product that successfully integrates all the features we envisioned. We also managed to deploy the app, which was a huge achievement given the complexity of the project. Completing our initial feature set within the hackathon timeframe was a key success for us. Trying to work with Python data type was difficult to manage, and we were proud to navigate around that. We are also extremely proud to meet a bunch of new people and tackle new challenges that we were not previously comfortable with.","We honed our skills in React, TypeScript, Flask, and Python, especially in how to make these technologies work together. We also learned how to containerize and deploy applications using Docker and Docker Compose, as well as how to use Defang for cloud deployment.","Moving forward, we want to add a business-facing interface, where restaurant staff would be able to view and fulfill customer orders. There will also be individual kiosk devices to handle order inputs. These features would allowHarvard Burgerto move from a demo to a fully functional app that restaurants could actually use. Lastly, we can sell the product by designing marketing strategies for fast food chains.","canva, css, defang, docker, ffmpeg, flask, html, javascript, openai, python, react, typescript","https://github.com/GrantDeWaay/HackHarvard2024, https://bbavoso-frontend--80.prod1a.defang.dev/?_gl=1*18ojt43*_ga*MTI3MDc1MDM3NS4xNzI4Nzc0MjE1*_ga_QVJVWN44CW*MTcyODgyMzUwMy43LjEuMTcyODgyMzUxNC4wLjAuMA.."
HackHarvard 2024,ParkPoint（Parking no worries）,https://devpost.com/software/parkpoint,False,"Urban parking remains a persistent challenge in many cities, contributing to traffic congestion, increased emissions, and driver frustration. We were inspired by the idea of harnessing smart technologies to create a more efficient and user-friendly parking experience.The Smart Parking Management System (SPMS)was developed with the goal of bridging the communication gap between parking authorities and parkers, while utilizing data-driven insights to optimize parking infrastructure.",,,,,,,"css, html, python, streamlit",https://docs.google.com/presentation/d/1aO6ONwEJVDaSg9cy-ssolsEGUhKRGXzqOEb2qsDdBlE/edit?usp=sharing
HackHarvard 2024,Theraplane,https://devpost.com/software/theraplane,False,,,,,,,,"css3, flask, google-calendar, html5, javascript, openai, python, sqlite, tailwincss",https://github.com/ChowdhuryFarzana/Theraplane_AI
HackHarvard 2024,Teleport,https://devpost.com/software/teleport-n79fal,False,"Having decided to come to a far off place, 8,000 miles away from home to participate in a hackathon at Harvard without a team and having no one to support me in the midst of heavy competition. I missed my family. And the whole point of understanding that being a global citizen means cities that are meant to make people closer, and transportation systems that enable the proximity of 2 individuals, it was almost near impossible to fly 8,000 miles now to just meet and talk to my family. Let's say in a village, city or country, you want to travel. You can travel either by road (though cars/uber/bus), trains and flights.  Taking the example of an Uber ride from Amherst to Harvard, it took 45 minutes of waiting and spending $180. Why?
Because there were no drivers around. Why?
Because those Uber drivers have less incentive to be in that area when compared to popular cities like NYC and Boston. Why?
Because BIG cities are the hubs to work and people come here to do that. Why?
Only certain places have access to those opportunities to connect. And so, we wanted to change that. Just because a country lacks good public transportation system, lack of Infra and other things, shouldn't stop people to connect. So, how do we enable EVERY place in the world, and give it the ability to go anywhere you want? Thinking out of the box, you need Dr. Strange's portal to teleport. Oh wait, teleportation is possible? With the advent of the metaverse it is. It'd be nice if our family could check in on how we're doing thousands of miles away without disturbing us 24/7.","Teleport, is a solution, that is powered using Deep Learning to segment your face, using an IoT Network to create a reverse proxy for your localhost computer that essentially negates paying a cloud provider for server space to run this and connect , and finally using Augmented Reality, bring an immersive experience using ONLY your phones (no need for big expensive Meta Quests/ Apple Vision Pro) to connect with the people you love, business deals or just checking in by ""teleporting"" you to their world!","By first creating a video conferencing tool using Node.js and Express.js. And then creating a separate module using Python to make use of dlib, a Machine Learning framework for facial detection, and doubling down by using Nvidia SegFormer Deep Learning model to only segment the face part to be displayed for the AR feature. And used AR.js to embed the AR/VR module to be loaded on a browser.","Initially, creating a web socket that concurrently handles the connections between 2 people via browser was a challenge. Then while using primitive ML models for facial segmentation, the model would not recognize parts of the face and it had black patches within the face, which is why we integrated Deep Learning to make it more robust. And one of the biggest problems, is integrating the video that we get from the model segregated face to appear on a browser in an AR context.","I did all this despite my team not being there due to emergency, as a full stack developer with no prior knowledge on Deep Learning, Integrating a IoT network, and AR embeddings, I was able to learn and implement these modules in a very short time (<8 hours)","Deep Learning, Machine Learning, IoT Networks, AR Integration on Environment","Create an end-to-end solution where all of them can be accessed in a single website URL. Like a metaverse based connection platform but uses live AR to make it immersive in your own world space, Make Meeting in your space.","ar.js, augmented-reality, css-frameworks:-flask, deep-learning, dlib, express.js, flask, html, iot, javascript, machine-learning, ngrok, node.js, opencv, python, pytorch, segformer, transformers, transformers-platforms:-local-machine-(for-development)","https://github.com/JudahJacinth/teleport, https://github.com/JudahJacinth/video-chat"
HackHarvard 2024,Traffix,https://devpost.com/software/traffix-bqny0s,False,"Growing up in Michigan, we experienced potholes and construction countless times on the roads. We wanted to create a scalable way that takes advantage of existing surveillance to monitor the health of all roads and provide recommendations for future repairs","Provides a dashboard with live feeds and detailed statistics for roads intersections, using data-driven calculations to proactively inform cities when repairs are needed.","We began by reading in the live stream frames and fixing delays in our video. Then, we incorporated OpenCV models to detect vehicles on the frames. Next, we preprocessed historical traffic and weather data with Pandas and created a database. Finally, we created several API routes with FastAPI that use our database to respond to queries and created a frontend dashboard with NextJS.",,,,,"fastapi, next.js, opencv, pandas, python, vercel",https://github.com/shlswnt/hackharvard2024
HackHarvard 2024,SkyScale,https://devpost.com/software/sky-scale,False,"While working on my own AI research, I routinely relied on cloud computing. Last summer, I encountered a major setback: after three weeks of work, I had under-allocated compute resources for my experiment - all my progress was lost. Frustrated, I turned to my mentors for advice, only to discover that this issue—either over-provisioning or under-utilizing cloud resources—was a common problem they had faced as well. I realized this same issue had to apply to the wide variety of cloud computing customers.","SkyScale helps organizations optimize their cloud computing usage by providing tailored recommendations. A user would first fill out data about their organization as a whole and upload documentation (think business plans, slide decks, proposals, papers, etc.) SkyScale analyzes this data, extracts pertinent information, determines the necessary compute units needed, and determines the best cloud computing services for the use case. Unlike all other existing cloud optimization softwares, SkyScale looks at your use case as a whole and determines the best allocation of services for your specific application. After a user is up and running, SkyScale connects with cloud computing services to continuously real-time monitor your usage to forecast future needs. This allows organizations to dynamically scale and adjust allocations to pay for only what they are using.",,"We ran into numerous challenges over the past 36 hours - too many to record.
One of the biggest challenges we ran into was understanding what the real problem was with cloud computing over-expenditure. There are many existing cloud optimization softwares on the market, yet the problem continues to exist. Our understanding was the diverse range of cloud computing customers caused for these softwares to be overly general. We opted for a personalized approach. 
We also struggled to figure out a way to make it simple for users to input their data. Most of our users do not have significant cloud computing experience or know specifics about what services they desire. We decided the best approach is to extract data ourselves from their business model and documentation using LLMs.
Additionally, we ran into a lot of hurdles with fine tuning our model. Based on our product requirements, we needed to utilize LLMs that have online search capabilities that can also be adjusted and fine-tuned. Due to the need to query real-time data about cloud compute providers, GPT-4o was our top option for data integration. In fine tuning, we scraped information from online sources about cloud compute prices, FinOps, and best practices per industry. These were used to adjust the model through API fine-tuning and system call requirements. A great deal of our fine-tuning also came through prompt engineering. Using tailored prompts behind the scenes, we adjusted model settings to account for user input nuance and real-time queried data.","Of course, we are most proud of what we built. We spent a lot of time ideating to truly find a unique unmet need and think we built a tool that actually addresses it. As researchers, we would genuinely use our tool.
We are also proud of how much we learned this weekend. We came into the project with not a ton of experience working with Next.Js or building projects, but came out of it feeling a lot more confident with developer tools.","Again, there is too much that we learned than we can list here. To list a few:
Cloud computing workflows
Relational Databases
Enterprise payment schemas
Fine tuning models
NextJs
Building user friendly APIs
Integration OAI keys
Handling multimodal inputs
Deploying systems on Replicate","We built an MVP for SkyScale this weekend. Our next step is to test our prototype with users to receive feedback and better understand how we can best address this problem. By talking to users, we hope to gain a better grasp on cloud computing over expenditure (much better than we could in 36 hours) and build SkyScale into an end to end solution that can benefit organizations.","cloudflare, next.js, openai, replicate",http://sky-scale.vercel.app
HackHarvard 2024,AI in Quality Engineering,https://devpost.com/software/ai-in-quality-engineering,False,"In the bustling city of Apex, North Carolina, a group of passionate engineers and tech enthusiasts gathered for a weekend of innovation at the annual AI Quality Engineering Hackathon. Among them was Satheeskumar, a seasoned quality engineer with a deep interest in artificial intelligence and a drive to solve complex challenges.
The theme of the hackathon was clear: to push the boundaries of AI while ensuring the highest standards of quality and reliability. As the event kicked off, Satheeskumar and our team brainstormed ideas, aiming to create an AI system that could revolutionize software testing.
Our project, named “AI-QE,” was designed to automate the detection of software bugs using advanced machine learning algorithms. The goal was to create a tool that could not only identify issues but also predict potential problem areas before they became critical.
Throughout the hackathon, the team faced numerous challenges. We had to ensure our AI model was trained on diverse datasets to avoid biases, and they needed to implement rigorous testing protocols to validate their results. Satheeskumar’s  expertise in quality engineering was crucial in navigating these hurdles, ensuring that every aspect of our project met the highest standards.
As the final presentations approached, Satheeskumar and their team worked tirelessly, refining their code and running countless tests. Their dedication paid off when they demonstrated AI-QE to the judges, showcasing its ability to detect and predict software bugs with impressive accuracy.
The judges were not only impressed by the technical prowess of the project but also by the team’s commitment to quality and reliability. AI-QE was awarded the top prize, and Satheeskumar’s team was celebrated for our innovative approach to integrating quality engineering principles into AI development.
The hackathon concluded with a sense of accomplishment and inspiration. Satheeskumar and our team had not only created a groundbreaking tool but also highlighted the importance of quality engineering in the rapidly evolving field of AI. Our journey served as a reminder that excellence in technology is achieved through dedication, collaboration, and an unwavering commitment to quality.",,,,,,,"amazon-web-services, bdd, cucumber, playwright, python",
HackHarvard 2024,WellBite,https://devpost.com/software/wellbite,False,"   We were really enthusiastic thePatient SafetyandAllHealthtracks, we wanted to harness that energy into building something worth it! WellBite.aiis an online AI assistant designed to help patients make food choices that doesn’t harm their health, considering their medical dietary restrictions. A patient can take a picture of their meal, and using AI, the ingredients are analyzed along with portion sizes and nutrition facts. Based on the patient’s profile, it offers instantaneous feedback on whether the meal complies with their dietary restrictions, helping to avert health risks. In addition, doctors have the capability to access and modify patients' dietary restrictions from an intuitive dashboard, ensuring adherence to the diet in real time and improving overall health outcomes. We developedWellBite.aiusing: One major challenge was updatingMongoDBcollections from the frontend which took considerable time to debug. We also faced challenges with securing user account management and ensuring the AI could accurately analyze a wide variety of foods. We’re proud to have created afunctioning appwhere patients, doctors, and AI interact in real time. We’re especially proud of integrating advanced AI for meal analysis, securely handling patient data, and creating an intuitive interface for users. We gained valuable experience in: We aim to further improveWellBite.aifor official deployment inhospitalsandhealth centers. Our future goals include:",,,,,,,"ai, cloudflare, flask, mongodb, python",https://github.com/moisedk/wellbite.ai.git
HackHarvard 2024,CiviClick,https://devpost.com/software/civiclick,False,"Let’s say you’re walking through the streets of Cambridge and you trip over a pothole, or you notice a small child almost get hit at an intersection, or you walk under a broken streetlight. These are things which make your city less safe, less habitable. Who do you tell?  Our project spun out of the understanding that we live in an age of limitless information, but limited reach to our municipal leaders. And our municipal leaders live equally in an age where there is unlimited information about what us, their constituents think about their leadership and local issues, as well as limitless information on the open source about where theres crime, or construction needs, where themes people get ticketed, and every other small and large issue that ranges from making daily life a nuisance to real natural disaster and life and death prevention. As we approached this, we asked, what is a smart city? And to us, a smart city means a city is a data driven city. It’s a city where municipal leaders have access to maximal information and citizens have access to maximal information too. Every person knows what issues are occurring in their town, where they are occurring, and who to contact to fix them — and that action is taken. Politicians know who and what issues are being face, what matters to their constituents sand they run on those issues. That was our inspiration for CiviClick - 260 access and insights from data. Data about everything that’s available publicly, for everyone, to incite real change. Smarter cities, and a better future, all through a single prince of software.",,,,,,,"fastapi, nextjs, openai, postgresql, python, yolo","https://hackharvardfrontend.vercel.app/, https://github.com/anushkarde/CiviClick, https://drive.google.com/drive/folders/1iKrZnOBwd6Uci-g2WnHuj1zWPZFtuP5w?usp=drive_link"
HackHarvard 2024,Local.ly,https://devpost.com/software/local-ly-joc2e1,False,"We were inspired by the untapped potential of local businesses and the challenge they face in gaining visibility within crowded marketplaces (especially in major cities). We wanted to create a platform that empowers communities to easily discover and support these hidden gems, while giving businesses the tools to thrive in the smart city ecosystem.","Local.ly connects users with local businesses, helping them discover new places and redeem exclusive offers. It provides a seamless platform for businesses to create profiles, promote special deals, and engage directly with their community. Users can browse, find unique businesses, and access deals—all in one place.","We built Local.ly using a combination of frontend and backend technologies. On the frontend, we used React and Tailwind, and for the backend, we used Express.js, Prisma, and PostgresQL. For the backend, we integrated a relational database to more easily manage user and business information, and used the Google OAuth protocol to allow users to sign in with their Google accounts without using a pre-built app that handles the state management of the back and forth between the client, server, and Google.","One of the biggest challenges we ran into was actually using Prisma with the CRUD operations to our PostgresQL database. We ran into many issues, including some where we faced were circular dependency / infinite recursion and null values from our Prisma query args, which we ended up not being able to fully solve due to a limit of time we had to actually test our solution. For example, we had models Business and Promotion in our schema, where Business has fields locations and promotions, and Promotion has field business, and for some reason the prisma include args would return some data as null, even though we checked the database and the values were actually present.","We’re proud of building a platform that empowers local businesses while creating a streamlined experience for users. Successfully overcoming the OAuth challenges and designing a user-friendly interface were also major wins for us. Additionally, our platform highlights businesses in a way that feels both fresh and essential.","Integrating Prisma was more complex than anticipated, especially when managing nested relationships and optimizing queries for performance. We encountered issues with circular dependencies in our data models and had to refactor our schema multiple times to ensure the integrity of our data. This process deepened our understanding of database design and how to efficiently fetch and transform relational data.","We plan to expand Local.ly by adding advanced features such as personalized recommendations, real-time business analytics, and integrations with smart city technologies to provide businesses with more insights and users with a tailored experience. We also aim to onboard more local businesses and grow our user base to foster stronger community connections.","adobe-illustrator, canva, css3, docker, figma, git, github, google-gmail-oauth, javascript, lint, node.js, oauth, prisma, react, typescript, vite, yarn",https://github.com/alaneng-neu/hackharvard2024
HackHarvard 2024,Priceflix,https://devpost.com/software/priceflix,False,"Priceflix - Movie Ticket Price Comparison
This project, Priceflix, is a web application designed to help users find the cheapest movie tickets by comparing ticket prices for various movie showtimes. The current version is focused on scraping real-time ticket prices from AMC Theatres using web scraping techniques and dynamically updating the data every five minutes. Key Features:
Web Scraping: The application extracts data from AMC Theatres' website, including movie showtimes, ticket prices, and available discounts. We use xpaths for precise scraping of this data.
Dynamic Data Updates: The ticket price and showtime data is refreshed every five minutes through a while loop to ensure users always see the latest information.
Data Storage: Scraped movie showtimes and ticket prices are stored in a file that is continuously updated. This allows the application to provide accurate, up-to-date ticket pricing information.
Custom Webpage: The application includes a custom-built webpage that pulls from the stored data file and presents it in an easy-to-read format. This web interface allows users to browse current ticket prices and identify the cheapest option for their preferred movie.
API Integration (Future Feature): While the MVP uses web scraping, future versions will integrate with official movie ticket APIs for even faster and more accurate data retrieval. Potential APIs include AMC Theatres API, Fandango, or Atom Tickets.
Project Structure:
Data Collection: Scraper that runs every five minutes, using Python and libraries like BeautifulSoup, Selenium, and Cloudscraper to bypass browser security features and gather movie and ticket price data from AMC's website.
Data Storage: The scraped data is stored in structured files (e.g., JSON or CSV format), which update dynamically as new data is retrieved.
Web Interface: A front-end interface created using HTML, CSS, and JavaScript to display the movie listings, showtimes, and prices.
Price Comparison: Logic built into the web interface allows users to sort or filter results based on price, movie time, or other factors, helping them find the best deal.
Technologies Used:
Python for web scraping and data handling.
Selenium and BeautifulSoup for scraping movie showtimes and ticket prices.
Cloudscraper API to bypass browser securities and access viable data using scripts.
HTML, CSS, and JavaScript for building the front-end user interface.
GitHub for version control and collaboration.
Future Enhancements:
API Integration: Incorporate official APIs to fetch movie and ticket data more reliably.
User Authentication: Allow users to save favorite theaters or movie preferences and receive notifications when prices drop.
Advanced Filtering: Add more filtering options for users to find tickets based on time, location, or ticket types (e.g., matinee vs. standard).
Mobile-Friendly Design: Create a responsive design for better usability on mobile devices.",,,,,,,"beautiful-soup, cloudscraper, css, html, javascript, python, selenium",https://github.com/nishant1823/HackHarvard-2024.git
HackHarvard 2024,Autodine,https://devpost.com/software/autodine,False,"Main Menu Choice of Order type given to user List of Available restaurants shown on a map Dashboard before order Dashboard after order Database Imagine you're heading home after a long, exhausting day. You’re hungry, craving something quick and easy. A fast food drive-thru sounds perfect. You pull up, but the speaker crackles. The employee struggles to hear your order over the noise of traffic and the sound of the fryer. You repeat yourself, growing more frustrated. When you finally pull around to the window, you find the order is wrong—again. What should’ve been a quick and convenient stop becomes an experience full of delays and confusion. This isn't just an occasional hiccup; it’s a systemic issue fast-food chains face every day. McDonald's recently tried automating their drive-thru system with IBM to tackle this, but the results were far from perfect. The very technology that was supposed to speed things up became a roadblock. So, when we heard about the partnership falling apart, we knew there was an opportunity to do better—to reimagine the entire experience. And that’s how Autodine was born. We didn’t want to simply fix the problem. We wanted to elevate the drive-thru experience into something truly futuristic, part of a bigger vision for the smart cities of tomorrow. Picture this: a world where ordering at a drive-thru is not just efficient, but seamless and even enjoyable. No more misheard orders, no more repeating yourself over a crackling speaker. Instead, you enter a digital space where you can see your order in real-time, communicated clearly and accurately. Whether it’s background noise, heavy traffic, or the busiest lunch rush, Autodine handles it all with ease. Our goal is simple but powerful: to rebuild trust in fast food service by tackling the inefficiencies that frustrate customers and hurt businesses. Through Autodine, we're offering a smarter, more reliable way to handle orders that enhances the customer experience while reducing operational stress for the chains. Welcome to the future—a future where cities are smarter, services are seamless, and technology helps us, not hinders us. Imagine stepping into a world where everything just works, where you no longer have to worry about miscommunication, long waits, or order mix-ups at a drive-thru. This is the worldAutodineoffers—a glimpse into the smart cities of tomorrow, where ordering food is as effortless as it should be. At Autodine, webring you into our visionof a smart city. Using the latest cutting-edge technology, includingUnreal Engine 5for immersive visualizations, we allow you to see what ordering food in the future will look like. Gone are the days of shouting your order through a crackling speaker. Instead, you enter a sleek, virtual space where youinteract directly with our smart systemsanddrive-thru agentsin real time, viewing your order with clarity. Our app isn’t just about making orders easier—it’s aboutbringing you into the futureof urban living. Through Auto Dine, you experience howautomated systemswill transform smart cities, making services faster, smoother, and more reliable. We’re not just offering a solution to current problems—we’repainting a pictureof howtechnology will work for usin tomorrow’s cities. At its core, our solution has three critical components, working together seamlessly: AI-Powered Large Language Model (LLM)We designed an almost real-time system where text-to-speech conversion allows drivers to place their orders naturally and quickly. Using clever prompts and API calls toOpenAIand the restaurant’s web server, the system processes each order with precision, ensuring that there’s no room for error. It creates a fluid interaction between the customer and the ordering system, making communication fast and effective. Web Server Built on DjangoTo manage the restaurant's operations, we built a robustDjango-based web server, modeling a database that reflects real-time restaurant activity. This server handles everything from creating orders to updating the database with incoming information, providing a structured backbone for AutoDine. It’s the beating heart of the system, ensuring that everything runs smoothly behind the scenes. Dynamic Front-End DashboardThe customer-facing side is a beautiful, interactive dashboard that reflects real-time updates from the web server. UsingServer-Sent Events (SSE), the web server continuously feeds the front end with the latest status of orders, so customers always know what’s happening with their food. It’s an interface designed for clarity and simplicity, making sure users have a seamless and engaging experience. Each of these components plays a vital role in delivering a futuristic solution to an age-old problem—improving the drive-thru experience. But AutoDine goes beyond just food orders: it demonstrates the power of automation, AI, and real-time systems to shape the smart cities of tomorrow. Autodine isn’t just solving today’s problems—it’s paving the way for a smarter, more connected future. 1) Getting the real-time text to speech was challenging because we were trying to achieve the most human-like conversation possible, so figuring out when to stop recording and process the audio asynchronously was challenging. We were fortunate enough to find the RealtimeSTT library that uses different techniques to achieve almost real-time speech-to-text. 2) Having both a speech-to-text and text-to-speech operating simultaneously, which both access the operating system and interfere destructively sometimes. 3) The prompt engineering turned out to be a little harder than expected. We used an LLM to parse the user input into a particular format and also interact with the web server, but it wouldn't always follow the format. So we had to chain prompts and insert system messages in-between user messages. We are proud of our execution speed. We came up with this idea, then checked the internet to see whether people had done it before, and apparently McDonald and IBM failed at it. But the uncertainty of whether it would even work didn't stop us from trying, and in two days, voilà. We are proud of the scalability of our system; for example, we could use it at hospital. Why? Our product is NOT an LLM, i.e., a text generator (though it is crucial for parsing information). Our product is a system that can interact with people and push updates to other services. So we can imagine a device in a consultation room that listens to diagnoses and conversations, or even a video camera that watches people and regularly updates information of interest. We learned that though accredited professionals may take months to develop products and fail sometimes, it doesn't mean that a group of students that don't even know each other are guaranteed to fail. Is that not AWESOME?! User Feedback and Iteration: Expand Use Cases: Integration with More Services: Enhancing AI Capabilities: Mobile App Development: Marketing and Awareness: Partnerships and Collaborations: Focus on Data Security and Privacy: Scalability Enhancements: Long-Term Vision:",,,,,,,"css3, django, html5, ispeech-text-to-speech, javascript, mycaption-speech-to-text, openai, python, sqlite","https://github.com/arielfayol37/AutoDine, https://pitch.com/v/audiodine_pitch_deck-4rww5h, https://drive.google.com/drive/folders/1LYaABseU_lmZ6md2Uwb24F1SHXtx-QsA?usp=sharing"
HackHarvard 2024,#FairSight,https://devpost.com/software/fairsight,False,"echnology has brought us incredible advancements—instant access to information, global connectivity—but it's also flooded the political landscape with misinformation and manipulative rhetoric. In the context of U.S. presidential debates, voters are left trying to sift through a sea of empty promises and confusing arguments. We were inspired by the need to empower citizens with clear, unbiased data, enabling them to make informed decisions in an era where trust in the political process is waning.","News bias detector
Debates fact checking","We used Whisper voice recognition software to identify speeches in live congressional broadcasts and classify statements by speaker, then integrated Gemini AI for argument validation. The platform uses data analysis to generate visual charts that showcase trends, speech impact, and argument strength. With Ollama the news can be explained to us in a more simpler way, and the web application also provides friendly-learning resources.","Another hurdle was integrating argument validity analysis in a way that could be easily understood by non-technical users, while maintaining the objectivity of the data.","We are incredibly proud of having built a tool that can genuinely empower voters by providing clear, data-driven insights from the often confusing political debates.",We learned to work with unfamiliar technologies and grew our skills in AI and data analysis.,,"gemini, next.js, python",https://github.com/Valenriquez/FiarSight
HackHarvard 2024,Twilight,https://devpost.com/software/twilight-cu8l5s,False,"Self-driving cars are transforming the automotive industry, not to mention empowering the future of sustainable transport. According to recent studies conducted by theNSF and MIT, up to 95% of all vehicles on the road could be fully autonomous by 2050. In order to prevent collisions between autonomous vehicles, it is beneficial to store all vehicles on a network and share data when required. This process is called cooperative perception. But how do we determine edge cases in cooperative perception? Furthermore, how can we integrate data from vehicles, sensors, and other nodes tobenchmark smart systemsand ensure robustuser safety?","Our project Twilight is an interactive UI that allows users to create custom environments and landscapes, consisting of vehicles, obstacles, sensors, and non-drivable regions. Users can run simulations on these environments, both with and without the cooperative perception component. This leads to a greater understanding of the network's overall behavior across various physical scenarios. By integrating data from nearby sensors and surrounding vehicles, we can also recover collision footage and determine the overall effectiveness of the system, including its performance in failures.","The frontend is a fully-integrated UI hosted on Flask. We constructed backend scripts and our main run file in Python, but most of our smart contracts, which connect to the Midnight Proof Network, are in TypeScript or JSON. We began by creating a simple framework of the frontend and then established different object classes for vehicles, sensors, barriers, etc. Once the layout was finalized, we encoded functionality on the backend side, such as handling user data and clicks; lastly, we create smart contracts that send and verify system data using the Midnight Contract API.","Our project is highly complex. One issue we encountered often during our hacking was translating commands from the backend to the frontend/UI. Combining components was difficult, due to mismatches in data type. We found ways to work around this issue by creating a Python server that could send tuples of data to the frontend. Another issue we experienced was the difficulty of working with APIs and software tools that were entirely new to us. Since neither of us had used Midnight prior to the event, we spent a significant chunk of time reading up on the Docs and familiarizing ourselves with the ledger, contract, and contract-cli formats.",,"We feel that completing this project was an overall incredible learning experience. Mihika learned how to interface with Insomnia, Docker, and APIs to deploy smart contracts for decentralized networks. Vishal and David worked extensively with the backend, generating innovative algorithms for collision detection and cooperative perception. Julianne discovered how to debug frontend issues with the help of generative AI tools and learned about using Socket.IO to drive client-server operations.","It may be getting dark, but we are burning the midnight oil on this one (literally). Our plans for Twilight are to expand the user interface to include even greater functionality, such as importing aerial images and converting these into a grid format. We also wish to implement a feature that allows users to see an aggregate view of a vehicle's vision - before and after cooperative perception.","blockchain, css, docker, html, javascript, json, node.js, python, typescript",https://github.com/mihikadusad/twilight
HackHarvard 2024,CareSync,https://devpost.com/software/caresync-b8m7gd,False,"The idea forCareSyncwas born out of the need to address the healthcare challenges faced by underserved communities, especially in third-world countries. We were inspired by the potential oftechnology to bridge gapsin healthcare access, improve patient understanding of their conditions, and assist overworked doctors by automating the report generation process. We wanted to create a solution that would empower doctors and make medical information more accessible for patients.","CareSyncis a virtual doctor’s assistant that usesvoice recognition technologyto generate medical reports based on a doctor's spoken input. The system then automaticallysimplifies complex medical jargoninto patient-friendly language. It also supportsmulti-language translationsand cultural sensitivity to ensure the information is clear and easily understood by patients, regardless of their background. In addition, CareSync providescustomizable medical report templatesand highlights key details such as dosages and lifestyle recommendations.","We built CareSync using a combination ofSpeech-to-Text APIsfor voice recognition andNatural Language Processing (NLP)to simplify medical terms. Our system was trained onmedical vocabulary datasetsto ensure it accurately captures and transcribes clinical information. Additionally, we integratedmulti-language supportto cater to non-English speaking regions. We createdcustom templatesfor medical reports to ensure flexibility and ease of use for healthcare providers.",,,"We learned that building technology for healthcare requiresprecision, sensitivity, and user accessibility. We gained a deeper understanding ofNLPand how it can be leveraged to simplify complex terminology. Additionally, we learned how to integratecultural and language considerationsinto technology to make it truly accessible for all.","Next, we plan to expand CareSync by addingreal-time doctor-patient interactionwhere doctors can directly dictate patient notes during consultations. We also aim to further improve theaccuracy of medical term recognitionand addmore languagesto our platform. Our goal is to partner with healthcare organizations in developing countries to deploy CareSync and start making an impact in underserved communities.","cloudflare, css, html5, javascript, mongodb, ngrok, node.js, npm, openai, visual-studio",https://github.com/SethJohnson333/HealthBuddy
HackHarvard 2024,HackrMap,https://devpost.com/software/hackrmap,False,"Hacking is fun, but daunting at the same time to many. We truly believe that people should not be scared of creating things and be more confident to ""hack"" for the better. For this reason, we bring outHackrMap, a solution for a Happy Hacking experience.",,,,"Working on the app Full-stack in 36 hours was a blast! Here are what we are very proud of. Our team is consisted of people of different backgrounds and experience with coding and hacking. However, we made it through by learning endlessly, trying a lot to arrive at the final product within just a short amount of time. Once the program processes all user-provided data, it generates a detailed roadmap that outlines the essential steps to bring your project to life. The program intelligently assigns tasks to each team member based on their skill levels, ensuring that everyone plays to their strengths. Additionally, it recommends relevant technologies, skills, and resources tailored to each task, equipping participants with the necessary documentation to familiarize themselves with any specific tools they may not know.",,,"api, flow, github, mern, node.js, openai, react","https://github.com/JoeyCorbett/hackrmap, http://hackrmap.com"
HackHarvard 2024,re$ipe,https://devpost.com/software/re-ipe,False,"We wanted to motivate people to cook at home, promoting healthier eating while making it fun and rewarding. The idea was to combine cooking with an incentive system that allows users to earn rewards for preparing home-cooked meals.","Re$ipe offers a variety of recipes across different cuisines. Users can browse, purchase recipes, and earn re$ipe coins. These coins can later be redeemed for gift cards such as Amazon or Visa. The app makes cooking engaging by rewarding users for their efforts.","We built Re$ipe using React for the front-end, Firebase for the back-end, and authentication. Firebase stores user data, recipes, and manages the reward system. The app’s design focuses on a smooth, intuitive user experience, making it easy to browse recipes and track rewards.","Our biggest challenges were designing a user-friendly interface and implementing the virtual currency system for rewards. Ensuring a smooth balance between ease of use and functionality was key, especially when handling user data and tracking rewards.","We successfully created a platform that encourages healthier eating with a rewarding experience. We’re proud of the clean, responsive design and the integration of a rewards system that motivates users to keep cooking and earning.","We learned the importance of building a user-focused platform, and balancing simplicity with features. We also gained experience using Firebase for real-time data handling and improving our UI/UX skills to enhance engagement.","We plan to add more rewards, social features for sharing recipes, and meal-planning tools. Expanding the recipe library and partnering with local businesses for more rewards are also on the roadmap. Additionally, a block chain implementation would be interesting for adding smart contracts.","css, firebase, google-maps, javascript, react",https://kennethc12.github.io/HarvardHack
HackHarvard 2024,SafeSpace,https://devpost.com/software/safespace-z6jsch,False,"Our project was inspired by the everyday challenges patients face, from remembering to take their medication to feeling isolated in their health journey. We saw the need for a solution that could do more than just manage symptoms—it needed to support patients emotionally, help prevent medication mistakes, and foster a sense of community. By using AI and creating a space where patients can connect with others in similar situations, we aim to improve not only their health outcomes but also their overall well-being.","Our project helps patients stay on track with their medication by using Apollo, our assistant that reminds them and tracks how they're feeling. It keeps a journal of their mood, sentiment, and actions, which can be shared with healthcare providers for better diagnosis and treatment. Users can also connect with others going through similar challenges, forming a supportive community. Beyond that, the platform helps prevent errors with prescriptions and medication, answers questions about their meds, and encourages patients to take an active role in their care—leading to more accurate diagnoses and reducing their financial burden.","We built multiple components so that everyone could benefit from our voice assistant system. Our voice assistant, Apollo, reads the user's transcript using OCR and then stores it in a DB for future retrieval. The voice assistant then understands the user and talks to them so that it can obtain information while consoling them. We achieved this by building a sophisticated pipeline involving an STT, text processing, and TTS layer.
After the conversation is done, notes are made from the transcript and summarized using our LLM agents, which are then again stored in the database. Artemis helps the user connect with other individuals who have gone through similar problems by using a sophisticated RAG pipeline utilizing LangChain.
Our Emergency Pipeline understands the user's problem by using a voice channel powered by React Native, evaluates the issue, and answers it by using another RAG-centric approach. Finally, for each interaction, a sentiment analysis is done using the RoBERTa Large Model, maintaining records of the patient's behavior, activities, mood, etc., in an encrypted manner for future reference by both the user and their associated practitioners.
To make our system accessible to users, we developed both a React web application and a React Native mobile app . The web app provides a comprehensive interface for users to interact with our voice assistant system from their desktop browsers, offering full functionality and easy access to conversation history, summaries, and connections made through Artemis.
The React Native mobile app for Emergencies brings the power of our voice assistant to users' smartphones, allowing them to seek help easily in case of an emergency","One of the key challenges we faced was ensuring the usability of the system. We wanted to create an intuitive experience that would be easy for users to navigate, especially during moments of mental distress. Designing a UI that is both simple and effective was difficult, as we had to strike the right balance between offering powerful features and avoiding overwhelming the user with too much information or complexity.","One of the biggest accomplishments we’re proud of is how accessible and user-friendly our project is. We’ve built an AI-powered platform that makes managing health easier for everyone, including those who may not have easy access to healthcare. By integrating features like medication reminders, mood and sentiment tracking, and a supportive community, we’ve created a tool that’s inclusive and intuitive. Our platform bridges the gap for those who may struggle with traditional healthcare systems, offering real-time answers to medication questions and preventing errors, all while fostering patient engagement. This combination of accessibility and smart features empowers users to take control of their health in a meaningful way, ensuring patient safety.","Throughout this project, we gained valuable experience working with new APIs that we had never used before, which expanded our technical skills and allowed us to implement features more effectively. We also learned how to better manage project progress by setting clear goals, collaborating efficiently, and adapting to challenges as they arose. This taught us the importance of flexibility and communication within the team, helping us stay on track and deliver a functional product within the tight timeframe of the hackathon.","In the future, we plan to enhance the platform with a strong focus on patient safety by integrating a feature that checks drug interactions when a prescription is provided by a doctor, ensuring the well-being of patients and preventing harmful combinations. Additionally, we aim to implement anti-hallucination measures to prevent false diagnoses, and safeguarding the accuracy of the assistant’s recommendations and promoting patient safety. To further protect users, we will incorporate robust encryption techniques to securely manage and store sensitive data, ensuring the highest level of privacy and security for patient information.","gemini, huggingface, mongodb, multiagent, openai, pineconedb, python, pytorch, rag, react, stt, transformers, tts",https://github.com/Swastik3/HackHarvard2024
HackHarvard 2024,EZ-PT,https://devpost.com/software/ez-pt,False,"We've all faced challenges with recreational sports injuries and have struggled to access physical therapy services due to complex schedules. Finding a reliable, guided recovery program has been difficult, especially through online sources. Our project aims to tackle this by empowering users to be their own trainers, supported by both trained professionals and our latest technologies.","EZ-PT provides guided, self-led, and professionally aided physical therapy from anywhere in the world, leveraging modern machine learning and computer vision techniques.","We utilized TensorFlow, NumPy, and a pre-trained ResNet model (MoveNet) for pose estimation. Video frames were streamed to the Python backend via WebSockets for classification using KNN, and a vector database was used to store pose embeddings. We connected this with server-side rendering via Next.js, using an ORM (Prisma) to communicate with our MySQL database hosted on Railway. Our frontend was hosted on Vercel.","We tried several tech implementations, including optimizing video streaming, which slowed us down due to its complexity. Initially, we attempted to run the pose estimation model in the browser for better latency, but eventually had to shift all our logic to the backend, which drained a lot of time.",,,"Prioritizing our focus on streaming and pose estimation on the web. Additionally, we are expanding our dataset to cover a wider range of physical therapy routines and enhancing the connection between professionals and their patients.","mysql, nextjs, numpy, opencv, python, railway, tensorflow, vercel",https://github.com/owenm-26/Easy-PT
HackHarvard 2024,docGen,https://devpost.com/software/docgen-v25jla,False,"As devs, we usually don't like the documentation process. It takes time, and is frankly quite boring. Our tool helps jumpstart that process.","Via CLI (or web browser) a user can input a github repo, and our tool creates a markdown file for each file in that repo. These markdown files contain explanations of the code using ChatGPT-3.5. They also contain snippets of the code with the intention to be a template, allowing the developer to add more info to each function faster.",Used Python (with libraries such as git) and ChatGPT to temporarily clone a GitHub repository that the user passes to our tool. It then parses through each file and makes an ChatGPT API call to get an explanation of each file. We created different methods to extract the functions/methods from each file as well. We also have another function that writes to each file with the generated file explanation and the code snippets. We also used Next.js to have a deployed version online where you can simply pass the github repo URL and view the markdown files on a web browser.,,,,,"nextjs, openai, python, tailwindcss",https://docgen-amber.vercel.app/
HackHarvard 2024,Attenbot,https://devpost.com/software/attenbot,False,"YouTube is not incentivized to keep your kids from brain rot like Mr. Beast and Ryan's World. This is how YouTube makes their money. The only alternative to protect your kids is to pre-approve every video manually (a feature that YouTube hides anyway), which is extremely tedious and not feasible for most parents. We want a world that's notskibidi toiletand is aligned with nurturing the next generation, so we decided that this problem would be the focus of our hack for impact.","It uses an LLM to filter the videos a child sees on YouTubedynamicallybased on the parent's filters/values. The parent defines things like ""I don't want anything that's clickbait whose purpose is to take my child's attention"" and ""Joey has been interested in dinosaurs in school so emphasize videos like that this week"", and Attenbot takes care of the rest. Your child will have a clean feed whenever they open up YouTube. The parent gets emailed weekly updates on the videos their child is watching. This provides an opportunity for the parent to refine Attenbot's choices, helping it better decide the content that meets the child's needs. Parents can now avoid relying on the YouTube algorithm, which does not prioritize the well-being of their child.",We have a Chrome extension that dynamically blocks the content on YouTube and makes API calls to our NextJS server endpoints to get the filtered videos. The filter is based on the structured output from gpt-4o given the YouTube videos that the child would have seen without the filter. The filter prompt is seeded with the filters defined by the parent and by the negative/positive examples that the parent defines from their weekly reports.,"Chrome extension development is painful. It was a fight against YouTube preventing us from changing the DOM how we'd like, but we got it done.",The filter is good! We're happy with the result and think it'd be useful for parents which is great.,Chrome extension development was a big thing we got better at.,Getting it into the hands of users. We're excited to try that out because we think this is a real problem.,"openai, postgresql, typescript",https://attenbot.vercel.app/
HackHarvard 2024,Moodivation,https://devpost.com/software/moodivation,False,"Moodivation was inspired by our shared interest in mental wellness and the different ways we connect with it. After thoughtful discussions and analysis, we realized this app could make a meaningful impact.","The app features a streak system, rewarding users for logging their moods, completing wellness activities, and sharing positive messages with friends.","We started with a prototype design, then built the app using various technologies, including Python, Firebase, React, and TypeScript.",We successfully trained and tested a Sentiment Analysis AI model in MATLAB but faced difficulties deploying it to the web app.,"we’re proud of our teamwork, creativity, and what we achieved in such a short time.",We learned new technologies and improved our communication and collaboration skills,"Next, we plan to integrate the AI model and enhance the app’s features.","firebase, flask, mathworks, node.js, python, quotesapi, rag, react, typescript, uizard","https://github.com/Sonnyywithanyextray/MoodMarathon, https://app.uizard.io/p/a5f82c46, https://www.loom.com/share/3cb67c5a81d644a98062b422a08ac8a3?sid=4b5cc94a-fd65-482d-9a12-e0e3bbadc778"
HackHarvard 2024,MoleculeIQ,https://devpost.com/software/moleculeiq,False,"The idea for Molecule-IQ Waste Management was inspired by the increasing need for efficient and sustainable waste management solutions. With growing concerns around waste disposal and the environmental impact, we wanted to create a platform that not only streamlines the waste reporting process but also motivates users to take eco-friendly actions. By incorporating AI for real-time waste verification and using gamification to engage users, we aimed to create a community-driven approach to addressing the waste crisis.","Molecule-IQ Waste Management is an AI-powered platform that simplifies waste reporting and rewards users for making sustainable choices. The platform leverages real-time AI verification to ensure accurate waste categorization, and gamification features—such as points and leaderboards—encourage users to engage more actively. It promotes a community-focused solution to waste management by rewarding users for their eco-friendly actions and providing a seamless, user-friendly experience.","We built the platform using a modern, full-stack tech stack, including:
    • Next.js 14 for fast, server-rendered pages and seamless API integration.
    • Tailwind CSS for responsive, elegant design.
    • Google Gemini AI for real-time AI-assisted waste categorization and verification.
    • Drizzle ORM to handle database interactions with Neon Database.
    • Web3Auth for secure and decentralized authentication.
The entire platform was designed with scalability and performance in mind, ensuring that as more users engage, the system remains efficient and responsive.","We faced several challenges throughout development:
    • Database Connection Issues: Integrating Drizzle ORM with Neon Database led to numerous hiccups. Ensuring stable database connections was a recurring issue, especially when working with real-time data.
    • Deployment Errors: While the platform worked perfectly on our local network, deploying it on Vercel introduced unexpected authentication errors, particularly around Web3Auth. Debugging and solving these deployment-specific issues took more time than anticipated.","Despite the challenges, we successfully built a beautiful, fully functional full-stack web application. The platform features a working database, responsive design, and operational APIs, all seamlessly integrated into a cohesive user experience. We’re particularly proud of the fact that we created a visually appealing product that functions as intended and addresses a real-world problem.","Originally, we were working on a different project related to drug research, but after realizing that it lacked a clear problem statement, we pivoted to Molecule-IQ Waste Management. This experience taught us the importance of recognizing when to stop and change direction while staying focused on the broader goal. We had to adapt quickly and work efficiently under time constraints, learning several new technologies in the process. We also gained deeper experience working with Git, especially in managing version control and collaborating effectively on a team.","Moving forward, we aim to take Molecule-IQ Waste Management to the next level by getting real users on board. Gathering feedback is crucial for improving the platform, so we plan to establish a feedback loop where user suggestions can directly influence future updates. Our goal is to refine the user experience, enhance the AI-powered features, and expand the platform’s reach to become a staple solution in sustainable waste management.","api, css, database, drizzle, google-gemini-ai, google-maps, javascript, neon, nextjs, orm, typescript, vercel, web3auth","https://github.com/vishomallaoli/mol-iq-waste-mgmt, https://mol-iq-waste-mgmt.vercel.app"
HackHarvard 2024,Albatross,https://devpost.com/software/albatross,False,"There are many ingredients that go into Smart Cities. But one that often gets forgotten amidst the great technological advancements and futuristic dreams is safety. The reality is, without safety for everyone, a city can't truly be considered ""smart."" Many of us have experienced moments when our navigation apps have led us into areas that felt unsafe or uneasy. That is precisely what we are here to prevent.","Albatross, like the bird that never fails to find its way home, will use AI to help you reach your destination safely by calculating the safest and fastest routes. Was there a recent robbery? Albatross knows all and will reroute you accordingly! By aggregating criminal history data with real-time traffic and city layout information, Albatross  provides quick, safe, and efficient routing.","We utilized Databricks for data storage and processing, integrating crime data into a Delta Lakehouse to generate and update crime hot-zones. MLflow powers the clustering and creation of these zones, enabling vector comparisons and embedding models to categorize different types of crimes. Cloudflare Workers handles the routing and navigation due to its speed and lightweight capabilities, while the Here Routing API was leveraged for optimal pathfinding.","Handling large sets of real-time crime data through new platforms like Databricks and Cloudflare Workers was a lot of fun but challenging. Designing a system that could efficiently process a steady stream of crime data and dynamically cluster it into hot-zones was quite complex, and took a lot of iterations. We also faced issues with flexible polyline encoding, which required workarounds due to limited library support.","Despite the challenges, we’re proud to have completed a project with all the core functionalities we set out to achieve. We embraced the spirit of hacking by diving into new technologies and implementing them successfully within a short timeframe.","We gained experience with various frameworks and tools, including Databricks, Cloudflare, and geo-routing APIs. This project also allowed us to revisit previous tech stacks such as Vue.js, Scala, and data warehousing.","Safety is a never-ending battle. We plan to introduce additional features such as real-time notifications, personalized safety recommendations, and advanced AI crime prediction models. Our goal is to continue enhancing urban safety, paving the way for a true Smart City.","amazon-web-services, cloudflare, databricks, here-routing, javascript, openai, python, scala, vue.js","https://albatross-hack.netlify.app/, https://github.com/orgs/HackHarvard2024-Team/repositories"
HackHarvard 2024,Proximity,https://devpost.com/software/proximity-1bzcke,False,"Proximity was born from the realization that despite our hyper-connected digital world, many people feel disconnected from those physically around them. We wanted to create a platform that bridges the gap between online social networking and real-world interactions, encouraging people to form meaningful connections with others in their immediate vicinity.","Proximity allows users to discover and interact with people within a one-mile radius. Users can set up profiles with different labels for various contexts (professional, dating, casual chatting), and the app matches them with nearby users who have similar interests or goals. Once connected, users can send meetup requests and engage in real-time chats, all while maintaining control over their privacy and visibility.","We developed Proximity using a modern web stack: Frontend: React for a dynamic and responsive user interface
Backend: Node.js with Express for a robust API
Database: MongoDB for flexible data storage
Mapping: MapboxGL for location visualization
Real-time Communication: WebSocket for instant messaging We integrated geolocation services to track and update user positions and implemented a matching algorithm based on user preferences and proximity.",,,,,"express.js, mapbox, mongoose, react, websockets",https://github.com/khanhdo05/proximity
HackHarvard 2024,PrepCheck,https://devpost.com/software/prepcheck,False,"PrepCheck was inspired by a story close to our hearts. One of the team members reported experiencing a case where a family member had to go through surgery. Far from reassuring, key information was hurried over or left out when the patient felt lost in the sea of 'big words' used in pre-op consultations. The confusion and uncertainty this caused were unnecessary stresses added to an already difficult situation. It, therefore, was apparent that accessible information was a common problem within healthcare. We realized that patients had the right to understand what was happening to their body and how to prepare for surgery in terms they actually understood. The personal experience ignited our drive to create a tool that would bridge the communication chasm between caregivers and healthcare patients, thereby empowering them with the knowledge to feel informed and confident.","PrepCheck is an AI-powered pre-operative assistant built with the purpose of simplifying the pre-surgery process for both the patient and the healthcare provider. It clearly explains the surgical procedure, what one should expect, potential complications, and all the pre-surgery do's and don'ts instructions. The AI converts a complex medical explanation into understandable terms; hence, the patient will understand how to prepare and what to expect. Moreover, PrepCheck promotes accountability by normalizing information given in pre-surgical briefings and ensures that critical information does not get excluded.","We built PrepCheck using a combination of Python and Flask to provide it with a simple yet effective backend infrastructure. Natural Language Processing was quite central in the development of clear, natural communication of the AI with patients. We trained it on sets of medical terminology and patient education materials so that the AI can provide responses which are not only accurate but also with appropriate detail. Our approach here was to break down the preparation process into smaller chunks and to address very specific questions and concerns that a patient may have. We also created a feedback loop for continuous learning - AI improves on real-world usage.","One of the most major challenges we needed to face was to make sure the AI responses were not only factually correct but appropriately nuanced. Further, the most challenging tasks included simplifying difficult medical terms to make them understandable, yet without losing vital information in the process; hence, trying to find an optimal balance between clarity and completeness. Besides, AI training that would fit into various types of surgeries and individual needs of patients was harder than expected because of the wide scope of medical procedures. Similarly, there were technical questions regarding how to attach the NLP functionality seamlessly to the system without compromising the system's performance and responsiveness. Things we are proud of We are proud that, in this timeframe, we have managed to develop a working prototype that fills the existing gap in the field of communication for the care of patients before surgery. We consider it gratifying to realize that we will contribute to a solution aimed at minimizing anxiety in patients while improving safety within surgical procedures. We also consider ourselves proud because we have managed to overcome serious technical difficulties with the NLP part of the work-a thing that actually allows AI to craft explanations that could be easily understood by patients. But what really demonstrated the team collaboration and problem-solving was how we were continuing to work out PrepCheck and make it as useful and user-friendly as possible.",,"In the development of PrepCheck, we came to learn about the various complications possible with healthcare communication, as well as about the diverse needs of patients. We realized how important it was to provide information in a manner that would be easy to digest, though complete. Equally, we improved our skills in natural language processing and AI development-skills of tuning algorithms for accuracy and user experience. Beyond the technical skills learned, which were empathy in technology design, at every turn, it had value: keeping the patient in mind. It truly matters in the success of our project.","The future for PrepCheck is great, and we look forward to continuing to work on enhancing the capabilities of the AI to cover a wider range of surgical procedures and conditions. Further down the road, we will incorporate multi-language support and accessibility features for patients with special needs to ensure that PrepCheck can assist some of the most diverse populations. We also want to integrate with EHR systems so that patient information is streamlined, making the pre-surgery process even more automated. We envision future collaboration with hospitals and healthcare institutions on the eventual piloting of PrepCheck in real-world settings, continuous improvement in its accuracy and usefulness based on real patient feedback.","elevenlabs, flask, github, openai, python",
HackHarvard 2024,EcoCoin,https://devpost.com/software/ecocoin-2iez17,False,"We saw that people weren't provided ease of access to the emissions allowance opportunities that companies have. Emissions trading, often referred to as cap-and-trade, is a system designed to reduce greenhouse gas emissions by creating a market for carbon allowances. Companies are given a set number of allowances that permit them to emit a specific amount of greenhouse gases. If they emit less than their allowance, they can trade or sell the excess to others, providing a financial incentive to lower emissions. This market-driven approach encourages companies to innovate and reduce their environmental impact while working within a regulated emissions cap, helping to fight climate change efficiently and sustainably. EcoCoin allows the everyday consumer to access these emissions allowances, giving them an opportunity to grow and benefit from this hidden market that not many people are knowledgeable about. Accessing these allowances can be done through the following coins: Toucan Protocol: Nature Carbon Tonne, Base Carbon Tonne, Moss Carbon Credit. Ecocoin makes it easy by providing all the necessary information to make the best decision on one page. Additionally, we provide users with possible ways to use their investment. They could use them by using them towards volunteering events, fundraising events, and staking opportunities. EcoCoin has the potential to empower the everyday consumer and help them approach the goal of financial freedom.",,,,,,,"blockchain, fastapi, google-could-search-api, google-firebase, neural-prophet, react",
HackHarvard 2024,Parkara,https://devpost.com/software/parkara,False,"On our way to Hack Harvard, we lost way too much time hunting for parking near the library. Being from NYC, we thought we could outsmart the system, but nope. After realizing there’s no app for finding free street parking, we decided to create one ourselves—because no one should have to endure that pain again.","We provide a heatmap that represents public parking availability in dense urban areas. Our focus is on free, street-level parking, including metered spots, but not on private garages or paid parking services. The app gives users insights into the probability of finding an open parking spot or the average time it takes to secure one. While driving, users receive assistance in finding parking through the display of likely open spots via a heatmap. This prediction is based on historical data about parking trends, factoring in variables like the time of day and the day of the week. Once users open the app, their driving routes are recorded, and any spots along the way where parking is unavailable are marked. When a user successfully parks, they simply tap a button to confirm, and we ask a few follow-up questions, such as whether they noticed parking availability elsewhere or if accessibility was an issue. To encourage user participation, we’ve introduced a contribution-based system. Users must share parking data to continue accessing the heatmaps. Initially, new users are granted a limited number of free requests, after which they must contribute more data to unlock additional insights. Users can earn points through various actions, such as marking when they leave a parking spot. Points are awarded based on the difficulty of finding parking in that area, and users can see their live-time rank as they contribute more.","We used React for the front end, MongoDB for the database, and Heroku for hosting.","We encountered several challenges with integrating the Google Maps API, particularly with implementing the heatmap, pinpointing user locations accurately, and ensuring the UI was smooth. Additionally, designing the algorithm to predict parking availability (hot and cold spots) proved more complicated than expected.","We’re really proud of the heatmap we were able to create, along with the shared database that connects all users. Initially, our app was limited to local testing, but we transitioned to a shared online database that aggregates parking data in real-time. In fact, to gather realistic data for our map, we went out and drove around late at night! The algorithm we built to calculate parking probability is something we’re especially proud of.","We deepened our knowledge of the Google Maps API, especially the Roads API, which turned out to be trickier to work with than we anticipated.","We want to enhance the gamification aspect to further incentivize users. Additionally, we plan to incorporate data on no-parking zones and parking restrictions (like fire hydrants or tow-away zones) to avoid marking these as false positives.","google-cloud, google-maps, heroku, hosting, mongodb, react, swift",https://github.com/Ajoukov/Parking
HackHarvard 2024,Vizzi,https://devpost.com/software/vizzi,False,Hearing the horrific stories of pregnant women in prison inspired us to advocate for change. Their resilience ignited our passion to raise awareness and provide essential resources to ensure these women receive the care they deserve during this vulnerable time.,"This project serves as a comprehensive platform aimed at supporting pregnant women in correctional facilities by addressing their unique nutritional and healthcare needs. It facilitates the collection of vital information regarding the dietary intake and health status of pregnant inmates. Correctional officers input specific data about meals consumed, as well as any vitamins or supplements taken, which will be analyzed to assess the nutritional health of the pregnant inmates. Based on this input, the system provides personalized nutritional feedback tailored to the specific needs of pregnant women, offering suggestions for dietary improvements to ensure that both mothers and their babies receive adequate nutrients.",React + Open AI,It was our first time using Open AI API so it was tricky,"We are proud of our accomplishment in creating a resource specifically for pregnant women in prison, a topic that is often overlooked. This initiative aims to provide essential information and support, raising awareness of the unique challenges these women face. Through this, we hope to contribute to discussions around their health and well-being during pregnancy.","Throughout this project, we developed crucial skills in time management and effective task delegation. We learned to prioritize our responsibilities, ensuring that each team member could contribute meaningfully without feeling overwhelmed. To prevent miscommunication, we held small meetings throughout the night. We also expanded our technical skills, such as gaining experience in using React and other UI/UX tools.","There are several features we wish to incorporate into our project but were unable to due to time constraints. We plan to continue developing this project in the future. One key feature we aim to implement is a database system to track the dietary intake of each pregnant woman in prison. This would provide more accurate output and generate useful data for healthcare professionals to assess potential health complications.
Additionally, we want to create a public user login feature. While specific information about individual inmates will remain confidential for privacy reasons, this feature will allow users to access aggregated data on the nutritional care conditions within each correctional facility. This transparency will help the public understand how women in critical situations are treated and advocate for better care and resources.","css, html, openai, react",
HackHarvard 2024,Stylete.app,https://devpost.com/software/stylete-app,False,"I've been inspired to create Stylete.app after learning about the massive amounts of fabric scraps, unsold inventory, and discarded clothing that end up in landfills. I saw an opportunity to connect waste generators with potential users, turning what was once considered trash into valuable resources for creatives, as well as anyone interested in being eco-conscious.","Stylete.app is a mobile marketplace that connects fashion companies, individual sellers, and eco-conscious buyers. It allows:",We built Stylete.app using a combination of technologies:,,,"I not only learned how to persevere, but I've also learned how to use PHP for backend. I've received a lot of guidance from not only mentors, but my friend, Andrew as well, and they've all taught me so much on how to integrate frontend and backend together.",,"css, html, javascript, json, php, phpmyadmin","https://stylete.app/, https://github.com/UberFoots/HackHarvard2024, https://www.canva.com/design/DAGTV0d3yFk/QHIJT91KEPt9B0M3xR8CSQ/edit?utm_content=DAGTV0d3yFk&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
HackHarvard 2024,VitalPath,https://devpost.com/software/vitalpath,False,"As international students in the United States, we've faced the daunting challenge of navigating an unfamiliar and complex healthcare system. Far from home and family, we've experienced firsthand how expensive and difficult it can be to access proper healthcare. This struggle inspired us to create VitalPath - a platform designed to bridge the gap between individuals and healthcare resources, ensuring people can stay informed about their health and seek attention before situations become critical.",VitalPath is a user-friendly web platform that:,Our development process involved several key components:,,"Despite the challenges, we're proud of several achievements:",This project was a significant learning experience for our team:,"We're excited about the future of VitalPath. Our next steps include: Our ultimate goal is to evolve VitalPath into a comprehensive platform for health monitoring and assistance, with a particular focus on serving underserved communities and individuals navigating unfamiliar healthcare systems.","javascript, machine-learning, node.js, python, react, typescript",
HackHarvard 2024,MiRTH,https://devpost.com/software/mirth,False,Countless people and families are affected by Alzheimer's. How can we help?,"We designed MiRTH (Mind Rehabilitation for Therapeutic Health) to minimize the effects of Alzheimer's in people, specifically helping the elderly. We are also focused on maintaining family connections and safety measures for those with Alzheimer's by maintaining a self-submitted daily journal entry of the person's day, then using AI to create habit patterns picked from the journals. Our LLM also assess how much the user's Alzheimers has developed, notifying families and friends listed as contacts within our app.","We used ReactNative, NodeJS, and AWS cloud services to create our app.",Creating a working environment for React proved especially difficult given our small timeframe. We created a small working prototype with React and the cloud services.,,"We learned a lot about web development, React, Javascript, and cloud services in our project!",,javascript,"https://github.com/OMarzLearns/Alzheimer-Helper, https://docs.google.com/presentation/d/1fpB9dWMZEJT6E781MrCXp2i85rKErejexI60cuG3oFU/edit?usp=sharing"
HackHarvard 2024,RareCare,https://devpost.com/software/rarecare,False,"One of the major inspirations of this project is when we got to know about the case of our team member who earlier met with a car accident. According to him, he didn't get the proper guidance from the doctors as the different doctors gave him different solutions. We have tons of examples from our society who needs help for their case from the right minds. That's how we figured it what to build for the patients who need help. We thought to create a platform where a panel of doctors can work together for a patient to provide a better solution.","RareCare is a web application designed to facilitate the management and research of rare diseases. It provides separate portals for patients, doctors, and researchers, each with specific functionalities:
Patients can view their health data, search for doctors, and book appointments.
Doctors can manage appointments, review patient cases, and conduct video consultations.
Researchers can access publications, collaborate with other researchers, analyze data, and provide insights on case studies.","Next.js as the React framework
TypeScript for type-safe JavaScript
Tailwind CSS for styling
Custom components (like GlassCard) for consistent UI elements
React hooks for state management
Tabs component for organizing different sections within each portal
Some AI tools","Creating a unified yet role-specific interface for different user types
Implementing secure authentication and role-based access control
Designing an intuitive UI for complex medical and research data","Developing a comprehensive platform that serves patients, doctors, and researchers
Creating a visually appealing and modern UI with a consistent design language
Implementing features like video consultations and data analysis tools","How to structure a complex application with multiple user roles
Techniques for creating reusable components in React
Best practices for handling medical data and research information","Implementing real authentication and database integration
Adding more advanced features like AI-assisted diagnosis
Expanding the data analysis capabilities for researchers
Enhancing the collaboration tools for the research community
Implementing a mobile app version for better accessibility","ai, css3, next.js, react, reacthooks, tailwind, typescript",https://github.com/iftekharanwar/RareCare
HackHarvard 2024,Envest,https://devpost.com/software/envest-otfpqy,False,"Sustainability is no longer a nice-to-have, it’s essential for future-proofing investments and businesses. Investors struggle to find companies that align with their sustainability goals, while companies face challenges identifying best practices in environmental responsibility. Our team saw the need for a solution that helps both investors and businesses thrive in the sustainable economy.",Our platform helps investors and companies alike by:,"We utilized cutting-edge technology to bring this vision to life. For the semantic search, we used a vector database (SingleStore) and cosine similarity to allow investors to search for companies based on specific sustainability traits. We integrated Hugging Face models to generate embeddings that power the semantic search. On the data analysis side, we used NumPy and scikit-learn to implement linear regression, predicting future environmental risk scores for companies. The environmental risk predictions and comparisons with the S&P 500 are visualized through Chart.js, giving users clear and insightful data representations.","Building an app with this level of complexity came with its challenges. Implementing accurate semantic search required careful tuning of the embeddings and vector search algorithms. Collecting and standardizing environmental data across a large number of companies was also a big hurdle. Additionally, fitting linear regression models to predict future risk scores based on historical data required rigorous testing and validation to ensure accuracy.","We are proud of the seamless integration of advanced technologies like semantic search and predictive analytics. Our vector search engine allows investors and sustainability teams to find relevant companies with incredible ease. Our linear regression models give forward-looking insights, helping investors and companies make data-driven decisions to improve sustainability. Most importantly, the potential for this tool to drive real-world environmental impact makes this project truly rewarding.","Throughout this project, we learned the power of combining machine learning with sustainable business practices. We deepened our understanding of semantic search algorithms, vector databases, and regression modeling for predictive analytics. We also realized how crucial well-organized data is to create meaningful predictions and insights. Building a product that bridges sustainability with investment strategy was complex, but it gave us experience in balancing technical precision with user-friendly design.","Our next step is to expand the platform by incorporating more dynamic data sources, including real-time environmental news and company sustainability reports. We want to broaden our recommendation system, providing even more refined suggestions for portfolio improvement. On the company side, we plan to introduce tools that allow businesses to simulate the impact of implementing specific sustainability practices, helping them visualize potential gains in environmental scores.","bootstrap, huggingface, javascript, mysql, node.js, numpy, react, scikit-learn, singlestore",https://github.com/Boris713/HackHarvard
HackHarvard 2024,Infinite Roads,https://devpost.com/software/infinite-roads,False,One of our HackHarvard team members was late because of the poor public bus transportation.,Game where you build your own public transit system in a subsection of Boston.,Using Unreal Engine with the blueprint system,Creating roads with the Unreal Engine spline system. Polishing a game project in a short time period.,"Creating an engaging gameplay loop, getting our playtesters to have fun.",We learned how to use many different Unreal systems and how to quickly organize work in a large Unreal project.,"If we had time to add more features, we would add a wider range of public transit and create multiple clearly defined levels in different subsections of Boston.","blender, maya, unreal-engine",https://jmandelbrot.itch.io/infiniteroads?secret=r7FLf3NYoTH6F2TQX8ErOErP8
HackHarvard 2024,mindshort,https://devpost.com/software/mindshort,False,Youtube algorithm. The same way Youtube is able to generate a short feed a user should be able to see how their previous history is impacting future recommendation,Labels shorts at run time.,https://github.com/jonss0777/HackHavard,"Finding a way to retrieve Youtube short’s description information. Options include using the YouTube Data API for video details or employing web scraping if the API doesn’t suffice. Understanding YouTube's HTML structure is crucial, along with adhering to any usage restrictions. Model user behavior: analyzes how users interact with YouTube Shorts, focusing on metrics like viewing duration, engagement (likes, comments, shares), and demographics. By applying data analytics and machine learning, trends can be identified to inform content recommendations and enhance user experience. Training labeling algorithm: involves developing a machine learning model to categorize YouTube Shorts by content type. It begins with compiling a labeled dataset for supervised learning, allowing the algorithm to improve its classification accuracy. The goal is to automate labeling, making content easier to organize and discover.",Facing the different challenges involved in creating a chrome extension.,"How chrome extensions work.
How to manipulate large html documents",Continue to build on our ai model. Include more parameters and create a website where the user can visualize the create content data.,"css3, html5, javascript, python",https://github.com/jonss0777/HackHavard
HackHarvard 2024,Luna Tech,https://devpost.com/software/luna-tech,False,"Before 1993, women were rarely included in clinical trials. Even today, women still remain underrepresented in medical research, vastly limiting our understanding of how women, particularly women of color, react to and experience drugs and other medical procedures. Existing research published in medical journals typically remains unseen from the general public, and nuances of the research are easily lost when relayed through news articles or social media. We aim to consolidate all existing medical research about women's health and make findings more accessible to the general public.","Our website allows users to search questions (""How can I increase my fertility?"", ""How can I feel less bloated during my period?"") and receive well-informed, research-based advice about lifestyle changes and dietary supplements to take. Our model takes into account personal demographics (age, race, previous medical history) and live-time data (BPM, menstrual cycle duration, oxygen levels, sleep duration) and consolidates it with published research papers to make a well-informed decision. Live-time information, such as BPM, menstrual cycle duration, and sleep duration, is also displayed on the dashboard.","For frontend, we used Node.js. 
The model itself utilized an algorithm similar to mixture of experts. Many small LLMs, each an expert on a specific topic (sleep, menstrual cycle, drug interactions) generate advice with a corresponding confidence score. A larger LLM takes these responses and produces a distilled piece of advice that is well-informed, accurate, and comprehensive.","After training the smaller LLM models, we discovered the advice was too generic and sometimes even included other genders. This was due to one of our decisions early on, where we realized our original idea of finding the 50 most relevant research papers to train our small LLMs was infeasible for the scope of the project in terms of time, resources, and compute. We decided to stick with our original decision to create smaller, less knowledgable LLMs, and take the tradeoff for accuracy with functionality. In the end, we prompted the model with the target gender and shelved the task to make more accurate LLMs for the future.","One accomplishment we're proud of was our task delegation. All four team members were experienced in different parts of the tech stack, and our distribution of expectations and features to ship were evenly distributed. Everyone collaborated, communicated, and worked together, and learned about the other parts of the tech stack as well. Being collaborative came naturally to us, and as a team we worked very efficiently. Additionally, we are proud of the realistic pacing of the scope of the project. We finished just on time, and the features that we deemed infeasible had alternatives that worked almost just as well. Overall, we worked well as a team, and decided on a challenging yet doable project to complete.","We learned a lot of technical skills while working on this project. None of us had worked with LLMs before, so learning how to train such a big model and learning to tailor our queries successfully took a lot of research. We also became familiar with different parts of the tech stack that we weren't familiar with before- for some of us, it was our first time learning how to write APIs and how front-end and back-end were connected!","For Luna Tech, one of our first priorities is to improve the smaller LLMs by training them on larger, more curated datasets tailored to women's health. With more time and compute, we can train our LLMs to become true experts that are up-to-date with the latest publications. With this, we can also include more topics for our smaller LLMs, including ones knowledgable about microbiomes, exercise, and diet. In the future, we can use unsupervised learning to select the topics that are most relevant to the search criteria. Furthermore, we hope to expand wearable device data integration. By integrating wearable devices like FitBit or Apple Watch, we can track additional biomarkers like glucose levels and heart rate variability to give the model a better picture of the user and their needs. In the far future, we hope to create a device that can track hormones, which would be extremely useful in assessing health during menstruation, pregnancy, and postpartum.","ai, cloudflare, css, flask, javascript, llms, ml, postgresql, python, scikit-learn",https://github.com/assasin2gamer/HackHarvard24B
HackHarvard 2024,HackEnzyme,https://devpost.com/software/hackenzyme,False,"Protein Language Models are an exciting new area of development. We wanted to see how they could be applied outside of merely obtaining protein embeddings—to generate meaningful proteins. Enzymes are almost unparalleled in their ubiquity and usefulness throughout nature. Therefore, we saw it a natural fit to try to generate enzymes for our project.","HackEnzyme takes in molecule strings called SMILES, and outputs the predicted amino acid sequence that breaks down or combines the substrate.","Our first goal was to acquire a dataset, we scraped multiple Rest Api’s to have a label: enzyme sequence: with corresponding: chemical compound strings, smiles. From this, we utilized models to map smiles strings into embeddings, and then appending back to its corresponding enzyme sequence. We fine tuned Roslab.Prot_T5_Large on our bioinformatic dataset that we created earlier, integrating into our web application. We implement esm-fold which maps protein sequences to 3d structures to better understand the output of our model.","Lack of datasets
Mapping smiles sequences to embedding space
File type embedding complications
Problem with Implementing the Model 
Bypassing encoders
Training the Model 
Lack of Computational Power
API integration with the front-end
The model
3D Protein Sequence Modelling
Chat-GPT LLM Integration","We are proud of our ability to collaborate on a topic that the majority of the team lacked understanding of. There was common discussion about different project ideas because of our lack of confidence; however, our perseverance and restlessness are what made our project pull-through.","Better understood transformer layers
3D Protein Modelling and Full-Stack Integration
A lot of Biology","Optimizing the base model
Using more parameters
Training the model on a bigger dataset
Fine tuning on multiple sequence alignments, chemical properties, and functional annotations to create a model that can be used to generate enzymes with specific properties outside of their input and output.","bootstrap, css3, esmform, html5, huggingface, javascript, numpy, pandas, python, pytorch, transformer",https://github.com/Klahadore/HackEnzyme
HackHarvard 2024,Aria,https://devpost.com/software/aria-sc10g2,False,"     In many parts of the world, access to high-quality medical care is delayed, limited, or prohibitively expensive, often leading to worse outcomes for patients. This disparity is even more pronounced in rural or underserved regions, where specialist care is scarce or non-existent. As we face the reality of aging populations and a rising demand for healthcare, the need for quick, reliable, and accessible medical expertise has never been more pressing. Even in well-developed healthcare systems, patients often experience long wait times for appointments with specialists, which can be frustrating and sometimes dangerous. Bureaucracy, administrative delays, and the complexities of navigating healthcare systems can make it difficult for people to get timely diagnoses and treatments. This is compounded by the heavy paperwork and insurance hurdles that make even routine care a drawn-out process. Aria is meant to deconstruct these barriers. By creating a highly automated, real-time conversational medical expert, we’re providing people with fast, specialized care wherever they are. Whether it’s through an accurate diagnosis, immediate referral to a nearby physician, or a detailed health report, Aria is always at service of her patients. In a world where every minute counts, Aria delivers high-quality healthcare faster, cutting through the red tape and providing critical insights when they’re needed most. Most of all, she is very easy to use, with minimum technological barrier to entry, and suitable for patients of all ages, educational levels, and backgrounds. The potential for this technology to save lives, reduce healthcare inequities, and alleviate the burdens on strained healthcare systems is immense. It’s not just about convenience; it’s about transforming healthcare accessibility for millions. Thus, Aria's name is inspired by ARIA (Accessible Rich Internet Applications), underscoring our foundation in and commitment to healthcare accessibility. Aria is the world's first fully-automated medical expert that can make reliable diagnoses from natural conversation. Patients can describe symptoms, ask questions, provide their medical history, and even show video evidence of symptoms on the body in real time to reliably receive diagnoses and medical advice. Key Features: Aria is a culmination of various moving parts, seamlessly integrated into one solution. Real-time communication with Aria was a challenge due to interruptions and the complexity of making the AI persona follow conversations smoothly while maintaining speed. We found that Anam’s beta API didn’t allow for interruption or natural-flowing conversation when using a custom intelligence pipeline, so we had to build a lot of the intermediate infra using a separate audio stream detection package called Hark, which we used to build our own interruption/free flow conversation capabilities for Aria. 
We also struggled with building a fast and accurate decision-making pipeline, given the numerous inputs like medical knowledge, images, and user interactions. We worked very hard to make Aria very fast while also handling the heavy processing required to deliver reliable medical advice in real time. We also worked on building robust knowledge graphs for Aria to traverse in her Q/A actions, as we wanted more deterministic outcomes in diagnoses and decision making, but we struggled to incorporate them efficiently into our low latency workflow. However, that’s definitely something we want to do in the future. State of the Art:No large language model today is able to provide consistently reliable medical advice. Aria, however, beats multiple benchmarks that demonstrate performance exceeds those of large language models such as GPT-4o. This can be accredited to the massive knowledge base (many thousands of documents) of high-quality medical data (Medline, PubMed, etc papers) that Aria sits on top of. In particular, we comparing the recommendations provided by ChatGPT and similar tools with Aria's recommendations, Aria was able to detect nuances among the symptoms and give specific (and accurate) diagnoses, while other tools give broader recommendations. Speed:Aria is the world's first real-time visual persona for medical purposes. Real-time personas are at the cutting-edge of present work, and not many people have been able to nail them down. For our particular application, not only are we able to build a persona that operates in real-time, but we are able to do so despite the maby operations and processes that occur in the ""brain"" of the persona. Medical decision making is a heavy process, and especially with so many sources of input to this decision-making process (knowledge base of thousands of documents of expert knowledge, image input, symptom descriptions, personal health records) latency can increase very easily. We pride ourselves on Aria's ability to have such seamless conversation despite having so many sources of information and so many decisions to make. Seamless conversation is critical to making healthcare accessible as everyone should be comfortable using this product. Anam Testing:Anam is a pre-seed startup building realtime AI personas capable of having conversations at talking pace, giving faces to many headless agentic AIs. Although they have built an impressive product that has great potential to create value, their API is not perfect and has many issues (as it is still in a closed beta), which is natural for such an early stage startup. As beta testers for Anam, we were building in the dark and found many points of breakage in their product. Communicating directly with the founders to iterate quickly, we not only accelerated our own pace, but also helped a valuable company do the same. In doing so, we stripped their product down to to its core, reverse engineered certain components, built a better ""brain"" (that generates the reasoning for the agent) for our particular use-case, and contributed to Anam's product roadmap by enlightening the founders to ways they hadn't seen developers would use their product. Developing Aria taught us the complexity of maintaining fast, accurate, real-time conversations, especially in a field as dense as medicine. Processing voice, image, and text inputs rapidly, while ensuring reliable medical advice, was one of our biggest challenges. We quickly realized the vastness of medical knowledge and the difficulty of extracting relevant information in real time. Handling thousands of medical documents and providing personalized diagnoses required immense technical refinement. Medicine is not just data-driven but deeply personal. We learned that empathy and human-like interaction are crucial in building trust with patients, which led us to prioritize Aria’s conversational abilities alongside her diagnostic expertise. Working with Anam, an early-stage startup, exposed us to the challenges of imperfect technology. We were often problem-solving on the fly, which not only enhanced Aria but also contributed valuable insights to Anam’s product development. The grand vision for Aria is to be an end-to-end one-stop-shop for all health advising. Future versions of this product will include integration with hardware that could collect more data that doctors collect (such as heartbeat, blood pressure, etc.). They will also include support in more languages, which is imperative to reach the goal of accessibility. Furthermore, we aim to build partnerships with healthcare providers and insurance companies to streamline patient referrals and coverage for the treatments Aria recommends. Such partnerships will also increase Aria's personalization, as all patient health data will become integrated. In addition, future integrations would include wearable devices to offer continuous health monitoring and real-time recommendations based on the data collected, further reducing the gap between the onset of symptoms and receiving medical care. Our long-term vision is to create a comprehensive healthcare ecosystem where Aria doesn't just diagnose and refer but actively participates in a patient's ongoing care journey, from prevention to treatment and beyond. Made with ❤️ by Lavik Jain, Dinesh Vasireddy, Pranav Ramesh, and Nikita Mounier",,,,,,,"anam, beautiful-soup, firecrawl, groq, llama, llava, mixtral, openai, python, react, supabase, typescript","https://ariamed.vercel.app/chatroom, https://github.com/DineshTeja/aria, https://youtu.be/lL6MhEyyubA"
HackHarvard 2024,Query on Reality,https://devpost.com/software/query-on-reality,False,"Behold the glory that is Query on Reality, the shining beacon of clarity in the sprawling wilderness of data overload. Data scientists, researchers, and those brave enough to embark on the treacherous path of exploratory analysis, rejoice! Gone are the days when you would sit, staring at an ocean of numbers, waiting for the magic of insight to wash ashore like a forlorn message in a bottle. No longer must you bow before the chaos of datasets! With Query on Reality, your destiny lies in creating a beautiful, ever-expanding mental map of connections, relationships, and jaw-dropping realizations about the world around you. Yes, I said jaw-dropping. It's science.",,,,"Oh, where do we even begin? First, we harnessed our deep, arcane knowledge of data science to set up key prompting questions for our API endpoint. With this in hand, our initial exploratory data analysis wasn’t just good—it was legendary. We seamlessly combined insights from research papers into the analysis, effectively bridging the gap between raw data and scholarly wisdom like the grand wizards of academia. But it didn’t stop there. Oh no. We unlocked a secret power: API threading optimization. The more nodes we added to the graph, the more our approach seemed to laugh in the face of long response times. With threading magic in full swing, we found a loophole that allowed us to cut down waiting time significantly, even as the graph grew more intricate than the plot of a sci-fi epic. Of course, we had some technical dragons to slay along the way. One particularly troublesome foe on the front-end side was creating new nodes without overlapping them with previous ones. Cue our algorithmic wizardry. We devised an ingenious method to ensure that each new node maximized its distance from the others, achieving harmony in our ever-expanding data universe. On the backend, we waved our metaphorical wands and abstracted away all those pesky details like node definitions and the mechanics of generating new nodes. Now, instead of getting bogged down in minutiae, we stand proud, watching as our tool spawns beautifully structured graphs at the click of a button. In short, we didn’t just overcome technical challenges; we sent them running for the hills.","Ah, the timeless tale of discovery. Like brave adventurers navigating the treacherous terrain of software development, we realized that an effective product is more than just lines of impeccable code—although, yes, we did write some of those. The real magic lies in balancing the clean, coherent logic of the backend with a creative, intuitive interface that makes the user feel like they’re gliding through the Matrix (without the red pill). The biggest challenge, however, wasn’t just building these pieces—it was figuring out how to integrate them smoothly.* Picture this: each of us, like master artisans, was crafting our own features in our own distinct style, employing models and methods that were as different as night and day. But when it came time to stitch them all together, the middleware—ah yes, the middleware—became our proverbial mountain. Integrating everything was like a puzzle where each piece was shaped by a different philosophy. And so, we learned the hard truth: effective communication is not just a ""nice-to-have,"" it’s the glue that binds the project together. We realized that while short-term clarity is crucial (because, let’s be honest, explaining why your code is a work of art to someone who sees it as a Jackson Pollock is a unique joy), the long-term vision matters even more. We learned that sharing a common understanding of how all our parts fit into the larger system is essential. It wasn’t just about making sure our individual features worked—it was about seeing the bigger picture and creating something greater than the sum of its parts. And let’s not forget the unsung heroes of this journey—our friendship and a hefty supply of Celsius energy drinks. Together, they fueled the all-nighter of all nighters, where we dug deep into our strengths, learned more about each other’s superpowers, and found a rhythm that synergized our workflows like a well-oiled machine. In the end, we didn’t just build a product—we built the kind of teamwork that legendary startups are made of.",,"ai, exa.ai, fast-api, next.js, opendata, react, react-flow, tailwind",https://github.com/Eli6th/HackHarvard2024
HackHarvard 2024,I CARE 4 DOCTORS,https://devpost.com/software/i-care-4-doctors,False,"We 3 are doing an Internship at Boston Children's Hospital, and we want to improve medical care.","It is a numerically-driven fatigue detection system that leverages biometric data to assess and monitor healthcare workers’ fatigue levels in real-time, aiming to reduce medical errors, improve decision-making, and enhance both worker well-being and patient safety by enabling timely interventions and promoting optimal performance during critical tasks.","We used pre built machine learning eye recognition algorithms, Matlab, Python libraries and the dashboard for data visualization was made with Sveltekit and the MongoDB Atlas cloud database","Time, Integration of components, new technology exposure.","Explore a new field we have not worked on in our hospital, we made a functioning machine learning image recognition program. We made a full stack application with a framework we never used before (SvelteKit)","Implementation of machine learning data and AI for data extraction, Server side rendering.","Collect Training Data & Calibrate the Model
Determine how many of the variables measured would be enough to categorize the doctor with fatigue.
Adapted learning for each individual.
Conduct clinical validation studies.","javascript, matlab, python, sveltekit",https://github.com/GilVA1/ICARE4DOCTORS
HackHarvard 2024,DocScribe,https://devpost.com/software/docscribe,False," In 2019, the World Health Organization found that medical errors are a leading cause of death and injury, amounting to a deficit of 37.6 to 50 billion USD loss through added health care, disability, and lost productivity. These errors are defined as the failure of a planned action to be completed as intended or the use of a wrong plan to achieve an aim. [1] DocScribe focuses on tackling both definitions of error by removing the variability between the doctor and the patient. Potential points of variability could include: At the start of the appointment, the doctor and patient will engage in a standard conversation about concerns and symptoms. During this, DocScribe will instantaneously transcribe all of this information and provides further insight upon medications and diagnoses that may be undercovered from human mistakes. This insight has many implications. It can range from a simple transcription of what happened during the appointment to analyzing and validating the responses that both the doctor and patient make. For example, if the patient contradicts themselves by saying they have migraines daily, but later admits to only having migraines occasionally, DocScribe will flag this as a concern and will prompt the doctor to question the patient about the inconsistency. In the same light, if the doctor prescribes a medication without providing reasonable side effects, DocScribe will alert the patient to question if this medication will be right for them. To make sure that all questions are answered properly, clarifying questions must be answered before the doctor can progress to the next screen. After the appointment is complete, the doctor has access to a private screen, which has holistic evaluations of their performance, predictive improvements based on trends that can be made for future appointments, and an updated patient history PDF. This PDF is in the same format as the entrant PDF, which allows DocScribe to adapt to fulfill both the doctor's and patient's inconsistencies in appointments. In the long-term, these three tools will independently improve a doctor's performance for appointments. Having summaries is important for the patient. On average, after a week of the visit, 15% of patients erroneously recall and 36% need a prompt to recall information, so having a summarized, easily accessible, and mutable source is key to information security and adaptivity. [2] Similar to the approach to the problem, DocScribe was created to solve specific issues. There are several issues in the current process between patients, doctors, and researchers. In 2013, Invokana hit the market and was considered a revolutionary drug. However, there wasn't a lot of research to determine that it was a drug with a major downside, and it resulted in thousands of direct amputations, ultimately leading to this drug being black-boxed. Since DocScribe utilizes AI, it is adaptive and is constantly updating to match whatever research is happening in the field so that a mistake like Invokana doesn't happen again. [3] In a similar light, Avandia was a drug that was intended for patients with diabetes. However, there were conflicting studies on whether it raised heart problems or whether it only had positive effects. Since there wasn't a holistic databse to this information, different doctors prescribed different medications, but later research justified the increased heart problems to be legitimate. This drug was also black-boxed, and DocScribe could have been transparent with the conflicting evidence rather than doctors only evaluating one side of the story. [4] DocScribe has an individualist, engine mentality. For the best functionality, previous PDF reports will be used to maximize efficiency and minimize errors during each visitation. This means that each patient can be assured that they are receiving care that fits them rather than a one-size-fits-all solution. DocScribe is not a replacement for a doctor. This allows patients and doctors to maintain autonomy over the decisions that they make. It is merely a tool that listens into conversations to not only identify plausible mistakes, exaggerations, or missed information during appointments but to also help the doctor understand which information to clarify with their patient before concluding their appointment to significantly decrease the chances of medical errors being made. DocScribe's last screen, Reports, is also very crucial to helping the doctor improve their diagnosis and understanding of a patient's symptoms and conditions with the AI-generated feedback coming from the original Appointment transcript. The purpose of this project is to really narrow down the chances of medical errors made during appointments and medical diagnosis, between both the patient and the doctor. As stated before, the goal is to not only narrow down on the deficit of added healthcare costs and lost productivity, but also to ensure that patient safety is secured and prioritized, and while it does not guarantee that medical errors will be erased, it significantly brings them down. We focused on creating a minimum viable product (MVP) during HackHarvard. This leaves room for growth. The below are sample regions for improvement: Q: Will DocScribe work if the doctor cannot be present (i.e. post-operation)?
A: Doctors will supply DocScribe with supple information, and DocScribe will become more authoritative in the one-on-one interaction with the patient. Q: How will DocScribe work with patients that don't speak English, patients that are deaf, and non-verbal patients?
A: This information will be stored in the PDFs under the Patient Info tab. This would require modifications to the LLM prompts, utilizing a webcam feed for sign language, and a textbox with pop-up notifications respectively. Q: How will DocScribe improve the long-term quality of doctor reports?
A: DocScribe will store the improvements from the PDFs over an extended period time, analyze common trends that the doctor messes up at, and provide predictive solutions to improve the quality of future visits. Q: DocScribe's current system implies that a patient will see the same doctor throughout future visitations. What if a patient sees a different doctor?
A: DocScribe will divide patient reports and doctor reports. This will add better adaptations to the model during the visit. Q: What if patients are not comfortable with sharing details about their life to an AI?
A: DocScribe will use aggregates (for example: if cholesterol runs in your family, you have a heightened chance of heart failure) to allow patients to be informed while maintaining anonymity. A couple of implications are that there is a microphone that picks up both the doctor and the patient and that both are understood by the model. [1]https://www.ncbi.nlm.nih.gov/books/NBK519065/#:~:text=According%20to%20the%20Institute%20of,and%201%20in%20854%20inpatient[2]https://pubmed.ncbi.nlm.nih.gov/29389994/[3]https://www.fda.gov/drugs/fda-drug-safety-podcasts/fda-drug-safety-podcast-fda-confirms-increased-risk-leg-and-foot-amputations-diabetes-medicine#:~:text=On%20May%2016%2C%202017%2C%20based,of%20leg%20and%20foot%20amputations.
[4]https://www.npr.org/sections/health-shots/2010/07/09/128406505/fda-documents-show-diabetes-drug-avandia-increases-heart-attack-risk",,,,,,,"chatgpt, dall-e, fitz, flask, git, github, langchain, natural-language-processing, python, react, spacy, spline, tailwindcss, whisperapi",https://github.com/ppilli1/DocScribe
HackHarvard 2024,BurnGPT,https://devpost.com/software/burngpt,False,Wanted to do the funny track :)),LLMs roast each other,"Claude, GPT, and LLAMA",Pivoting last minute,,Not to pivot too last minute.,More user involvement!,"claude, gpt, javascript, llama, nextjs",
HackHarvard 2024,HelpSignal,https://devpost.com/software/helpsignal,False,"On average, an EMT can take 10 minutes to arrive at the scene of an emergency while incidents such as choking or heart attacks can turn fatal within 3 minutes. Those 10 minutes between the start of the emergency and when help arrives are vital in the patient's survival.","Any surrounding good-Samaritan may use the app, press SOS, use their voice to explain the situation, and the app will ping nearby certified CPR, EMT, or any person will relevant experience who can arrive on the scene before 911 can. HelpSignal is used to make the most of the time between the start of an emergency and when ambulances arrive","We used React Native and Expo Development to build the application, targeting Android for live voice transcription from expo-speech-recognition and sending the transcription after recording to Cloudflare Worker. The Cloudflare Worker then uses the BAAI general embedding model to vectorize the transcription. The categories of needed certifications or experience are in a vector database, and vector search is done to get the most relevant person for the situation. The account system is on Amazon RDS, as well as the current emergencies. After an emergency is categorized, it's put onto the database, which is called on every refresh by people with accounts and certifications. A map is shown on the page to show locations of emergencies.","We had difficulty implementing the audio as none of us had access to iOS development kit, nor macOS laptops for running Expo Development on iOS. In order to record and collect audio to transcribe live, an Android system was needed. We spent a considerable amount of time setting up the Android SDK.","Throughout this project, we encountered many different roadblocks, which required determination and flexibility to get around. As a group, we were able to effectively communicate and pivot roles on the fly. As a result, we all stayed occupied and spent all 36 hours wisely designing and implementing different systems. Our feature of using Cloudflare Workers to use vector search was a big accomplishment for us, as well as getting authentication and accounts working with the stored certifications and experience, and an engaging UI/UX.","Coming into this project, few of us had experience with React Native, and some of us had no experience coding with TypeScript and React in general. This seeming roadblock forced us to learn syntax and techniques for working with the technologies on the fly. Additionally, getting Expo Development working with Gradle and running on an Android simulator was a big learning experience for how Android development works.","Being able to grow HelpSignal through advertising and social media would not only allow HelpSignal to become more popular, but would also improve the app. As more and more users get onboarded, there's more people available to help others, and therefore more of a chance that there's people to help in case of emergency. Using WebSockets instead of database updating for the emergencies would also let updating of emergencies be more instantaneous, and push notifications would allow for people not currently using the app to be notified when someone needs help. Connecting users with 911 while submitting an emergency would also allow for police to still be notified as normal. ","amazon-rds-relational-database-service, amazon-web-services, cloudflare, expo.io, flask, javascript, python, react-native, typescript",https://github.com/josephHelfenbein/HelpSignal
HackHarvard 2024,Queue-1-1,https://devpost.com/software/queue11,False,"Our inspiration for Queue-1-1 (Q11) came from witnessing the immense strain on small-town 911 call centers, particularly during high-traffic emergencies. Long wait times for critical calls in smaller communities can be life-threatening, and we wanted to create a system to help prioritize those calls and optimize response times. At the same time, we realized that large cities also suffer from similar issues, inspiring us to find a solution.","Queue-1-1 is an intelligent triage system designed to assist 911 responders by prioritizing emergency calls during periods of high volume. By leveraging machine learning and AI, our system analyzes live call recordings to extract key information about the emergency and assigns a priority score. Providing a priority score allows dispatchers to focus on critical situations during heavy traffic. Q11 also integrates cloud telephony to play automated prompts, record calls, and notify callers that their information has been captured and will be addressed as soon as possible.
We also wanted the 911 operator to be able to view all the essential variables of the incoming calls on a straightforward interface. Thus, we created the dashboard for Queue11, which displays an organized triage system of calls. The system displays this information on a dashboard that includes the time of the call, address, caller details, and a brief description of the emergency. It also groups related incidents during crises, making it easier for operators to manage high volumes of calls.","We first implement Plivo, a leading cloud telephony provider, to play an automated message to callers, prompting them to provide their name, address, and description of their emergency. Plivo then recorded the call, and we used OpenAI's Whisper API to transcribe the audio. After getting a text transcription of the call, we condensed the transcription to extract key information using OpenAI's GPT-3.5 Turbo. This text transcription was run through a machine learning model that predicted the priority level based on the condensed description. This involved extensive data preparation and feature engineering. We used Pandas for data manipulation, the TF-IDF vectorizer from Scikit-learn to convert the call descriptions into numerical features that the machine learning model uses, and the Random Forest Classifier was then used for classification. This model works by creating multiple decision trees that each classify the call based on subsets of the data. This processed information was then securely stored in an Amazon Web Services S3 bucket, making it readily accessible to emergency personnel. We then used Plivo again to make a call-back to our callers, informing them that their data was successfully captured and that they would receive a call-back once an operator was available. We used JavaScript, HTML, and CSS as the foundation of our website, and Figma was used to draft the prototype. Our code employs JavaScript arrays to store and display caller information, including name, address, and time of the call, for the 911 dispatcher. We also implemented custom JavaScript functions to detect and group duplicate calls about the same incident, improving the system's efficiency.","One of the significant challenges we faced was working with various voice providers. We had difficulties with poor recording quality and integrating different services like Plivo and AWS. We spent hours and hours attempting various voice providers but ran into multiple obstacles, such as low recording quality and paywalls. This led us to delve into the specifics of their APIs to understand how they worked and how to make them perform better. 
Another challenge was optimizing the machine learning model to handle unstructured emergency data and ensure it provided accurate call prioritization.","We're incredibly proud of integrating various complex technologies into one cohesive system. Creating all the puzzle pieces was one challenge, but having a grounded framework that consistently works was a significant accomplishment.
Initially, we worked with 2.9 million call entries, but with SMOTE (Synthetic Minority Over-sampling Technique), we expanded this to nearly 7 million, allowing for more accurate training of our model. As a result, our model achieved an impressive accuracy rate of 93.71%.","This project gave us a deep understanding of telephony APIs, specifically how to integrate and troubleshoot Plivo with Open AI. We also learned how to handle and preprocess large datasets for machine learning, specifically with imbalanced emergency call data. Our team honed our skills in both front-end and back-end development, and we strengthened our ability to work under pressure and adapt to technical challenges.","We plan to expand Queue-1-1's functionality by incorporating more advanced speech recognition features and integrating location-based services to identify nearby emergency responders better. We also aim to improve the machine learning model's ability to prioritize calls based on historical data trends and explore the potential to add datasets across North America. Additionally, we would like to implement more features that could be used in different crises. For instance, we could integrate the number of available resources, like fire trucks, ambulances, etc., to Q11 to allow for other approaches on how some calls may not have resources available to respond to the emergency.","amazon-web-services, css, figma, gpt-3.5-turbo, html, javascript, joblib, machine-learning, openai, pandas, pip, plivo, python, randomforestclassifier, scikit-learn, smote, td-idf-vectorizer, vercel, whisper-api",https://github.com/abelalexander/Queue11
HackHarvard 2024,CogniSpeak,https://devpost.com/software/cognispeak,False," Our project aims to leverage cutting-edge machine learning and clinical cognitive computing (MLCC) techniques to detect early signs of dementia using voice recordings. Dementia is a progressive condition that affects cognitive functions like memory, thinking, and communication, and early detection is crucial for effective intervention. More than 55 million people live with dementia, with 10 million new cases occurring each year. In addition more than 60% of dementia patients live in low and middle-income countries, highlighting the need for accessible monitoring services. Research has shown that cognitive decline often manifests in subtle changes in speech patterns long before more noticeable symptoms appear. Our system focuses on detecting these changes using machine learning, enabling a quicker and non-intrusive method of early dementia detection.
This innovative approach provides a low-cost, accessible tool that can help in proactive cognitive health management and potentially slow down the progression of dementia through early intervention.",,,,,,,"cloudflare, defang, fastapi, javascript, next, python, scikit-learn","https://github.com/TheFloatingString/CogniSpeak, https://cogni-speak.vercel.app/"
HackHarvard 2024,WHEN???,https://devpost.com/software/when-ht7x9p,False,"In the heart of evolving smart cities, with digital technologies to reimagine and redefine urban living, we noticed a very important lacuna: collaboration tools. WHEN?? draws its inspiration from our vision to contribute toward a smart city by finding one of the most vital solutions for urban challenges: making it easier for busy people to find the time to connect. This chance to make a huge difference in time management and collaboration came at such a time when cities were looking at how technology could increase efficiency, sustainability, quality of life, and services. Whereas existing tools such as When2meet can work to some degree, they aren't utilizing digital capabilities for the dynamic needs that modern city life throws at them. We set out to develop not just a solution which would simplify the way of scheduling but do so in a way that stays true to the core principles of smart cities: Our inspiration was informed by the belief that, in a smart city, even the most mundane yet fundamental tasks of city living-such as meeting and event coordination-can be improved through appropriate deployments of technology.",WHEN?? introduces the concept of team scheduling with a set of features that are intelligently designed for today's urban dweller. It includes:,,,"As we reflect upon our progress, which began at HackHarvard 2024, we are proud of how far this has taken us in bringing WHEN?? into reality: These are huge milestones toward changing the way city people will schedule. Each piece that we finish gets us closer to a complete-and-easy-to-use environment that will ultimately enhance collaboration and time management for residents in smart cities.","Our journey in developing WHEN?? at HackHarvard 2024 has really been an innovation crucible that pushed us to forge our ideas into reality under that time pressure. Really, this has been a class in tech and teamwork, tucking insights that are sure to shape our future efforts for life. Herein lies the art of digital urban design:
In designing the WHEN?? interface, we also came to understand the balance required between functionality and simplicity. From login to team scheduling, every page designed was another lesson in digital space creation-feeling intuitive to navigate, just as well-planned city streets should be. This was more than a competition; it was a miniature of the very urban environment we were trying to improve with its rapid pace, its challenges, and its opportunities for innovation around every corner. As we go forward in continuing to develop WHEN??, we bring with us not only new skills but a renewed passion towards creating technology that pulses with the lifeblood of our cities.","Next steps that extend our achievements at this time include the following: Some further ideas to enhance the impact of our tool on urban life and beyond include: As we build out WHEN??, our vision will not change-developing a scheduling solution that is able to answer not only the call of living smart city-style but, importantly, also contributes positively toward upgrading both the quality of life and work effectiveness of citizens in urban centers around the world.","api, css, figma, flask, html, javascript, python, react","https://www.figma.com/proto/E6ijL6ydgvzU1qMEwrP8Y6/WHEN%3F%3F-browser?node-id=0-1&t=gIptu5k8jzgxU3YD-1, https://github.com/doreendai2005/WHEN_SmartCityMustNeed"
HackHarvard 2024,Doctor Doctor,https://devpost.com/software/doctor-doctor,False,"Long waiting queues! Who would like to speed up processes that take forevvveeerrrrrrrrrr....
We were imagining to ourselves: WE SCREAMED AT THE SAME TIME ""ALLHEALTH"". Not really, but you get the gist. This is when we came up with an idea","Our app is an AI-powered diagnostic tool that collects multi-modal user input—such as images, audio, and text—and processes this data to offer potential health diagnoses and advice. It’s designed to work like a virtual medical assistant, allowing users to input symptoms through a conversational interface and receive quick diagnostic feedback. We have a family built in functionality to accommodate multiple users on one app. It also helps doctors to quickly access data from users if users wish to share to particular hospitals, providing invaluable data sets and easy response from doctors.",,"With this being our first ever hackathon, we faced several challenges, from time management to technical hurdles. One of the biggest obstacles was integrating multiple technologies—such as audio processing with Pydub, image classification with pre-trained models, and using the OpenAI API for NLP—into a cohesive app. We also encountered difficulties in ensuring seamless communication between the React frontend and Flask backend, especially when handling large files like audio and images. Debugging these real-time interactions and ensuring cross-browser compatibility added additional complexity, but we persisted and learned a lot throughout the process.","This hackathon was both of our first, and the fact that as duo, we were able to come out with a working product, that we believe has the potential to become something much bigger than just a hackathon submission is an accomplishment in itself. From waking up 13 hours late into the competition to finishing our final working demo only 30 minutes before the submission, we're proud to announce the release of Doctor Doctor. We're proud to have accomplished a convoluted model integrated into a chatgpt wrapper with a touch a Fourier on the side... We're proud to have created a front end which is stylish and cool and wow.... Lastly we're proud to have made a tool that may one contribute to making the world a better place (Ooo dramatic)","Nothing... we're the best(Lol jk). In all seriousness, we've learned quite a bit over the last 36 hours: 1) Don't oversleep at the start of the competition(take shifts if need be)
2) Maybe have a team of 4 damn it was rough with only 2 people
3) Come to the competition with some level of understanding of whos doing what
4) General new technical concepts such as: random-forest, most of react cause wow leetcode != building a product from scratch, the fundamentals of working with and storing data to use at a later date, and a lot of css concepts I never though I'd stuggle so much on. Okay, for real we really did learn:","Doctor Doctor has the potential to shake up healthcare in a big way, especially for hospitals and underserved regions. Imagine reducing those crazy long waiting times at hospitals by using quick, accurate assessments to figure out who needs urgent care first. Plus, it could bring AI-powered medical knowledge to places that really need it, like third-world countries where access to doctors is limited. This app could help doctors with initial diagnoses, give personalized health advice, and empower people to take control of their health. The idea is to make healthcare more accessible, especially for those who usually get left out, and help ease the load on already overwhelmed systems. It’s all about making healthcare faster, smarter, and available to everyone who needs it. Sponsorships from hospitals and markets could improve the drug market and make it much more accessible.","css, flask, fourier, html, javascript, json, machine-learning, numpy, python, pytorch, random-forest, react, scikit, tensorflow",https://github.com/DannyHelou/hackharvard
HackHarvard 2024,DecentraHealth,https://devpost.com/software/decentrahealth,False,"Front End AWS EC2 Solidity Contract Zero knowledge proof circuit  (circom) Our project focuses on creating a decentralized federated learning system for hospitals, ensuring privacy and data confidentiality while enabling collaborative AI training across different institutions. The system allows hospitals to train AI models on their own medical data (like chest X-rays), and then securely share model parameters with a global server that combines and averages these models without ever accessing the sensitive data itself. What inspired us to work on this project is the growing need for privacy-preserving machine learning, especially in the healthcare industry. With increasing concerns about data security and strict regulations like HIPAA, it’s critical to find ways to leverage data without violating confidentiality. Federated learning, combined with blockchain and cryptography techniques like zero-knowledge proofs (ZKPs) and CKKS encryption, offers a solution to these challenges. Through this project, I learned how decentralized systems can foster innovation and collaboration without compromising data security. We integrated technologies such as Ethereum-based smart contracts and privacy-preserving cryptography to ensure model integrity and correctness, even without accessing the data itself. Data Privacy and Model Training: We designed two isolated hospital environments where each hospital trains a CNN model on its dataset.
Federated Server and Blockchain: Each hospital sends its model’s weights and training metadata (loss function, optimizer, model architecture) to a global server, which uses CKKS encryption to perform privacy-preserving weight averaging.
Zero Knowledge Proofs: To verify the training integrity without revealing sensitive data, ZKPs are employed, allowing the server to confirm the validity of training without accessing the actual data.
Smart Contracts for Verification: The verification process is handled using Ethereum-based smart contracts written in solidity and built over Rust-based Circom circuits, that ensure the authenticity of the training logs.
Frontend for Transparency: The frontend displays two key components: (1) Information about the global model, including its type, optimizer, and weights, and (2) Decentralized training logs, showing client IDs and training information. Challenges Faced One major challenge was implementing privacy-preserving technologies like CKKS encryption and ZKPs into the federated learning workflow while maintaining the efficiency of the system. Another was designing the smart contract to handle training log verification and ensuring secure communication between hospitals and the global server. We also had to overcome resource constraints, particularly in handling large model weights in a decentralized and encrypted manner.",,,,,,,"amazon-web-services, css, django, flask, html, javascript, python, solidity, tensorflow, typescript",https://github.com/ishanrev/harvard
HackHarvard 2024,Resumify,https://devpost.com/software/resumify-rz0eon,False,"With an increasingly more competitive job market, applicants need a way to stand out from the crowd. One such way to do so is by tailoring and fine-tuning your resume to the job application's specific description and requirements. However, with the volume of job applications most applicants submit, it is difficult to maintain consistency while adjusting for each company's needs, which is where we got the inspiration for our project from.","This project is an AI-powered resume enhancement platform. It allows users to upload their resumes, typically in PDF format, and uses OpenAI's API to analyze, improve, and restructure the content into a professional and job-tailored format. The AI can correct grammar, enhance phrasing, and even suggest improvements based on job descriptions, which is provided by parsing the link to a job application. It returns the updated resume back to the user where they can either download or provide feedback on changes and send it back for revision with the user's prompt as the main point of revision.","The user would input a resume into our web application, along with a link to the job they want to apply to, where it is then sent to our backend that's run on FastAPI,. It is then processed into a text-file and used as input to openAI's API where we prompt GPT-4 to convert the text-file into a strict JSON file based on a typescript template and make changes based on the parsed job description. This JSON file would then be compiled using a custom made compiler that converts the JSON into latex and returns the latex to the frontend as a pdf for the user to view","File types were a major issue we ran into. Our project only supports pdf file types since multiple types file types have different ways of parsing their data and we cannot account for every single file type. Conversion between file types were also a big challenge, along with prompt engineering. It took a while to get GPT-4 to output the specific JSON that we wanted.","Creating a live demo of real-time resume revisions and allowing the user to provide feedback on those revisions. Creating an endpoint that does the work of compiling the revised resume into latex which is then converted to pdf, creating a visually pleasing and nicely formatted resume.","Utilizing FastAPI to create endpoints for backend
Prompt engineering to get the most accurate response that suits our needs","We want to make this more accessible, by that we mean we want to create a chrome extension. Here, we can automatically parse the job description and requirements along with the user's resume","fastapi, latex, openai, python, react.js",https://github.com/JoblessWei/Resumify
HackHarvard 2024,QuickDeets,https://devpost.com/software/quickdeets,False,"Three of the people on the team have family members in healthcare services. After talking with their family members, we found many ways in which we could improve patient care and automate some of their tasks.",It is a website that is capable of adding new patients or importing patient data from an EHR system. It then takes this data and populates a table of patient harm indicators. A healthcare professional can then record their own notes and optionally add an LLM's analysis of their notes to the table. All appended notes are timestamped.,"In Python, there is a Audio Recording class that handles the function of starting and stopping audio recordings. In JavaScript, requests are made to Python using the API created in Django. These requests will start and stop the recordings. Once a recording is stopped, it is passed into a ChatGPT API prompt to return a JSON file through the API and back to the requester to update the website. A user can decide to update the website with the suggested updates or they can edit the suggestions and save those changes. Using an FHIR compatible server and Synthea, we created mock patient data in the format that you would see at a hospital. This data was then used in the website and each patient was assigned an ID. With this ID, you can pull up the data related to that patient and you will see the patient harm indicator data for them as well.",There are a lot of moving parts and there were conflicts at certain points during development. We also initially struggled to find open source data that is in a format that hospital systems use.,We were able to resolve the conflicts between the separate components to create a working project.,We learned that communication is paramount when working across different languages and frameworks. We had some communication related issues that set back our progress that could have easily been avoided.,We plan on scaling the project and creating more security features for deployment in real-life settings. There is also room for a lot more AI and machine learning components to personalize the project's use for any patient's condition.,"bootstrap, chatgptapi, css, django, fhir, html, javascript, python, synthea, whisper",https://github.com/rafaelolal/hackharvard2024
HackHarvard 2024,MediTrack,https://devpost.com/software/meditrack-o7b3kf,False," Prescription costs are skyrocketing, and for many patients with chronic illnesses, affording medication has become a struggle. The rising costs often lead to missed or delayed doses, which can have serious effects on patient outcomes. The alarming statistic that 75% of Americans struggle to take their medication as directed—and the fact that approximately 135,000 deaths per year in the United States are linked to medication nonadherence—served as a powerful motivator for us to find a solution. We envisioned a system that not only tracks medication intake but also provides real-time insights into patient adherence. Our goal was to reduce the number of medication-related complications and hospital admissions, and that's why we builtMediTrack—an automated RFID-based patient tracking and reporting system. We wanted to create a seamless, reliable way to help patients stay on top of their medication schedules while reducing the burden on healthcare providers. The app also uses a custom-coded regression model to predict whether or not our patient X will take their medication on time. Based on its training, we can predict whether our patients are at high-risk, medium, or low-risk. This makes it easier for healthcare providers or caretakers to respond to urgent needs immediately. Throughout our journey, we learned the importance of clear data structuring and agile development practices. Here are some of our key takeaways: We builtMediTrackusing a combination of cutting-edge technologies to ensure reliability, scalability, and seamless integration with existing systems: Building a solution like MediTrack came with its fair share of challenges, including: Despite the challenges, we’re proud of several key achievements: We're not stopping with medication tracking. Our vision is to expand MediTrack's RFID system to enhance other areas of healthcare, including: With MediTrack, we aim to transform healthcare through smarter technology, improving patient outcomes, reducing healthcare costs, and making hospitals more efficient. MediTrack: Smarter Medication, Safer Lives.",,,,,,,"javascript, mongodb, next.js, node.js, python, react-native, twilio",https://github.com/Shhreya-Anand/MediTrack
HackHarvard 2024,Beacon,https://devpost.com/software/beacon-clh8kw,False,"Beacon was inspired by the belief that AI can be a powerful tool to transform lives, particularly for those with visual impairments or physically challenged. Despite some existing tools for assisting, most are either outdated, inefficient, or not user-friendly. We wanted to create a solution that not only works seamlessly but also provides real-time, accurate assistance. Beacon is designed to bridge the gap, offering an advanced AI-powered assistant that empowers users to navigate independently and confidently. This has never been done before at such a high level of integration and accuracy, and the need for such a solution has never been greater.","Beacon is an intelligent assistant designed to provide real-time navigation support for visually impaired and physically challenged users. Using a smartphone’s camera, it detects objects, recognizes scenes, and performs text recognition while providing voice feedback to the user. With commands like ""What's in front of me?"" or ""Read this sign,"" Beacon helps users understand their surroundings instantly. Its unique blend of AI models for object detection (YOLOv7), scene recognition (Places365), and Optical Character Recognition (OCR) allows it to process multiple tasks simultaneously and relay accurate, real-time information to the user in a natural, human-like voice. The app also supports intuitive voice commands, making it completely hands-free.",We built Beacon by integrating several state-of-the-art technologies:,"One of the major challenges was integrating multiple AI models to work together in real-time. Synchronizing YOLOv7 for object detection, Places365 for scene detection, and OCR for text recognition was complex. The decision to use Places365 for indoor environments was strategic, as it’s highly specialized for recognizing indoor scenes, but switching to outdoor models like DeepLab or U-Net when needed added a layer of complexity. Additionally, ensuring the voice feedback was both accurate and responsive to the user's queries required careful tuning. Managing model efficiency and balancing the demands of real-time processing with voice accuracy pushed our skills to the limit.","We're proud of buildingan eye for the blind— a working prototype of a system that can assist visually impaired users in navigating their environment independently and with confidence. The system's ability to seamlessly integrate multiple AI models for object detection, scene recognition, and text reading in real-time while delivering accurate voice feedback is a huge accomplishment. Moreover, the app's user-friendly voice interaction and hands-free operation elevate it beyond just a functional tool to an indispensable daily aid for visually impaired users.","Throughout the process, we learned thattime management is key, especially when working on complex, multi-model systems. Things don’t always go according to plan, but our ability to adapt and persevere allowed us to deliver a fully functional system in just 36 hours. We realized the immense potential of our skills and how much of an impact we can make when focused and determined. On the technical side, we honed our skills inPyTorch, OpenCV, and model training and deployment. Most importantly, we learned that it's always about the end user. Understanding their needs drove every decision we made during this project, and it was this mindset that helped us overcome technical challenges and build something truly meaningful.","Beacon has the potential to grow into afull-blown appthat goes beyond individual assistance. Future updates could include sendinglive location updates to loved ones, so family members or caretakers can keep track of the user's whereabouts and ensure their safety. We also plan to enhance the scene recognition model for outdoor environments, refine the accuracy of the voice assistant, and introduce more features for contextual awareness. The long-term goal is to make Beacon a comprehensive navigation assistant that provides emotional support, navigational guidance, and safety alerts, making the world more accessible for visually impaired individuals.",python,https://github.com/VrabelPeter/beacon
HackHarvard 2024,ReliefToYuah,https://devpost.com/software/relieftoyuah,False,"The recent hurricanes that have hit Florida, and the South Eastern US regions","This is a web application that helps people in natural disaster zones, get help, and resources.","We used defang for a baseline, with python django to build the applications inside, with a SQLite database, and used html, css, and javascript for the appearance, and some interactions.","When combining all the code, some of the code was not working together as intended, and url's seemed to not function properly.",Learning how to use languages that aren't always used together properly.,"Python, and web dev languages are rough to use together.",Fixing broken links.,"css, django, docker, html, javascript, python, sqlite",https://github.com/Sherlockabc731101/ReliefToYuah
HackHarvard 2024,Sleep Soon,https://devpost.com/software/sleep-soon,False,"In the spirit of staying up ungodly amount of hours trying to build a project, what inspired us to build Sleep Soon is the everyday hacker who is in desperate need of a good sleep schedule. Like many college students and working adults, we all could use some more sleep especially considering how busy people have become.","Sleep Soon takes the inputs from the user, particularly the amount of sleep they want to have, tasks they need to do, time to do those tasks, and the time for them to start the day and creates a scheduled tailored for them to finish those tasks and get them to bed by 9:00 pm. It also encourages the user to take breaks with healthy activities.","We developed Sleep Soon using Python to power the backend, processing user inputs and generating personalized schedules. The frontend was built using HTML and CSS to provide an intuitive interface for users to input their preferences and view their custom schedule. Flask was used to tie everything together, making the app interactive and responsive. By focusing on simplicity and usability, we were able to create an app that anyone can use to get their work done efficiently while still prioritizing sleep.","One of the biggest challenges we faced was handling the complexity of scheduling tasks and balancing them against a fixed sleep window. We had to ensure that tasks, breaks, and sleep time fit into a 24-hour day while also making the experience seamless for the user. Another challenge was validating user inputs to ensure realistic schedules—making sure people weren't trying to cram too many tasks into an impossible timeframe. Not only that, we had a hard time trying to connect all of our teammates work together in a way that would look and feel smooth.",We are proud of the fact that this is our first time building out a project and we were able to come out with a finished product. This is big for us especially as First Time Hackers. We are also proud of our perseverance and our ability to problem solve as a team with this project.,"We learned that we are able to use different languages in different files to help us build a better experience. Furthermore, we were fascinated that multiple programming languages can work in conjunction with each other. We also learned how to technically communicate with each other. Finally, we also learned how much fun and ridiculous a hackathon can be.",We hope to continue to work on this project and customize it to make it feel memorable and meaningful.,"css, defang, flask, html, python",https://github.com/kyleenax/sleep-soon-clone
HackHarvard 2024,BioSync,https://devpost.com/software/biosync,False,,,"We began by identifying the problem, validating it through research, and understanding the needs of the users. From there, we researched existing solutions and devised our approach by addressing the shortcomings we identified in those solutions. To start the building phase, we prototyped the UI using Figma and mapped out our database design using an ER diagram. For the UI development, we used HTML, React, Bootstrap, APIs, CSS, and JavaScript within Visual Studio Code. To collaborate efficiently, we used a shared repository where each team member worked on individual UI pages in their own branch and then merged them after completion. For the backend, we designed our database in MySQL and integrated it with the frontend system using Gradle and JavaScript extensions. Additionally, we utilized ChatGPT as a learning tool to help us address unfamiliar challenges we encountered during coding.","As beginners in the software development industry, we encountered many challenges due to our limited knowledge in areas such as database management, frontend and backend integration, programming languages, and API integration. To overcome some of the obstacles, we sought guidance from mentors and collaborated with nearby groups for support. Additionally, we used search engines and AI to find solutions while continuing to learn throughout the process.","~ Gaining knowledge about database structures, associative entities, technical terms, and ER diagrams.
~ Implementing tables and integrating APIs.
~ Attempted to integrate backend and frontend systems.
~ Improving frontend development skills using React and Bootstrap.
~ Network and met amazing people","~ Despite building a functional user interface, not all programming languages are compatible with each other, which required a minor switch in language.
~ Collaboration with peers and clear communication of goals and outcomes leads to a successful team.
~ Don’t be afraid to step up and take the lead in areas where you may be unfamiliar.
~ Gaining technical knowledge and hands-on experience is invaluable.","~ Incorporating RFID and sensors into the project.
~ Integrating more useful APIs to create a more functional product.
~ Experimenting with an alternative database.
~ Improving the User Interface (UI) and User Experience (UX) on the website.
~ Enhancing frontend and backend integration.","api, bootstrap, css, design, er-diagram, figma, google-cloud-console, html, intellij-idea, javascript, mysql, qrcode, react, red-hat, sql, vs-code","https://github.com/vivianli1337/biosync, https://biosync.netlify.app/content/summary"
HackHarvard 2024,SlideProf,https://devpost.com/software/slideprof,False,"Many professors upload their slides without annotations, making it super frustrating for students to review afterwards. One of them taught my class. It was midterm season so my friends and I met to review together before an exam. We tried rereading the lecture slides hoping to understand better, only to get more confused. Random equations popped out of nowhere. Slides went from 1 + 1 = 2 to ways to solve B-tree with doubly-linked lists using dynamic programming at O(1/n). It was 2am so there was no way to get 1-on-1 help with a TF or professor. Spamming ChatGPT kinda helped but eve this was limiting because we couldn't picture or visualize the concepts in our minds. This is why I created SlideProf for those who are like the me back then trying to learn better with visualizations.","You upload your lecture slides and the system will scan them. You can read through your slides and review them as you normally do. If you don't understand a certain part, you can click the chat, easily hold click and drag to select the region and ask our SlideProf chatbot. SlideProf will explain just like how your professors normally would in a lecture with their tablets, by explaining while annotating and drawing things so the students can better understand the concepts. There is also another feature where you can put in your lecture Echo360's video and it will create annotated notes taken from your professor's audio. A good tool for those who are sick!"," When you upload some slides, our Vercel frontend built with Next.js and Tailwind will send it to the data processing service deployed on Defang. Deploying a FastAPI app on Defang was a lot simpler than we expected and we got our own domain too. Would definitely recommend. In the backend, the slides will be formatted and then sent to our other service to be analyzed. First, depending on your question, it will choose which visual format that fits best with your problem. Then, it will look around to see if there's any space to insert it in, and if not it will erase that blocking region. Then through GPT-4o API, it will answer your question normally like ChatGPT. The twist here is that it splits its answer into two parts, one for speaking with a voice model, and one for rendering on the slides. Our app will try to sync them so that the displaying and talking feel as natural as possible.","At first, we couldn't get the OpenAI API's responses to fit with our format so there were a lot of trials of drafting the right system prompt. The hardest part, though, was how to position the shapes and the equations so that they look natural, as well as how to sync the speaking and the rendering times. The positioning was tricky because the PDF's dimension and the dragbox's coordinates on the screen were different, so we normalized them so that the coordinates only go from 0 to 1. It's not the best way still as things can be too far or too close to each other. For rendering shapes so that it feels natural, I cut them into different parts with timestamps that match with the speech.  These two problems are what got us up all night, but we shalt persist and we did (no we got cooked).",Learning how to display the visualizations in an intuitive way - Eric Shaking hands with David Malan was definitely a big win since he was the one who got me into coding - Bao Sleeping on four lecture hall chairs for a night - Hung Got ranked up in leagues and learned React I hope - Vu,Learned what it felt like to use GPT without worrying about money (Thank you our goats David and Julianna) I don't know I need sleep - Hung I learned so much I don't think it fits in my tiny brain - Eric Zzzzzzz - Vu,,"d3.js, defang, fastapi, gpt-4o, next.js, ngrok, react, shadcn, tailwindcss, vercel","http://github.com/weebao/slideprof, https://slideprof.vercel.app"
HackHarvard 2024,UniRoute,https://devpost.com/software/uniroute,False,"Isaac Park, a freshman from South Korea, noticed that back home they had super convenient all-in-one transportation apps, but when he came here, he realized there’s nothing like that. It was frustrating having to jump between different apps for shuttles, trains, and figuring out routes. That’s where the idea for UniRoute came from — a way to make commuting easier for university students by putting everything in one place.","UniRoute is designed to make students’ commutes smoother. Instead of juggling multiple apps, it combines shuttle and train info into one easy-to-use platform. The app shows shuttle times, connects you to train schedules, and helps you plan your trips so you don’t have to deal with delays or missed rides. It’s all about saving time and avoiding the headache of switching between different apps when you’re just trying to get around.","We built UniRoute using Apple’s MapKit API, which let us integrate maps and transportation info into the app. As first-time hackers and newbies to mobile app development, we hit a lot of bumps along the way. Initially, we tried using Google Maps API, but after spending hours trying to fix dependency issues, we just couldn’t get it to work. In the end, we switched to Apple’s API, which worked out much better for us.","Being new to this, pretty much everything was a challenge! We had trouble setting up the right dependencies and figuring out how to handle complex route calculations. We ended up using static data in the app just to get something working in time, but that helped us push through and build a prototype.","Despite all the challenges, we’re really proud that we made an actual working app. It might not be perfect yet, but we learned a ton, and getting the app up and running, even with static data, feels like a big win for us as first-time hackers.","We learned a lot about mobile app development, especially how hard it can be to combine different transportation data into one place. We also got a much better handle on how APIs like Apple’s MapKit work and how to manage time under pressure.","Next up, we want to actually implement the algorithms we had planned for real-time data and fully functional route calculations. We’ll also refine the app’s design and make it more user-friendly, so students can rely on it to make their commutes easier every day. The dream will be to actually post the app on App Store and create impacts for many students' life.","apple, swift, swiftui, xcode",https://github.com/bisrat415/hackharvard24-deis
HackHarvard 2024,Smart Traffic Light Simulation,https://devpost.com/software/smart-traffic-light-simulation,False,We came up with this idea based on our combined concerned for long commute times.,Its a 3d simulation comparing the differences between regular street lights and our advanced light system.,we built it using Unity 3d and combined research.,"implementing smart traffic light mechanism and dealing with different outputs.
finding statistics for research.
last day the the project broke.","finishing the project.
getting our hypothesis to work.
getting the simulation within a 5% accuracy.","building a simulation is hard.
Unity randomly breaks projects.
working with data is difficult, specially when combined with a simulation.","making it more accurate for research purposes.
build a tool that can be used by researchers.
add more complex real world scenarios to have a more accurate and practical end product.","c#, github, unity",https://drive.google.com/file/d/185hkpkPAQRvmqls4ml9KiRQB_2C4kVkB/view?usp=drive_link
HackHarvard 2024,AIDERS: AI-powered Detection and Emergency Response System,https://devpost.com/software/aiders-ai-powered-detection-and-emergency-response-system,False,"Recognition of choking Our journey as a diverse group of students with backgrounds from Morocco, Ghana, Afghanistan, and China brought us to HackHarvard with a shared desire to make an impact on the world of healthcare. Our experiences and those of our parents, who come from various parts of the world, made us acutely aware of the gaps in healthcare systems globally. Whether it’s a lack of access to emergency care or inadequate monitoring systems, these experiences deeply influenced our decision to work on something meaningful. We were inspired by the Patient Safety 101 workshop, which highlighted the importance of addressing patient harm and the urgent need for innovative solutions to prevent medical errors. Hearing that nearly 70% of Americans have never even heard of patient safety made it clear that this is an area where technology can—and should—make a difference. The Pittsburgh Regional Health Initiative’s commitment to patient safety technology and its emphasis on reducing preventable harm pushed us further to take up this challenge and do our part. Thus, we decided to create AIDERS—a real-time patient monitoring system utilizing a vision model to detect emergencies like falls or choking incidents in hospitals. By promptly alerting healthcare professionals and designated contacts, we aim to enhance patient safety and bring peace of mind to patients and families alike. Through countless hours of work on the technical stack—leveraging YOLOv8 for detection, combining front-end technologies like Go, Handlebars, and JavaScript, and building our backend in Python—we built a system that not only recognizes critical incidents but also ensures rapid response. As a team, we have learned that creating a tech-driven solution for healthcare is not just about coding or models—it’s about empathy, safety, and giving back to the people who need these services the most. Our work on AIDERS is a step toward contributing to a safer healthcare environment and engaging our tech-savvy generation in building solutions that truly matter. We hope to expand AIDERS to support additional emergency scenarios and integrate it into more healthcare facilities. We will continue refining the model for improved accuracy and broader usability.",,,,,,,"academicresearch, bioengineering, css, env, express.js, go, hbs, html5, javascript, nodemodules, pt, python, ultralytics, videomonitoring, yaml, yml",https://github.com/naweensar/HackHarvard
HackHarvard 2024,AthenaPrep,https://devpost.com/software/athenaprep,False,,"AthenaPrep has two functions, 
Orientation: Uses an LLM to create a back and fourth conversation with a student, that creates thought provoking questions making connections with details provided, to narrow down a compelling essay topic. After 12 questions, the AI creates more final questions to provide closure, and then provide a detailed closing statement stating good approaches for a solid essay the student could consider.
Essay Editor: Provides rich and detailed feedback to a student provided essay, focusing on general broad topics like cohenrency, and qualities that would be appealing to an admissions officer. Feedback is shown in the style of google docs comments, with text boxes shown beside the essay, and relevant text being highlighted.",,,,,,"flask, gunicorn, node.js, openai, python, react, tailwindcss, typescript, vite",https://athenaprep.org/
HackHarvard 2024,Signify,https://devpost.com/software/signify-ezb6aw,False,,"It allows to convert ASL (American Sign Language) to English text. The goal is to make it easier for deaf people to communicate with people who don't know ASL, which is most of us.","We first trained our model using images that we took ourselves, and then used to model to detect common phrases in ASL. We then used OpenAI API to convert the detected phrases to English sentences, since ASL does not respect correct grammatical rules.
For the frontend, we used React and Vite.
We used AWS for the hosting.","We did not find the datasets we needed on the internet. Most datasets online included one picture of each phrase which was not enough to train our models. For this reason, we had to capture all the images from different angles ourselves. This was time-consuming but necessary.
Another challenge was cleaning the array of detected words in a way that we wouldn't get duplicates so that our final sentence would make sense.","We're proud of having been able to achieve what we did in such a short amount of time. It was a challenge to do everything from scratch but at the end, we were able to make something that has potential to become a real application in the future.","We learned a lot about both coding and how the ASL works. We did not know how deaf people communicated with each other before this hackathon. Also, we did not know how to train a model using images, which we learned during this hackathon.","To make Signify more useful, we would definitely need to include many more phrases to it. The model is currently limited due to the limited time that we had to train it, but with more time, we will have the chance to complete our dataset and make the application more powerful.
Also, to make it more convenient for the users to use Signify, we could convert the detected ASL phrases to other languages than English and also generate speech.","amazon-web-services, fastapi, openai, opencv, python, react, scikit-learn, vite","https://github.com/Qischer/signify_backend, https://github.com/Qischer/signify_frontend, https://github.com/AliTaladar/Signify"
HackHarvard 2024,"Vota: One vote, one voice",https://devpost.com/software/vota-one-vote-one-voice,False,"With the 2024 presidential election – a historical and critical event – coming up in less than a month, we noticed that there was a surplus of election information on the internet, but a lack of organization. At a more targeted level, even less people are informed about their state senators and representatives, despite their direct impact on voting issues. To address this issue we created Vota, which aims to centralize representative information for voters – helping to streamline the process of making informed election decisions. Existing websites and resources for centralizing information for voters tend to be convoluted and difficult to navigate due to the quantity of information that they store. Vota’s focus on congressional positions allows for a more targeted user friendly experience.","Vota provides general election information to a user, and prompts users to enter their hometown state; from the input, a list of state representatives and senators is retrieved from the API and displayed in a user-friendly format. A chatbot is also provided to ask additional questions pertaining to representatives, voting, and election information, which aims to answer any questions that may arise. Through these functionalities, we hope to provide an easily accessible and centralized method of learning about state representatives and elections!",,,"Coming out of this hackathon, we’re proud of developing a functional website with both a front and back-end! We also learned many of the frameworks we used for the project during the 36-hour duration of the hackathon.",,"We hope to find a more comprehensive API to provide the most up-to-date data. With enough data, Vota could be expanded to local elections. Vota’s chatbot also has potential for growth through partnership with a more established AI model in the future.","css, flask, html, javascript, python, react",https://github.com/angelinaa-chen/vota
HackHarvard 2024,Lucidity,https://devpost.com/software/lucidity-tsnufp,False,"While shopping on Amazon, my partner and I noticed nearly all the search results were self-preferenced, low-quality, high-polyester/polyamide/nylon clothing options. Without clicking each and every option and reading the often garbled product description, it was impossible to quickly tell whether an item was sustainably produced or not. The few-and-far ethically produced clothes were buried between scourges of fast-fashion filler. So, we developed Lucidity: with a quick glance, see what clothes are fast-fashion, which clothes are not, and sustainable alternatives to that once 95% polyester piece that you might really love the look of.","Lucidity combs through Amazon results in batches and checks a few factors to determine whether each clothing item is considered ""fast fashion"": fabric materials, ratios of said materials, clothing manufacturers, and price. If items are deemed fast fashion, they are flagged. For flagged items, links to eBay, Facebook Marketplace, and Depop are provided to encourage shopping secondhand if such items are still desired.","We developed Lucidity to run conveniently in your web browser through a chrome extension. There's no need to manually activate it--it will get to work when you pull up Amazon. The codebase was built using JS, ScraperAPI, and a bit of design work in Figma/Photoshop.","The algorithm initially processed and classified each listing individually, slowly working through the products on the search page. We optimized the algorithm for concurrent batch processing to accelerate sustainability calculations so the user knows which products to buy instantly.","We're proud of how simple, yet how essential a product like this is. 310 million people use Amazon, and the vast majority are stripped of the ability to make smart, sustainable choices by Amazon's self-preferencing, fast-fashion favoring algorithm. Lucidity makes it easy for anyone to shop sustainably by highlighting the ""good"" and the ""bad,"" no research required.","Through the process of putting together our algorithm, we researched the concept of fast fashion, it's market penetration, and why fast fashion is unsustainable. We also learned how to develop and ship a chrome extension.","Lucidity will be published to Google Chrome Extension marketplace, free for all shoppers on Amazon. We're hoping a frictionless way to shop sustainably will encourage more people to adopt better shopping practices. We will also iterate and improve Lucidity, aiming to reduce barriers to shopping sustainable.","chrome, dom, javascript, scraper",https://github.com/23JacobH/lucidity-chrome
HackHarvard 2024,EcoKitty,https://devpost.com/software/ecokitty,False,"We were inspired to create Eco Kitty by the pressing need to educate children about environmental issues in an engaging and accessible way. We realized that many kids are naturally curious about nature and the world around them, but often find traditional environmental education boring or difficult to understand. We wanted to harness the power of modern technology to make sustainability fun and interactive, hoping to nurture a generation of eco-conscious individuals from an early age.","Eco Kitty is an interactive educational app that makes learning about sustainability and environmental consciousness fun for children. It features an AI-powered chat assistant called the Eco-Buddy, which provides eco-tips and answers questions about the environment. The app's innovative recycling scanner allows kids to use their device's camera to identify recyclable items and learn proper disposal methods. To help children understand their environmental impact, Eco Kitty includes a leaderboard system that fosters some healthy competition between family members to adapt to eco-friendly habits. The app also incorporates gamification through daily eco-friendly quests, challenging users to develop sustainable habits. By combining these engaging features, Eco Kitty aims to nurture environmentally responsible behaviors in young learners, making complex environmental concepts accessible and exciting.","We built Eco Kitty using a modern tech stack centered around Next.js 14 with TypeScript. The frontend was developed using React components styled with Tailwind CSS for a clean, responsive design. We leveraged Next.js API routes for our backend logic, which helped streamline our development process. Supabase served as our database and authentication solution, providing a robust and scalable foundation. The AI chat feature was powered by the OpenAI API, while we used Zxing and Cheerio for UTC code recognition as well web scraping public data to find products","Making Eco Kitty was fun but tough. We wanted to teach kids about the environment in a way that was both correct and easy to understand. It was hard to explain big ideas in simple words without losing important details.
The AI chat part was tricky too. We had to make sure it talked to kids in a friendly way and taught them the right things. We spent a lot of time making the AI say just the right things and adding ways to keep it safe for kids.
Another big problem was using lots of different computer tools to build our app. Many of these tools had old or confusing instructions. This made it really frustrating to figure out how to use them. But we didn't give up! We looked everywhere for help - in the official guides, on forums where other coders talk, and even in other people's projects that used similar tools. We had to put together bits of information from all these places, which was like solving a big puzzle. This made us think in new ways and come up with clever solutions to make Eco Kitty work better. We learned that being able to adapt and find new ways to solve problems is super important when making apps. In the end, the hard parts helped us make Eco Kitty even cooler than we first imagined.","We're incredibly proud of what we've accomplished with Eco Kitty. Our biggest achievement was creating an app that makes complex environmental concepts accessible and engaging for children. Along the way, we significantly expanded our technical skills. We dove deep into Next.js 14, mastering its new App Router and strengthening our TypeScript expertise. Integrating various APIs, including OpenAI for our AI chat feature and image recognition services for our recycling scanner, pushed us to learn rapidly and adapt to new technologies. We overcame challenges with outdated documentation by collaborating and finding creative solutions, which not only solved our immediate problems but also deepened our understanding of API integration. The project also allowed us to refine our skills in React, Tailwind CSS, and Supabase. Perhaps most rewarding was seeing our vision come to life - we worked so hard as a team and couldn't have gone anywhere with the support we provided for each other. Though moments were intense and hard we stuck together and fought to the very end.","Developing Eco Kitty was a fantastic learning experience for our team. We significantly improved our skills in Next.js 14 and TypeScript, while also gaining valuable experience with Supabase and the OpenAI API. The project taught us the importance of clear communication and teamwork, especially when solving complex problems like creating child-friendly AI interactions or designing engaging educational content. We learned to adapt quickly when faced with challenges like outdated API documentation, which pushed us to find creative solutions. Working on an environmentally focused app also showed us how technology can be used to educate and inspire positive change. Overall, this project not only enhanced our technical abilities but also reinforced the value of creating purposeful technology that can make a real difference in the world.","• Conduct extensive user testing with children and parents
• Expand educational content with more environmental topics and quests
• Localize the app for different regions (recycling guidelines, languages)
• Create a parental dashboard for progress monitoring
• Enhance gamification with rewards, levels, and competitions
• Develop resources for school curriculum integration
• Continuously improve AI chat responses and accuracy
• Add community features for users to share eco-friendly achievements
• Partner with environmental organizations to enhance content and reach","cheerio, css, figma, html, next.js, openai, postgresql, react, supabase, tailwindcss, typescript, zxing",https://github.com/AshleighWong/eco_kitty/
HackHarvard 2024,Eventure,https://devpost.com/software/eventure-1ey42c,False,CO,,,,,,,by-voice,
HackHarvard 2024,CopayHero GPT,https://devpost.com/software/copay-hero-gpt,False,,"CopayHero GPT is a smart platform that helps users compare prescription drug prices in real-time across various pharmacies. It also finds available discounts and coupons, ensuring that patients can save as much as possible on their medications.","We built the platform using a combination ofGPT-4 for natural language processingto help guide users in finding the best price,Pythonfor backend development, andAPIs from major pharmaciesto pull in real-time price data. Our front-end interface was developed usingReact.js, ensuring that users could easily navigate and search for their prescriptions. To ensuresecurity, performance, and scalability, we implementedCloudflarefor managing DNS, protecting against DDoS attacks, and optimizing content delivery. Cloudflare’s caching and security services helped us maintain fast load times and safeguard user data.","One of the biggest challenges was ensuring accurate price data across multiple pharmacies. Some pharmacies don’t openly share their pricing information, making it difficult to provide comprehensive comparisons. Additionally, integrating various APIs and maintaining consistent data flow was a technical hurdle. We also faced challenges in filtering out unnecessary information for a smooth user experience. ImplementingCloudflareadded a layer of complexity, as we had to ensure proper configurations to balance security with real-time data flow.","We’re proud of successfully building a functioning beta that accurately compares drug prices and identifies potential savings for users. Our integration of GPT-4 to offer a more personalized and intuitive user experience was another major milestone. ImplementingCloudflareenhanced the platform’s security and performance, ensuring that we could deliver a seamless experience to our users. Additionally, we managed to create a sleek, user-friendly interface that simplifies a typically complicated process.","We learned a lot about API integration, especially when working with real-time data. Additionally, building a healthcare-focused application requires an understanding of both the technical and user experience challenges involved. We also gained deeper insight into the complexities of drug pricing and how to present this information clearly to users. ImplementingCloudflaretaught us valuable lessons about managing cloud infrastructure for security and performance at scale.","Next, we plan to expand our platform to include more pharmacies and add advanced features like price alerts and personalized medication savings recommendations. We also want to develop partnerships with pharmaceutical companies and pharmacies to enhance our discount offerings. On the technical side, we will continue optimizingCloudflareconfigurations for better performance and security. Ultimately, we aim to launch a full public version of Copay Hero GPT and further optimize the platform based on user feedback.","css, html, python",https://github.com/Sai-Vignesh/MedWallet
HackHarvard 2024,EduCare,https://devpost.com/software/educare-9vc6an,False,"Healthcare is one of the most crucial and rapidly evolving industries, yet communication gaps between doctors and patients continue to pose a significant challenge. Often, post-appointment documentation is complex, medical jargon-heavy, and difficult for patients to fully understand. Studies reveal that 60-80% of medical information provided by healthcare professionals is forgotten by patients immediately after the consultation, with half of the remembered information recalled incorrectly (Journal of Royal Society of Medicine). Furthermore, the lack of clear, accessible medical records can leave patients confused about their diagnosis, treatment plans, and next steps. Current solutions available focus solely on the doctor’s convenience, automating voice-to-text to generate reports but leaving patients behind in the process. These reports often remain medical-centric and inaccessible, preventing patients from understanding their health journey effectively. EduCare was born from the need to address these gaps—creating a post-appointment reporting tool that is accurate, doctor-approved, and easily understandable for patients, while also providing concise summaries for doctors to review.","EduCare transforms post-appointment documentation into concise, doctor-approved summaries and digestible, patient-friendly reports. Here’s how it works: Voice Transcription: During the appointment, the doctor’s voice notes are transcribed into text in real-time using Deepgram API.
Medical Parsing & Coding: The transcribed text is processed through NLTK and a custom parser to extract medical codes (CPT/ICD), symptoms, diagnostic procedures, and medications.
Doctor Approval Stage: The summarized medical report is sent to the doctor for review and approval through the app interface, ensuring accuracy and mitigating hallucinations or errors from the AI-generated text.
Humanized Patient Report: Once approved, the data is passed through OpenAI API to create a patient-friendly, layman’s version of the report, enriched with explanations.
Language Translation: To improve accessibility, the final report can be translated into the patient’s native language.
Seamless Delivery: The finalized report is stored in Firestore and sent to the patient via the platform for easy access and reference.","The development of EduCare required seamless integration of multiple APIs and technologies to deliver an efficient, error-free experience for both doctors and patients. Here’s a breakdown: Backend Development: Deepgram API: Used to transcribe the doctor’s voice into text in real-time.
PyAudio: To capture and stream audio input from the microphone to the backend server.
NLTK (Natural Language Toolkit): To summarize and process medical text, extracting important elements such as symptoms, diagnoses, and medications.
Custom Medical Parser: Converts the raw text into CPT/ICD medical codes, providing structured medical information.
OpenAI API: Used at two stages—first, to generate an AI-enhanced summary for doctor review, and second, to produce the final patient-friendly report.
Firestore Database: Stores the summaries, parsed codes, and final reports in real-time.
Flask Backend: Handles API calls between the frontend and backend services, ensuring smooth data exchange.
Frontend Development: React.js: Builds the web interface for doctors and patients, enabling easy approval and report access.
Firebase Firestore Integration: Provides real-time synchronization of approved summaries and final reports.
Responsive Design: Ensures that the web app functions seamlessly on both mobile and desktop devices.
Workflow Integration: Approval Mechanism: The app features an approval stage where the doctor reviews and fact-checks the auto-generated summary before it is finalized.
API Coordination: We ensured smooth integration across multiple APIs—transcription, summarization, parsing, and translation—so the reports flow seamlessly from creation to approval and delivery.","Building EduCare presented several technical and logistical challenges, including: Handling Real-time Transcription: Integrating PyAudio and Deepgram API to capture and transcribe audio in real-time required careful coordination between streaming input and API responses.
Accuracy of Summaries: Managing the risk of hallucinations or inaccuracies in AI-generated text was crucial. We implemented an approval stage to mitigate errors.
Medical Parsing & Coding: Creating a parser that correctly interprets CPT/ICD codes while also generating understandable summaries was a complex task.
API Integration & Data Flow: Managing the seamless flow of data between multiple APIs and the frontend-backend systems required extensive testing and debugging.
User Experience Design: We had to design an interface that was simple for doctors to approve summaries while also producing clear, digestible reports for patients.","Empowering Patients: We successfully created a tool that shifts the focus from a doctor-centric workflow to one that empowers patients with clear, understandable medical reports.
Accurate, Doctor-approved Reporting: We developed a robust approval mechanism that ensures all reports are accurate and fact-checked before being delivered to patients.
Seamless API Integration: We integrated multiple APIs and achieved smooth interoperability between the transcription, summarization, and coding systems.
Real-time Updates: With Firestore’s real-time database, we enabled instant report generation and updates, ensuring timely delivery to patients.
Language Accessibility: By including a translation feature, we broke language barriers, making healthcare information accessible to non-English-speaking patients.","Balancing AI and Human Oversight: We realized the importance of combining AI capabilities with human oversight in sensitive areas like healthcare to ensure accuracy and safety.
API Coordination: We gained valuable experience working with multiple APIs and learned how to optimize data flows between the frontend and backend.
Healthcare Documentation Needs: We deepened our understanding of healthcare documentation challenges and the need for concise, yet comprehensive reports.
User Experience in Healthcare: Designing interfaces for both doctors and patients taught us how to balance simplicity and functionality across different user groups","Expanding Language Support: We plan to add more languages to the translation feature, ensuring that patients from diverse backgrounds can access their medical reports.
Mobile App Development: A mobile version of EduCare is in the works, allowing patients and doctors to access reports and summaries on the go.
Integration with EHR Systems: We aim to integrate EduCare with existing Electronic Health Record (EHR) systems, streamlining workflows for healthcare providers.
Voice Command Features: Implementing voice command functionality will allow doctors to interact with the app hands-free, further enhancing usability.
AI-powered Insights: We plan to leverage AI analytics to provide patients with personalized insights and recommendations based on their health data.","api, custom, deepgram, firebase, firestore, flask, language, medical, natural, nltk, openai, parser, pyaudio, react.js, sci, scipy, spacy, toolkit, typescript",https://github.com/kcnakamu/EduCare
HackHarvard 2024,PAWPAW - the Privacy Analyzer with Personality and Whimsy,https://devpost.com/software/pawpaw,False,"TOS, EULA, SLA, and other user agreements can act as a vehicle for abuse by online service providers, embedding shady practices within dense legal-ese—incomprehensible to most consumers. As security-minded individuals, our team members are aware of how difficult it can be to maintain your data security and be cautious about the myriad contracts we have to sign in our daily lives, so we wanted to make a tool that would help less security-inclined folks take care of their agreements and data. We also wanted it to be funny, so we made it a sarcastic old man!","When users navigate to PAWPAW, either on the hosted project or on localhost, they can input a URL to a TOS or other user agreement site. After submitting, the site generates an overall ""score"" on the practices of the provided company, a summary of key points on their practices outlined in the given document, and a series of highlighted blocks of text in the provided document that can be navigated to and hovered over for further exposition on their importance.","We used Next.js and Vercel for the web components of PAWPAW, building our front end with tailwind and framer motion for clean styling and animation. On the backend, we query a fine-tuned subset of OpenAI's GPT-4o which inspects the given document and summarizes parts of it, helping us assign an overall score. The model also points out specific important instances within the text, which we highlight using a highlighting and fuzzy finding algorithm developed by us.","We originally hoped for PAWPAW to be a Chrome extension, but ran into issues with developing on Manifest V3 that took too long to fix for it to remain feasible. We also had several difficulties related to our highlighter and fuzzy finder, which ended up being much more complicated than we anticipated.","Most of our group consists of systems-oriented programmers who don't have much (or any) experience with web development, especially front-end tasks. For all of us, making a responsive, modern, and beautiful website was a massive accomplishment. We'd also never done a project involving the use of AI for parsing and analyzing documents, which was a very fun challenge.",That our friendship lasts through adversity (all-nighters and arguing over page styling).,"We hope to add a feature for lazy loading pages for faster performance, along with a feature that would allow PAWPAW to conduct ""background checks"" on given companies—scraping the web for articles related to data misuse, misconduct, etc.  We're also looking at fine-tuning our own LLM instead of using an external service.","css, exa, html5, next, openai, react, typescript","http://pawpaw-analyst.vercel.app, https://github.com/spoonmilk/pawpaw-analyst"
HackHarvard 2024,SafeRoute | Safety Prioritized in Google Maps,https://devpost.com/software/saferoute-safety-prioritized-in-google-maps,False,"Safety is a universal concern—whether it's fear of car accidents or walking through unsafe areas. For many, these fears have already become their reality. In the U.S. alone, 6 million car accidents occur annually, and 40,000 lives are lost in these crashes. Meanwhile, 50% of women fear walking alone at night. Despite the nation's access to cutting-edge technology and resources, safety remains a pressing issue in our communities. We’ve personally faced these dangers and seen friends become victims of avoidable incidents. With the power of big data, we knew we could engineer a solution. Why not use the resources available to make a real difference?","SafeRoute is a Google Chrome extension that enhances Google Maps by integrating a safety-first approach to route planning. Once users activate ""Safety Mode,"" routes are ranked based on safety rather than just speed. The extension utilizes factors like historical crime data, accident history, and real-time weather conditions to guide users through safer paths.","Back-end: We used Python to collect, analyze, and model data. Leveraging libraries like Requests and Beautiful Soup, we pulled crime and weather data via REST APIs and analyzed it with Pandas and Matplotlib. To rank the safety of each route, we used a Decision Tree Regressor (Sklearn) to process crime rates, historical accidents, and weather. Historical accident data was especially challenging—we converted a massive 60,000-row CSV into a 350,000-row JSON file to link accidents with geographic data along routes, generating a comprehensive safety score for each path. Front-end: Built with JavaScript, HTML, and CSS, the extension seamlessly integrates into Google Maps, allowing users to switch between ""Safety Mode"" and regular navigation with ease.","Handling 350,000 lines of data required us to optimize performance. Initially, our algorithm took 2 minutes to calculate a route, but through caching, refactoring loops, and efficient data handling, we cut it down to just 7 seconds. Integrating the back-end and front-end posed its own difficulties. Team communication and alignment were key to overcoming these obstacles, ensuring the code was both functional and cohesive. Our machine learning model also presented a steep learning curve, particularly around data cleaning, encoding qualitative data, and fine-tuning the model for accuracy.","We’re proud of overcoming technical challenges, diving into unfamiliar technologies, and efficiently collaborating to break a massive project into smaller, manageable parts. The result is a functioning product that can truly make an impact on user safety.","Analyzing and cleaning up data before using it was huge, as we didn't parse the crash CSV file properly the first time, which caused headaches. We learned how Chrome extensions are deployed, how they manage data and how to manipulate info on a browser page.","We plan to refine our safety model further, integrating more real-time data sources like police reports and user feedback. We also envision expanding SafeRoute’s functionality to more platforms, enabling safer navigation experiences across different modes of transport and regions.","html/css, javascript, json, machine-learning, python",https://github.com/aliisona/maps-extension
HackHarvard 2024,DermaDrill,https://devpost.com/software/dermadrill,False,The inspiration for this app arose from two key insights about medical education.,"This app provides an interactive learning platform for medical students, focusing on dermatological diagnosis. It presents users with real-world images of skin conditions and challenges them to make a diagnosis. After each attempt, the app delivers personalized feedback, explaining the reasoning behind the correct answer, whether the diagnosis was accurate or not. By emphasizing pattern recognition and critical thinking, in concert with a comprehensive dataset of over 400,000 images, the app helps students refine their diagnostic skills in a hands-on manner. With its ability to adapt to individual performance, the app ensures a tailored learning experience, making it an effective tool for bridging the gap between theoretical knowledge and clinical application.","To build the app, we utilized a variety of tools and technologies across both the frontend and backend. On the frontend, we implemented React with TypeScript and styled the interface using TailwindCSS. To track user progress in real time, we integrated React’s Rechart library, allowing us to display interactive statistical visualizations. Axios was employed to handle requests and responses between the frontend and backend, ensuring smooth communication. On the backend, we used Python with Pandas, Scikit-Learn, and Numpy to create a machine learning model capable of identifying key factors for diagnosis. Additionally, we integrated OpenAI’s API with Flask to generate large language model (LLM) responses from user input, making the app highly interactive and responsive.","One of the primary challenges we encountered was integrating OpenAI’s API to deliver real-time feedback to users, which was critical for enhancing the app's personalized learning experience. Navigating the complexities of API communication and ensuring seamless functionality required significant troubleshooting. Additionally, learning how to use Flask to connect the frontend and backend posed another challenge, as some team members were unfamiliar with this framework. This required us to invest time in researching and experimenting with different approaches to ensure proper integration and communication between the app's components.","We are particularly proud of successfully completing our first hackathon, where we built this app from concept to execution. Despite being new to many of the technologies involved, we developed a full-stack application, learning the theory and implementation of tools like Flask and OpenAI's API along the way. Another accomplishment was our ability to work as a cohesive team, bringing together members from diverse, interdisciplinary backgrounds, both in general interests and in past CS experiences. This collaborative effort allowed us to combine different skill sets and perspectives to create a functional and innovative app that addresses key gaps in medical education.","Throughout the development of this app, we learned the importance of interdisciplinary collaboration. By combining medical knowledge, AI, and software development, we were able to create a more effective and engaging tool than any one field could produce alone. We also gained a deeper understanding of the technical challenges that come with working on large datasets and implementing adaptive feedback systems.","Looking ahead, there are many areas our app can expand into. With AI identifying the reasoning behind a certain diagnosis, we can explore the potential for diagnostic assistance, where AI can identify areas that may be abnormal to ultimately support clinical decision-making, giving physicians another tool. Furthermore, in other fields that are based on image-based diagnosis, such as radiology or pathology, we can apply a similar identification and feedback system. Future applications of such an app can enhance clinical diagnostic abilities while acknowledging the complexities of real world practice.","flask, numpy, openai, pandas, python, react, scikit-learn, tailwindcss, typescript",https://github.com/heewonseo7/HackHarvard
HackHarvard 2024,Just Hurry!,https://devpost.com/software/just-hurry,False,"The devastation caused by recent hurricanes in Florida highlighted the shortcomings in post-disaster relief efforts. Thousands of people were left without homes and access to essential resources. As we watched these events unfold, we felt compelled to dig deeper into the problem and understand how we could make a difference. With communities suffering due to slow and inefficient resource allocation, we knew there had to be a better way to help. This realization inspired us to create Just Hurry!, a platform dedicated to transforming disaster relief efforts by providing faster and more efficient support to those in need.","Just Hurry! is a comprehensive platform designed to bridge the gap between disaster relief organizations and the vulnerable communities affected by hurricanes. Using real-time data and predictive ML modeling, Just Hurry! identifies the regions most in need of immediate assistance, based on factors like socioeconomic vulnerability, historical hurricane data, air pressure, wind speed, and population density. Organizations can register their available resources, such as food, water, and medical supplies, while the platform ensures these resources are allocated efficiently based on real-time requests from users in affected areas. In addition to resource allocation, Just Hurry! allows individuals to request emergency help or sign up as volunteers to provide aid where it’s most needed.","We built Just Hurry! by leveraging advanced data analytics and machine learning to create a custom model that calculates real-time risk factors for different communities. Using Palantir’s Foundry, we analyzed critical variables such as air pressure, wind speed, and population density to predict the impact of hurricanes and determine which areas would be most at risk. We then developed a platform with a seamless user interface, making it simple for organizations to register their resources and for users to request help or volunteer. Throughout the development process, we ensured that Just Hurry! prioritizes user experience, data accuracy, and the efficient distribution of resources.","One of the biggest challenges we faced was accurately predicting the areas that would be most affected by hurricanes. With so many variables to consider, including weather patterns, socioeconomic factors, and historical data, developing a model that could consistently provide accurate results required extensive research and testing. Additionally, building a platform capable of handling real-time data and ensuring that resources are efficiently allocated presented technical challenges. Balancing the need for a user-friendly interface while managing the backend complexities of resource allocation and disaster response was another major hurdle we had to overcome.","We’re proud to have built a tool that can genuinely make a difference in the way disaster relief efforts are conducted. Just Hurry! is more than just a platform; it’s a solution that can save lives and help communities recover faster. We successfully developed a real-time model that predicts hurricane impact zones and created a platform that optimizes the allocation of resources. Furthermore, we’re proud of our ability to streamline communication between organizations and those in need, ensuring that help reaches the right people at the right time.","Throughout this project, we learned a great deal about the challenges of disaster response and the importance of data in making informed decisions. We discovered that while technology can’t prevent natural disasters, it can play a significant role in mitigating their effects by improving the speed and efficiency of relief efforts. We also learned about the complexities of resource management and the importance of collaboration between relief organizations and communities. This project taught us the value of persistence and innovation when tackling large-scale problems like disaster relief.","Our vision for Just Hurry! extends beyond hurricane relief. We want to expand the platform to be adaptable for various natural disasters and emergencies, whether it’s earthquakes, floods, or wildfires. Our next steps include refining our predictive model with more data and incorporating partnerships with additional relief organizations to broaden the platform’s reach. We also plan to integrate features that allow for more efficient volunteer coordination and community-driven support systems. Just Hurry! is more than a temporary solution—it’s a long-term vision for a more resilient, prepared society capable of responding swiftly to crises and rebuilding stronger than before.","foundry, kaggle, nextjs, palantir, python, react, sklearn, tailwindcss, tensorflow, vercel",https://github.com/AnandVishesh1301/just_hurry
HackHarvard 2024,Realytics,https://devpost.com/software/realytics,False,"As new adults and near-graduates entering a challenging job market, we've faced the uncertainty of housing affordability and real estate decisions firsthand. The complexity of fluctuating markets—driven by location, economics, and global events—can make homeownership and investment feel out of reach. We realized this wasn’t just our struggle; millions are asking the same questions: ""Where should I invest?"" or ""Can I afford to live here?"" Our project was inspired by this shared challenge. Existing tools often lack accessibility or accuracy for the average person, so we set out to build a solution that simplifies real estate data. By using two decades of historical trends and future projections, we empower users with clear, actionable insights into the market, helping them make smarter decisions.","Our project predicts real estate value for any city that the user inputs. It analyzes past real estate trends over the last two decades and generates real-time analytics and future projections for the coming years, all the way until 2035. The core features include: Our submission to the ""Open Source Data"" track emphasizes the power of shared data to drive innovation and solve complex, real-world problems like affordable housing and investment planning. By utilizing open-source data, we’re not just predicting market trends—we're also advocating for a future where data accessibility helps people make better, informed decisions.","We used multiple open source datasets from Hugging Face, mostly tracking Zillow databases, as the main data source for our models. We utilized Scikit Learn, Numpy, PyTorch to train the AI prediction models and perform testing and evaluation.
For the backend, we used Flask and SQLite to create API endpoints to the model, AI-powered suggestions, and the AI assistant. For the frontend, we used Next.js and Tailwind CSS. Lastly, we used AWS to host and deploy.","Because of the numerous number of API endpoints, we had challenges during the hosting and deployment process, getting all of it to run live. We also worked on the deployment for almost six hours, and we're so proud it worked finally!!!!",We will able to create a model that predicts real estate prices pretty well as well as with a very interactive user interface with helpful suggestions and assistant features. The visuals and analytics are easy to understand and show very detailed projections.,"We learned a lot about training and finetuning a model pretty much from the ground up using open source databases, as well as a lot about hosting our own backend through AWS. Each member originally had pretty different strengths, and though we played to them, our collaboration allowed us to learn a lot of skills in areas we had less exposure in before.","Compare two cities function so that users can compare the analytics for two different regions, as well as adding even more regions/cities to the map for users to explore!","amazon-web-services, flask, next.js, python, pytorch, scikit-learn, sqlite, tailwind","https://realytics-two.vercel.app/, https://github.com/taipioca/hackharvard24"
HackHarvard 2024,Unify,https://devpost.com/software/unify-23bgjx,False,"Patient safety, the wellbeing of a patient during a visit, is frequently harmed due to medication, medical complication, surgical, infections, and diagnostic errors. While some of these problems can be pinpointed to specific causes, most arise due to system-wide inefficiencies and errors that compound on one another. A major cause is the siloing of patient data across different healthcare providers, leading to incomplete information and inefficient care coordination. Current solutions are ineffective, dated, or lacking an incentive for hospitals to get involved.","Unify is an all-in-one, fully anonymized medical database analysis tool, patient search engine, doctor communications platform, and case study generator. The application serves as a central database where medical personnel can upload new patient data or search through existing treatment cases. If a doctor has questions regarding how a specific patient was treated, they can anonymously chat with the original healthcare provider for advice. On the home dashboard, important hospital statistics are shown to allow staff to keep track of improvement in patient safety.","In our project, we utilized OpenAI embeddings to represent patient data, which allowed us to calculate similarity scores between different cases. These embeddings transformed the patient information into high-dimensional vectors that encapsulated key features such as medical history, treatments, and diagnoses. By comparing these vectors, we could determine how closely related two patient cases were, which helped in identifying similar conditions or recommending treatments. On the frontend, we used Next.js to design the user interface, with React for building dynamic and interactive components, and TypeScript to ensure robust typing and styling across the application. For the backend, MongoDB was used to store patient cases, and FastAPI acted as the bridge between the database, machine learning model, and frontend. FastAPI enabled seamless interaction between the various components. When users searched for similar patient cases, the system leveraged OpenAI's embeddings to compute similarity scores, allowing the frontend to display relevant and related patient information quickly. This approach facilitated a more personalized and informed recommendation process in the medical chat application.",,"We’re very proud of our similarity based patient search function which relies on the cosine similarity of transformer-based embeddings. This allows doctors to freely enter their data without having to strictly adhere to cumbersome formatting/documentation rules. For example, a doctor listing a patient’s language as “ENGL” will score very similarly to “English”. This reduces the burden placed on medical staff and will lay the foundation for connecting Unify with hospitals across the globe and the different data conventions they use.","We learned a lot about using OpenAI’s API in ways other than making a chat bot. We also learned a lot about the medical industry and the role that data plays in it. Along the way, we improved our skills in the tech stack we used, including Next.js, TailwindCSS, and FastAPI.","We would like to make Unify HIPAA compliant by updating its database to follow the Safe Harbor Method, which deidentifies protected health information (PHI). The 18 PHI identifiers would be omitted from patient case entries. To facilitate easier deployment into hospitals, it would also be nice if Unify was backwards compatible with current EHR systems. Lastly, we would want to broaden the scope of Unify to international countries by making it support different medical protocols (e.g: CPT and ICD-10)","fastapi, mongodb, next.js, openai, python, react, sqlite, tensorflow, typescript",https://github.com/TrexKalp/hackharvard24
HackHarvard 2024,elo-fi,https://devpost.com/software/elo-fi,False,"elo-fi was inspired by the growing need for secure, easily manageable public WiFi in smart cities. We wanted to empower non-technical business owners, like cafe and restaurant owners, to maintain safer, high-quality networks without complexity.","elo-fi assigns elo ratings to public WiFi networks and users, allowing automatic AI powered scoring and even removal of harmful users. It also provides business owners with simple tools to improve and monitor their network ratings.","We built elo-fi using a combination of network performance tracking, security analysis, and an elo rating algorithm. Our backend server collects data from WiFi networks, and we integrated a user-friendly dashboard for non-technical users.",One of the main challenges was translating technical data into an intuitive and simple interface for non-technical users. We also encountered difficulties in ensuring the accuracy of WiFi rating assessments across diverse environments.,"We’re proud of creating an accessible platform that empowers non-technical users to maintain high-quality, secure WiFi. Building a scalable solution that could enhance smart city infrastructure is another significant achievement. Additionally, none of us had experience with blockchain or working with routers in Openwrt, so it was really fulfilling for us to see it work in the end!","We learned how to balance technical complexity with user simplicity, focusing on making advanced technology accessible to everyday users. Additionally, we deepened our understanding of smart city network infrastructure.","Next, we plan to expand elo-fi by partnering with local governments and smart city initiatives to further enhance public WiFi infrastructure and safety. We’ll also explore adding more advanced security features and real-time feedback for users.","ganache, go, llms, natural-language-processing, nextjs, openwrt, solidity",https://github.com/elofi-hh
HackHarvard 2024,ChaosNet,https://devpost.com/software/chaosnet,False,"We love simulations. A few weeks ago, we decided to simulate options pricing using some complex math, and after we did that, we thought... why dont we try to simulate global trade, across the entire world? The goal of this project is to see as we change trade policies between two or more countries (Nodes in our network), how it globally affects the entire world, as well as those countries economies. The things we found that shocked us were as we change small % of inputs (i.e., an import tariff increase on agricultural goods in chile by .1%) can lead to affecting countries who are not even a part of the trade. We are shocked, and impressed.","We developed a real-time visualization that showcases the simulations. We calculated the Partial Equilibrium, which allowed us to model and find the probability that as we change input parameters, we can find the ripple affect across various countries. Things like changing one trade policy can affect how a country inflates its currency, finds new trade partners and more. Our visualization shows all the relations in real time. Our application uses Knowledge Graphs to store Countries, Industries, and Relationships between them as nodes. We then run our simulator, iterating over all neighboring nodes and find probabilities of economical change as we alter the inputs. We passed 99 industries to our model, and end up restricting it to only 6 because there is so much data uncovered.","We used a wide-range of technology, we did a lot of the data processing in python, with pandas and numpy. We wrote the math formulas out in Matlab to take advantage of vector and matrix operations. We found this speed increase to be crucial for our projects run time. We spun up a front-end with React, Next.Js and produced real-time chart visualizations in D3.js. The charts are dynamic and interactive, so please play with them. Finally, we deployed our application to vercel and gave it the hostname:https://chaosnet.world/.","There were many challenges. We thought simulating this would be easy, but as we dug into the math, we realized there are so many paths our simulation can take. For example, when a country imposes a tariff on another, the responding country has the option to trade elsewhere, inflate their currency against the others in some way, or impose retaliation tariffs. This is just a few examples, and modeling these three was a challenge in such a short time! Our charts are dynamic, and we found that as we simulate our relationships, there are so many outputs and possible paths. Limiting our data was a challenge, because we wanted to show it all, but we cannot due to resource constraint, and the size of our network growing large.","We are super happy to say that we completed the project, hosted it, and provided real-time visualizations of all the simulations. The end result is a knowledge graph that shows the relationship network of all connected nodes, and shows the probability of how the countries respond to their inputs changing. This can be the probability that the country decreases their workforce for a specific industry by a certain %, or the probability that the stock market surrounding this specific product increases by X %. The possibilities are endless, and we are super happy we were able to showcase the probabilities in relation to world trade simulations.","We learned a lot about world trade, and we learned a lot about performing real-time simulations and producing visualizations to showcase it. We had a lot of fun figuring out all the different paths a simulation can take, and the math behind it. We learned a lot about matlab, and D3.js.","We want to continue simulating more scenarios. Trade policies are endless, and things like politics, global events, economical market events can drive simulation outputs to change. We would like to simulate as many scenarios as we can and keep having fun developing it.","d3.js, matlab, nextjs, python",http://chaosnet.world
HackHarvard 2024,Call Me Maybe,https://devpost.com/software/call-me-maybe-lxrwaf,False,"On average, people receive around 120 to 150 spam calls every month, with some even experiencing daily interruptions. These unsolicited calls can consume valuable time—between answering, listening to automated messages, or dealing with unwanted pitches, individuals often waste upwards of 10 minutes a day. This adds up to significant frustration and distraction, interrupting both work and personal life. Call Me Maybe offers a solution by redirecting unknown calls to an AI assistant, filtering out spam and ensuring you only engage with the calls that truly matter.","The AI assistant evaluates incoming calls by interacting with the caller, asking targeted follow-up questions to gather more context. It classifies each call as either 'Important' or 'Not Important' based on criteria such as emergency services, urgent medical appointments, and critical financial notifications versus non-urgent inquiries like telemarketing or surveys. If the call is deemed to be spam or unimportant, it is automatically declined. For calls classified as important, the assistant seamlessly redirects the caller to the appropriate responder.","We designed the interaction between the AI assistant and the caller using a four-step process: caller audio, caller text, AI text, and AI audio. First, we record the caller’s voice and convert the audio to text using JavaScript Speech API. The AI then generates follow-up questions and responses using Generative AI based on the caller's input. This exchange continues until the AI can classify the call as either important, leading to a redirect, or not important, resulting in a decline. This efficient process facilitates effective call management based on the caller's needs.","During development, our biggest challenge was accurately classifying calls from unknown numbers into two categories: important and not important. Achieving reliable differentiation often led to false positives and negatives, complicating the user experience. To solve this, we established criteria: important calls include emergency services, critical financial notifications, and urgent medical appointments, while unimportant calls encompass telemarketing, advertisements, and non-urgent inquiries. The AI asks follow-up questions after each prompt to gather more context without revealing the classification. Additionally, we faced challenges with user data security, gaining user trust, and integrating with various phone systems, all while adapting to evolving spam tactics.","We are proud to have developed a potential app that effectively distinguishes between important and unimportant calls. This innovative concept has shown strong promise and garnered interest from peers and industry observers. Early feedback suggests that our approach could significantly enhance user experience by reducing interruptions from unimportant calls. We believe this solution addresses a real-world problem, helping users manage their calls more effectively. Overall, these accomplishments highlight our commitment to advancing call management solutions through this promising framework.","Through this project, we learned the immense value of teamwork in creating and developing our idea. Collaborating effectively allowed us to brainstorm and refine our concept while building a clear product roadmap. Conducting thorough market research was crucial in understanding user needs and preferences, which informed our approach. Additionally, preparing and delivering our pitch helped us articulate our vision and gain valuable feedback. Along the way, we also gained important technical skills, which enhanced our capabilities and confidence in bringing our idea to life.","Looking ahead, our next steps involve refining our app prototype based on feedback from initial presentations. We plan to conduct further market research to deepen our understanding of user needs and identify potential features that could enhance functionality. Additionally, we aim to develop a comprehensive testing plan to ensure the app's accuracy and reliability in sorting calls. We will explore opportunities for collaboration with industry experts to validate our concept and gain insights into best practices. Ultimately, our goal is to move toward a beta launch, where we can gather real user data and iterate on our solution to create a more effective call management tool.","canva, github, google-generativeai, javascript, python, speech-to-text, text-to-speech",https://github.com/anjieliu121/HackHarvard2024/tree/main
HackHarvard 2024,mood,https://devpost.com/software/mood-lf4rqk,False,"Over a fifth of the U.S. adult population suffers from mental health conditions. With economic pressures, social media, politics, personal relationships, and humanitarian crises worsening personal stress, people feel increasingly isolated. In this environment, music has emerged as a powerful tool for emotional connection and healing. By allowing users to create personalized playlists based on their mood—whether it’s a simple emotion like sadness or a more complex narrative—our AI-driven platform provides an outlet for emotional expression and relief. The ability to select genres and favorite artists gives users more control over their listening experience, which can be vital in managing stress and anxiety. Research shows that music can help lower cortisol levels, improve mood, and even enhance cognitive function. By curating playlists tailored to an individual’s emotional state, we aim to offer a therapeutic experience that promotes mental well-being. Making users feel seen and understood through music, mood alleviates mental pressures through the healing power of music.",mood generates a Spotify playlist based on your mood description and genre/artist preferences.,"Tech stack: React.js, CSS, Javascript, Flask, Python, OpenAI, Spotify API",,,,"In the future, mood can receive user feedback on the playlists to create improved generation. For example, users will be able to rank song suggestions. mood will also be able to generate more precise playlists for descriptions that are written as narratives.","ai, css, flask, javascript, ml, openai, python, react.js, spotify",https://github.com/ricchen/mood
HackHarvard 2024,RideCast,https://devpost.com/software/ridecast-am39xz,False,"Every college student has used a Ride Sharing at some point in their lives. The issue with Ride Sharing APPs are often the price but the inevitability of using the APP is unavoidable. What if we could help not just students, but everyone to save a little bit of money while also making a more efficient way to compare prices between different Ride Sharing APPs.","RideCast is a downloadable Google Maps plug-in that pulls the APIs of Uber, Lyft, and Curb's APIs to utilize their database to analyze and create trends that can be implemented into predicting the future prices of car rides which also takes into account distance, weather and time. Another perk of RideCast is that it shows the comparisons in prices among the large RideSharing APPs, and will automatically show the cheapest price on top.",,One of the main challenges was that Lyft had a very secure API and must go through a long process to be approved to view the Data they have. Due to this we recorded data by hand every 30 minutes to check price changes over 16 hours. In the future once we are fully approved by Lyft it will be a much easier process to predict future trends. Another challenge that we faced was that it took a long time to decide on an idea. Our original idea on tracking heat and building a more heat resistant areas in the Cambridge area was not as viable as we once thought.,"As a group that most likely has less experience in coding compared to others, we spent most of the weekend attending workshops and watching YouTube videos in order to learn the coding we wanted to utilize. We are also very proud to have been able to become great friends over the weekend, it isn't often that people can become so close during such a small amount of time and, especially being able to work together without major arguments. We are all so glad to have met each other and hope that even after HackHarvard we can still stay in touch.","A better world is about taking action and trying to implement it. The importance of consistency, persistence, and trusting in your vision are key abilities to bringing a better ideas into real life. Furthermore, we learned about how to proceed when we are facing bottlenecks. How to convince and encourage each other to make it through the goal. Tech wise, some of us has never previously used Figma, and never trained models. We are picking up a lot of new information and skills that will be invaluable to our project, and future lives.","To create another model that predicts the probability of getting a driver at a certain time and location. Imagine you are in the middle of Michigan, you have a 5AM shift and you need a ride at 4AM. You reserved an Uber but still there was no driver; or you have an Uber, but the reserved price is like 10 plus dollars than you directly call it at the moment. We are also going to let the Google Maps Plug-In capable of requesting rides with APIs and without redirecting the users into the apps. It is also foreseeable to add features like directly requesting a ride by least wait time, price, comfort, etc, across all the platforms.","databricks, python, sklearn",https://github.com/oolongmilktea-a/HackHarvard2024.git
HackHarvard 2024,CrisisCompanion,https://devpost.com/software/crisis-companion-xigj0e,False,"When our team first came together in the days preceding HackHarvard, we agreed that if nothing else, our project would focus on helping society. Hannah told us about the nerve-wracking experience of watching from afar as her loved ones in Tampa prepared for and dealt with hurricanes Helene and Milton. It takes a significant amount of time, effort, and money to plan for the storm’s devastation. We were inspired to consolidate verified and reliable information about the steps needed before, during, and after an impending hurricane to minimize damage and cost, and be able to find resources, shelters and evacuation routes anytime.","Once we decided on the issue to tackle, we spent a couple of hours planning the implementation to best serve our audience—people planning for and affected by hurricanes. We decided to design and build an app available on both Android and iOS devices to enhance accessibility. The app features:","We divided our team into roles: The frontend team developed inReactNativefor cross-platform compatibility (iOS and Android).
The backend team implemented services withPython,Flask, andcloud hostingusingDefangand a backupLinodeserver for optimal performance. We containerized the backend usingDockerto ensure a stable, continuous and high-quality experience that would scale without issue.",Some challenges we encountered include:,"We are incredibly proud of the project we built. Though our primary focus for now, was to provide immediate assistance in hurricanes, the name""Crisis Companion""stands for our goals to extend the product’s capabilities to support responses for other natural disasters. We aim to unify disparate datasets to serve under-duress people and communities more effectively.",Our team learned:,"Moving forward, we aim to:","defang, docker, flask, google-places, linode, python, react-native, selenium","https://github.com/raoanmol/HackHarvard, https://www.canva.com/design/DAGTcmJSNxw/K2Pg65KtIKhufpj4Fal47w/edit?utm_content=DAGTcmJSNxw&utm_campaign=designshare, https://youtu.be/cxQSPFLX7e4"
HackHarvard 2024,HavenLink,https://devpost.com/software/havenlink,False,"As all three of us are from China, we were particularly inspired by a recent technological initiative implemented in major cities. This initiative focuses on assisting elderly individuals in navigating the digital landscape for taxi calling services. In China, nearly all taxi services have transitioned to online platforms, which poses significant challenges for seniors who may struggle with digital technology and accessing information online. This experience highlighted for me the critical importance of cities integrating devices and services that cater to diverse needs and bridge the technological gap.","In this context, we turned our attention to the issue of homelessness, which presents its own set of challenges. Many homeless individuals face significant difficulties in accessing essential resources, often exacerbated by limited or no internet connectivity. Without easy access to information about shelters, food banks, and healthcare services, they are left isolated and without the support they need. Our goal is to address this gap by developing solutions that provide essential information and assistance to vulnerable populations, ensuring that everyone can access the resources necessary for their well-being. Moreover, this device is not limited to homeless individuals; it serves as a new initiative within our infrastructure, providing support to a wider range of people. By acting as a community resource hub, the device can assist those facing various challenges, such as low-income families, individuals experiencing temporary hardships, or those seeking information about local services. It fosters a sense of inclusivity by ensuring that everyone, regardless of their circumstances, has access to vital resources. The integration of a voice model allows users to interact easily with the device, making it more user-friendly for individuals who may not be comfortable with technology. This initiative aims to create a more connected and supportive community, enhancing the overall quality of life for all residents.","Because we do not possess an actual IoT device, we decided to use a website to model the IoT device placed around the city. The website will serve as an interactive platform for users seeking assistance. Upon entering the homepage, users will encounter an integrated voice model that allows them to speak directly to the device regarding their needs. The voice model will categorize the request and guide users to the appropriate resources they are looking for. To enhance accessibility, the website will also offer a text-to-speech option for users who may have difficulty reading or prefer auditory information. Additionally, users will have the option to click into one of six predefined categories, each representing a specific type of assistance, such as shelters, food banks, healthcare services, mental health support, addiction resources, and emergency services, weather, and AI chatbox. This structure ensures that users can quickly and efficiently find the help they need while navigating the challenges associated with homelessness or other hardships. The shelter button will provide users with information on the three nearest homeless shelters, including their addresses and a Google map for easy navigation. Similarly, the food bank button will list food bank locations along with relevant details. The emergency calling service and health support will allow users to directly dial the appropriate phone numbers. Additionally, the weather function will prompt users to enter a specific ZIP code and return a week's worth of weather advice. For any further inquiries or emotional support, users can engage with an AI chatbot for assistance. By modeling the IoT device through this website, we aim to create a user-friendly and efficient resource that fosters greater accessibility and support for all community members.","Since this was our first time participating in a hackathon, and it was also the first experience for all three of us in building a full-stack working project, we faced numerous challenges along the way. Our lack of familiarity with front-end and back-end development, various frameworks, and API calls made the process particularly daunting. We often found ourselves troubleshooting issues that arose from our inexperience, which slowed down our progress. Moreover, we struggled at first to identify the project we actually wanted to pursue. The brainstorming sessions were filled with great ideas, but we had difficulty narrowing them down to one cohesive concept that we felt passionate about and could realistically execute within the time constraints of the hackathon. This uncertainty added to our initial frustration, but it also provided an opportunity for us to collaborate more closely and learn from one another as we explored different possibilities. As the hackathon progressed, we gradually gained confidence in our abilities and started to enjoy the process. We learned to leverage our individual strengths, and despite the setbacks, we remained committed to pushing through the challenges together. Overall, this experience taught us valuable lessons about teamwork, adaptability, and problem-solving, which we hope to apply in future projects!","As a team of three whose members have a) never collaborated together on any major programming tasks and b) have varying levels of experience and familiarity with different languages, we are proud that we were able to come up with a functional website integrating technologies that were, for the most part, new to us. Additionally, we believe that what we have created can help mitigate the societal issue of homelessness. Although we only have a demo, we believe that the core idea that our project follows can benefit society.","First and foremost, working as a team isn't easy, especially with members with different levels of experience and language preferences. Additionally, we learned (the hard way) that version control is a) extremely important and b) extremely easy to mess up when you don't know exactly what you're doing. Lastly, while this experience has taught us that while not all things will follow ideally according to plan due to a variety of factors, we can still build innovative and impactful projects if we put our minds to it.","We plan on perfecting our current set of features - extending language support to more than just English for speech, adding a TTS service (was WIP but didn't have time to finish) for the visually impaired, expanding and improving our databases from a simple mock to a real database etc. Additionally, since the project was intended for real devices on the streets, we have plans to refactor the project into a suitable format for a larger scale as well as server-edge device communications, and actually implement the devices. All in all, the future holds much potential, and we will be looking to improve our project to better serve the community.","google-maps, javascript, langchain, openai, openweathermap, python",https://github.com/yDing121/HavenLink
HackHarvard 2024,Teaching Assassin,https://devpost.com/software/teaching-assassin,False,"The inspiration behind Teaching Assassin came from our collective experiences as students and teaching assistants. Krish is a TA at Northeastern University, Monishwar taught classes online, Ganeshkumar taught Java, and Ayaman conducted research on how students learn effectively. As a team, we all shared a passion for education and recognized the common challenges students face. We wanted to address issues such as unclear lectures, high teacher-to-student ratios, lack of personalized feedback, and not knowing the most effective way to study for a subject. Many students struggle with how to structure their learning for tests, midterms, or assignments, not knowing what questions to ask themselves. With Teaching Assassin, we set out to create an edtech platform that provides personalized learning techniques tailored to each student's unique needs, helping them discover the best way to study.","Teaching Assassin is designed to help students identify their optimal learning strategies. Here's how it works: The app asks four preliminary questions based on research in learning methodologies.
The first two questions help determine the type of resource a student might need. Students rate the subject on a scale from 1 to 4, with 1 being declarative knowledge (factual) and 4 being procedural knowledge (how-to).
Then, students choose a level from Bloom's Taxonomy, which helps the app suggest a learning activity—whether it's a quiz, flashcards, long-answer questions, or project-based learning.
The platform presents these suggestions in an intuitive, interactive UI. Students can engage with flashcards, take quizzes, answer questions, and verify their learning.
Additionally, users can save their study materials as a PDF or download them in a reusable format, making it convenient for future use.","To build Teaching Assassin, we conducted research on various learning methodologies and their effectiveness, particularly focusing on how technology can help students by delivering these benefits at scale. We chose a web app format to ensure easy access for all users. The tech stack includes: Frontend: React with Bootstrap for building a clean and easy-to-navigate interface.
Backend: Node.js and Express, with integration of the OpenAI API for generating relevant questions and study materials based on user inputs. Deployment: The app is hosted on Cloudflare and Heroku to ensure stability and scalability. Throughout development, we focused on creating a smooth user experience with a clean design and calming color tones to make studying a pleasant and focused experience.","Choosing the Right Tech Stack: Balancing ease of development with the performance requirements of the platform was initially tricky. Structuring User Input: It was challenging to turn unstructured user input into valuable, structured output in a meaningful way. Optimizing GPT Responses: We had to spend time figuring out how to prompt the OpenAI API effectively to generate relevant learning materials for different use cases. Frontend-Backend Integration: Dealing with CORS errors while integrating the backend with the front end was a significant hurdle. Deployment: We faced a few complications with deployment, ensuring that the platform remained stable and accessible to users.","Successfully deploying the app and overcoming the challenges we faced. Building an intuitive, user-friendly interface that helps students learn in a more structured, effective way. Integrating academic research into our platform and utilizing modern technologies like GPT to provide personalized learning resources.","We deepened our understanding of frontend and backend development, learned how to deploy web apps more efficiently, and improved our skills in integrating APIs and leveraging large language models (LLMs) like GPT for specific tasks. We also gained valuable experience in designing interactive UI elements to enhance user engagement.","AI-Based Performance Tracking: We plan to implement adaptive question difficulty metrics that adjust to the student's progress. More Learning Techniques: We aim to expand the types of learning techniques available to better serve different types of learners. Study Schedules: Teaching Assassin will eventually provide tailored schedules for upcoming deadlines, midterms, or exams to help students stay on top of their studies.","bootstrap, cloudflare, heroku, node.js, openai, react","https://hh24.monishwar.com/, https://github.com/NotMoni/hackharvard2024"
HackHarvard 2024,Buzzly-AI,https://devpost.com/software/email-ai,False,"Our project idea came from the common frustration many businesses face when managing email campaigns. It's frustrating to send bulk emails manually, especially when you're trying to get important updates out quickly, like notifying employees about their paychecks. Furthermore, tracking the success of these emails, such as understanding open rates or click-through rates, is often disjointed and difficult to analyze. We knew there had to be a better, more efficient way to manage this process. With AI and automation, we saw a huge opportunity to simplify email campaigns. We wanted to build something that simplifies the entire process, from designing and sending emails to analyzing performance and optimizing future campaigns. Our goal was to help people save time, boost engagement, and make email marketing smarter and more effective.","Our platform provides an all-in-one solution for creating, sending, and managing email campaigns. Here’s what it offers:",Building the platform involved several technologies and key steps:,,"We successfully built a fully functional platform that integrates multiple advanced features, from AI-generated email templates to payment processing via email.",,"Next for Buzzly-AI, we're focusing on taking personalization to the next level by refining our AI model to create even more tailored email campaigns based on recipient behavior and preferences. We also plan to expand payment options, adding flexibility with options like cryptocurrency and in-app invoicing. In addition, we’re working on an automated follow-up feature that will re-engage recipients who didn’t open the email or clicked specific links, ensuring no opportunity is missed. Our analytics will get even smarter, with predictive insights to help users optimize campaigns before they go live, making campaign success more achievable. Lastly, we’re looking to introduce team collaboration tools, allowing groups to work together on campaigns, share templates, and track performance in real-time, streamlining the workflow for teams.","chequebook-api, google-cloud, llama3.1, mongodb, next.js, semantic-search, websockets",https://github.com/sajanpoudel/Buzzly
HackHarvard 2024,TechedIn,https://devpost.com/software/techedin,False,"Although students studying Computer Science or related fields are equipped with the foundation needed to pursue a career in technology, their education often misses out on providing students with the relevant experience gained through internships and personal projects. Hackathons are a great opportunity for the creation of these projects and learning valuable teamwork and communication skills. Otherwise, students create solo projects on their own timeor, worse, graduate without any real experience under their belt and struggle with finding full-time jobs.","TechedIn is a social media platform that uses AI analysis to match student technologists to one another so that they can share project ideas and build their dream team. Students can input their interests, post their own projects in search of teammates, explore other people’s projects through the AI-recommended homepage, and instantly connect with students who resonate with their ideas. The platform acts as a bridge for collaboration,enabling students to take their concepts from mere ideas to actual production and deployment.","The platform features a responsive design using Tailwind CSS and Mantine. The fixed layout features a menu,a list of the most popular repositoriesthat is pulled and filtered from the GitHub API as a constant source of inspiration for your next big idea, and four core pages:Home,Notifications,Projects, andProfile.
TheHomepage, built with React and Node.js, allows users to create posts to share their own project ideas or view a list of recommended projects, ranked using AI analysis with Cloudflare and Python. Our machine learning algorithm for ranking was conducted based on:Intereststhat users select upon registering for an account and can edit whenever they want,Experienceextracted from analyzing the user’s GitHub profile and statistics for certain skills, tools, topics, and languages,Locationbased on user colleges, so that relevant projects by students closest to each other are prioritized.
Users can express interest in a project with the click of a button. This sends anotificationto the project owner, who can then assess the interested user’s eligibility for the project by perusing theirprofile, which includes the user’s interests andgraphically displayed statistics of their experiencegathered via theGitHub API.
If the project owner accepts the user onto their team, the new team member receives a notification that allows them to instantly email the project owner with a pre-populated messageand get their project started!Our backend consists of two parts: the AI analysis using CloudFlare and the routes for retrieving data from our database using Flask.Terraformhelped us maintain the infrastructure for the complex backend of our project to run on both AWS and Railway.
These components all come together to preview what an end-to-end process from project ideation to team creation would look like on TechedIn.","We spent a significant amount of time planning out what features to include, designing an MVP, and specifying our functional goals. A social media platform incorporates so many moving parts that keeping track of everything and ensuring our frontend and backend are interwoven seamlessly was the biggest challenge. With constant communication and a clear plan, we were able to mitigate any ambiguity and stay ahead of the rough patches.
Another challenge was creating an effective AI recommendation system that accurately matches students with projects based on their interests and skills. Although Cloudflare was a new technology to us, it was the tool that aided us in obtaining a list of recommended projects for our users.","We are proud of successfully creating a platform that not only connects students but also enhances their collaboration experience through AI-driven recommendations. Our team’s ability to stay in communication with each other, collaborate on roadblocks, and keep each other’s spirits up left us with a memorable and satisfying hackathon experience. We’re excited for the foundation of TechedIn that we’ve developed and can’t wait to continue to collaborate with each other, in the spirit of our platform’s mission.","Throughout the development process, we learned the importance of effective communication within a team and the value of adaptability when facing challenges. We also gained insights into machine learning algorithms and their practical applications in real-world scenarios, particularly with the help of technologies like Cloudflare. Understanding user experience principles helped us create a more intuitive interface, making our platform accessible to all students, and keeping in mind the architecture of our web application allowed us to keep all the moving parts of the app in mind.","Looking ahead, we plan to expand the platform's features by incorporating additional tools for project management and collaboration, such as real-time chat and video conferencing. Our demo displays a logged in view of how the site would work, but next steps would include incorporating proper authorization. We also aim to enhance our AI algorithms for better project recommendations and explore partnerships with educational institutions to broaden our user base. At the end of the day, projects are an opportunity to make lifelong connections and have fun sharing your passion with like minded peers, and we would want TechedIn to be a welcoming platform to be able to do just that.","amazon-ec2, amazon-ecs, amazon-web-services, api, cloudflare, docker, express.js, flask, git, github, github-api, javascript, machine-learning, mantine, node.js, postman, python, railway, railwind, react, socket.io, sql, supabase, tailwind-css, terraform","https://github.com/huda-ayaz/techedin, https://phenomenal-puppy-05c189.netlify.app/"
HackHarvard 2024,Health Guardian,https://devpost.com/software/health-guardian-nuwcth,False,My project is inspired by real patient safety problems like RSI when surgical tools are carelessly left inside a patient after surgery causing infections or even organ damage.,"My website provides step by step guidelines specific to the surgery and notifications to remind doctors if they have corrected the basic steps. It will show the most basic step like all surgical instruments are counted before starting the surgery and ask the doctor to mark it as completed when done. It will digitally log each tool used during operation and track their usage status. To make it hands free I have incorporated voice assist where doctors can set a timer based on the priority of the step and get a voice assisted alert.
Before closing the surgical site, the system will notify the team of any card marked as incomplete acting as a safeguard against human error.","The app was built using HTML, CSS and JavaScript.","-Coming up with an innovative solution that was not of the existing solutions. 
-Faced problems integrating voice for the notifications.
-Continuous integration of LLM with my website was challenging.","-Being able come up with a solution that is helpful and addresses the existing problem.
-Creating a functional website and being able to deploy it online","Building a functional website using CSS, HTML and JavaScript",Integrating LLM(s) using API so that all the surgical procedure and their individual steps is up to date.,"css, html, javascript","https://moshiumniah.github.io/HealthGuardian/, https://github.com/moshiumniah/HealthGuardian"
HackHarvard 2024,VeriPharma,https://devpost.com/software/veripharm-8u2pli,False,"We were inspired by the grave impact of counterfeit drugs and the alarming rise of indistinguishable fakes, which pose serious risks to public health. 
Tragically, the people most impacted by these fraudulent pharmaceutical products are often those in need, yet unable to afford their retail price.","VeriPharma leverages blockchain technology to bring transparency and peace of mind to the entire pharmaceutical supply chain, from production to the end user.
Each drug is assigned a unique serial number, with its entire supply chain recorded immutably on the blockchain. This allows end users to easily verify the authenticity of their medications by inputting the serial number, ensuring that counterfeit or compromised drugs are swiftly identified and removed from the market, protecting public health.","We built a decentralized web application using Ethereum's Sepolia testnet for smart contract deployment. The core functionality revolves around several smart contracts that manage drug registration, supply chain transfer, verification, and flagging faulty batches. Stakeholders can use MetaMask to interact with the system, ensuring that every step of the supply chain is recorded immutably on the blockchain.",We encountered challenges in integrating and deploying the smart contracts with our system's front-end.,Being able to successfully deploy our smart contracts onto the Sepolia Testnet and integrating MetaMask was particularly rewarding as we had initially struggled with the environment.,"Through this project, we gained valuable insights into blockchain development, smart contracts, and the intricacies of decentralized systems. We learned how to seamlessly integrate web3 technologies with traditional web development, manage gas fees, and optimize smart contracts for practical applications. Most importantly, we discovered that blockchain's potential extends beyond finance, offering meaningful solutions to support communities in essential industries healthcare.",We'd like to explore partnerships with healthcare officials and regulatory bodies to promote widespread adoption.,"css, html, javascript, python, solidity",https://github.com/aanxshka/VeriPharma
HackHarvard 2024,SOS,https://devpost.com/software/sos-4h501v,False,"Our inspiration emerged from a realization of the need of rapid response in many cases of danger, and how a minute can make a life-altering difference.",,"We spent considerable time brainstorming and defining the app’s essential features. With a dedicated team of three, SOS utilizes several technologies:
Mobile Framework: Next.js
Backend Services: Firebase and Node.js
Database: Firebase Firestore
APIs: Geolocation API, Health Monitoring API, SMS Sending API","Finding reliable partners proved difficult until we matched amazingly. We painfully even paid for an API service that ultimately didn't work haha. We faced persistent 404 errors, which required extensive debugging and collaboration.","We successfully created a fully functional MVP that meets the needs of users in emergencies, showcasing our teamwork and perseverance.","We learned the importance of thorough research and ideation, effective communication, and resilience in overcoming technical challenges. And we also learned the thin difference between persistence and wasting time in some cases.","Moving forward, we plan to enhance the app's features, improve error handling, and explore partnerships for additional functionalities, ensuring a robust user experience. We also aim to implement camera enabling and Apple Watch seamless integration.","apple, firebase, firestore, gcp, google-maps, health-monitoring-api, next.js, node.js, twilio","https://www.github.com/Thabhelo/sos, https://sos-hackharvard.vercel.app"
HackHarvard 2024,FoodBridge,https://devpost.com/software/foodbridge-6frkw9,False,"With half of our team working in the food industry, we saw firsthand the staggering amount of food wasted daily. This inspired us to act, creating a solution that not only reduces food waste but also helps those who are uncertain about where their next meal will come from. FoodBridge was born from our drive to make an immediate impact on both restaurants and the community.","FoodBridge is more than just an app—it's a non-profit 501(c)(3) initiative that gives local restaurants the opportunity to donate their untouched, excess food in exchange for valuable business services. From waste management and deep cleaning to streak-free glass maintenance and free publicity, FoodBridge supports sustainability while offering practical benefits to restaurant owners.","We started with big ambitions, planning to use Next.js and Google Firebase. However, with the intense 36-hour hackathon timeline, we made the strategic decision to focus on the essentials. We pivoted and built the project using HTML and CSS, prioritizing a sleek, functional front-end to ensure our vision was not just a concept but something we could present while we keep building the structure of the app.","During the hackathon, we faced several challenges that required adaptability and quick decision-making. Designing a NoSQL database schema to efficiently manage user data and interactions was a significant hurdle due to the steep learning curve. Additionally, we developed two distinct home pages for logged-in users—one for restaurants and one for recipients—further adding to the complexity. However, time constraints proved to be our greatest challenge. Although we initially planned to utilize more advanced technologies, we quickly realized the importance of adjusting our approach. We streamlined our tech stack, focusing on the essential elements of our project, which allowed us to maintain the integrity of our core idea while meeting the tight deadline and delivering a functional, high-quality product.","Despite the intense timeline and scaling back, we successfully created a fully working app that brings our vision of sustainability and business support to life. Our focus on creating something tangible and impactful within 36 hours is a huge win for our team.","Throughout the project, we gained valuable experience in both technical development and team collaboration. Tackling a NoSQL database schema for scalable data management was a steep learning curve, but it taught us the importance of structuring data efficiently for diverse user interactions. We also honed our skills in React.js and Firebase, learning how to handle authentication, real-time data storage, and dynamic routing for different user experiences.
Beyond the technical challenges, we learned how to adapt quickly in a fast-paced environment. Time constraints required us to prioritize features and focus on delivering a functional product, which taught us how to make strategic trade-offs without compromising the quality of our core idea. Working as a team under these conditions also improved our collaboration and communication skills, ensuring all components of the project came together seamlessly. Finally, we deepened our understanding of user-centric design, tailoring distinct experiences for both restaurants and recipients to create a practical and intuitive interface.","Scalability options: We came into this hackathon with a mission, and through creativity, teamwork, and relentless execution, we’re confident we’ve built something truly game-changing with FoodBridge.","css, firebase, html, next.js",https://github.com/AnasAA98/FoodBridge
HackHarvard 2024,moodeng,https://devpost.com/software/moodeng,False,"The inspiration for this project came from my own need to balance multiple responsibilities across academics, research, and personal life. I often found myself struggling to manage deadlines and plan effectively. Existing tools, such as to-do lists or calendar apps, did not offer the level of personalization or integration I needed. So, I envisioned an AI assistant that could automatically learn from my activities and help me manage not only my tasks but also my habits and overall well-being.",,,,,,,"calendar, google, notion, python, rag, typescript",https://github.com/LiSiruiRay/moodeng
HackHarvard 2024,Educrate,https://devpost.com/software/educrate,False,"The inspiration for Educrate came from the growing need for personalized learning in an increasingly diverse educational landscape. As we observed the limitations of traditional online courses, it became evident that a one-size-fits-all approach was inadequate. We envisioned a platform that could leverage advanced large language models to create tailored and personalized educational experiences, making learning more engaging and effective for users of all backgrounds.","Educrate is a web application that generates personalized courses similar to platforms like Coursera and edX. By utilizing large language models, Educrate tailors course content to meet individual user requirements, allowing learners to create customized educational paths. Whether for personal development, professional training, or academic enrichment, Educrate transforms learning into a dynamic and user-centered experience.",We built Educrate using a robust technology stack that includes:,"During the development of Educrate, we encountered several challenges:",We take pride in several accomplishments throughout the Educrate project:,"Throughout this journey, we learned valuable lessons, including:","Looking ahead, we plan to: Educrate is just the beginning of our mission to revolutionize personalized learning. We are excited about the future and the impact we can make in the education sector!","clerk, drizzle, firebase, gemini, git, github, javascript, neon, next.js, node.js, postgresql, react, shadcn, tailwind, vercel, youtubeapi","https://educrate-product.vercel.app/, https://github.com/ronak-wani/Educrate"
HackHarvard 2024,Gladiator Go,https://devpost.com/software/gladiator-go,False,"We got our inspiration from the various trends and buzzwords that currently dominate the tech space. From established companies to startups to hackathons, everyone seem to be irrationally chasing the newest, hottest thing, even for cases where the technologies do not contribute positively to the user experience. We noticed that many of these trends, such as AI and Blockchain/Cryptocurrency, involve the hoarding of large amounts of GPUs, so we made a game that is based on the phenomenon.
Gameplay wise, we wanted to take that idea to two extreme ends of a spectrum. On one end, you can slowly make money by solving the problems by hand with a pencil and paper; on the other end, you can harness the power of the sun to provide enough energy for an excessive number of computations, getting you a vast amount of wealth.","Our project is a functional idle game. In it, the player can make money over time by buying better computer hardware, or they can actively pitch startup ideas to gain a large boost of funds.",We used the Unity game engine and the C# programming language.,N/A,We are quite proud of our final product and think it has plenty of charm and entertainment.,"Throughout the process, we learned better ways to organize our code and assets which vastly improved our ability to work on further improvements.","The game is very easily expandable now that we have laid the foundation. We wanted to add additional mini-games to play, but we were only able to make the one. In the future, we may add additional things to buy and fill out the rest of the intended mini-games.","c#, unity",https://github.com/mrehmet/gladiatorGo
HackHarvard 2024,PatientVault,https://devpost.com/software/patientvault,False,"place where you put in wallet address and then input data takes in the data, outputs the insights log in landing page Inspiration PatientVault was inspired by the need for patients to have control over their medical data. With increasing concerns about privacy and security in healthcare, we envisioned a platform that empowers individuals to manage their own health information securely. What it does PatientVault allows users to securely store and manage their medical records using blockchain technology. It employs smart contracts for secure data sharing and AI analysis to provide valuable insights about individual health conditions, enhancing the overall healthcare experience. How we built it We built PatientVault using a combination of blockchain technology and AI. The backend utilizes secure blockchain protocols to store and manage data, while the frontend is designed for user-friendly interactions. Smart contracts automate data sharing and permissions, ensuring that only authorized parties can access sensitive information. Challenges we ran into We faced several challenges, including ensuring data privacy and security while implementing blockchain, as well as developing an intuitive user interface that effectively communicates complex information. Integrating AI analysis in a way that provides meaningful insights without overwhelming users was also a significant challenge. Accomplishments that we're proud of We successfully developed a secure platform that meets industry standards for data protection. The integration of AI analysis has provided users with actionable insights, and we have received positive feedback during user testing, validating our concept and design. What we learned We learned the importance of user feedback in the development process. Iterating based on real user experiences helped us refine the platform significantly. Additionally, navigating the complexities of blockchain and AI integration provided valuable insights into emerging technologies. What's next for PatientVault Moving forward, we plan to enhance our AI analysis capabilities to provide even deeper insights into health data. We also aim to expand our user base through partnerships with healthcare providers and institutions, making PatientVault a go-to solution for secure medical record management.",,,,,,,"css, html, javascript, node.js, openai, python",https://github.com/Ridgelow/PatientVault
HackHarvard 2024,EcoCompute,https://devpost.com/software/ecocompute,False,"Logo Landing Page Laboratory Dashboard  We were inspired by a friend's story about his family’s energy company in France, where they sometimes faced negative pricing—meaning they had to pay to offload surplus electricity during periods of low demand. It seemed unfair that people generating clean energy could be penalized for overproduction. As of 2023, over 60 countries generate more than 10% of their electricity from wind energy. However, when supply exceeds demand, prices can drop below zero, forcing providers to pay others to take excess electricity. A striking example occurred in Germany in 2016, where prices fell to -130 euros per megawatt-hour. Meanwhile, data centers consumed 416 terawatt-hours of electricity in 2023, about 3% of global power. This figure is rising, especially with the growing AI sector. We thought—why not harness that excess renewable energy for cloud computing? This idea became EcoCompute. EcoCompute is a cloud computing platform that allows energy providers to monetize surplus renewable energy during periods of negative pricing. By offering affordable compute resources, it converts wasted energy into valuable computing power. Providers can deploy nodes, which dynamically allocate workloads based on real-time energy production. This gives users access to low-cost computing while helping providers avoid paying to offload excess energy. We developed a React-based front end that operates like a cloud notebook, providing an interactive experience for users. This front end connects via WebSockets to a FastAPI server, which serves as the backbone of our application. The server manages connections to energy provider nodes, which are orchestrated using Docker. These nodes not only handle compute tasks but also utilize real-time data to adjust workloads according to energy availability. We focused primarily on German data for wind power, which made it easier to access open-source information. This decision allowed us to build a more robust and reliable platform since Germany has a wealth of data available and is a leader in renewable energy production. Additionally, we incorporated statistical models that predict energy production and pricing trends, allowing us to optimize the allocation of compute tasks during surplus periods. This proactive approach ensures that we maximize the use of renewable energy while providing users with efficient computing resources. Finding open-source data on renewable energy and managing WebSockets effectively were significant challenges. However, by focusing on German data, we found it easier to obtain relevant statistics and trends. We also learned that Europe, being a leader in wind energy, experiences the most negative pricing situations. We’re proud to have created a platform that turns wasted energy into useful compute power. Not only does this reduce energy waste, but it also provides an eco-friendly, cost-effective solution for compute tasks, benefiting both energy providers and users. Through this project, we gained a deeper understanding of renewable energy markets, particularly the complexities of balancing energy production with computing needs. We learned that while battery storage is crucial for managing surplus energy, it can be prohibitively expensive. By focusing on regions with existing infrastructure for energy storage, such as dams in Québec, we can better manage excess energy. Our next step is to enhance the computing experience by allowing users to upload their own containers for execution on the platform. We plan to expand EcoCompute by partnering with more energy providers worldwide and integrating additional renewable sources like solar and hydroelectric power. Our goal is to make EcoCompute the go-to platform for turning surplus energy into sustainable cloud computing.",,,,,,,"colab, docker, fastapi, ngrok, python, react, statistics, weather, websockets","https://ecocompute.live, https://github.com/Olivier520100/EcoCompute_HackHarvard_2024"
HackHarvard 2024,Grapevine,https://devpost.com/software/grapevine-fe5i4j,False,"We wanted to work on a project that 1) dealt with maps, 2) could benefit any urban environment regardless of how others view it, and 3) had a sense of intimacy. We found many of our initial ideas to be too detached—solutions that lacked a personal connection with the communities they aimed to serve. Then we came up with the idea of an application where users could simply look at a map and see all the areas that are recommended by locals, rather than popular locations that overshadow smaller and underrated areas in a community. From this, we expanded our idea to improve upon inaccurate and sometimes predatory apps claiming to protect users from dangerous incidents, yet only warning users when they are proximity to a ""high-crime"" area. By simply showing how often crime really happens in a much more realistic area, users have more knowledge and freedom to decide and understand what's going on in the local community around them. This, combined with local recommendations, lets users get the ""word on the street"" - they would hear it through the grapevine.","Grapevine is an application designed to make it easier for people to get the inside scoop on an area, based on local reports and recommendations. Locals can anonymously submit incident or recommendation reports, with the corresponding mark showing up on the map. Visitors can then search a location and get a map of their immediate surroundings that shows any reports in the area. They can also specify the radius and filter for certain types of reports. Reports also have an upvote/downvote system.","We knew we wanted to build a web application, and so we decided on trying out Node.js and Express.js as our backend framework. Given this, we also decided to use MongoDB to complete the well known ME(no React)N tech stack, and also because of its popularity and reputation for being relatively easy to setup and use (which it was). Our frontend was built very simply with HTML/CSS. For the maps on our frontend, we used Leaflet.js, an interactive map JavaScript library that allowed us to easily display user recommendations and reports.","This was our first time using MongoDB/Express.js/Node.js so there were many difficulties learning these tools on the fly. There were a lot of complications involving missing forward slashes and a good portion of our time was spent trying to figure out how to route pages. Fortunately, we were able to adapt and create a solid code structure that made the rest of our working process easier. 
We also thought that, given how GitHub is way easier when people aren't making contributions every 30 minutes, it would be better to use VSCode's Live Share feature to work collaboratively at the same time. However, this turned out to be more difficult than expected, especially when only the host can see what their code changes do. Despite this, we were able to push through and develop a good finished product that does exactly what we envisioned it to do.","We’re very proud of being able to split the work efficiently and being able to stay organized on top of all of our contributions (given that we were using Live Share instead of Git).
We are also proud of being able to implement the tech stack and use it in an application. We also successfully used Leaflet, an interactive map library for the first time, which was a new learning experience for us.","Since this was a full-stack project that included everything from backend to frontend, there were many aspects that some of us did not know how to do/work with, but learning how to use different resources available to us online, reading documentation,  and just using trial and error until we found something that works out helped us a lot as well in learning how to build an application with this tech stack.","We would like to scale this internationally and find a way to be able to optimize the search function. It would also be good to create a way to verify locals vs non-locals, perhaps through user login and personal information authentication (but still give the option of posting anonymously). We also have ideas of adding routing to the map, so that a user could input a destination and see local reports and recommendations along their route. Finally, we would like to flesh out the upvote system (differentiate between local/visitor feedback).","express.js, leaflet.js, mongodb, node.js",https://github.com/JeffreyWangg/grapevine
HackHarvard 2024,TrueCritic,https://devpost.com/software/truecritic,False,"In today’s world, we’re bombarded with choices, but quantity often overshadows quality. TrueCritic was built to redefine urban exploration by curating only the best-rated venues in smart cities, focusing on genuine quality and recognized awards to cut through the noise.","TrueCritic ranks the top restaurants and hotels, drawing on both user reviews and elite industry accolades. By aggregating data from Google Places, Reddit, and award sites like James Beard and Forbes Travel, it highlights the most acclaimed venues in any area. This focus on quality makes TrueCritic an ideal fit for the smart city ecosystem, where efficiency and informed choices are key.",,,,,"Next, I plan to expand TrueCritic to cover more cities and add categories like cultural attractions to diversify its scope. I’m exploring predictive analytics to spotlight emerging hotspots, keeping TrueCritic relevant and valuable as smart cities continue to evolve.","google-places, kaggle, next.js, tailwind, vercel","https://truecritic.vercel.app/, https://github.com/flyr102/truecritic"
HackHarvard 2024,Traffic Eye,https://devpost.com/software/traffic-eye-dah7ql,False,"We noticed one of the tracks involved creating a better environment for cities through the use of technology, also known as making our cities 'smarter.' We observed in places like Boston & Cambridge, there are many intersections with unsafe areas for pedestrians and drivers.Furthermore, 50% of all accidents occur at Intersections, according to the Federal Highway Administration. This can prove to be enhanced with careless drivers, lack of stop signs, confusing intersections, and more.","This project uses a Raspberry Pi to predict potential dangerous driving situations. If we deduce that a potential collision can occur, our prototype will start creating a 'beeping' sound loud enough to gain the attention of those surrounding the scene. Ideally, our prototype will be attached onto traffic poles, similar to most traffic cameras.","We utilized a popular Computer Vision library known as OpenCV, in order to visualize our problem in Python. A demo of our prototype is shown in the GitHub repository, with a beeping sound occurring when the program finds a potential collision. Our demonstration is built using Raspberry Pi & a Logitech Camera. Using Artificial Intelligence, we capture the current positions of cars, and calculate their direction and velocity. Using this information, we predicted potential close calls and accidents. In such a case, we make a beeping sound simulating a alarm to notify drivers and surrounding participants.",One challenge we ran into was detecting the car positions based on the frames in a reliable fashion. A second challenge was calculating the speed and direction of vehicles based on the present frame & the previous frames. A third challenge included being able to determine if two lines are crossing based on their respective starting and ending coordinates. Solving this proved vital in order to make sure we alerted those in the vicinity in a quick and proper manner.,We are proud that we were able to adapt this project to multiple levels. Even putting the camera up to a screen of a real collision video off Youtube resulted in the prototype alerting us of a potential crashbefore the accident occurred. We're also proud of the fact that we were able to abstract the hardware and make the layout of the final prototype aesthetically pleasing.,"We learned about the potential of smart intersections, and the benefits it can provide in terms of safety to an ever advancing society. Surely, our implementation will be able to reduce the 50% of collisions that occur at intersections by making those around the area more aware of potential dangerous collisions. We also learned a lot about working with openCV and Camera Vision. This was definitely a unique experience, and we were even able to walk around the surrounding Harvard campus, trying to get good footage to test our model on.","We think we could make a better prediction model, as well as creating a weather resilient model to account for varying types of weather throughout the year. We think a prototype like this can be scaled and placed on actual roads given enough R&D is done. This definitely can help our cities advance with rising capabilities in Artificial Intelligence & Computer Vision!","ai, opencv, python, raspberry-pi",https://github.com/syedshazli/Traffic-Eye
HackHarvard 2024,Mood Map,https://devpost.com/software/mood-map-igfd5t,False,"Our inspiration was to leverage multiple libraries and technologies to apply machine learning models to real-world data while maximizing the breadth of impact. We saw the potential for facial recognition and emotion analysis to assist in improving the analysis of individuals' reactions to stimuli for various fields. We captured the essence of this through the implementation of a system that assists therapists in improving the analysis of their patients through emotional trends through visual and auditory expression. By storing this data, therapists can establish a comprehensive database for future analysis, improving patient care through detailed emotional tracking. A patient can have a profile compiled of their sessions with the therapists in which unrealized trends can be made apparent, or trends can be further analyzed by the professionals.","Mood Maps recognizes a patient's emotions through facial recognition and analyzes their spoken words during a therapy session. This data is then stored, enabling therapists to track and reference emotional patterns and verbal expressions with improved results over time. It provides therapists with a tool to better understand their patients' mental states, allowing for improvements in diagnosis and treatment planning.","We used ChakraUI, a React-based bootstrap for React applications for the user interface, OpenCV to create a live video feed, DeepFace AI to analyze facial features, Supabase to quickly and reliably restblish a relational database utilizing 3 tables and 2 foreign keys for effective data storage and retreival. We utilized WebSockets for interoperability between our python-based facial recognition and dynamic front-end. We discovered that facial recognition models (e.g., the DeepFace model) could detect emotions based on facial expressions, while we used Whisper for natural language processing to analyze the patient's speech. We developed the backend using python and Flask, along with many libraries like pandas and numpy for graphs. We stored our relational database and three tables in Supabase. Chart.js was used to visualize emotional data in real-time, giving therapists a clear, interactive view of the patient's state.","Our team knew little about facial recognition and their associated models, so developing an application that analyzes data based on facial recognition was challenging. Having various operating systems among our team members also made progress slow at times due to varying results for varying platforms. We collaborated to discover ways of displaying data that would be most useful to an end user. To tackle this challenge we dedicated time to thoroughly analyzing tradeoffs, and optimizing the development process simultaneously. While having facial and audio based emotion recognition data is interesting, having an adequate way of utilizing it further empowers the information to drive innovation and discovery.","Our creation of Mood Map neatly gathers data through a UI and facial recognition model that is ~75% accurate. We were able to showcase this data in a digestible format for potential healthcare professionals, connecting the backend and frontend with a very dedicated team. Also, the Mood Map platform provides therapists with a dashboard for their patients, a live recording page, and an analytical page all in the name of health. The real-time graphical display of emotional fluctuations highlight the efficacy of the therapist's analysis may improve.","We learned how to utilize AI to streamline the process of making an idea tangible, driving curiosity and creativity to new heights. Through the integration of  a facial recognition model and analytical interpretation into a web-based user interface, we learned how to make data communicate between parts of a system that utilize different platforms to handle and share data. A WebSocket was opened to simulate communication between client and server such that we may run the facial recognition program in tandem with a real-time camera feed. We also learned how to design a system while reoptimize as needed as our system grew.","Our prototype displays the effectiveness of this extra assistance as a tool to the therapist and client. Mood Map will be provided to healthcare providers such that they are equipped with tools to help improve the well-being of others. The next steps to improve our software will require that we train on a custom model that will train against more data to achieve higher emotional accuracy estimates for voice and video. Improvements will be made in terms of processing by moving the platform to the cloud. These steps would assist in improving overall mental health as profiles built to better understand patients are improved per session. The internal framework that differentiates moods and transcription also has the potential to be used in many fields i.e., UX testing, marketing, 3D graphics optimization, special needs accommodations, and more!","ai-models, chakraui, css, deepface, html, javascript, jsx, mediapipe, node.js, numpy, opencv, pandas, papaparse, python, react, react-router-dom, supabase, websockets, whisper",https://github.com/im-a-robo/jint_ws.git
HackHarvard 2024,Muscle Intelligence,https://devpost.com/software/muscle-intelligence,False,"Many fitness enthusiasts struggle with maintainingproper exercise form, leading to injuriesand ineffective workouts. Observing these common issues motivated us to create a tool that provides real-time feedback to enhance workout safety and effectiveness.","Muscle Intelligenceanalyzes users' exercise formsin real-time using computer vision and machine learning, offering actionable feedback on what and how to fix to optimize performance and prevent injuries.","We developed Muscle Intelligence by integrating several cutting-edge technologies. For movement detection and analysis, we utilizedTensorFlow and OpenCV, enabling accurate recognition of various exercise forms. The frontend was built withReact Nativeto ensure cross-platform compatibility, allowing users on both iOS and Android devices to benefit from the app. On the backend, we implemented real-time data processing to deliver instant feedback seamlessly within the app. By combiningmachine learningmodels with an intuitive user interface, we created a smooth and engaginguser experiencethat effectively guides users through their workouts",One of the main challenges we faced wasensuring the accuracy of movement detection. Fine-tuning our machine learning models and building data pipelines to interpret complex body movements required extensive data collection and iterative testing.,"We are particularly proud ofdeveloping highly accurate machine learning modelsthat can analyze a wide range of exercises with precision. Creating a user-friendly interface that is both intuitive and engaging was another significant achievement, ensuring that users of all technical backgrounds can easily navigate and benefit from the app. Additionally, achieving seamless real-time feedback has greatly enhanced the user experience, allowing immediate adjustments to improve exercise form.","Through the development of Muscle Intelligence, we gained deep insights intoadvanced computer vision techniquesand their application in real-time scenarios. We learned how tooptimize machine learning modelsfor better accuracy and performance, ensuring that our app provides reliable feedback. Our experience inuser-centered designunderscored the importance of creating interfaces that prioritize usability and engagement, making technology accessible to all users","Looking ahead, we plan toimplement Dynamic Time Warping (DTW)metric due to its ability to handle slight delays in movement (temporal misalignment). Separately, we will include a broader variety of exercises and types of workouts (cardio, boxing). We aim to developpersonalized trainingplans that adapt to each user's performance and goals, providing tailored guidance to enhance their fitness journey. Integrating with wearable devices like fitness trackers and smartwatches is also on our roadmap, allowing for more comprehensive data collection and analysis. Furthermore, we intend to leverage advanced AI to offer more personalized and detailed feedback, making Muscle Intelligence an even more effective tool for users striving to achieve their fitness objectives. Stay fit, stay intelligent with Muscle Intelligence!","expo.io, firebase, javascript, numpy, opencv, python, react-native, tensorflow",https://github.com/Zhandolia/aura.ai
HackHarvard 2024,Beat the Receipt,https://devpost.com/software/beat-the-receipt,False,"As college who have all recently moved into apartments for the first time, we found that we were wasting more food than we could've expected. Having to find rotten yet untouched lettuce in the depths of the fridge is not only incredibly wasteful for the environment but also harmful to our nutrition. We wanted to create this app to help other students keep track of the items in their fridge, without having to wrack their brains for what to cook everyday. Our goal was to both streamline mealtime preparations and provide a sustainable solution to everyday food waste.","Our app is meant to be simple and intuitive. Users are able to upload a photo of their receipt directly from our app, which we then process and extract the food items. Then, we take these ingredients, calculate expiration dates, and produce recipes for the user using the ingredients that they already have, prioritizing ingredients that are expiring sooner.","Our tech stack consisted of React-Native, Express, MongoDB, Open AI API, and OCR. We used React-Native for our frontend and Express for our backend support. MongoDB was used to store the data we parsed from user receipts, so that way our app would not be memoryless. To actually process and recognize the text on the receipt, we used OCR. To generate recipes, we utilized Open AI API and engineered prompts that would yield the best results.","For this project, we wanted to really challenge ourselves by using a tech stack we had never used before, such as React-Native, Express, OpenAI API, and OCR. Since essentially our entire tech stack was unfamiliar, we faced many challenges in understanding syntax, routing, and communication between the frontend and backend. Additionally, we faced issues with technology like Multer in the middleware when it came to sending image information from the front end to backend, as we had never used Multer before either. However, we are incredibly proud of ourselves for being able to persevere and find solutions to our problems, to both learn new skills as well as produce our MVP.","We are incredibly proud of being able to produce our final product. Though it may not be the best, we hope that it symbolizes our learning, development, and perseverance. From getting our MongoDB database set up to getting our frontend to properly communicate with our backend, we will be taking away many accomplishments with us.","As previously mentioned, we learned an entirely new tech stack. We got to experience React-Native, Express, OpenAI API, and OCR for the first time. It's hard to verbalize what we have learned without talking about our entire project process, since we truly learned something new every time we implemented something.","Originally, we wanted to implement our in-app camera, but due to an unfamiliar tech stack, we didn't get a chance to implement it for this iteration, but are already working on it. Additionally for the future, we hope to allow users to choose recipes that better cater to their tastes while still using soon-to-expire ingredients. Eventually, we would also like to implement a budgeting option, where users can visualize how much of their budget has been spent on their groceries, with our app handling the calculations.","express.js, github, mongodb, ocr, openai, react-native","https://github.com/alicehan8/hackharvard, https://docs.google.com/presentation/d/1wcC5NqYyG6WwZ4v8FJZgiSzgTHeG1F9rOCc0mntDjEY/edit?usp=sharing"
HackHarvard 2024,UrbanEye,https://devpost.com/software/urbaneye,False,We were inspired by the large time delays between when citizens report an issue and when first responders are able to process the information and take action. We realized that these delays are especially pronounced in urban areas where there may be high volumes of calls due to the condensed population. We recognized that the speed of first responder action is crucial in building any community and set out to build a solution.,UrbanEye allows residents to call any phone number and explain any and all details of their emergency to a system that will present this information to first responders in the form of a summary that is organized in our user dashboard. UrbanEye is intended to be used when call traffic limits the amount of human attention each problem gets,"This project was primarily built using Node.js, with Twilio integrated to enable call answering and voice-based interactions. To generate responses, ChatGPT and its APIs were utilized, employing prompt engineering to guide ChatGPT's role and conversational position. Ngrok facilitated communication between the website, private backend, and public Twilio accounts, ensuring smooth integration between local and internet-based functions. Additionally, the Hugging Face API was employed for supplementary tasks like using All-MiniLM-16-v2 for basic sorting.","The primary obstacle I encountered while developing 911 Responder was effectively integrating the diverse API services. Given the backend's critical role in supplying data for significant portions of the frontend through numerous API calls, this challenge was particularly acute. To complete the project, I had to rapidly adapt and familiarize myself with several new APIs. The most significant hurdle was integrating Ngrok, which was not initially part of my design but became essential as the backend matured. Learning Ngrok's applications quickly was a defining experience of the hackathon, highlighting the importance of adaptability and problem-solving in such a fast-paced environment.","Using Ngrok could slow down our 911 Responder app, especially when handling lots of data. We also had to work hard to make sure the app didn't crash if something went wrong and to give users helpful error messages.","Through building the project we learned the invaluable importance of a development schedule, particularly when tackling complex projects. By meticulously estimating the time and effort required for each feature, we were able to effectively prioritize tasks, allocate resources, and anticipate potential challenges. This strategic approach enabled me to overcome obstacles more efficiently and ultimately deliver the project within the desired timeframe. The experience reinforced the critical role of planning and organization in successful project management.","Next up for UrbanEye, I will be finalizing our offerings for all kinds of calls. This will expand the project's utility and provide immediate development opportunities. Additionally, we will further explore the ChatGPT Whisper and Hugging Face APIs. The former was introduced by a mentor, and the latter offers features that can enhance the application. To ensure the highest level of security, we will also prioritize implementing robust data protection measures throughout the development process.","chatgbt, ngrok, node.js, twilio",https://github.com/YeIIcw/urbaneye
HackHarvard 2024,Climately: Your Weather Calendar,https://devpost.com/software/calendar-weather-app,False,"We created Climately to change how people plan their days. Imagine being able to see real-time weather information right next to your schedule! By combining these features, we want to boost productivity and help users make better daily decisions, no matter the weather. The best part? There’s no need to switch to an entirely new calendar; it effortlessly integrates as an extension of Google Calendar, one of the most popular calendars available, enhancing your experience without any hassle.",Easily see your events with real-time weather updates and personalized scheduling suggestions. This helps you plan outdoor activities better and stay prepared for any weather surprises.,"Our Chrome extension delivers real-time weather updates directly to your browser, offering quick, accurate forecasts and alerts without the need for external websites. We chose React for the front end because it makes development easier and helps us create a dynamic user experience. Now, for authentication, we use Google OAuth, allowing users to send their Google Calendar schedules and preferences to the backend, which they can easily adjust in the extension. Our technology stack is designed to create a seamless user experience, with Java Spring Boot powering the backend. We handle multiple APIs to ensure smooth integration, allowing weather-related events to be directly added to your calendar. We dynamically update these events using the Weather API and Google Calendar API. We utilize the OpenAI API for intelligent AI suggestions, evaluating whether to offer recommendations based on weather conditions and user event context, and providing options like rescheduling or alternative plans. We’re hosting our backend on Digital Ocean, and we have exciting plans to launch our Chrome extension on the Chrome Web Store (currently under review), making it accessible to everyone!","We encountered challenges with Google authentication and sending data to the backend from our extension, which involved navigating some complexities. Since this was our first time building a Google Chrome extension, we also had difficulties with data storage on local devices because of permission restrictions. On the backend, we encountered issues with intricate network processes that frequently caused our API calls to crash. We worked hard to fix these problems, which improved our application's stability and performance. Despite these challenges, we persevered and built a product we are proud of.","We take great pride in our innovative idea, which modernizes calendar applications that have long needed an update. The seamless integration with Google Calendar ensures ease of use, eliminating the need for users to switch to a different platform. We are also proud of overcoming the numerous challenges we encountered throughout development, particularly in combining multiple APIs and synchronizing the frontend with the backend—a task that is no simple feat.","Working with the OpenAI API and Google authentication taught us how to integrate advanced features into our app, significantly enhancing its functionality and user experience. Additionally, we navigated the exciting challenge of building and deploying browser extensions, which was a new venture for us. We encountered several issues with HTTP server ports, but we’re proud to say that troubleshooting these problems deepened our understanding of how they operate. Our mentors played a crucial role during this process, guiding us through various challenges and helping us maintain composure while resolving issues. Their support not only improved our problem-solving skills but also made our development journey more enjoyable. Many hurdles had to be overcome to orchestrate the front-end, Calendar, Weather, and Open AI APIs to provide a seamless and reliable experience. Effective use of documentation, Hackathon resources (mentors, workshops), and effective use of AI dev tools were key to the successful completion of the project.","We’re thrilled to announce our upcoming launch on the Chrome Web Store, making the Climately App accessible to a wider audience! We're also focused on introducing new exciting customizable preferences that empower users to personalize notifications and weather insights to fit their unique lifestyles!","chrome, digitalocean, extension, google-calendar, google-calendar-api, google-gmail-oauth, java, javascript, oauth, openai-api, react, spring, springboot, tailwindcss, weather-api","https://github.com/sanijcreates/Climately/tree/temp-branch, https://docs.google.com/presentation/d/10LiCuOTU6kWVbHRiiQqUDDFpyStK5fHX/edit?usp=sharing&ouid=104511934439861259787&rtpof=true&sd=true"
HackHarvard 2024,City Vibes,https://devpost.com/software/city-vibes-g2ps8e,False,"City Vibes was inspired by the idea of making city exploration more personalized and intuitive. We wanted to create a platform where users could easily discover locations that match the vibe they're looking for, whether that’s a cozy cafe, a lively music venue, or a quiet park. Many people rely on reviews, but sorting through them to find a place that fits their specific mood or preference can be difficult. City Vibes aims to simplify that process by integrating user-generated reviews with real-time mapping to create a smarter way to explore cities.","City Vibes allows users to search for locations that match their desired atmosphere by typing in a description of the vibe they want. The app then matches their description with user reviews, filters the results by rating, and displays the top 10 locations on a map. Users can also explore random locations with fewer reviews and submit their own reviews for places they visit. Additionally, each recommended location includes an image, name, rating, and a snippet of a user review to help guide the user’s decision-making. For now, it is just in Boston.","We built City Vibes using a combination of technologies. On the backend, we used Flask to handle the server-side logic and API requests. We stored location and review data in a JSON file and used Python for text matching between user input and reviews. For the frontend, we employed HTML, CSS, and JavaScript to create a responsive and user-friendly interface. The interactive map is powered by Mapbox, which allows users to visualize location recommendations and zoom into specific areas based on their search. The dynamic matching of user descriptions to reviews is handled by a custom algorithm that uses simple keyword matching.","One of the biggest challenges we faced was integrating the Mapbox API for real-time mapping and geolocation. Ensuring that the map could dynamically update based on user preferences, while also displaying filtered locations accurately, took time to get right. We also encountered some difficulties in making the text-matching algorithm robust enough to handle various user descriptions. Ensuring that images were properly displayed in the recommendations section, without breaking the layout, was another challenge we had to resolve.","We are proud of successfully integrating all the different technologies into one seamless user experience. The dynamic mapping feature, which allows users to see their top recommended locations on an interactive map, is something we’re especially excited about. Additionally, building a functional text-matching algorithm that gives meaningful results based on user descriptions was a significant achievement. We’re also happy with the clean, modern design of the interface and how easy it is to use.","Throughout this project, we learned how to effectively integrate several APIs, manage dynamic data, and create a responsive user interface. We gained experience working with Flask for the backend and learned how to store and retrieve structured data in JSON format. We also deepened our understanding of JavaScript and how it can be used to make web pages more interactive. Additionally, we learned a lot about handling asynchronous requests and ensuring that our application remains responsive while data is being fetched. Of course, most importantly we learned that we have a lot more to learn asw this project could have been put together with greater efficiency and even become enhanced with better coding and web development skills.","Looking forward, we want to enhance the text-matching algorithm to use machine learning techniques, allowing for more accurate matches between user descriptions and reviews. We also plan to expand the app’s functionality to include more cities and locations, potentially incorporating real-time data for events and new openings. Another goal is to improve user engagement by allowing them to contribute photos and share their experiences more easily. Ultimately, we hope to make City Vibes a go-to tool for anyone looking to explore their city in a more personalized and meaningful way.","css, flask, geocoding, github, html, javascript, json, mapbox, python, scikit-learn",https://samuel-del-bosque.github.io/HackProject/
HackHarvard 2024,Unikeyboard,https://devpost.com/software/unikeyboard,False,"9:25 AM, New Haven, CT. Ian was in math class, trying and failing to type his notes in LaTeX. Sida in Linguistics class, struggling to juggle paying attention with typing IPA symbols in his notes. Eric was working on web dev, and wished he had a tool to help him with colors and hex numbers. Millions of people around the world were swiping, trying to find that one perfect emoji they wanted to use. Why do different symbols require different input methods? Why do you need specialized softwares and tools to do simple text transformations and replacements? Enter Unikeyboard,the last input method any STEM major—and anyone—will ever need.","Entering Unicode has never been easier. Now, you can type STEM notes directly into Google Docs or Word, without having to use a LaTeX editor or other specialized program. Out of the box, we supportLaTeX symbols,International Phonetic Alphabet,emojis, and more. Withsmart semantic search, Unikeyboard knows exactly which Unicode character you want, even if you forgot its exact name. Type "":japanese culture:"" and get a plethora of related emoji. Or, type ""\union"" and the correct LaTeX symbol (which is called ""\cup"") will come up. If you're a developer, Unikeyboard is for you. Any text conversion you needed a specialized website for, you can now directly perform it as you type—fromRGB/HSVtobinary/hex/decimal, convert them hassle-free.",,"Our biggest challenge, ultimately, is that all three of us had never done Swift before this hackathon. We decided to make this experience a learning experience that reaches beyond our comfort zone into something we are passionate but uneducated about. And it turns out that native development is much more challenging than web development. The first issue we ran into was the very limited and outdated Apple documentation for the IMK SDK. After extensive research, we realized we were one of maybe five public projects that used the SDK, and there's very scarce guidance to navigate through the APIs. We also ran into issues with Apple Code Sandbox, and had to change our approach to loading semantic data from the server. We are especially grateful for CloudFlare's super-easy-to-deploy Worker AI product that helped us set up a production-ready API in 30 minutes, after we battled with CoreML, PyTorch, sandboxes, and whatnot for 6 hours.","First of all, we make a really cool tool that we will actually use in class. We are the first users of our own product and our own experience will help driving it forward. Looking back, we are especially proud that we persevered, even though Google and StackOverflow were little help with the poorly documented SDKs. Within 36 hours, we went from absolute ignorance about Swift and macOS development to somewhat fluent app developers who can turn ideas into code. Finally, we are proud to have integrated many different aspects of technology—native app, web scraping, self-hosted model, ChatGPT API, etc.—into a single and coherent product, in a way that proves to work and works well.",,,"chatgpt, cloudflare, huggingface, python, swift",https://github.com/ericyoondotcom/KeyboardProject
HackHarvard 2024,Kormo Mela,https://devpost.com/software/kormo-mela,False,"Instant messaging Customer Login Built in Chat option Service Provider Login Customer Signup Requests recieved by customers of service providers who willing to work for them. Upcoming services to be recieved by the customer Confirmation Screen of Service Provider- Clearly outlines all upcoming and past tasks, alongside feedback/rating option for completed tasks. Job posting screen continuation List of services already recieved by the customers, alongside feedback/rating option. Homescreen of service provider which should all relevant job postings Details of the request recieved by customers of service providers, containing more specific information of the service providers. First screen after a customer logs in is the job posting screen. What it does
Kormo Mela is a platform designed to connect consumers with verified blue-collar service providers. Through the app, users can post job requests, review service providers based on their profiles and feedback from previous customers, and even watch introductory videos of the workers. The app also features direct communication between consumers and providers, making it easy to negotiate terms and finalize jobs quickly and efficiently. Ultimately, it aims to reduce unemployment by offering a streamlined, trustworthy way to find work and hire help. How we built it
We built Kormo Mela using React Native for the front-end to ensure a cross-platform mobile experience. For the backend, we relied on Django with PostgreSQL to manage user data and transactions. We focused on building a functional platform that included user authentication, job posting, and provider review systems, ensuring that users can easily find and hire verified service providers. Challenges we ran into
One of the key challenges we faced was building a robust verification system that ensures service providers are trustworthy and reliable. Given the time constraints, integrating the verification process, setting up real-time communication, and building a seamless user experience all required rapid iteration and problem-solving. We also had to balance creating an interface that is intuitive for users in developing countries, where tech literacy may vary. Accomplishments that we're proud of
We’re proud of building a functional platform during the hackathon. We managed to implement core features such as job postings, user verification, and a real-time communication system, making it easy for consumers to interact with service providers. The app addresses a real-world issue, and seeing it come to life in such a short time frame was a rewarding experience for the whole team. What we learned
During this hackathon, we learned how to efficiently collaborate under pressure, prioritize key features, and pivot when necessary. We also gained a deeper understanding of the technical challenges involved in building scalable applications for users in developing countries. Additionally, the importance of user experience design, especially for platforms serving diverse audiences, became even more evident as we worked through multiple iterations. What's next for Kormo Mela
Following the hackathon, we plan to refine the app’s features, especially around the user verification process, and further enhance the UI to cater to a broader audience. Our next step is to onboard more service providers and test the app with a small group of users. Ultimately, we aim to scale Kormo Mela across Bangladesh, integrating additional services such as home repairs and tutoring, and improving the app’s security and reliability.",,,,,,,"django, javascript, postgresql, python, react-native",https://www.figma.com/design/tNrJateDZXErW1O4MJiLOD/Hack-Harvard?node-id=0-1&t=oinN0MiNinDitlyi-1
HackHarvard 2024,steersafe,https://devpost.com/software/steersafe-yuh1fp,False,"Our inspiration forsteersafecame from a desire to make a tangible impact, particularly in the dense urban areas where we live and commute daily as students. Car crashes, especially those caused by distracted driving, are a universal issue that us as college students relate to firsthand, often witnessing them happen on our campuses. Distracted driving leads to countless accidents, injuries, and fatalities each year. We wanted to create a solution that rewards those making an effort, all while promoting the concept of safer, smarter cities.Watch our hype video!",,,,,,,"core-location, coremotion, firebase, firestore, swift, swiftui, tomtom",https://github.com/joonhoswe/SteerSafe
HackHarvard 2024,Vigilo,https://devpost.com/software/vigilo,False,"The name of our app, ""Vigilo"", is the Latin word for ""protect"" and also has similarities to ""vigilance"" in English, which we used to express the concept of a community-based platform that focuses on offering shelter and relief to those in need. The project is motivated by recent events of natural disasters that has wrecked havoc on various regions around the world, from Hurricane Milton in Florida to Typhoon Yagi in Southeast Asia, with an emphasis on supporting rural communities through these crises.","Vigilooperates through local volunteer members, who are willing and capable of supporting those in need during natural disasters. This enables government programs to cut down on costs of building more shelters and grows resilient communities in rural areas by increasing the support network during disasters. The program offers real-time location tracking as well as texts and calls between volunteers and clients, which also enables more efficient communication and helps individuals find shelter and relief sooner.Vigiloalso uses a geolocation API to assign nearby volunteers to clients and incorporate real-time weather forecasting data to help clients find the most efficient route to safety.","Our app was built usingThunkable, which is a software development platform that allows anyone to create an app without much coding experience. This was the most realistic choice for us, as we were new to app development and needed a simple platform that can perform complex tasks without having too much of a learning curve. We also developed our own database system using Google Sheets to ensure a more personalized user experience, as well as embedded Google Map to drop pins that enable volunteers and clients to track each other's locations.","Initially, we faced some challenges in narrowing down the scope of the project to something that we felt like was feasible with our skill levels and experiences as new hackers. While we had ambitious ideas, we had to scale down or change some of our initial plans as we discovered the challenges of implementing these features on theThunkableplatform, which is limited in terms of which platforms it is compatible with. A major challenge that we encountered was also storing and accessing the data, where we attempted to useFirebasebut was unable to connect it toThunkable. We ultimately resolved this issue by switching to a Google Sheet database system instead, which enabled us to transfer and store the data onThunkableto create a more personalized user experience for the volunteer app users through a registration and sign in portal. Lastly, we also faced challenges in integrating a disaster map into our system, where we realized how difficult it is to incorporate real-time data into our program and ultimately resolved to simplifying the app to a location drop instead.","We were able to get the map to display the data points of clients and volunteers. We were also able to run our system independently so that people can sign up and have their databases stored in our spreadsheets, which was surprisingly challenging throughout the hackathon.","We learned about control flow and how to manage real-time data in our system. Furthermore, in the process of finding which platform to use, we learned a bit about Swift, Firebase, and MATLAB, which are tools that we look forward to exploring after the hackathon but were unable to incorporate into our projects due to constraints in time.","After HackHarvard, we have some long-term vision & development for Vigilo. Firstly, we hope to incorporate interactive and easy-to-follow tutorials and tips to increase disaster preparedness within communities and focus on prevention, which has been shown to benefit individuals in becoming more resilient to natural disasters. We would also like to explore AI-powered tools for building prediction models instead of relying on real-time data to inform users on the weather, which could increase the disaster preparation time for both clients and volunteers. Lastly, we also aim to have reward incentives for volunteering to build a larger network of local shelters available and also add a transportation support feature to the app for clients who are unable to safely move themselves to the shelters.","google-spreadsheets, thunkable",https://github.com/gracefeng28/vigilo
HackHarvard 2024,Hospital Inventory Manager (2),https://devpost.com/software/hospital-inventory-manager-2,False,"Our inspiration for developing the Hospital Inventory Manager stemmed from the pressing need for efficient healthcare resource management and improved patient outcomes, particularly in the context of organ donation and allocation.","The Hospital Inventory Manager is a comprehensive platform designed to streamline various aspects of hospital resource management, including inventory tracking, scheduling, patient management, and organ matching. The system incorporates QR code functionality for quick access to vital information.",,"We faced several challenges, including:",We take pride in several achievements:,"Through this project, we gained valuable insights:","Moving forward, we plan to: We believe that with further development and refinement, the Hospital Inventory Manager has the potential to significantly enhance healthcare resource management and improve patient outcomes.","flask, postgresql, python","https://hospital-im.replit.app/, https://github.com/lucas-chu/care-cost"
HackHarvard 2024,BinIT Right,https://devpost.com/software/binit-right,False,We wanted to get some hands on experience with LLM APIs and both me and my partner were concerned about the problem of improper waste disposal that hampered sustainability because often people would be confused about the categories of waste disposable.,"The App, even though, it is not functional as of now, prompts the user to put a photo of the waste and if possible, the description of what it is and from these information, the App tells the user whether to recycle, reuse, compost, trash the waste or if it is a hazardous waste.","We built a user interface using html, css and javascript on the front end and in the backend, we used python flask, and API calls to chatGPT programmatically. The program collects the responds and sends it to the user.","We had to learn how to use flask, and css. We also had to learn using gitHub which made it easier to share the code and files. We also had to learn to use AI API integration and importing a lot of packages and imports to run these. The main challenge that we ran into was the version mismatch of chatGPT code that we got and the code that we were expected to have, we were not able to figure it out.","While, we were not able to figure out the version mismatch of chatGPT access, we are really proud that we even attempted this endeveour in the first place. It was fun to built this project as we learnt a lot of GitHub skills, some languages and AI API calls. It was great overall and we want to continue with this project.","We learnt JavaScript, HTML, CSS, Flask, GitHub, AI API calls. Most importantly, we learnt about ourselves and the work we can accomplish as a team. We also heard about other cool stuff happening in the field of Machine Learning, Data Science and AI that we are interested to checkout once we get enough experience.","We are planning to extend BinIT and make it better than it is right now. We want to make it a webpage that the college can accept so we can make qr links to the webpage that we will put in front of trashbins, compost bins and recycle bin that makes trash separation easy.","ai, api, css, flask, html5, javascript, json, pycharm, python",https://github.com/shamshertamang/HackHarvardProject
HackHarvard 2024,CareCost,https://devpost.com/software/carecost-e6gpb3,False,Ever book a hospital appointment and then find out the cost later? Many people struggle with understanding the potential costs associated with various medical procedures. This tool aims to fill the gap by offering a reliable estimate of these costs.,"We want to provide transparent and accessible healthcare cost information. CareCost is designed to estimate the costs of different medical procedures based on user input, such as procedure type and location (ZIP code). It can give users a breakdown of potential costs, including lower and higher estimates, typical insurance costs, and additional considerations like complications or recovery times.","CareCost is built using Flask as the web framework, with a frontend likely composed of HTML templates such as templates/index.html for user interaction. The backend uses Python to process inputs and generates prompts tailored to specific procedures using methods defined in app.py. The application integrates with OpenAI's API to find cost estimates.","Developing a comprehensive healthcare cost estimator involves handling diverse data inputs, ensuring calculations are precise, and making sure the tool is intuitive for users. Additionally, integrating with external services like OpenAI's APIs presented challenges regarding versions and response handling.",I'm proud that this hack didn't take me that long to make.,"Throughout the development of CareCost, I learned valuable lessons about healthcare pricing, web development (Flask, HTML/CSS), and API integrations.","Moving forward, CareCost could expand by adding more procedures, refining estimates with up-to-date data, and possibly incorporating user feedback mechanisms. Improvements in UI and further leveraging AI models for dynamic estimation are other potential areas for advancement. I also want to explore additional data sources to increase the accuracy and coverage of cost information. Additionally, improving accessibility and mobile-friendliness would ensure broader user adoption and satisfaction.","flask, html/css, openai",http://carecost.replit.app
HackHarvard 2024,ConnectHealth,https://devpost.com/software/connecthealth-d3ywha,False,"The healthcare industry often struggles with challenges related to data security, management, and interoperability between hospitals.ConnectHealthwas created to address these challenges by:","Ruff is a helpful and interactive feature designed to assist healthcare professionals with quick access to patient data and medical advice. Built using React, Firebase, and Axios, Ruff operates through a chat interface where users can type commands to retrieve specific information or get advice. Ruff is a lighthearted, dog-friendly companion, providing quick and accurate information!",,"We restarted 9 times!  Most of which revolved around integrating CORS with the front end react. Processing data in the backend while trying to fix these errors pushed us back several time. 
Being able to set up a data structure and being able to combine all the features together so they are able to automatically and seamlessly.","Ruff is a smart AI that evolves with the software. As it processes more data, it detects patterns in patient histories, treatments, and outcomes, continuously improving its accuracy. Over time, Ruff’s success rate increases, offering better recommendations for medications and treatments. Ruff adapts to new data, making it more effective at predicting successful outcomes, enhancing patient care, and streamlining decision-making for doctors. The more Ruff is used, the smarter and more reliable it becomes, providing valuable, data-driven insights to healthcare professionals. By being a team of 2 and being first time hackers it was a lot of pressure, however, we walked in with a goal on our minds and we knew each other’s skills that helped us innovate better, for the whole hackathon we had around 2-3 hours of sleep. But as we passed through multiple hours of coding it seemed just as another programming project.
Proper planning and time management allowed us to complete our project parts on time.","We learned to think quickly and rely on each other, understanding that if one person makes a mistake, it affects the whole team. This taught us the importance of collaboration and accountability, reinforcing the idea that success is a shared responsibility. Additionally, we gained valuable skills in organizing and structuring complex databases, ensuring they were efficient and scalable. We also dove into the process of creating AI, learning not just how to build these intelligent systems but also understanding the underlying mechanics of how AI learns, adapts, and improves over time.",,"checkbook, css, firebase, html, javascript, newsapi, node.js, openai, react, visual-studio",https://github.com/DiegoCico/connecthealth
HackHarvard 2024,Trashure Hunt,https://devpost.com/software/trashure-hunt-7x6241,False,"I was walking through the trail near our school when we all realized that there were lots of little pieces of trash on the sidewalks that no one bothered to pick up. It soon occurred to me that it was prevalent near all of our communities, and that there was no easy solution to this common problem. To address this, I created TrashureHunt. A platform where you can build a community but also play a game, two things that humans love. To add some statistics to back up our dilemma, sustainability faces huge impacts from trash that is improperly disposed, locally, North Carolina spent $15 million removing 7.5 million pounds of roadside litter. This means that it was not only an issue that affected my local community, but the whole state as well.","It is an app that is built on the idea of community and gamification. The app provides a platform in which users can create a profile and sign up for ""TrashureHunts"" hosted by other members on the platform. Hosts are verified by our app through a basic credibility process to maintain trust and are able to host trash collecting events that everyone else on the platform can sign up for. Just imagine, your friend is hosting a trash collecting event, wouldn't you go? Of course you would! It is a great bonding activity, and similar to games like PokemonGo, is a real-life version of collecting. When you go to an event, you can record how much trash you collect and how much time you spent to be put on our leaderboard.",,,,,"I plan on launching our app on platforms such as the app store so that a larger audience of people are able to access it, and I am truly able to make a difference with our creation. I may also work on redesigning the UI and making the ""game"" a little more fun, in order to attract more users. That is my plan for the future, and I am really looking forward to it!","css, figma, github, html, typescript",https://github.com/MayMing5/Trashure-Hunt.git
HackHarvard 2024,Optimized Trash Disposal System,https://devpost.com/software/optimized-trash-disposal-system,False,"We were inspired by movies where trash are burned, flown away, or simply zapped. We wanted to create some kind of system that makes trash an issue of the past.","Our project looks at sensors in trash cans to see how filled they are. From there, the program uses the sensor data to create an optimized route for trash disposal teams to pick up the trash in the most cost-effective and fuel effective manner. This ensures that the trash that needs to be picked up will be picked up, and the trash cans that aren't full can wait to be picked up.","The backend is built with Node.js and Express, handling sensor data and determining which bins need attention (if their fill level exceeds 80%). It also uses the Google Maps API to generate optimal routes. The frontend is built with React, displaying the bin locations on a map and showing optimized routes. MongoDB stores bin data like fill levels and locations. A Python script simulates sensor readings by sending data to the backend every few seconds, helping to demonstrate the system in action.","Some challenges we worked with included trying to work with google API's. Relying on external APIs (like Google Maps) also created some rate limitation issues. We also had to consider some scalability issues as adding trash cans, working with theoritical broken sensors, and thinking about traffic in route optimization.","We're proud to understand how to connect with multiple different coding sources, and split and deligated tasks in order to work together as a team. I think the primary achievement was figuring out how we can work together to make the best plan and strategy.","We learned how to primarly work with API's like google maps which taught us how to make API requests, handle responses, and troubleshoot issues like rate limits and authentication errors. We also learned how to handle large amounts of data and work with a team to discover, fix, and save errors in our code.","We need to impliment real-world data to see if this works realistically with things such as broken sensors, amounts of traffic, and other factors like different trash trucks, amount of garbage being held.","axios, express.js, gcp, google-maps, javascript, python, react",https://github.com/HackAtiksh/hackharvard.git
HackHarvard 2024,WanderWays,https://devpost.com/software/wanderways,False,"We love walking, but not everyone does so we wanted to find a way to make it more enjoyable as well as show more appreciation to the beautiful sights different cities have to offer!","WanderWays give you the most scenic route between two different locations, allowing the user to detour to landmarks, walk through gardens, and stroll past historical sights will still being efficient in terms of time.","We used the Foursquare Places API to gather data on scenic spots within a certain boundary given the coordinates of that location. We then created a path finding algorithm that weighted how ""scenic"" a spot was along with how on the way or out of the way it was from the shortest path. Finally, we used Mapbox APIs to create an interactive map where the user can enter their starting location and ending destination and get a scenic path.","One of the main challenges was identifying what makes a route ""scenic."" We had to analyze various factors to create a scoring system that accurately represents scenic quality.",,"Through building WanderWays, we gained significant experience in front-end development, particularly in creating an intuitive and visually appealing user interface. We also learned how to work with various APIs that we hadn't seen before.",,"foursquare, html, python",https://github.com/seaking117/location-map
HackHarvard 2024,Eye In The Sky,https://devpost.com/software/eye-in-the-sky-sqol5g,False,Our team was initially inspired to create this project by the concerningly high number of insecure cameras around the world. We wanted to raise awareness on how easily accessible these cameras are and their prevalence globally.,"Eye In The Sky is an interactive world map that geolocates and provides links to insecure cameras around the world. It utilizes OSINT techniques such as Google Dorking and anOpen SourceShodan.io script to find vulnerable security cameras, laptops, personal cameras and more. Additionally, we have a counter to keep track of the number of cameras found, with around 300 currently mapped.","For the frontend, our team used React, JavaScript, and HTML/CSS, and for the backend we used Python and Flask. The project is hosted in an AWS environment using Defang. The Shodan API was used to scrape web cameras and the IP2Location API was used to get locations from the IPs. Coordinate data and the URLs were put on a REST API using Flask-RestX. We used MapLibre to display camera markers on an interactive map.",Our biggest challenge was making the user interface smooth and efficient while also intuitive and quick-loading. We spent a lot of time working with the React front end to ensure maximum user friendliness and performance. These issues were resolved using memorization and debouncing.,Our team is proud that we were able to efficiently visualize the hundreds of unsecured cameras we found online. We believe that this project provides a new perspective to just how widespread unsecure cameras are on almost every continent of the world.,"While working on Eye In The Sky, we learned how to efficiently delegate tasks to each individual member of the team based on their individual strengths and prior technical background. This project also practiced our team's problem-solving abilities with unfamiliar technology such as Defang and MapLibre.","In the future, we plan on improving the user interface of the application and creating an authentication system for individual users. On the backend we will implement more precise Google Dorking techniques through the usage of LLMs and enhanced web scraping techniques.","amazon-web-services, css, defang, flask, flask-restx, html, ip2location, javascript, maplibre, open-source, python, react, shodan","https://eyeinthesky.live/, https://github.com/henmel1/hack-harvard"
HackHarvard 2024,Recycletron,https://devpost.com/software/recycletron-498ozq,False,,,"“Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of Detectron and maskrcnn-benchmark. It supports a number of computer vision research projects and production applications in Facebook.” - From Meta/Facebook GitHub README.md. Object identification is a class of novel computer vision algorithms that uses advanced methods to find multiple objects within one image. This contrasts with traditional image classification techniques that can only put one label on an image. It does this by first learning how to “split” an image into multiple possible regions where images could exist, and then runs a Convolutional Neural Network, Swin Transformer, or other computer vision models across each region individually. Detectron2 is a library that allows users to test multiple advanced methods, already trained on enormous data of everyday household objects. 
For more specifics, feel free to check out Detectron2’s detailed documentation.","The largest challenge was finding a class of models that would fit our use case, and being able to understand it well enough to use it. Traditional Convolutional Neural Networks would work okay, but our vision was to have a model that could take a pile of waste and determine what is recyclable and what is not; after all, in households and in recycling factories, where we hope to deploy this technology to facilitate the recycling process and incentivize individuals to recycle, there aren’t usually ways to individually separate each waste material into its own individual picture to then classify. We were able to find object identifying networks, but there are many versions, including YOLO models, R-CNN, and Weak Supervised Segmentation, and finding a codebase that had prepackaged models as well as useable modules was tough. 
We also ran into challenges when we finally looked to deploy our model. Though these models are extraordinarily fast, they used lots of memory which made it tough to deploy these models through virtual servers. Since we didn't have advanced GPUs, we had to push these models through our cpus, which made them a lot slower and harder to host. We got around this by hosting our backend on one computer, pushing all the calculations from there before sending it back to the frontend for the user to view.","We're proud of getting a model connected to a frontend service and able to run fast inference on any image given by a user. For many of us, this is our first time working with AI models, and object identification is an exciting field still undergoing development. We're excited to have been a small part of it.",,"We wanted to make Recycletron more gamified, so that users could log in, upload images, and be rewarded for recycling and also adding to our database of recycled items. We added a basic framework for username, password, and score storage but in the future we would like to fully develop this into a sort of game. 
While deploying our model, we struggled with finding a way to host the network. Virtual machines would get overloaded with their low memory, and our computers did not have GPUs, sometimes making inference slow, especially in real-time. In the future we'd like to get a small server with a GPU to have a centralized place for users. 
There are more advanced object identification models trained on over 20,000 different objects and ready-to-go. An example is the Detic model created by Facebook, which is much more robust than Detectron2. However, these models were too heavy for us to use for inference","css, detectron2, flask, html, javascript, python, pytorch, torchvision",https://github.com/samc5/Recycletron
HackHarvard 2024,FNAF VR,https://devpost.com/software/fnaf-vr,False,"I LOVEFive Nights at Freddy's(FNAF)! 😍 But... sometimes Ireallydon’t feel like studying 😅. So why not mash them together? 🎯 Now, withFNAF VR, you can play and study at the same time! Studying has never been this thrilling—answer questions or get spooked! 😱 Step into a spooky FNAF world where you’ll have to answer questions to stay safe! 🧐 If you get it right (and fast ⏰), you win! 🎉 But… if time runs out or you answer wrong… get ready for a jumpscare that’ll give you chills! 👾💥 This awesome (and scary 👀) mashup came together with: Getting the spooky lighting just right! 💡🕯️ We wanted it to be creepy enough to give you goosebumps but still clear enough to see the questions. Balancing horror with learning = not easy! 😅 We made our first-ever VR horror game!! 🎉🤩 We’re super proud of mixing FNAF thrills with a fun, educational twist. Learning has never been so spooky and exciting! 👻📚 We’ve got BIG plans! 🚀 Here’s what’s coming: So get ready tostudy or screamwithFNAF VR—because learning should be as fun (and terrifying) as a jumpscare! 😱🎉📚",,,,,,,"c#, gemini, unity",https://github.com/WilliamUW/HackHarvard
HackHarvard 2024,Med-O-Matic,https://devpost.com/software/medicinermanager,False,"Reading about issues with patient safety was... not exactly inspiring, but eye-opening. Issues that were only a matter of human error (understaffed or forgetfulness or etc) like bed sores seemed like things that could easily be kept track of to at least make sure patients could get a heightened quality of life. So we decided to make an app that tracked patient wellness and needs, not necessarily just concrete items, but all the necessary follow-up items from them as well.
We understand that schedulers for more concrete events like appointments already exist, but something that can remind providers to check up on patients in 3 days to see if they have had any side effects to their new prescription or any other task would be helpful.","The Med-O-Matic keeps track of patient needs and when they're needed, and sets those needs up in a calendar and matching to-do list for a team of healthcare providers in a hospital to take care of. Providers can claim tasks that they will get to, and can mark them down as they go throughout their day. This essentially serves as a scheduled task-list for providers.","To build the frontend, we used Vue.js. We have a database holding all the tasks on AWS DynamoDB.","Getting started was a bit difficult and we weren't really sure which direction we should take for Med-O-Matic. There were a lot of uncertainties about what exactly would be best for our application, so we had to delve in a bit deeper by thinking about what the current process is like at hospitals and clinics, and finding areas for improvement. This has led us to addressing a process issue in task assignment to reduce the number of errors associated with inattentiveness.","What makes our application different than others is that you can sequence tasks and use these sequences as a template. For example, a procedure like heart-surgery always has required follow up steps. You can create a heart-surgery template, that will be used to set all the required follow-up steps. After the template is created, we can easily reapply that template however many times we want!","We learned how to deploy using DeFang, and also how to connect our frontend with DynamoDB. And we learned more about the domain of our project, which is patient safety.","More automation would be next. We've already got some bit for making sequences of tasks, but features like a send-a-text feature for example to make the following-up-on process easier would be next- in other words, we'd add features that help do the tasks as well, instead of simply reminding providers of what they need to do.
We would also connect it to some medical scheduler like EPIC's API, like EPIC. This would allow us to 
really get the task sequencing working seamlessly with a real workflow, as something like a surgery can be scheduled in epic, happen, and then trigger the Med-O-Matic to create all the necessary follow-up tasks from that.","amazon-dynamodb, defang, javascript, node.js, typescript, vue",
HackHarvard 2024,POCKET-HELPER,https://devpost.com/software/pocket-helper,False,"As a clg student its very difficult to manage your day to day expenses and make a list of that ,I myself is suffering from same condition so to avoid this I love to use my programming skills to create a webpage that will record our data and show it to us when needed",It keeps record of all our previous transcition and shows us our  available balance and sents us msg when balance is less than 80% 0f the amount at starting,"We built is using HTML,CSS,JAVASCRIPT,JASON and PHP",Main challenges were to integrate backend in the project to save and retrive the data according to our will,we proud of successfully integrate all aspects into one making a beautiful project,,,"css, html, javascript, json, mysql, php",
HackHarvard 2024,HookBook,https://devpost.com/software/hookbook,False,"There is a literacy crisis in America. A study in 2022 showed only 31% of 8th graders were reading at or above grade level. 83% of teachers surveyed have noticed a drop in their students' reading stamina since 2019. Reading and communication is the most important skill in the modern economy, and the American economy loses an approximate $225 Billion every year due to low literacy levels (this is $1.19T globally). At the same time, young people are rarely reading independently due to addiction to technology, specifically platforms such as TikTok, YouTube Shorts, Instagram Reels, and more. If we can get young people to spend as much time reading and improving their literacy skill, as they do mindlessly scrolling through online content, they will rapidly improve their skill and subsequently their future success and quality of life.","HookBook makes reading addictive by making reading more like TikTok. In HookBook, users swipe and tap through an infinite feed of engaging 'hooks' about short stories. We have 1,000 real short stories from an open data repository that we have rewritten using AI to be accessible to young people, while keeping the plot and characters as dynamic and relatable as their authors have made them. After readers find a hook that catches their interest, they tap to read more, they can ask questions about the story as it unfolds, and when they finish the story, they answer comprehension and discussion questions to show what they just learned. Then, our recommendation algorithm learns the student's interest based on which stories they pick, and tailors the next story shown to those exact interests.","This was built using an open source dataset of short stories, a starter Django template we found–from which we customized to suit our needs, Figma for design work, Cursor for assisted development, and a plethora of other minor frameworks and packages.","Some challenges we ran into include finding the right short story data set, and then fine tuning our ML re-writes of them, creating a cosine similarity recommender system with limited resources, and deploying a full stack app on a tech stack most of us weren’t familiar with. Our idea is big and so are our responsibilities.",Some accomplishments that we are proud of is being able to support a working system that carries out the intended task. Trying out the product for ourselves was not only fun but amazing as well. HookBook serves its purpose as encouraging reading amongst teens who are constantly living within the digital world.,"We learned a lot of things at HackHarvard. One primary thing we learned is that it takes a lot of trial and error with figuring out how to create a product that solves a complex problem with an efficient solution. Although we communicated and cooperated thoroughly throughout, we also learned the value of self management and specialization. We also learned a lot of technical skills ranging from going from no knowledge of Django to writing a full stack application.","Future plans would be to contact schools and parents to sample current features. Within a year, we expect to see significant growth and feedback without our web app to have a sense of most demanded features. One feature we plan on implementing soon would be to have small and simple AI generated images that match each story and have them appear along the text as the child reads. Another feature we plan to include is augmenting the recommender system with user input (such as likes for good hooks). And last among the many other features we plan to implement is to tag stories with genres and allow users to explore the stories according to their desired genre.","css, django, elevenlabs, html5, javascript, openai, python, sass, sqlite, typescript",https://github.com/mchadaga/bookly
HackHarvard 2024,Mental Health App,https://devpost.com/software/mental-health-app-u4z9rf,False,"                     AI USED: LLAMA-8B-8192, FACE EMOTION RECOGNITION MODEL, GEMINI/ANTHROPIC/MISTRAL SUPPORTED (API_KEY REQUIRED) APP STATUS: ALL CORE FUNCTIONALITIES WORKING (Deployed NodeJs & Postgresql on Render) KEYNOTE : FINDINGS WHILE USING AI APIs DEPLOYMENT AND UNREFINED USAGEIS THE SIMPLISTIC APPROACH TO INTEGRATION. IT TAKES LESS TIME TO GENERATE THE CONTENT AND FETCH IT IN JSON Format for further processing by the DOMAIN LAYER entities. MANY OF THE PAID APIs ESPECIALLY ANTHROPIC-CLAUDE-SONET-3.5 AND MISTRAL-LLAMA-7.3B REQUIRESLIGHLTY MORE PROCESSING TIMETHEREBY INCREASING THE REQ-RES-STATUS TIME. FINE TUNING OF THE MODELSIS A TIME TAKING PROCEDURE AS THE DATASET GEN FOR REQUIRED OUTPUT IS TO BE MANUALLY GENERATED OR IMPLEMENTED. THE AI-API USED FOR ILLUSTRATION PURPOSES  ""LLAMA-8B-8192"" (previously ""GEMINI-1.5-FLASH"") DOES PROVIDE ACCURATE RESPONSE FOR THE FOLLOWING SITUATION (AS COMPARED TO PREVIOUS ONE) :TIME SYNCACROSS THE APP (i.e definition of morning etc.)PROPER JSON FORMATING(INCLUDES THE WORD ""json"" IN RESPONSE WHICH IS 
                    TO BE OMIITED SPECIFICALLY FOR THE INNER 
                    LAYER ENITITES TO FUNCTION) ANTHROPIC-AI/SDK API(claude-3-5-sonnet) WAS FOUND TO BE MOST SUITABLE AS COMPARED TO gpt-3.5-turbo. In today’s fast-paced world, where stress and anxiety have become all too common, I wanted to create something meaningful—something that could help people reclaim their mental well-being. That’s howMindful, a mental wellness app, was born. This post will walk you through the journey of buildingMindful, from choosing the right technologies to incorporating AI for personalized advice and relaxation features. Mindfulis a personalized mental wellness companion designed to help users manage their stress and anxiety. Through personalized AI-driven advice and a soothing music player,Mindfulaims to guide users toward better mental health and mindfulness. ADDED FOR EXTRA COMFORT... One of the core features ofMindfulis the personalized advice system powered byLLAMA-8B-8192(previously : Gemini), an AI developed by meta and fine tuned by me to offer mental health tips tailored to individual needs. WhileLLAMA-8B-8192(previously : Gemini) powers the current version,I’ve written the code for other APIswhich require paid API_KEYS FOR integration with more advanced AI systems likeAnthropic,Open AI, andMistral. They requirePAID API KEYS. To further enhance mindfulness,Mindfulincludes acurated music playerwith a library of relaxing tracks to help users unwind. Whether you're meditating or just taking a break, the app's music player offers a peaceful escape. Feel free to add your playlist to the db. Mental wellness apps should be stress-free to use. WithMindful, the focus is on simplicity and functionality. The user interface is designed to be minimalistic and intuitive, providing users with easy access to resources without overwhelming them. I found inspiration from various sources—more on that below! Building an app likeMindfulrequires a solid foundation. Here’s a breakdown of the technologies and architecture that power the app: The backend is built withNode.js, providing the scalability needed for growing user traffic.PostgreSQLstores user data (with your key) securely, whileFirebasemanages real-time syncing with auth, making the app fast and responsive.Tensorflowto create the FER Model using FER2013 Dataset. To ensure thatMindfulis maintainable and future-proof, I adoptedclean architectureand followed theSOLID principles. This modular approach separates business logic, user interfaces, and infrastructure, allowing for easy updates and testing as the app grows. ModelView-ViewModel patternto handle user requests and business logic. Separation of concernsfor better code management and maintainability. Extensibilitybuilt-in so new AI features and wellness tools can be integrated smoothly in future updates. If you're curious to see how the app is structured or want to contribute, head over to theGitHub Repository.
FEEL FREE TO DOWNLOAD THE RELEASE BUT MAKE SURE TO FIRST RUN THE SERVER. The AI FACE DETECTION engine is central to theMindfulexperience. I trained the initial model usingTensorFlow, and now the app provides correct detection based on user camera input. It can be further used for many purposes. Want to check out the model?Here’s theColab Notebookwhere I trained and fine-tuned the model. Feel free to explore, experiment, or even enhance the model if you're interested in contributing to the AI aspect! Designing a mental wellness app requires careful consideration of both aesthetics and functionality. I wanted the user experience to be as stress-free as possible, so I sought out minimalist, calming design elements. I found inspiration onFigma, where I designed and prototypedMindful’sUI. The app features soft colors, intuitive buttons, and easy navigation to reduce user overwhelm. Interested in the design?Check out theFigma Fileto see how I brought the UI to life. It’s a work-in-progress, so feel free to offer feedback! TheMindfulapp employs a clean architecture pattern, allowing for separation of concerns and enhancing maintainability. This design makes the app adaptable to future changes and ensures a robust user experience. By adhering to SOLID principles,Mindfulpromotes best practices that facilitate development and testing. To runMindfullocally, follow these steps: Clone the repository: Install dependencies: Setup environment variables (OPTIONAL, USE MINE):
Changes in API_KEY (if needed) to be done in : Run the Server: Run the App: Structure of DB: This is the structure of thesongstable in thepublicschema. I’m planning to continuously evolveMindful, with upcoming features like meditation sessions, enhanced AI capabilities, and more personalized recommendations. The goal is to build a complete mental wellness toolkit right in your pocket. If you’d like to contribute ideas or code, please do!Mindfulis an open-source project, and I’m excited to see how the community can help shape its future. You can submit pull requests or open issues on theGitHub Repository. CreatingMindfulhas been an enriching experience, and I hope it can positively impact users by helping them manage stress, anxiety, and overall mental health. The combination of AI-driven advice, relaxation tools, and user-friendly design makeMindfula comprehensive companion for mental wellness. Mental health is as important as physical health, andMindfulstrives to be a tool that helps users prioritize their well-being in a meaningful and practical way. Thanks for reading!I’d love to hear your thoughts on the project or discuss anything related to mental wellness tech. Feel free to leave a comment or reach out directly. Let’s make mindfulness accessible TO ALL, one step at a time.",,,,,,,"flutter, github-jobs, node.js, postgresql, redis, tensorflow",https://github.com/GYANENDRA1475/Mental-Health-App
HackHarvard 2024,Fish Outta Water,https://devpost.com/software/fish-thing,False,"Our inspiration behind the project came when we were discussing our hobbies and one of our team members mentioned that they had went fishing the week before and was talking about the terrible state of the lake. His nearby lake had been infested with the common carp and had killed of the diverse fish. After this we realized that this problem could be solved if people, particularly fisherman, were more aware of what are the invasive species in their local area and how to catch them.","Our website shows users a map the different invasive species of fish near their homes and across the US. We display scientific information upon click the fish icon on the map, tips on how to catch the type of fish as well as a picture so that fishers know what to look out for. Users are also given a link to directions to where they can find that type of fish. We also have a recipes tab which display a wide array of recipes for ever invasive species of fish in the US! Finally to better spread the message to other fisherman we created a social media tab where users can anything from their results out a certain pond to unique recipes.","The first thing we did was layout a plan for what we wanted our project to look like. We decied on a cholor scheme as well as the different tabs and features we wanted to integrate. The first course of technical action was to start clustering fish together as there was over 500,000 data points and we realized that displaying those on a map could be confusing for users and would have cause the website to crash. We then worked on creating a basic template for every page on our site. Using next.js allowed us to conserve allot of time as we could create components such as the nav bar and footer that we could use in every web page. We then worked on trying out different maps to display our Invasive Fish Tracker. We ended up choosing leaflet due to its lazy loading as well being able to use a csv for our pins. We then moved to our recipes section. Here we created box for every type of fish in our data base allowing you to search for a specific one. Then when we wanted to create a recipe for each fish we decided on prompt the OpenAI api to create one for us. We also decided that if we wanted to reach a larger impression it would be good to create a social media/form where people can talk about anything they would like. After we created all the base features we decided to add some smoothing as well as animation components to add a artistic aspect to our project. After this the team realized that new fishers might have a hard time catching and identifying the invasive fish from the ones that are not. We added feature where users can see a picture of what the fish they are looking for and some tips to catch it using the OpenAI api.","Our first big challenge was in clustering data. We tried around 10 different clustering algorithms weighing  their pros and cons until we could find one which gave the results that we were looking for. The computation time was quite long as each model took around 30 mins to create an output as 500,000 entries along with the data being 2d proved to hard to work with. Next was picking out which map we were going to use, we wanted something that would highlight the terrain around but be easy on the server side as the amount of pins we had were still allot. We found leaflets lazy loading to be the best solution to our problems as rendering sections of the map at a time created better load times for the map. After this we attempted to add tips using and adding images to the pins on the map. This took the most time as when we got it to work the map kept glitching causing the map to move on its own. We found a work around for this by editing the leaflet map directly.","We’re incredibly proud of creating a fully functional, user-friendly tool that can serve both amateur and professional fishers alike. One of our biggest achievements is successfully clustering and visualizing a large dataset of 500,000 entries without crashing the site. It was a technical challenge that took a lot of optimization and innovation. Another achievement is integrating OpenAI’s API into the platform to generate helpful fishing tips and custom recipes for each invasive species. We’re also proud of the interactive map, which offers users the chance to explore their local area and identify invasive species, complete with images and directions to help them catch and manage those species.","Through this project, we learned how to manage and optimize large datasets for performance, especially in a web app environment. We explored various clustering algorithms and analyzed their strengths and weaknesses to find the best solution for our dataset. We also gained experience in frontend development using Next.js and enhanced our design skills by incorporating smooth animations and transitions to improve user experience. Furthermore, by integrating APIs such as OpenAI’s, we learned how to streamline the process of retrieving and displaying dynamic content. Finally, since this was the first hackathon for most of our members we are very proud that we were able to combine our mixed skill sets to create such an amazing product!","We plan to expand the scope of our platform beyond the United States by integrating data from other countries and ecosystems. This will involve acquiring more data on invasive species worldwide and further improving our clustering algorithms to accommodate a growing database. We also want to create a mobile version of our website to make it even more accessible for users on the go. Another exciting feature in the works is a leaderboard for fishers to share their catches, encouraging friendly competition and promoting conservation efforts. Lastly, we’d like to collaborate with environmental agencies to provide real-time data updates on invasive species and develop partnerships with local communities to help manage their waters more effectively.","api, css, javascript, leaflet.js, next, openai, openai-api, python, tailwind, typescript",https://github.com/RajivJonnala/Hack-Harvard
HackHarvard 2024,friday,https://devpost.com/software/friday-5ix4ro,False,demo in person!,,,,,,,cloudflare,http://github.com/tanmaik/friday
HackKU25,DocDoc Goose,https://devpost.com/software/docdocgoose,True,"Almost 70% of American adults have to take one prescription medication a day, and approximately 50% of patients don't take it as prescribed. This can have serious effects -- I know someone who forgot to take their seizure medication at the correct time and as a result ended up having a seizure. Our goal is to reduce those experiences as much as possible.","Our app reminds you when to take medication, as well as listing all the medication you have to take for the day and the time. You are able to input the medication manually or by scanning the barcode, which will automatically populate detailed descriptions of drugs from the FDA. The app also uses OpenAI's API to detect potential conflicts in medication and allergies, giving the user not only ease of use but greater safety precautions.","Front-endWe used React Native Expo for the mobile app and utilized RNUI as a UI component library. Most styling was done with Nativewind. Back-endMidnight operates with the Midnight Lace Wallet to manage the user's tDUST token. We got the tDUST tokens via Faucet. Ansh used the bulletin board example to implement a smart contract on the Midnight Blockchain. Smart contracts ensure that sensitive information is stored immutably on the Midnight Blockchain, keeping data secure and verifiable.","Midnight was really difficult to use, as documentation was convoluted and filled with difficult jargon.","We're proud of challenging ourselves to use Midnight since we had no prior experience, instead of going safe with something we've used before like Gemini.","We learned how to use Figma and Midnight, and improved our skills with React-Native.","We really enjoyed making the goose mascot and plan to incorporate him with interactive animations. We also introduced functionality to store medical documents, and we hope to integrate OCR/LLMs to incorporate this data into our app.","midnight, react-native, supabase, typescript",https://github.com/AndrewChang-cpu/HackKU2025
HackKU25,SafeStep,https://devpost.com/software/safestep-nv67j1,True,"This shows a modeling of the acceleration data for falls (red) and nonfall events (blue) used to determine the optimal threshold value. This is an email alert sent out after a detected fall that includes the specific location of the fall with a link to it. This is the screen that appears on the app when a fall is detected. The user can verify the fall or after 30 seconds an alert will be sent. These show two different screens - the emergency contacts and settings - that are in the app. We developed SafeStep, an iOS application with a mission to improve safety and peace of mind for individuals at risk of falling. Using the iPhone’s built-in motion sensors, the app detects falls in real-time and provides a simple, accessible interface for emergency contact management. As our loved ones age, ensuring their safety becomes increasingly important, and falls - especially when undetected or unnoticed - can pose a significant threat to seniors. Every year, as many as3 millionemergency department visits are caused due to older people falling. In 2019,83% of hip fracture deaths and 88% of emergency department visitswere due to falls, which the CDC also finds are the most common cause of traumatic brain injuries. Having medical alert systems in place for the people most at risk of falls can help them age with a greater sense of independence, confidence, and safety. Having free, accessible, and quickly-responsive fall detection capable of just fitting in your pocket is critical to reducing the response-time after a fall. Not only are falls dangerous, but the danger only increases the longer it takes to get up. Most fall cases, especially those that happen to people alone, result in 10-20 minute waits to get help - at a minimum. That can get up to as long as an hour or more, which is categorized as a 'long lie'.78% of individuals 65 or olderwho fell required some form of help getting up from the floor, exacerbated by the fact thatover half of falls occurred with them being alone. This extended wait can lead to any additional symptoms ranging fromdehydration and tissue damage to psychological damage. In short, the longer people are forced to wait after a fall, the worse the situation gets. That's why we found a way to make fall detection accessible and responsive for everyone by utilizing built-in hardware in all iPhones to detect fall events and alert emergency contacts. 📱Intuitive UI for SeniorsDesigned with accessibility in mind — large text and clear buttons with a very simple interface and UI 🚨Real-Time Fall DetectionUses CoreMotion's accelerometer data to detect sudden movements or impacts that may indicate a fall and respond quickly 📇Emergency Contact ManagementAllows users to add and manage emergency contacts for rapid response",,,,,,,"express.js, node.js, python, swift, vercel",https://github.com/aidenhburke/hackku-25
HackKU25,EirAI,https://devpost.com/software/eirai,True,"Landing Page — Access to patient accounts SOAP generation in progress SOAP generation output Possible ICDs Search input for extra configuration Final rating based off historical datasets Results page for easy data viewing SOAP notesare a standardized method of documentation used by healthcare providers to record patient interactions. The acronym stands for: ICD codes(International Classification of Diseases) are critical for billing treatments, but remembering the right code out of over73,000is overwhelming. Nurses and medical codersspend countless hours searching for correct codes just to ensure insurance providers accept a treatment—time that could be spent on patient care. Doctorsface their own bottleneck: writing detailedSOAP notesfor each visit, adding to their administrative load. We spoke directly withworking healthcare professionalsto validate the need for smarter, AI-powered ICD prediction tools.",,,,,,,"nextjs, postgresql, supabase, tailwind, typescript",https://github.com/aelxxs/hackku-2025
HackKU25,Git Lucky,https://devpost.com/software/git-lucky,True,Git Lucky is inspired by TikTok Minecraft videos and the ease of use of Tinder.,Git Lucky is democratizing pull request maintenance. You get a random PR in a random repo that has installed our service. You can see the code and vote on the PR in the app. The PR is merged based on the will of the people,We used Axum for the backend and Dioxus for the frontend. The server is deployed on fly.io.,"We ran into several issues trying to deploy to the server. Open SSL was not found, then the CA certificates. We ran into issues with the mobile app version of the front end, which we think is an internal error in the framework. We tried forking the underlying code, but eventually pivoted to a web/desktop app instead",The sliding card CSS animation and deploying to fly.io,More about CSS. What certificate authorities are.,,"axum, fly.io, rust",https://github.com/ealmloff/gitlucky
HackKU25,MindClash,https://devpost.com/software/mindclash-haltev,True,"We wanted a more fun, less frustrating way to learn Python. All of us had hit the same wall: learning code felt dry and disconnected. Inspired by the Game Pigeon game Filler, we decided to reimagine coding as something competitive and addictive, where the goal is to “fill” your knowledge one challenge at a time.","Mind Clash is an interactive online coding game that transforms learning Python into an engaging experience. Whether you're playing solo or competing in real-time with others, Mind Clash challenges players to solve Python-based problems and level up their coding skills while having fun. It’s the perfect blend of education and gamification, making coding accessible, social, and addictive",Frontend: Gatsby (React-based) Backend: Node.js with Socket.io Realtime Database & Auth: Firebase Multiplayer Room Logic Code Challenges,"One of our biggest challenges was keeping the game state synced in real-time across multiple players without lag. Handling sudden disconnections without breaking the game logic took careful planning, as we had to ensure the experience stayed smooth and fair. Another major hurdle was designing a UI that felt playful and game-like, while still clearly presenting the coding challenges, we went through several iterations to strike that balance and make learning feel fun","We’re proud that we built a fully functional multiplayer coding game in just one weekend. It wasn’t just about finishing the project, we had a great time collaborating as a team and really enjoyed playing the game ourselves. Along the way, we learned a lot about real-time systems, multiplayer architecture, and what it takes to design a game that’s both educational and genuinely fun.","We learned a ton throughout this project, especially about the tools that powered it. Firebase was a game-changer. Its real-time features and simplicity made building a multiplayer experience much easier without needing a heavy backend. On the frontend, we got comfortable working quickly with Gatsby and React to build a clean, responsive interface. We also gained hands-on experience with real-time sockets, managing race conditions, and designing smooth, collaborative user flows that kept the game running seamlessly.","We want to expand beyond pre-written challenges next, we’ll allow users to upload or generate challenge files, so the game can cover more topics and grow with the user’s skill level","css, express.js, firebase, gatsby, javascript, netlify, node.js, react, render, socket.io, tailwindcss",https://github.com/navyan02/MindClash
HackKU25,The Note Distillery,https://devpost.com/software/the-note-distillery,True,"As a college student, I was inspired to create this project while attempting to organize my vast repository of handwritten notes. I then realized the potential of an app that could read notes straight from a notebook, and store those notes as data.","The project, a web app, is capable of parsing text from image inputs using ai; it's also capable of taking in plain text input. This input is then cleaned up, and organized into a series of notes. After cleaning, each note is passed to a database; the user can then re-access these notes through a visual file system in the browser. Upon loading a note, the user is presented with a host of learning tools such as an ai-generated quiz, flashcards, and summary; the user can also prompt an ai with questions pertaining to the note.","We built the app using Next.js and React, and stored our data with Firestore; we also leveraged Gemini for all of our ai calls.","Next.js was a big learning curve for all of us; I think it ended up being the right fit in the end, though.","We're very proud of the app's ability to pull text from handwritten notes, as well as the smoothness of the mobile experience.","We learned a-lot about AI, seeing as that's the driving force behind a-lot of the functionality of our website. We also learned how to persevere through sleep deprivation and debugging to ultimately release an app we're all very satisfied with.","We plan to add semantic searches on all of a user's notes; this would allow functionality like grouping and searching notes based on concepts, or similarity to other notes. Another plan is to leverage the notes data to generate a visual mind map of concepts learned in and out of the classroom.","firebase, firestore, gemini, gemini-api, google-cloud, next.js, react.js, vercel","https://the-note-distillery.vercel.app/, https://github.com/ThisisJackRyan/The-Note-Distillery"
HackKU25,Monster Muscles,https://devpost.com/software/monster-muscles,True,The theme and some of our member like lifting and working out so making a health website was our goal.,It shows you workouts and has a chatbot that can create workout plans for you. It also has a diagram of a person so you can click on a muscle and see the exercises.,"We built it in Visual Studio code, and we used many coding languages.",Coding and getting the AI chatbot to work were some challenge we ran into.,Getting the chatbot to work and having a somewhat decent website.,We learn some more code,Advance production,"css, gemini, html, javascript, php",https://github.com/The-Monster-Monsters/KUHackathonWebsite
HackKU25,"Get Big, Man",https://devpost.com/software/get-big-man,True,"We wanted to represent this year's themed track, ""Health and Well-Being,"" more creatively and engagingly. While the physical aspect of going to the gym is what people often see, people don't think of the mental challenges of trying to go to the gym for your first time. The title, ""Get Big, Man,"" displays the pressure some feel from others when starting out at the gym. We created this game to empower those who struggle with the mental challenge of getting up and going to the gym.","""Get Big, Man' is an immersive roguelike game that puts the player, ""Big Man"", in a loop where he must defeat rogue gym equipment to progress. The objective of the game is to defeat enemies to earn coins to get ""Big Man"" bigger as he progresses through push, pull, and leg day. Each time ""Big Man"" fails and is overtaken by this gym equipment, he is put back at the start, emphasizing the power of commitment in achieving your goals.  The player wins by progressing through push, pull, and leg days. The conclusion of the game shows how hard work and consistency can empower the player to achieve their goals without fear of being judged along the way.",Unity Game Engine: Used in compiling world and player assets all in one place. Offered all the tools needed to run the game and debug. GitHub:The backbone of our collaboration and file transfer. Allowed multiple people to work with unity software seamlessly. Kept every user up to date with the correct version. LibreSprite:Used in creating all custom character and enemy sprites. Allowed us to easily transfer sprite sheets into unity to use for character and enemy animation. Created custom buttons and backgrounds. FL Studio:Used for background music and sound effects.,"""Get Big, Man"" came with many challenges. As it was many of our first times using Unity as well as GitHub for collaborating, we expected to run into some issues. Git Issues: Our issues came from the GitHub and unity version control. We tried our best to keep everything in separate branches, but despite our best efforts, we dealt with many corrupt files. We ended up getting so used to exactly which files would corrupt that we kept a saved drive of these as backups just in case. Unity Issues: As mostly first-time users, we ran into a lot of unknown errors. Lots of time was spent learning the basics of unity. Collaboration and Delegation: Each of us had to learn what our end goal was as we went along. Each of us did what we were best at and stuck to it. This being music, game development, and art.","We are each most proud of each other's dedication to this project. Over the 36 hours we had to complete this game, someone was working on something during almost every hour. Some of our most valued accomplishments include: Building up a complete game: Building this game from nothing to being something complete with a beginning showcases how much we all had to learn and adapt to all of the challenges we faced. Custom Art: Creating the art for the game presented a custom challenge that we had never experienced before. Figuring out appropriate workflows for each asset and designing convincing movement was a skill that was gained during development. Custom Music: Each sound and background music track was made custom. Object Sounds were made using our voices and background music composed in FLStudio","""Get Big, Man"" presented many opportunities to learn something new. Unity Proficiency: Each of us gained a much better understanding of the Unity game engine, expanding our abilities further than we could have expected. Collaboration: While we used GitHub to send files, collaboration was done outside of the coding process. We learned to work effectively as a team by respecting each other while being confined to a small space together for such an extended period. Mental Health Issues:  As we developed further, we began to see the greater application and purpose of this game. We gained an understanding of how others around us are struggling and became more aware.","From the start of ""Get Big, Man,"" we have been very ambitious. Almost every moment we would come up with something new to make the experience more enjoyable. Story aspects: While some of the story is implied by the nature of the enemies and the game title, we want to immerse the player even further in the character's struggle and how that struggle is overcome. Environmental features: New enemy types with different ways of attacking, such as a dash or projectile, and a more immersive and detailed environment. Longer, more detailed levels with more power-ups and items. Rotating Items in the shop Movement Features: To enhance the experience of the player, we want to add various movement features for a more engaging and fluid combat system, Alternate player attacks.","c#, flstudio, github, libresprite, unity","https://github.com/sonicboom989/HackKU25-2D.git, https://ziverz.itch.io/get-big-man"
HackKU25,TARS,https://devpost.com/software/tars-1b3cwz,True,Our inspiration came from the movie Interstellar and the character of TARS.,"It is a functioning scaled-down replica of the character TARS from the movie Interstellar. It has a functional screen, a speaker, and 4 servos to control the movement. It can ""walk"", converse with you, and perform simple tasks.","We custom trained a TensorFlow model for the activation/wake word, used Vosk for on-board speech recognition, and the Google Gemini API for the conversational character.","There were plenty of challenges. We constantly ran into issues with connectivity with the hardware and small issues that weren't even part of the code. Thankfully, we were able to overcome the vast majority of them.","Being able to have a physical product that actually does something makes us feel very proud and fulfilled. This year's HackKU was a blast, and we're happy to be able to walk away with many new skills. We learned how to implement voice recognition speech to text and then integrating that with gemini to text to speech. We successfully implemented a psuedo-PID controller using an approximate distance based methodology for steady state error.","Most of us had never dealt with this small of a time crunch for a hardware-based project, nor had we ever trained our own machine learning models.","Adding more commands, fleshing out movement, and adding even more expressiveness to him. Eventually... space.","3dprinting, assembly-ai, gemini, hardware, openai, python, raspberry-pi, servo, speech-to-text, text-to-speech",https://github.com/m-riley04/TARS
HackKU25,Dream Dragon,https://devpost.com/software/dream-dragon,True,,"Therefore... IntroducingDream Dragon 🐉!! Dream Dragon is the perfect solution to create a healthier life style with sleep (one dragon at a time 😉).
With Dream Dragon, users can: We also wanted to create this app to make it as engaging for the users to build up their habits. This is why we made a gamified element to our app: This helps motivate the user to try to engage more with their sleep and encourage a healthier lifestyle.","We built our app using the Flask Python Framework, Gemini API for sleep analysis software, MongoDB atlas for the backend database applications, ChatBase for the ChatBot, HTML-5 Canvas for the UI and game effects, and we all learned how to use JavaScript for creating APIs and the connections.","Although the end-results were quite rewarding, the challenges were many. First, we had a lot of integration issues, especially when trying to integrate our MongoDB back-end to work with functionality on the front-end.
We were all very new to UI-design and front-end work, so we faced major challenges in learning how to use JavaScript, HTML, and CSS - especially for the game and pages part.
Another issue that we ran into was overloading the server from running the code too much and at the same time.
To overcome these issues, we used teamwork to solve through some of the more difficult integration issues.","We were all very new to creating front-end work, and so we were very happy to be able to integrate and make a full-fledged web app with a user-friendly interface, especially since we had not used HTML-5 Canvas like this before. We also were very excited to use our own drawings and creations for the UI (curtesy of Anna's drawings).
Our team was proud of using new and advanced tools that we hadn't used before, such as the Gemini API, since we knew we wanted to level up our application with an AI sleep analysis.",,"For the future we hope to improve our website by adding more games to help provide users more variety. Additionally in order to help users stay motivated and help them improve their sleep schedules alongside their peers, we hope to implement a leadership board and also allow users to form groups with their friends and family to help keep track of who has achieved the most sleep. By doing so we hope to motivate users to improve their sleeping cycles as it is the most integral part of one's health! https://www.hopkinsmedicine.org/health/wellness-and-prevention/health-risks-of-poor-sleephttps://www.nhlbi.nih.gov/health/sleep-deprivation","canvas, chatbot, css, flask, gemini, google.generativeai, html5, javascript, mongodb, python",https://github.com/SAJacob7/DreamDragon.git
HackKU25,Souperb,https://devpost.com/software/souperb,True,"Our inspiration for Souperb came from our deep love for soup. It has always provided us comfort on days when we were feeling under the weather, and we recognize its health benefits. With this in mind, we wanted to create an app that helps people discover ingredients and recipes for soups, promoting better health.","Souperb offers a variety of features to enhance your soup experience. Users can easily log in or sign up to access the web app. It includes a quiz or an AI chat feature to help find the perfect recipe. You can save your favorite soups and cans to a personal shelf. Carroqueen, the app’s assistant, answers questions related to soup ingredients, recipes, or even general inquiries. Additionally, the app provides a list of ingredients, making it simple to gather everything you need to create delicious soups.","We built Souperb using a combination of powerful tools. For the front-end, we used Astro, while Supabase manages the backend database. API calls are powered by Huggingface, and we utilized mistralai/Mistral-7B-Instruct-v0.3 to create the model. The app’s UI/UX was designed in Figma, with custom illustrations made in Krita and Procreate for a more personalized touch.","During the design process, we faced some time constraints, especially when it came to adding intricate details to the artwork and the time required for each task. We also encountered issues with design files and exporting them correctly. A significant challenge was waiting on others to finish their tasks, which prevented us from completing our own work due to the limited time. Additionally, we struggled with running out of time to debug and implement all the features we wanted, which highlighted the over-ambition we had given the tight timeline.","Our ideas for this hackathon were one of our biggest accomplishments, and executing them within the time constraints was a major achievement. The login page was also a significant milestone for us. We successfully made a variety of different elements work together, even though they weren’t necessarily designed to integrate. Additionally, working with cutting-edge technology and training a unique model was a rewarding challenge that we’re proud of.","For some of our team members, this was their first hackathon, making the experience an incredible learning opportunity. We gained valuable insights into how documentation evolves and the importance of keeping up with updates in technology. We also learned a lot about improving design choices, color palettes, and the finer details of art creation. Additionally, this was our first time working together as a group, which taught us a lot about collaboration, communication, and teamwork.","Our next steps for Souperb include enhancing user interaction by adding more comments and communication features. We also plan to integrate a map that locates grocery stores, along with the ability for users to order ingredients online directly through the app.","astro, figma, huggingface, javascript, krita, mistralai/mistral-7b-instruct-v0.3, procreate, python, supabase",https://github.com/2406mmartin/souperb2
HackKU25,medIQ,https://devpost.com/software/mediq-2gs7ct,True,Our  inspiration of medIQ stemmed from the lack of patient knowledge and the hassle of being on multiple medications. We took these 2 problems and created a easy to use solution to bridge some of the gap between patient and doctor.,"medIQ is a mobile app that pulls medications that the user should be on from FHIR. This ensures that users can easily keep track of numerous medications that they may be on as well as when and how much of the medication they need to take. medIQ also takes into account the user's schedule, for example, if the user is going to be absent for a long time, i.e. a vacation, then it will remind them to refill their medication ahead of time so that they don't run out during the vacation. We also provide a timeline for users to check on their diagnosis so that they can be a part of the process as well. Users are able to see their past diagnosis and current diagnosis. Users will also be able to access a summarized, easy worded version of their own medical records with text to speech as well as multi-language support features.","For the frontend, we used React Native with Expo as well as Expo Router for file-based navigation, we also used TypeScript for static typing. We used luxon for date handling, axios for making API calls, and ngrok for tunneling and remote access.","The biggest challenge we ran into was designing our UI/UX to be extremely intuitive, as we realized our target demographic most likely did not have much  experience with modern technology. We went through multiple iterations of UI/UX to reach a satisfactory level.",We're very proud of the amount of information that is condensed and summarized. We're also proud that we were able to make he flow of information so smooth.,We learned how to query patient data from the HAPI FHIR api and how the system worked to query information such as patient condition.,"medIQ has a lot of potential as a patient centered app with room to grow, with features that we thought of adding but having scopes too big to fit within the timeframe of a hackathon. Features such as, a clinical hotline for connecting patients to readily available healthcare workers working anywhere to help solve the manpower shortage through video chat, a live diagnostic timeline with addition of doctor's notes as well as summarization of the notes, and a ride share estimate to doctor's appointment.","axios, expo.io, hapi-fhir-api, luxon, ngrok, react-native, typescript",https://github.com/ewbyf/hackku-2025
HackKU25,FoodSafe,https://devpost.com/software/foodsafe,True,"Our inspiration for creating this app came from a very personal place. One of our group members has severe food allergies and was even hospitalized after unknowingly eating a dish that contained peanuts. As anyone with allergies will tell you, they have to constantly check what they’re eating. But when food is unlabeled or information isn’t clear, it’s all too easy to make a dangerous mistake. We wanted to build something that could help prevent allergic reactions and give peace of mind to others in similar situations. That’s how we came up with FoodSafe, an app that empowers people with allergies to feel safe, informed, and in control. But wait there's more, lets say you are at a restaurant and you're unsure if there are peanuts in your food, but the server/cook doesn't speak the same language as you. We have a built in interpreter that will ask that question for you! FoodSafe has you covered no matter where in the world you are!",,,,,,,"fast-api, google-cloud, google-text-speech, google-translate, javascript, python, react-native",https://github.com/Dheeraj9489/FoodSafe
HackKU25,PeTAI,https://devpost.com/software/petai,True,"As we were brainstorming ideas, Gage and I (Carlos) were discussing how hard it is to get perfect form at the gym, let alone at home when following tutorials for workouts. That's when it hit us, what if we use Computer Vision and AI to provide user's feedback, what if we made a Personal Trainer AI, PeTAI.","PeTAI uses the camera from either an iPhone, iPad,Desktop, or Laptop to provide real time feedback on exercises you're doing. For instance, if you're following a Yoga or Dancing video from YouTube PetAI will tell you how similar your movements are to the teacher's. Likewise, if you're doing squats, PeTAI will notify you if you're at risk of injury or getting close to perfect form. As the saying goes, Perfect Practice Makes Perfect.","PetAI is a cross platform experience. For iPhone, iPad and Apple TV, we're using Swift, SwiftUI and Apple's Vision framework in order to offer on-device analysis on the video streams from the cameras. We also have a web app in order to support non-apple devices, that offers similar functionality via Google's ML Kit and a website built with Javascript.","Setting up the real time tracking was relatively simple. Our main challenge was using the data we were collecting and offering feedback to the user. As it's very difficult for someone to have a perfect 90 degree squat, we had to find a set of constants that are doable and safe. Likewise for the dynamic exercises, it'd be difficult for someone to be perfectly in sync with a tutorial they're watching for the first time, we also had to take that into account. Moreover, implement Apple Watch data analytics was also quite involved.","Getting all of this working! We honestly didn't know if we'd be able to build this, so it's super cool. Especially the feedback functionality.",We learned how to make multi platform apps and the importance of good form when exercising.,,"css, google-mlkit, html, javascript, swift, vision",https://github.com/carlosmbe/PetAI
HackKU25,Let's Eat!,https://devpost.com/software/app-3-6kxg4q,True,"We were inspired by Stardew Valley for its charming 2D pixel art style and its ability to make everyday activities fun. Initially we had a Papa’s Pizzeria-style concept in mind, where tasks felt like a game. As college students, we know that cooking isn't always the most exciting part of the day, so we wanted to create an app that would help find recipes and plan meals more fun.",The Recipe Finder App is designed to help college students find fun and easy recipes based on the ingredients they have on hand. The goal was to make cooking more enjoyable and less intimidating by gamifying the experience.,"Front-End Development: The front-end was built using CSS and HTML, which was our first time implementing, so we had to learn as we built it. However, we had a Figma design mockup which made the process a little better. Back-End Integration: We integrated Flask to handle the back-end, which allowed us to process and display the recipes. This added another layer of complexity to the project, especially in maintaining a seamless connection between the front-end and back-end. Design & Layout: We started with a Figma layout to visualize the app's interface and user flow. The design was heavily inspired by the 2D pixel art style, focusing on making the app visually appealing and fun to use. However, merging this style with functional elements proved difficult, especially since we wanted animations and game-like features to be part of the experience.","Design vs. Development: Combining the fun, game-like design with the actual coding was tough. Back-end development in Python, especially with Flask was difficult as we had to balance design elements with the the front end side. Game-Like Features: We struggled with making the app feel truly gamified while still being practical and easy to navigate. Incorporating animations, sound, and interactive elements would have taken a lot longer and had to resort to simplifying it. Back-End Integration: Connecting the back-end (Flask) to the front-end presented its own set of challenges. Making sure the recipes were properly displayed while keeping the app’s design intact was tricky.","First Hackathon Success: Completing our first hackathon and delivering a fully functional app in a short time was a huge achievement. Teamwork & Collaboration: We worked well together, improving our skills in planning, time management, and communication. Targeting College Students: Most of all, we’re proud to have created an app that makes cooking enjoyable for college students, turning a chore into a fun experience.","Using Web Frameworks, Learning to Quickly Understand Topics: The biggest challenges we faced involved combining the final components of our design. As a team, we had to learn how to use the web framework Flask to make our front-end and back-end design interact. This experience taught us how to efficiently learn necessary bits of a language or framework in a short amount of time, as we've been accustomed to exploring subjects in breadth with ample time. Real-World Team Organization Skills: We also learned how to effectively manage our commitments as a group and worked to better streamline our design process, experience we aren't often able to gain through our academic coursework!","Improvements to the app would revolve around adding features beyond the ingredient list maker. A great feature to include down the road would work almost inversely to how the ingredient finder works, taking a list of ingredients on hand and returning a variety of meals that can be made with them. And of course, bug fixes and UI tweaks are not to be forgotten! Thanks for checking out Let's Eat!","copilot, css, figma, flask, gemini, github, html, javascript, python",https://github.com/aliviahanes/hackku2025.git
HackKU25,CreditNexus,https://devpost.com/software/creditnexus,True,"The inspiration for CreditNexus stemmed from theRipple Sponsor Track Challenge, which encouraged us to build an innovative finance application using RLUSD on the XRP Ledger (XRPL). We identified several key opportunities in the credit and lending space:",,,,,,,"bcrypt, express.js, ipfs, jest, jsonwebtoken, next.js, node.js, pinata, railway, rlusd, tailwind-css, typescript, xrpl-(xrp-ledger), xrpl-javascript-library",https://github.com/adithya70/creditnexus
HackKU25,Renmo,https://devpost.com/software/renmo,True,"We found problems with traditional transaction apps like Venmo and CashApp, and we wanted to use the decentralized nature of XRPL to improve these applications.",Renmo is a transaction app that uses XRPL and RLUSD to send transactions and connect users via their XRPL wallets.,"We used TypeScript, NextJS and XRPL to create a seamless user experience and secure transactions.","We had difficulty navigating XRPL as we had never built an app on it before, but were able to use the documentation and adapt our existing Web3 knowledge to complete the application.","We are proud of the wallet persistence system, using IPFS and Pinata to store account information.","We became more familiar with the XRPL ecosystem, and building DApps in unfamiliar environments.",We want to increase network activity and improve ease of access.,"nextjs, rlusd, tailwind, typescript, xrpl",https://github.com/CalebHite/renmo
HackKU25,Decora,https://devpost.com/software/ar-room-builder,True,"Earlier this year, I was full of anticipation searching for a house. However, after every tour guided through desolate rooms I was left disappointed with no sense of what actually living there would feel like. Fast forwarding to this weekend, I was amazed after experiencing the potential of an Augmented Reality engine at our first workshop. Instantly, I was brought back my house hunt and how useful it would have been if I was able to carry furniture in my pocket. So we made Decora, an AR powered web app to enhance interior design.",Allows user to:,We utilized Niantic's Augmented Reality engine to take a real world space and edit it with virtual objects. Using the engines framework we created furniture objects with downloaded GLB files. Our own custom TypeScript component files take user input to allow the user to create and remove 3D objects from their Augmented Reality.,Learning Curve: User Interface:,,,,"niantic8thwallengine, sketchfab, typescript","https://andrewhowell.8thwall.app/room-builder/, https://github.com/andrewHowell81/Decora"
HackKU25,DUZK,https://devpost.com/software/duzk-guard,True,"Did you know that over the last 14 years healthcare data breach statistics show a clear upward trend?
In 2023 alone, over 133 million medical records were exposed or impermissibly released through data breaches. The Department of Health and Human Services Office of Civil Rights publishes all of the instances of data breaches on medical records. These disclosures show that in 2023 alone, there was over a 230% increase in hacking related data breaches. In fact, in one incident in 2024, Change Health Incorporated got hit by a ransomware attack that resulting the the breach of over 190 million individuals' medical records. This put 2024 at record breaking high of 274 million individuals records being breached, including an estimated 1 in every 2 Americans. This has severe consequences for individuals all across the world. UnitedHealth, the parent company to Change Health, stated they can't specify what data was compromised for each individual, but a notice on their website said that the compromised data could include Social Security and passport numbers, patient diagnoses, medical records, billing information and health insurance plan data. Additionally, the data breach halted medical claims processing, payment platforms and pharmacy network services. Between devastating negative consequences and the waxing nature of the frequency and severity of these data breaches, we knew that this was a major problem in the current health services industry. Conversely, decentralized blockchain networks have been consistently shown on the cutting-edge of data security. Therefore, we created DUZK, a decentralized system that allows for the secure storage and usage of individual's medical records.","DUZK Utilizes Midnight, a data-protection blockchain to safeguard medical records for our users. Midnight allows DUZK to facilitate confidential interactions, view sensitive medical information, and maintain patient medical record's integrity. Midnights signature feature is it's ability to validate user data without ever having to share it. It does this through what's called a ZK Snark. ZK Snark is a type of zero-knowledge proof that stands for zero-knowledge succinct non-interactive argument of knowledge. All forms of zero-knowledge proofs are used to verify user data without sharing anything, but some unique advantages of the ZK snark specifically are its succinctness and the fact that its non-interactive. Succinctness allows us to have a proof size that remains constant, regardless of the complexity or size of the statement being proved, and the non-interactive aspect allows us to skip multiple rounds of communication with the verifier. DUZK uses these functions to allow a user or hospital to have secure access to location on the blockchain where a patients medical records can be retrieved. It accomplishes all this without sharing any user data over the network. Now, once a user has been verified, we have additional levels of security in place around the retrieval of the medical files. We elected to use Pinata to securely store the files. When a file is uploaded by DUZK to Pinata, Pinata returns a hash CID string that DUZK stores on the blockchain along with a hash of the patient's private key. The key feature here allowed by ZK Snarks is the fact that the user's private key is never shared to the network. Additionally, because Pinata returns CID as a hash, Pinata does not care about the format of the files being stored, making the system completely data-agnostic. Pinata contains protocols that allow us to pull a selected file with a IPFS gateway, API key, and the public CID. Pinata keeps the API key and the gateway private and passes out the CID every time a file is added or updated. As long as the modifying party is the hospital, verified by their unique hospital key, Midnight will store the new CID to the blockchain.","Using Midnight's documentation and model, we created our own model of a smart contract that would be used by both patients and hospitals to adhere to HIPAA data safety.  After modeling how patients and hospitals would use Midnight, we were starting to hit a wall in how data would be stored when a friend from KU Blockchain Institute told us about Pinata. Pinata's data agnostic and accessible framework fit perfectly with the nature of our project, so making it work with DUZK was a ""gimme"". After working for hours through Midnight's maze of documentation and libraries, we finally had a few basic working examples. Shifting the examples to our will was a Herculean task of Javascript packaging, but after getting our contract modeled properly, we were able to make a simple interface that can upload files via Pinata to get our final minimal viable product of DUZK bringing the best of the two worlds of Midnight and Pinata together.","The first and most blatant challenge we faced was the one we brought with us. No one on our team had any experience working with blockchains at all, much less Midnight or Pinata. While we chose our project with this in mind with the hopes of vastly increasing our knowledge base on the topic, we grossly underestimated the rate the concepts involved can be abstracted. The most notable example is exemplified in a two hour conversation we had on how to construct a smart contract that would allow unique permissions to be granted to patients and hospitals. After that extensive deliberation, we created an opportunity to more easily integrate our system with hospitals' current systems. The system currently works to grant hospitals access to the blockchain where they can pass read-only access to their patients Midnight was an uphill battle almost the entire time. From not having access to the  wallet extension to trying to get the smart contract to compile, Midnight's documentation density was almost impossible to parse through. We overcame this issue through force of sheer will. After approximately 3 hours of throwing various codes and fixes at the compiler it eventually stuck.","One clear accomplishment to us is the fact that it works. One of the main benefits in tackling the challenges offered by this project was the knowledge we'd all gain considering our lack of prior experience. We are incredibly proud of how far our practical knowledge has grown this weekend and were incredibly excited to possess tangible evidence of that in a working project. Deciphering the API and the CLI examples given by Midnight to engineer our project was another point of pride for us. None of us were as experienced as we wished in Javascript, so trying to understand the processes that were occurring was no small challenge. Due the complexity of the resources Midnight provided against our prior experience, we were thrilled to come out the other side with a fruitful product","Considering our shared lack of knowledge on this topic, this whole weekend was full of learning experiences for us. We collectively went from never having heard of a zero-knowledge proof to implementing its functionality within a smart contract that updates a blockchain. This required of us to be quick studies in both TypeScript and concepts of blockchain architecture. We also learned Compact, a language unique to Midnight that is used exclusively to write smart contracts. A significant proficiency was necessary to achieve some of the features we wanted, allowing us significantly bolster our understanding of smart contracts and the potential features possible with them. One of our members had never used Linux or Github before, and for them the knowledge gained was incomprehensible. He learned how to work within Github repositories as well as how to navigate around a Linux terminal. By the end of the project he had a working knowledge of all the elements of the project.","We want to add a secure indexing system for doctors to track and access specific patient medical data. We also talked about implementing a biometric aspect to the project that allows the patients to have a unique private key, such as some form of hash of their fingerprint. This would allow patients anywhere to have read only access to all their medical records. One of our main interests in improving on is our UI, we hope to have a more user friendly interfaces that makes it as easy as possible for hospitals to integrate our software.","blockchain, compact, javascript, midnight, pinata, typescript",https://github.com/delster1/duZK
HackKU25,spot-a-song,https://devpost.com/software/spot-a-song-ufcm9k,True,"We wanted to create an implementation of AI that had the capability of introducing new creative content rather thanAI Slop. Withspot-a-song, AI can bring about an exposure to fresh and incredible art, pushing forward creativity instead of replacing it.","Spot-a-song allows users to generate a playlist of songs with a similar bpm, length, and genre. This provides the opportunity for listeners to expand their music pallet to other tracks and artists.","ImplementingGemini API,Spotify API,HTML/CSS/JS, andFigmawe created an interactive application to intertwine the two API's. We first worked with the Gemini API to generate up to 51 songs with similar bpm, length and ""vibe"" formatted into a dictionary of titles and artists. This dictionary was then given to the Spotify API where the various information of each song was organized into arrays. The data in these arrays when paired with HTML/CSS/JS provided us with an interactive and creative playlist display. Ultimately creating an application with the capability of creating fully built spotify playlists generated by AI.","We ran into some difficulty with the Spotify API, specifically generating the access token. This token expires every hour, and unfortunately we did not discover this fact right away. This ended up leading to quite a bit of confusion and struggle as our code would simply not run and we had no errors . Our other main problem turned out to be efficient and effective prompt engineering for the Gemini API. It was difficult to understand at first what we needed to instruct the Gemini AI to complete and what information was needed for the AI to complete the task correctly. However we worked with a variety of resources and we managed to create an effective prompt leading to diverse selection and correct storage of the songs.","We are extremely proud of our UI/UX design. Implementing Figma allowed us to expand what we believed was possible with HTML/CSS/JS, creating a dynamic and exciting way to interact with our software. Our design lead hand drew a few of the elements creating flow and connection throughout the site, while the two frontend leads strived to turn the Figma maps into real working code. Overall the software we created is something that's exciting and fun to work with and we are delighted with the outcome.","Invoking APIs to create solutions with data was something our group had never experienced before. API's are powerful tools that can be used in a variety of situations leading to more efficient and dynamic solutions. With this experience, creating programs that interact with real world technologies becomes a reality, one that we could not be more excited to contribute to!","Our group would like to expand the features of spot-a-song to include AI input and storage on spotify jams with friends. This feature would allow users to save the songs played in jams to a playlist automatically, and even have an option for Gemini to suggest the next track. With this implementation listeners would be able to expand their scope of music with friends and others alike! We also would love to expand the current software to other music platforms like iTunes and Youtube Music.","css, figma, firebase, gemini, github, html, javascript, procreate, spotify","https://github.com/Thomas-writes/HackKU-25, http://hack-ku-25.vercel.app"
HackKU25,bldr,https://devpost.com/software/bldr,True,"App Demo Our Flowchart The idea for bldr was born out of frustration. Our university’s only scheduling tool was deprecated right before enrollment opened. We were left navigating spreadsheets and broken systems to build our course schedules. We knew there had to be a better way—so we created one. bldris a fully integrated scheduling assistant designed to simplify course planning for college students. Upload your academic advising report or unofficial transcript, and bldr will parse your completed classes and suggest the optimal set of courses for your degree path. Our built-in chatbot, powered by the Gemini API, lets you build or refine your schedule through natural conversation—whether you're asking for classes that fulfill core requirements or telling it to avoid 9 AMs on Mondays and Wednesdays. Our Flowchat We plan to refine our recommendation engine to factor in more personal data—like workload, commute times, or extracurricular activities. We’d also love to explore collaborative features where students can compare schedules with friends, plus expand the chatbot’s capabilities for more personalized academic advice.",,,,,,,"beautiful-soup, express.js, flask, framer-motion, gemini, google-calendar, mongodb-atlas, next.js, node.js, postgresql, selenium, shadcn",https://github.com/shivanshshrivas/bldr
HackKU25,Hack Draw,https://devpost.com/software/hack-draw,True,We were inspired by collaborative art boards and Reddit's r/place event,Allows participants to draw with pixels on a canvas with other people,"Hack Draw is a Web-App built with Svelte/SvelteKit, TailwindCSS, SkeletonUI, MongoDB Atlas, and a GoDaddy registry",Server communication was a big problem and still isn't perfect.,Getting the server communication to a state that allowed the program to work well enough. www.hackdraw.design https://github.com/dean-blick/HackDraw,,,"godaddy, mongodb, skeletonui, svelte, sveltekit, tailwind",https://hack-draw.vercel.app/
HackKU25,RustBucket,https://devpost.com/software/rustbucket,False,,RustBucket has the ability to remotely execute some funny commands to mess with your friends.,"First, we started out building individual tools. We started with the keylogger and clipboard and then built on top. Once we had a good set of tools, we transformed it into a server-client like design for remote execution.",There was a lot of learning to do and just forming an idea was a roadblock at first.,,"We all have never done much with Rust, so we learned a lot.",Next we plan to add many more tools so that there are endless ways to have fun.,rust,https://github.com/bschulte1515/rustbucket
HackKU25,Fireball Clicker,https://devpost.com/software/fireball-clicker,False,,,,,,,,c#,https://github.com/IslandRaptor3/HackKU_2025/tree/Fireball_Implementation
HackKU25,Road to Recovery,https://devpost.com/software/road-to-rehab,False,"The inspiration for Road to Recovery came from a personal experience with physical therapy and the challenges faced during the recovery process. Many people going through physical therapy struggle to: These challenges often lead to slower recovery times and less effective treatment outcomes. We realized that while there were many fitness tracking apps available, there wasn't a comprehensive solution specifically designed for physical therapy patients.",,,,,,,"gemini, github, google-ai, mongodb, node.js, react, swift, xcode",https://github.com/brobertiello/RoadToRehab
HackKU25,Merlin - Your personal wizard assistant,https://devpost.com/software/merlin-p2kahm,False,"Merlin is inspired by AI agents being developed today, and a need for a simplified, elegant command-line version of those same tools. It also draws inspiration from fictional helpers like Jarvis and the fantasy theme of the hackathon and its themed rooms. Finally, we drew inspiration from our daily frustrations with bash commands and attempting to run similar commands across OS we were unfamiliar with.","Merlin uses speech-to-text technologies to turn your words into reality within your computer. It connects with Google's Gemini 2.0 Flash to generate a list of terminal commands and execute those commands, checking/fixing errors and asking you, the user, for further guidance. Merlin narrates commands and thought process, making it more user-friendly.","We used OpenAI's Whisper model and Google's AI Studio and Gemini 2.0 Flash to translate speech into usable terminal commands, no matter what the configuration. By enabling Gemini to read command output and cache a prompt-response history, we were able to utilize Gemini's >1 MILLIONtoken content window and make the most of our API key. We integrated both of these services, along with a command execution schematic, into Python and managed packages using virtual environments. Finally, eReader was used to implement Merlin's voice.","As it turns out, running GPU-based architecture is difficult on tiny laptops. Our integrated graphics handle speech processing very inefficiently, so we need to use the lighter weight models at the cost of having more errors in transcription. This was relatively mitigated with prompt cleaning, but still was a major drawback.
There was a second challenge that arose out of hosting our own speech-to-text model. Python packages were often large (in some cases more than1 GB!) This slowed development and stalled us as we quickly lost patience waiting for Pip to load these modules.
The largest challenge was our time limitations. We were not allowed to stay on-site overnight and left around 11PM Friday and Saturday.","One of the coolest things we learned was prompt engineering, and making sure that Merlin was able to create responses within the guidelines using Gemini's structured response feature. 
Another awesome achievement was integrating a chat history to allow Merlin to work similar to a chatbot, making CRUD operations much more possible and simple, and making Merlin more synced with its environment. This also allows Merlin to have recursive error handling.
We also implemented guardrails to prevent Merlin from accidentally wrecking havoc on an environment. Merlin has some pushback from assuming sudo.","We learned tons about working as a team, and our all-highschool background with two freshmen provided a unique experience for our developers. We also learned about error-handling and taking care of ourselves despite the hackathon sending us into insanity.","The next step for Merlin is making our speech-to-text work better. Once we do that, we open up to a myriad of possibilities. We tinkered with real UIs using tkinter, text-to-speech to give Merlin a voice, and even more efficient contextualization using cached history. The possibilities are endless, as the terminal can do anything in terms of interacting with the system.","ereader, gemini, openai-whisper, python, virtual-environments",https://github.com/JerrySMith13/hackku-hacklets
HackKU25,Medi ErrSaver,https://devpost.com/software/medicine-error-detection-companion,False,To empower patients with personalized insights and protect them from medication-related errors by using their own health history—transforming data into actionable safety.,"It analyzes patient-entered data, past medical documents, and current prescriptions to detect harmful drug interactions or contraindications, and prepares patients for safer doctor discussions.","We used a multi-agent system powered by LLMs and integrated statistical tools with drug interaction datasets (e.g., FDA Side Effect data). We parsed patient records (PDFs/DOCX), matched them with prescribed medications, and generated personalized alerts and question prompts.",Integrating LLMs to reliably interpret nuanced medical records while ensuring deterministic safety analysis was challenging. Balancing statistical outputs with natural language reasoning required careful orchestration.,We built an AI assistant that not only detects potential medicine errors but also proactively empowers patients by generating doctor-ready questions. We're especially proud of our multi-agent orchestration.,"That real-world patient safety challenges are deeply contextual, emotionally charged, and often underserved by tech. Addressing them requires empathy, precision, and interpretability beyond raw data.",Integrating genetic data and personalized risk scores to predict how individual patients might react to certain medications — making the assistant even more adaptive and precise.,"fastapi, gemini, python",https://github.com/Raman369AI/mlhks
HackKU25,The Phog Log,https://devpost.com/software/the-phog-log,False,"As a freshman at the University of Kansas, I quickly realized how hard it was to manage nutrition while relying on on-campus dining options. I wanted something that could help students like me track their intake, build meal plans based on what’s actually available on campus, and do it all in a personalized, accessible way. So, I built The Phog Log — a KU-specific nutrition assistant powered by AI.",The Phog Log helps KU students:,"I built the backend in Python using Flask, and structured the app around reusable modules like users_info.py, food_logger.py, and build_prompt.py.
For the AI integration, I used Google Gemini (via the google-generativeai API) to dynamically generate meal plans based on the user's profile, calorie needs, and what's actually being served in KU dining halls that day.
The frontend is made with HTML, CSS, and Jinja templates, styled with Poppins, light/dark mode, flexbox/grid layout, and smooth transitions for a modern feel. It's fully responsive and optimized for use on both desktops and phones.",,,,,"css, flask, gemini-api, html, jinja, json, python",http://thephoglog.us
HackKU25,CarGoo 2,https://devpost.com/software/dr-obd,False,"All four of us were once aspiring mechanics with ODB scanners, but one look at that ancient-looking piece of technology deterred all of us from our dreams that we had once held for over a decade. This issue prompted us to become Computer Science majors and create a solution that would no longer deter aspiring mechanics who are OBD-illiterate by creating a prospective firmware and web app solution to solve this nagging problem and save their jobs.","Using a prospectively new invention that will substitute the role of an ODB scanner while also sending that data to the cloud, we built a web application that will process ODB data into real-time metrics with features built-in like a dashboard that will update values, live monitoring that visualizes data about RPM, vehicle speed, etc., diagnosing vehicles based on Diagnostic Trouble Codes, suggest repair workflows based on diagnostics, AI-generated reports that allow all users to understand of Diagnostic Trouble Codes, and an admin panel that will allow system administrators to track users, vehicles, the DTC database, and other logs.","For the front-end tech stack, we used Next.js and Shadcn. For middleware, we used GraphQL. For backend tech stack, we used rust, diesel, utoipa, and actix-web.","One of the major challenges we faced was interpreting and standardizing raw OBD-II data, which varies across different vehicle manufacturers and models. A big learning curve was understanding how we had to use the data and train the model in a way to be useful and applicable to our final product and systems. Building a reliable pipeline to convert this data into meaningful, user-friendly metrics required significant trial and error. Integrating multiple technologies—such as Rust for performance, GraphQL for flexibility, and modern frontend tools—also presented complexity in terms of system design and communication between components.","We are proud of building a full-stack application that bridges the gap between complex automotive diagnostics and accessible insights for users of all technical backgrounds. From real-time data visualizations and AI-generated DTC explanations to a robust admin panel, our platform achieves a comprehensive user experience. We also take pride in our seamless team collaboration, which allowed us to ship a technically challenging product within a short development window.","We gained a deeper understanding of how to work with complex and domain-specific data—in this case, vehicle diagnostics—and how to present that information in a way that’s both useful and approachable. We refined our skills in building scalable full-stack applications, working with tools like GraphQL, Rust, and Next.js in a real-world context. Breaking down the barriers of a technical field (vehicle diagnostics and maintenance) while also learning the ins and outs of the different tools we used was a big accomplishment. Additionally, we also learned how to design with both technical users and non-technical users in mind, ensuring clarity without sacrificing depth. Most importantly, we learned how to break down a problem that initially felt overwhelming into smaller, solvable parts through collaboration, iteration, and a lot of coffee. Bridging the gap between our technical expertise and the level of understanding our target audience has proved to be an important consideration while fine tuning our project.","Our immediate next steps involve prototyping the hardware component that will interface with vehicles and send diagnostic data to the cloud. The hardware would need to read and process the data from the car in a way that our models can run and predict the next steps and generate informative reports with. Once the hardware is functional, we plan to integrate it with our existing web application and conduct end-to-end testing. We also aim to expand our database of Diagnostic Trouble Codes and fine-tune our AI-generated explanations to be even more accurate and helpful. Beyond that, we’re looking to gather feedback from both everyday drivers and automotive professionals to guide future improvements. Ultimately, we want Cargoo 2 to become a reliable and user-friendly diagnostic tool that bridges the gap between complex automotive data and real-world understanding.","diesel, next, numpy, pandas, postgresql, python, radix, rust, scikit-learn, shadcn, tailwindcss, typescript",
HackKU25,GrocerGPT,https://devpost.com/software/grocergpt,False,"Based on the theme of HACKKU, which is about Health & Well-being, we decided to make something that could actually be useful for people","Grocery Analyzer transforms your shopping receipts into personalized nutrition insights and recipe recommendations. By scanning and analyzing your grocery purchases, the app identifies patterns in your food choices, suggests recipes using ingredients you already have, and provides tailored health advice based on your body measurements and dietary goals. The AI-powered platform helps you make better use of your groceries, reduce food waste, and improve your eating habits without the need for manual tracking or nutritional expertise. Whether you're trying to eat healthier, save money, or just figure out what to cook with what's already in your pantry, GrocerGPT turns your shopping data into actionable food intelligence.","GrocerGPT is built on a modern tech stack: Frontend: Next.js with TypeScript and Tailwind CSS for a responsive, accessible interface Backend: Node.js with Express for API endpoints Database: MongoDB for flexible data storage of users, purchases, and recommendations AI Integration: Google's Gemini API for recipe generation and nutritional analysis Authentication: NextAuth.js for secure user management Deployment: Vercel for continuous deployment and scalability","Receipt Data Standardization: Every store formats receipts differently! We spent hours building a flexible parser that could handle variations in formats. Health Recommendations: Balancing helpful advice without crossing into medical territory was tricky. API Limitations: The original plan to use specialized nutrition APIs hit a wall when we found most had severe request limits or high costs, we ended up taking the hybrid approach using Gemini's AI with nutritional databases.",,"Quy learned more about Next.js architecture and MongoDB integration, dove deep into AI integration, working with Gemini's API to create personalized recipe and health recommendations Dan learned more about web development, APIs, database integration and multiple different frameworks Kaiden learned more about web development, APIs, database integration and multiple different frameworks Blake tackled the data processing challenge, creating algorithms to extract meaningful patterns from grocery receipts, working with Gemini's API to create personalized recipe and health recommendations, learned more about Next.js architecture and MongoDB integration",,"api, gemini, mongodb, nextjs, typescript","https://grocery-analyzer.vercel.app/, https://github.com/ChinhQuy29/grocery-analyzer"
HackKU25,Calculator for Miller Unit Values,https://devpost.com/software/miller-unit-values-for-galactosidase,False,"What inspired your team to build this? β-Galactosidase assays are used frequently in the field of synthetic biology to understand the fundamental aspects of transcription, the central dogma of biology. β-Galactosidase assays contain eight measurements, generated by spectrophotometers which generally export text files. for each sample tested and on average approximately 100 samples are tested per assay. Historically, the computation for the miller unit (the quantification of transcription) has been manually copying all outputs of the assay into an excel template to calculate this unit. Seemingly when faced with this method of calculation, we were inspired to make a program that could interpret the outputs without manually having to enter them in, significantly decreasing the amount of repetitive work needed for calculating miller unit values. Our program asks the user to take three files of different file types. The first is a text file containing the initial absorbance values from a spectrophotometer measured before the experiment, the second is a text file containing the final absorbance values measured after the experiment, and the last is a csv file containing sample times.  This data will be processed using the miller unit equation that is defined by taking the difference of absorbance values at λ = 420 and 550 over the difference of absorbance values at Absorbents values at λ = 900 and 1000 over the volume (0.1, a constant) multiplied by the initial concentration and the amount of time in seconds the experiment took. Our program will calculate the miller units (MU) described by this equation and output them with the given sample from which the miller unit was derived. All code generated in this project was made using Python. As two students currently enrolled in EECS 168, we faced many challenges through our development of this program. Initially, we lacked confidence in our abilities to achieve our goal. However, after clever problem solving, we were able to achieve our goal using the tools we knew (or had recently learned in this hackathon). Our program accomplished our goal of computing miller values from a given data set of measured values! Remarkably, our program even found errors in previously computed miller unit values calculated by the historically used method of manually imputing into an excel template. This new method cuts down 67.5% of the work needed to calculate miller unit values as compared to the previously used methods. From this project, we significantly enhanced our file i/o abilities (especially when it came to new file types such as csv), as well as our general understanding of lists and other staple coding features. Additional features would include: (i) a graphical output with miller unit values given for each sample, (ii) Including an interface that could actively display statistics when calculating the error in β-Galactosidase measurements, (iii) implementing calculations for further miller unit calculations such as induction or repression ratios.",,,,,,,python,https://github.com/RyanHuffman0607/HackKU2025Group.git
HackKU25,OWL - Observational Watch Log,https://devpost.com/software/owl-observational-watch-log,False,We were interested in creating a tool that makes comprehensive notes for service workers to increase accountability and challenge bias.,"Our app takes an audio recording, transcribes it, and creates comprehensive summary lists about the information. Frontend: We used Streamlit as a frontend tool to create our website and input generated summary lists into a viewable format. Backend: We used Python because of the whisper python library’s ability to generate .json files for our Gemini Prompting. Machine Learning: We used Gemini API to summarize transcriptions of police bodycam footage by sending raw text data along with a detailed system instruction for concise, actionable summaries. The API processes the content and outputs a summary, which is then saved into a text file for further use.",Our product uses Python Whisper to discern from multiple people in a conversation and translates that text into a file. That file is prompted for a Gemini API Wrapper. The AI then summarizes the information in an easily digestible way that ensures high accuracy of objective information for indexible notes.,"Importing multiple libraries and ensuring that each section of code was able to be passed from one library to the next proved to be difficult
Creating applicable json files for the Gemini API",Working on our first ever front end development of a project,"Google genai libraries
Python Whisper
Transcription coding
Streamlit Project demo coming soon!",,"gemini, python, streamlit, whisper",https://github.com/NSP13737/HackKU2025.git
HackKU25,GuideBuddy - Interactive Speech Medicare Assistant,https://devpost.com/software/guidebuddy-interactive-speech-medicare-assistant,False,"Patient safety is at the heart of GuideBuddy's conception. Every year, millions of patients experience challenges in healthcare communication:",GuideBuddy is an innovative Interactive Speech Medicare Assistant that:,We developed GuideBuddy using a comprehensive technology stack:,,,,"GuideBuddy Version 1.0 is just the beginning of our transformative journey in healthcare technology. We are deeply committed to continuous innovation and improvement, with a clear roadmap for future development.",python,https://github.com/lokaesh2000/GuideBuddy---Interactive-Speech-Medicare-Assistant
HackKU25,NetCode,https://devpost.com/software/netcode,False,"The idea for this project stemmed from the need for a more efficient, interactive way to collaborate on coding projects in real-time. We wanted to create a platform that allows multiple team members to work on code simultaneously, ensuring the process remains seamless and interactive. The goal was to enable everyone to see changes in real-time, without the need for complex setups or version control systems.","Our project is a full-stack web application that allows users to write and run code in real-time. The platform supports multiple programming languages, including C, Python, Java, JavaScript, C++, Rust, TypeScript, and more. Users can choose the programming language they want to use and write code in the editor. The output of the code is displayed in real-time on the screen, allowing for instant feedback. The platform also supports real-time collaboration, where multiple users can work on the same code simultaneously. The code updates in real-time, so everyone involved in the project can see the changes immediately. Additionally, we integrated Google Gemini to provide automated code reviews and suggestions, ensuring code quality and best practices.","The front-end of our project was built usingReact, which enabled us to create a dynamic, interactive user interface. The key features of the front-end include: The back-end was built usingGo, which is known for its performance and scalability. The back-end serves the following purposes: We built aMakefileto automate the process of compiling and executing code. This makes it easier to run code in various programming languages. The system supports the compilation of languages like C, C++, Java, and Rust, while dynamically interpreting languages like Python, JavaScript, and TypeScript. We integratedGoogle Gemini, an AI-powered tool that helps with code review. It automatically provides feedback and suggestions, improving the code quality by detecting potential bugs, code smells, and offering best practices for writing clean, maintainable code. To handle large blocks of code efficiently in real-time, we utilized theRopes Data Structure. This structure allows us to manipulate long strings or text blocks without negatively impacting performance. It helped in:",,,This project taught us valuable lessons in both front-end and back-end development:,"Looking ahead, we plan to: Overall, our goal is to continue improvingNetCodeto make it a powerful tool for collaborative coding and code review, helping developers of all skill levels work together seamlessly and efficiently.","back-end, c, c#, c++, collaboration, css, front-end, full-stack, gemini, go, golang, html, java, javascript, makefile, multi-language, multi-language-support, node.js, npm, pages, php, python, react, real-time-collaboration, realtime, rest, restful, rope, rust, typescript, web-application",https://github.com/Xavanion/Hack-KU-2025
HackKU25,muscle maker,https://devpost.com/software/muscle-maker,False,,,,,,,,"css, html, javascript, vscod",https://github.com/Hadyntryon/demo
HackKU25,Sigma Nurse,https://devpost.com/software/sigma-nurse,False,"Opening page Nurse tending to a patient Start Page InspirationWe were inspired by the real struggles nurses face balancing care for others and themselves. What it doesIt’s an interactive game that simulates a nurse’s day, highlighting choices between service and self-care. How we built itWe used Unity and started with a hospital background and added nurse and a patient and the idea was to make automated patients and for the nurse to help them with their needs. Challenges we ran intoBalancing realism and fun was tough; we also had to learn how to tell a serious story through gameplay. We also had a system crash which made us loose some of our progress lol. Accomplishments that we're proud ofCreating a meaningful experience that resonated with nurses and students alike. What we learnedEmpathy is key in design—especially when representing real professions and challenges and never give up! (Not even when you system crashes.) and also to save the progress before you run the code. What’s next for Sigma NurseExpanding gameplay paths, adding more levels, and adding an outside which is bright with tress and a walking path around a lake nearby.",,,,,,,"art, c#, pixel, unity",https://github.com/v4ms12004/Sigma-Nurse
HackKU25,CashCraft,https://devpost.com/software/cashcraft,False,"Login Page Stock Analyst Site Leaderboard - Compete VS. Friends Logan, Gabe, and I (Nate) all share an AP Calculus class - in which one of our shared friends loves to trade stocks. Due to this, we wanted to try some ourselves, but all being under eighteen - we couldn't. Paper trading, practicing with stocks without money, is unfortunatly restricted to people who use apps where they can also trade real stocks (limiting it to being over eighteen), or paid software.
Our team is all fans of enabling people to learn and better themselves, regardless of age - so this was a clear problem to us. We want an app where we can trade stocks ourselves - learning how to make money with investments - but that wouldn't be original. ""Gameifying your life"" is a tradition that exists in various industrys - and using inspiration from how Pokemon GO's leaderboard enabled many people to go on walks and get physically active, we decided that this app should also be a game. CashCraft enables anyone to practice trading stocks - a valuable life skill in a world where the average cost of living has rapidly grown - especially compared to the average salary. It also encourages everyone to actively work on it due to its friendly competitive nature. This makes a fun learning enviroment for people of any age. Last year we designed with the idea of wanting to make something work that looked good. This year, we have been designing with the idea of scaleability. Our database is done in Golang for high scalability, along with MySql and caching stock prices to not overwelm the API resources we are using. I did the initial set up for the database and website while Logan implemented the stocks. Nick then polished database while Logan tied together the webserver with basic front end. Gabe worked on implementing frontend that the team had been sketching while on breaks.CONTINUE Challenges for this Hackathon came from the massive amount of data we needed to handle This is our first hackathon app in which the app completely works. We genuinely plan to use this app with our friend in the calculus class - and are aiming to have actual users for the app. Everything inside of this app is intended to have high scalability, so we have no concerns if the app was to actually take off. We all learned completely new technologies for this app, and contributed to the overall success of the team.",,,,,,,"crypto, fiber, finnhuub, go, gorm, mysql","https://cashcraft.vip, https://github.com/loganstottle/CashCraft"
HackKU25,Attendr,https://devpost.com/software/attendr-wdm147,False,"The inspiration for this project (in general) came from the theme of this year's hackathon: health and wellbeing. As college students, we see every day how the stress of college and everything to balance can become overwhelming. It can be very easy to fall into the rut of skipping more and more classes every week, and feeling lazy. This laziness leads to feelings of unproductivity, and eventually prolouged stress. The feeling of always falling behind is all too common. This is what inspired us to create Attendr.","Attendr is a mobile app that gamefies going to your lectures and earning points. This unique app also allows you to connect with friends and join groups, where you can comete against each other for the highest score. This friendly competition motivates students to attend their classes and to be more socially active, helping college students to feel more productive and stress free.","We build Attendr using a django-based backend to store all neccessary data with a frontend based in react-native/Expo. React-native allowed us to grab map data from both Google and Apple maps, allowing the map interface of the app to be easy to use. We were also reccommended to use Expo by the professionals at Pella, as we were looking for something that could easily create a mobile app for both Apple and Android.","We ran into many difficulties throughout the creation of this project. This was the first hackathon for all of us, and most of us had never used react-native, Expo, or django before. This turned the project into a very large learning experience, but it was super fun nonetheless. Our main challenges were based around learning all of the ins and outs of each syntax that was used.","We are proud of our learning capability and our organizational skills. We were able to learn and accomplish a lot in a short amount of time. This was made possible by splitting up the project: Alvin worked mostly on the UI and the web design, Nate worked on the front-end, and I (Vincent) worked mostly on the back-end.","We learned a lot about how to use important frameworks such as Expo, django, and react-native, as these are all powerful tools for modern developers. We also learned that it may be wiser in the future to choose a project we are more confident we can complete in the allotted time period, as we may have bit off a little more than we could chew this time.","If we had more time to work on this project, we would've wanted to implement a streak system for indivuduals and groups, (getting a higher streak based on the most consecutive lectures attended). We also entertained the idea (suggested by one of the professionals at Pella) to implement a betting system, allowing you and your friends to enter money into a pot that would be rewarded to the highest-scoring player.","css, django, expo.io, javascript, python, react-native, typescript",https://github.com/Pyrallux/Attendr/tree/main
HackKU25,TeroSecureMD,https://devpost.com/software/terosmd,False,"The inspiration for TeroSecureMD came from the need to empower patients with full control over their medical records while addressing the challenges of data security, privacy, and accessibility in the healthcare industry. We wanted to create a platform that leverages blockchain technology to provide a transparent, secure, and patient-centric solution for managing medical data.","TeroSecureMD allows patients to securely store, manage, and share their medical records on the blockchain. It ensures that patients have complete ownership of their data, enabling them to grant or revoke access to healthcare providers as needed. The platform also provides a transparent activity log, ensuring that all interactions with the data are tamper-proof and traceable.",We built TeroSecureMD using:,,,,,"eccrypto, etherium, firebase, javascript, next.js, node.js, pinata, react, vercel","https://github.com/TheOfficialBarn/tero, http://terosecuremd.vercel.app"
HackKU25,Mr. J,https://devpost.com/software/mr-j-ikgsqr,False,,"This is a minecraft bot that helps alleviate some of the menial tasks within the game with LLM integration that will talk to the user in natural English while also providing a back-up to incorrect task declaration. If the user inputs a task incorrectly or tries to run a task that doesn’t exist, our LLM will trigger a task from our list that does work. A list of tasks is as follows:
Move: Moves to location;bot move x y zHello: Says a message from our LLM integration;bot helloSetHome: Set’s the coordinates of the bot in question as the home for bots to return to;bot sethomeGoHome: Causes bots to return to the previously saved home location;bot gohomeSay: Causes the bot to say a message;bot say messageCome: Causes the bot to come to the users location;bot comeDrop: Causes the bot to drop a block of the specified block type;bot drop itemTypeRemember: Causes the bot to remember whatever is specified to be recalled later;bot remember string of informationValueOf: Recalls previously saved information;bot valueofFollow: Causes bots to follow the specified user;bot follow userAttack: Causes bots to attack the specified user;bot attack userGatherFood: Causes bots to attack a specific type of food;bot gatherfood pigInventory: Causes bots to list their inventory;bot inventoryStatus: Causes bots to list their current status;bot statusEquip: Causes bots to equip a pickaxe if in their inventory;bot equipSword: Causes bots to equip a sword if in their inventory;bot swordMine: Causes bots to mine one block;bot mineChestItem: Causes bots to go to the nearest chest and receive items;bot chestitem itemtype number The minecraft bots can work in plural and tandem using the swarm command. You treat the swarm command the same way you treat bot. Simply input swarm command and they will all act out the command together.","We began the project by programming the bot to enter our Minecraft server and creating the basic functionalities like moving and jumping. After this, we used Mineflayer.pathfinder to help the bot find the most optimal path to get to a location. We created functions that told the robot to go to a specific location, go to a preset Home location, and follow players around. After this we tested functions that interacted with the world such as mine and gatherFood. Both of these functions had to locate a target such as a block or animal so that the bot could perform its action. After a block was mined or an animal was killed, the bot automatically searches the area for dropped items that it can pick up. These functions interacted with other functions like attack and with the inventory to select the correct tool. We built the LLM through trial and error. First, trying different ways of hosting the model on our computers, locally, through a flask API, or on supercomputer clusters. None of which worked, finally settling on using a model’s API offloading the processing to their server and using allotted tokens. This comes with the downside of token prices and needing the application to be online for it to function, but frees up a dramatic amount of processing power on our machines. The next difficult part was engineering the prompt to give us the information we wanted. We iterated drafts of the prompt throughout the time we were allotted slowly fine tuning the results to better fit what we needed. The output was required to be highly specific for our code to run, and to keep the natural language side of the LLM.","The strict time deadline and number of functions the bot required caused pressure on us. We needed to develop a lot of simple actions and then quickly implement them into the more complex functions, where the complex functions would always reveal more bugs that were hiding in the simple implementation. The LLM integration proved to be far more challenging than any of us anticipated. First, we tried to host a model on our system, but the model we chose at first was simply not powerful enough to be useful. Trying larger models proved to slow down our machines too much to be remotely usable. To combat this, we tried hosting them on a local api, but this did not give us any speed back. Finally, we realized we had to use an API and take the always online route. After finally getting the LLM to work and communicate with us, we had to engineer the prompt. This proved to be rather time consuming as even when the LLM produced correct results, the eval function would simply not evaluate it correctly. There were hidden characters or various data types that Javascript’s eval function refused to evaluate correctly. Eventually we managed to get the LLM to provide the correct answer in the correct format most of the time, however, with such a short time we were unable to get the model and prompt at 100% efficiency. About halfway through the allotted time, we were faced with a difficult decision. As our LLM plan was failing and we were left with a lot of broken code, we needed to decide what path to take. Deciding that the LLM as it stood might not be the path, we diverted attention elsewhere because coming back to it. Changing plans and refactoring our team goal was a constant difficulty for us that we overcame to create our final product that we’re proud of.","The amount of code we were able to write in such a short amount of time. Even under stress we were able to work together and create a large amount of functionality for our bots. A well engineered LLM prompt that gives correct format in its replies. One of the more finicky and difficult parts of this project was dealing with and factoring the LLM, getting it to work and having it evaluate as functions was the most satisfying moment of the entire Hack-a-thon for me personally. Having multiple bots work in tandem using swarm to accomplish many simple tasks takes our bots and deepens their complexity.","Every project will inadvertently run into far more unexpected events that your team will have to find solutions for on the fly. No plan ever stays together and works right off the bat. Flexibility and on the fly thinking is an all important skill to have in a group setting. Before starting this project, almost all of us had very little to no experience in Javascript or AI programming. We’ve all learned a large amount about dealing with asynchronous functions, prompt engineering, AI bots, LLMs, using multiple libraries in tandem to make a better whole,and the javascript language. The strict time limit forced us to work together as a team and learn better communication. We were forced to face stress, failure, and things falling apart numerous times before coming together and finding solutions.","We will continue to work on the functionality of the code, implement new functions for our bots in game, continue to tune the LLM prompt, and iron out more of the lingering bugs. Our next steps are to expand upon the functions that are already written and tune the LLM so that the bots are easier to interact with. We would also like to add more functions so that Mr. J can help with even more aspects of the game. Mr. J is able to automate the repetitive and monotonous aspects of Minecraft and we want to make the user experience as streamlined as possible.","gemini-api, javascript",https://github.com/1ProCrafters/HackKU-Mr.-J
HackKU25,SideKick AI,https://devpost.com/software/sidekick-ai,False,"As a college student in this era of AI boom, I’ve always felt tools like ChatGPT or Gemini are insanely helpful — but they kinda miss the part I need most:understanding the approach. Sure, they can give you the answer. But if you don’t grasp thewhy, did you really learn anything? (Spoiler: no.) To be honest, I tried going ""AI-less"" for a bit to challenge myself — I lastedtwo days, and that too on aweekend🫠 Initially, I wasn’t even going to participate in this hackathon. I was busy, burned out, and planned to sit this one out. Which I successfully did... untilSaturday noon. See, all my friends were building stuff, asking what I was working on, and I was like, “oh, not participating 😌” — but that never ends the convo, right?“Why not?” “Skill issue?” “You got ditched?” “Bro you good?”I was just trying to vibe in peace and they hit me with the deep concern 😂 So I thought, alright — I've been meaning to explore Chrome extensions + AI anyway...Why not jump in and see what’s possible? And that’s howSidekickAIwas born:A Chrome extension thatsits with you, helps youunderstand, and gives you answersonly if you ask for them.Not a replacement — a learning buddy. A Sidekick. This isn’t just about solving stuff. It’s about learning how tothink throughstuff. Yeah so... Chrome doesn’t play nice withfile://pages,chrome://tabs, or blank tabs. Took a bunch of reloads, Google searches, and confused silence to sort that out. Felt like building a walkie-talkie. You send a message, no one replies. “Receiving end does not exist.”Mood. At first, I just shoved the answer into the first input box I could find. But that wasn’t smart. I tried detecting where the user selected the text and finding thenext closest input— didn’t fully finish it, but got the basic idea working, and it’s on my roadmap. Honestly, this was the most “full-stack” thing I’ve ever done inside a browser that wasn’t a traditional web app. No frameworks, no libraries — just rawJavaScript + Chrome APIs + UI tweaking. 📦 Justclone the repo,load as an unpacked extension,right-click any question, and you’re learning smarter — not harder. (you will unfortunately need your own gemini api, I was not able publish this extension in this short time frame ;(  )",,,,,,,"css, gemini, html, javascript, markdown",https://github.com/JayP04/SideKickAI.git
HackKU25,DepressoBot,https://devpost.com/software/depressobot,False,"Many people who have undiagnosed depression may not realize it as a condition applicable to them. It is shocking what people can see as normal when exposed to a condition like depression for an extended period of time. Our goal heading into this project was to simply serve as a wake up call, getting people to question their own mental health and potentially encourage them to seek out help.","We built a discord bot that takes in some basic personal data, and the plugs it into a machine learning model that tests your data against a large dataset of students to see if your data correlates with students diagnosed with depression, so that we can alert those who may be at risk and direct them to resources that can help.","We used the sklearn library in python to test several different machine learning algorithms against a large open-source dataset on students who were possibly diagnosed with depression. After we applied these models, we choose a model with high accuracy and precision, and used API tools to take in data from a conversation with a discord bot, and test that against the machine learning model",,,,,"discord, flask, pandas, python, seaborn, sklearn",https://github.com/ascaletty/HackKU2025/tree/main
HackKU25,Galdoran,https://devpost.com/software/galdoran,False,Our team YAKK has been working together for the past school year fueled by our passion for learning and love for video games. We want to learn as much as we can about computer graphics and the available tools while also making video games that we would want to play. This project was born from our interest in making fun good looking games highly accessible.,"Galdoran is an open world RPG game that runs in the browser. It is optimized to run on lower end devices making it highly available, not even needing to download anything to be able to play it. Galdoran provides a fun experience exploring a vibrant planet with a new race.","Galdoran was built mainly using ThreeJS, a java script library for 3D computer graphics. Several other tools were used to produce the environment and assets used in the project. Unreal engine was used to quickly plan and build out the vast environments then were exported to be imported, adjusted, and used Blender was used for some modeling and some textures.","There were a few large barriers we ran into during the production of Galdoran. One of the main ones was getting the environment to migrate from unreal engine properly. We leveraged the ability to quickly build environments with PCG in unreal engine but when it came to migrating them we ran into some issues with the materials and textures not carrying over properly. Some other issues came with us being new to JavaScript, and the limitations of the browsers performance capabilities.","We are very proud that we were able to take our understanding of higher level implementations of computer graphics concepts and build our own implementations of them at a lower level. Some examples of this being state machines for animations and decisions, and culling.","We all gained a lot of useful knowledge from working on this project, among them being JavaScript, state machines, blender, server hosting, and texturing. However, the most important thing we learned from this, is how useful and important a well built and organized plan is. In the future we will be better about building out concrete measurable tasks for us to reach.","Every member of out team intends on working together on Galdoran throughout the rest of the semester, with the intention of releasing the game around the end of the semester, So keep an eye out for Galdoran from YAKK Studios, coming soon!","blender, css, gimp, github, html, javascript, rapier, three.js, vite","https://galdoran.org/, https://github.com/KlaytonPagel/galdoran-HackKU"
HackKU25,GoodMoodFood,https://devpost.com/software/goodmoodfood,False,,,"We built it using html, javascript and css.",,We can actually make the UI/UX a bit more than something within so limited time.,,Further development on how the AI going to provide accurate meals recipe to the user with given data. There would also be different version that aim to some specific target audience like children with more playful and vibrance color choices.,"api, css, gemini, html, javascript",https://github.com/norman2k5/GoodMoodFood_HackKU25
HackKU25,ThriveTogether,https://devpost.com/software/thrivetogether-dye67f,False,"Webapp interface Wellness is a team sport—we help you build yours, starting with care tailored just for you. Preventative care often gets overlooked. Many of our relatives struggle with getting access to holistic care, and are unaware of how to best keep themselves healthy. They often don't have the tools or community to guide them. Inspired by these experiences of navigating personal health, we set out to build an easy-to-use tool that helps people take proactive steps towards living a healthier life style. Our web app suggests personal healthcare and screening recommendations along with lifestyle tips based on one's basic health data and medical history. Once you've received your care plan, our platform connects you with local community members who share similar health goals or challenges, because staying healthy is easier (and more fun) with others. Whether you're looking for someone to go on a walk with or accompany you to appointments, ThriveTogether aims to help you build a circle of support to keep you motivated and healthy. Paste the link to your video here.📽️Watch the demo",,,,,,,"css, gemini, html, mongodb, typescript",https://github.com/aliiyuu/hackku
HackKU25,Dopamine Tracker: Reward System Detox App,https://devpost.com/software/dopamine-tracker-reward-system-detox-app,False,"In today’s fast-paced digital world, we are constantly surrounded by tech designed to grab our attention and offer quick dopamine hits—whether it’s scrolling through social media, playing games, or consuming junk food. While these habits provide short-term satisfaction, they often lead to long-term mental and physical consequences like decreased productivity, poor health, and emotional burnout.
Dopamine Tracker was inspired by the need to create a tool that helps individuals break free from the cycle of instant gratification and create healthier, more intentional habits. The goal was to empower users to track and regulate their dopamine-driven behaviors and promote mindful living.","Dopamine Tracker is a web app designed to help users track activities that lead to dopamine spikes (e.g., tech addiction, unhealthy eating) and replace them with healthier alternatives. The app offers the following features:
• Dopamine Activity Tracker: Log activities that trigger dopamine spikes, such as social media, gaming, snacking, etc.
• Healthy Habit Swap Challenges: Replace addictive behaviors with productive and healthier alternatives.
• Personalized Insights: Analyze user data to provide insights on habits and suggest improvements.
• Progress Tracking: View visualizations of habit trends and the success rate of habit swaps.
• Push Notifications: Gentle reminders to stay on track and stick to healthier routines.
The app’s mission is to help users regain control over their time and well-being by fostering long-term behavioral change.","We built Dopamine Tracker using Flutter for the front-end development. The key technologies used include:
• Firebase for backend services, including user authentication (Firebase Auth), real-time database (Firestore), and analytics.
• Dart as the primary programming language for Flutter.
• FlutterFire to integrate Firebase functionalities seamlessly into the Flutter app.
• Flutter Widgets and Material Design principles to build a smooth, user-friendly UI.
The app starts by allowing users to sign in anonymously via Firebase Authentication. After logging in, they can track their dopamine-triggering activities and receive challenges to swap those habits with healthier options. We also implemented Firebase Analytics to track user behavior and help optimize future app features.","• Firebase Setup: Setting up Firebase with Flutter initially posed some challenges. We had issues linking Firebase services, especially for iOS and web configurations. However, following detailed guides and troubleshooting through Firebase documentation helped us successfully integrate it.
• User Data Privacy: Since we were working with personal behavior data, we needed to ensure that the app complied with privacy regulations and that users felt comfortable using the app. We implemented anonymous login to avoid storing unnecessary personal information.
• UI/UX Design: As beginners, designing an intuitive, visually appealing interface that was also functional required a lot of trial and error. We wanted the app to be simple and easy to use, while still delivering complex insights to the user.",,"• Firebase Integration: We gained a deeper understanding of how Firebase works and how to use it effectively for real-time data storage and user authentication.
• Flutter Development: Building the app with Flutter helped us understand cross-platform development better. We learned how to handle platform-specific issues and ensure a seamless experience across devices.
• UI/UX Design: We learned how to design a minimalistic and intuitive user interface using Material Design principles in Flutter.
• App Architecture: We learned how to architect a simple yet scalable app that can grow with additional features like social sharing or gamification.","• Gamification: We plan to add gamification features, such as badges and rewards, to keep users motivated and engaged.
• Social Sharing: We would love to incorporate a social aspect where users can share their progress and challenges with friends and compete on leaderboards.
• Customization: Allow users to create custom challenges based on their own habits and goals.
• Expansion of Features: Additional features like advanced data analytics, mood tracking, and more health-focused habit recommendations are planned for future versions.
• Android & iOS Deployment: The next step will be to deploy the app on the Google Play Store and Apple App Store.","dart, firebase, firestore, flutter",https://github.com/Ankitha-S/HackKU-Project
HackKU25,Claria,https://devpost.com/software/smartpitch-o5x8pf,False," Claria was born out of a recognition that sales professionals often juggle too many tasks during live calls—balancing conversation, note-taking, and remembering critical details about prospects. We were inspired by the potential of AI to simplify these challenges, drawing motivation from platforms like Assembly AI that showcase the power of real-time transcription and analytics. Claria is an AI-powered assistant designed specifically for live sales calls. It: Our development process combined state-of-the-art AI with thoughtful design: Developing Claria was not without its hurdles: Our journey with Claria taught us several key lessons: Looking ahead, we plan to:",,,,,,,"ai, amazon-ec2, beautiful-soup, docker, express.js, gemini, javascript, mongodb, node.js, pulse-audio, react, request, tailwind, webrtc, websockets","https://github.com/Rizwan1387/Sales_Agent.git, https://github.com/Rizwan1387/Sales_Agent.git"
HackKU25,Workout Warriors,https://devpost.com/software/workout-warriors,False,We were inspired by another app that one of our team members use: Level Up: Fitness. He thought that we could make an app that he could switch to. This would allow him to request ideas or functionality.,"It tracks your fitness progress, and it incentivizes you being gamifying the process. The app is currently not finished, and there isn't a lot of functionality, but we planned on having",We used the Flutter framework which allows multiplatform development.,The first problem we ran into was nobody knew anything about flutter or how it worked. Another problem that was ran into quite frequently was library compatibility issues.,"We are proud with how far we were able to get. The team consisted of one software engineer, one app designer, one graphic designer, and one audio designer.",We have learned that it is hard to complete a feature filled app in 36 hours.,"The next steps for Workout Warriors is to implement being able to start exercises. We also need to add functionality to social aspects, and we also need to work on implementing daily challenges. We also need to flesh out a profile screen where users can change some of the info about them.","dart, flutter",https://github.com/craig-zac-a/WorkoutWarriors
HackKU25,Rate My Roommate,https://devpost.com/software/ratemyroommate,False,College housing decisions are often made blindly. We've all heard roommate horror stories or experienced them firsthand - from the night owls disrupting your sleep to the ones who never wash their dishes. We created RateMyRoommate because students deserve to make informed decisions about who they'll share their living space with during such a formative time in their lives.,"Rate My Roommate is a platform that helps students share and access honest feedback about potential roommates. Users create profiles with their educational email addresses, search for previous or potential roommates, and leave structured reviews based on their living experiences. The platform includes ratings on key factors like cleanliness, noise levels, and overall compatibility, alongside written reviews that provide context and personal experiences.","We developed Rate My Roommate using Next.js for a responsive user experience, with TailwindCSS and  for a clean, intuitive interface. The backend runs on Supabase's PostgreSQL database to securely store user profiles and reviews.
We implemented Clerk for secure authentication with educational email validation to ensure our community remains focused on students.","Educational email validation proved trickier than anticipated, as we needed to verify countless institutions while preventing non-students from joining. Building a review system that's both helpful and protective against misuse required careful balance - we wanted honest feedback without enabling harassment or defamation.
Determining the right questions to ask in our review system was another challenge. We needed to capture the essential aspects of roommate compatibility without making the process tedious or intrusive.","We're most proud of our user interface design. We created a refrigerator-themed interface with sticky notes and a to-do list that perfectly mimics how roommates actually communicate in shared living spaces. This familiar, relatable design reflects the authentic roommate experience while making the platform instantly intuitive for students.
We've successfully married practical functionality with an authentic visual language that speaks directly to student life. Beyond our robust educational email verification and balanced review system, it's this attention to real-world roommate dynamics in our design that makes RateMyRoommate both useful and delightful to use.","Building Rate My Roommate taught us the importance of user privacy and data protection, especially when handling potentially sensitive information. We learned to balance transparency with responsible data practices.
We also gained valuable insights into the specific concerns students have regarding roommates and living situations, which helped us refine our platform to address real problems rather than assumptions.",We're planning to expand Rate My Roommate with several features:,"clerk, next.js, postgresql, react, supabase, typescript",http://ratemyroommate.co
HackKU25,IBD Tracker,https://devpost.com/software/ibd-tracker,False,"We were biased to make this app because Sam and I both suffer from IBD, a chronic autoimmune disease, which currently has no cure. Treatments to manage IBD are not well understood, so the process of finding what works for an individual is often tedious, taking many years to find treatments to allieve patients' suffering. Out of this need for a way to better recommand treatment and understand what leads to the large variance of treatment responses across individuals, we developed IBD Tracker -- the first product of it's kind -- whose aim is to help sufferers of IBD better understand their disease and contribute to our understanding of and ability to personalize treatment for IBD.","IBD Tracker consists of two parts -- the phone application for logging symptoms and receiving personalized insights, and the machine learning powering these insights. We utilize clustering and custom pattern recognition algorithms to detect groups in IBD patients who respond differently to medication and share distinct symptom profiles. We trained these algorithms from scratch utilizing open-sourced data (over 2 terabytes and 8 million repeated self-assessments). The power of IBD tracker, is the more our app is used, the better our models get, as they are continuously improving to understand IBD treatment better. The app has many features. In the home page, you can log entries and your daily symptoms (e.g fatigue, stomach pain, etc) on a scale from one to ten. you can also add a notes, for example I ate chicken today and I hit legs in the gym. This data is shown in the dashboard, so you are able to see your most severe symptom as well as graphs that show you how you felt throughout a given period of time. This dashboard encourages app usage with the mascot starting out sad and, as you use the app, it gets happier. Then you have the community tab, where you have chats in which you can post and connect with other people and discuss about topics like medication, nutrition, etc. You also have recommended healthy restaurants, and nearby hospitals. Finally in the profile tab you have information about the user and then you can export the pdf and send it to your doctor. This has your symptoms over a period of time, and also your notes. From experience this is something that gastroenterologists recommend when you suffer from ibd.","We built the app using a combination of react native and FastAPI. React Native was use for the frontend and we are really happy with the results. We were able to connect the frontend and the backend to achieve successful features but without complicating things to much. We utilized PostgreSQL, Sklearn, Pandas, and Matplotlib for training the models.","The dataset was 2 terabytes. We had to clean it, pre process it, one hot encode it.",We learnt a lot about databases and finally got to put what we've learnt in class to test. It's also the first time we compete in a hackathon so we are proud to be able to have a succesful demo.,"We learned a lot about team work. How to divide tasks, how to work under pressure and deadline and also how to push through fatigue and still maintain a positive attitude an creativity.","Connect all the features with the backend and add more features that can be useful for the user. A major goal would be a win-win situation in which the user is getting helped by using the app, but we, the creators are using their data, with their consent to train models and be able to help even more people.","custom, expo-print, expo-router, expo-sharing, expo.io, expo/vector-icons, fastapi, github, javascript, python, react-native, react-native-calendars, react-native-community/slider, react-native-svg-charts, typescript",https://github.com/f642f091/Hack-app.git
HackKU25,Flanner - AI meal planner,https://devpost.com/software/flanner-ai-meal-planner,False,"College students face unique nutritional challenges while navigating campus dining options. Many students want to make healthier eating choices but struggle with limited meal plan options, nutritional awareness, and time constraints. As students ourselves, we recognized this gap and created Flanner to help students on meal plans make more conscious, personalized eating decisions without requiring extensive nutritional knowledge or time investment. The motivation behind Flanner stems from our own experiences trying to maintain healthy eating habits while on university meal plans. We found that despite having access to dining halls, many students struggle to create balanced meals that align with their dietary needs and fitness goals. This challenge is particularly acute for students with specific dietary restrictions, allergies, or fitness objectives.","Flanner is an AI-powered meal planning application specifically designed for college students on meal plans. It transforms campus dining from a source of nutritional uncertainty into an opportunity for personalized, health-conscious eating. The app offers:",Flanner is built with a modern tech stack that combines robust backend services with a responsive frontend:,"We faced a few technical challenges going into this project. First off, University dining systems lack public APIs, therefore we built our own custom web scrapers to handle inconsistent HTML structures and frequent DOM changes. Since this is our first hackathon ever, and our first full scale web application, we had very fun time exploring the frontend and backend and how to integrate them together. Using Gemini API is also a challenge since we need to make sure to parse them correctly to our app. Overall, there are still bugs to squash but we are happy to have tackled them.",,,,"gemini, nextjs, python, typescript",https://github.com/trungle15/flanner/
HackKU25,Hardware Shenanigans - CRT Restoration,https://devpost.com/software/hardware-shenanigans-crt-restoration,False,"Box grid from the 240p test suite. Menu Screen, Final Fantasy VI (SNES, 1994) The Inside of the PVM-1271Q. Our focus is on the vertical board on the right - the deflection board. Capacitor gore Playing Super Smash Bros. Melee (Nintendo Gamecube, 2001) after the recap work. I hope I've kept you waiting. Back for yet another hardware hack, this year I am fixing up an early 1980s Sony Trinitron PVM-1271Q. With a nice 14"" picture tube, this is an excellent monitor for watching old movies and playing older games. As electronics age, the components in them can begin to fall out of spec, and while there is a tolerance that is usually afforded, they will eventually fall out of spec, which will result in issues cropping up. With electrolytic capacitors, we can also find that their electrolyte can leak out, which is corrosive and can damage the PCBs and other components on the board. Most electrolytic capacitors only have a shelf life of around 15-20 years, and a service life closer to 10 years. With this monitor being 40 years old... you can see how that's a concern. CRT Monitors and TVs can age in many different ways - worn down picture tube, screen burn in, geometry issues, deflection issues, and power issues are some of the most prolific. But they're of special interest for restoring and preserving, as they're completely and fundamentally different than LCDs and OLEDs in that they don't have physical pixels. Ever notice how old 240p or 480p content looks awful on a modern TV but looks perfectly fine on a tube TV? That's why - LCDs and OLEDs have aphysicalpixel grid which result in them having to very roughly scale up old 240p and 480p content. CRTs will nicely display them due to the them not having a physical grid, and is entirely based on the signal being put into them! In addition, since there aren't physical pixels, older pixel artreallyshines on old hardware. This particular unit is having some deflection issues, which results in some issues with geometry, as we can see from the top left corner stretching out in the pixel grid, as well as some image clarity issues. By recapping the board, we can solve the fundamental deflection issues that result in image clarity issues, and help us dial in the geometry. However, due to time, we're only going to focus on the recapping, rather than the geometry adjustments themselves - those only need potentiometer adjustments and adjustment of the yolk. Due to the way CRTs draw images, it is very difficult for one to capture a good image of one from a phone, so if you want to see good pictures of the end result of the recapping, you'll need to come by the demo!",,,,,,,"capacitors, hardware",
HackKU25,ObsCurity,https://devpost.com/software/obscurity,False,We were inspired by the visuals seen through Lidar technologies and by the bravery it takes for those without sight to navigate the world.,It allows you to experience a small part of how blind people explore the world and map their surroundings in their head. It aims to spread awareness of how difficult even ordinarily simple navigation can be when you can only rely on mental mapping and sound.,Using the unity game engine and a lot of time and brain power.,"Sharing the Unity project through GitHub ended up causing many headaches and not working at all, as there were many config errors we couldn't figure out.",We are proud of the technology we were able to come up with ourselves to get the primary game mechanic to work but are also proud of the story concepts we have that will eventually make this into a full game (that are not available yet in this demo unfortunately).,We have learned much about how difficult it is for those without vision to navigate even simple house layouts and have gained a huge respect for them.,We will work to extend this past just being a demo project and into being a full game that others can play. We believe spreading awareness of how amazing blind people are to be incredibly important and would love for it to reach as many people as possible.,"c#, github, unity","https://github.com/Voidyll/ObsCurity, https://voidyll.itch.io/obscurity"
HackKU25,peerGig,https://devpost.com/software/peergig,False," We noticed that many students possess valuable skills but lack an easy way to offer them to their peers. Whether it’s help with a resume, event photography, design work, or lab tutoring, opportunities are often scattered and informal. PeerGig was created to centralize and streamline this process for students. Students can post gigs they need help with—like designing a poster, coding help, or editing essays. Other students can browse gigs and apply based on their interests and skills. Posters can view applicants, accept or reject them, and once accepted, initiate in-app communication. Fully responsive and tailored for both desktop and mobile campus users. Frontend: React + Tailwind CSS (Vite) Backend: Express.js, MongoDB (Mongoose) Authentication: JWT-based secure login with college ID/email File Upload: Resume upload via multer, stored in a secure folder Real-time Communication (WIP): Implemented via Socket.IO for post-acceptance chat Managing real-time chat between users and ensuring the correct socket rooms were joined. Handling multiple user types (poster vs applicant) and conditional rendering based on status. Resume upload flow with authorization headers. Debugging deployment issues with Render and frontend-backend CORS handling. Building a full-stack MVP with authentication, CRUD operations, real-time elements, and responsive design. Creating a clean user experience for both job posters and gig seekers. Deploying a working version on Render with persistent data. How to implement secure authentication and role-based access Structuring a full-stack freelance platform with bidirectional workflows Working with Socket.IO for real-time features Handling dynamic forms and conditional UI logic efficiently 💬 Polished chat system with persistent message history 🏷️ Skill-based filtering and AI-suggested tags for gigs 📊 Dashboard analytics for students to track earnings and reviews 🪪 Verified badge system based on resume and previous gigs 🌐 Expansion to multiple campuses with admin moderation tools",,,,,,,"express.js, firebase, git, github, javascript, jwt, mongodb, multer, node.js, react, render, socket.io, tailwind, vite","https://peergigfe-59yi.onrender.com/, https://github.com/manavisharma14/test2"
HackKU25,ReLode,https://devpost.com/software/relode,False,AI searching through websites to find the best results,help people get info on places,GeminiAi,"Time, I have to split my time between workshops, coding, and other activities",First ever Hackathon,Ai produces a lot of bugs in larger programs,,"css, firebase, html5, javascript, python",https://github.com/JaidenTGreen/HackKU25
HackKU25,Polar Lizard,https://devpost.com/software/polar-lizard,False,"The C programming language distinctly lacks many things that many programmers take for granted today, meaning it can be an intimidating choice for performing even simple tasks. Yet, you can do anything with C, and there's something liberating about its simplicity. So, we set out with the simple goal of making a game in C, and this is where it brought us.",,"We used Raylib, a simple library wrapping common functionalities needed in games. Much of the game state is contained in a ""Level"" struct, which contains most of the items on screen at any given point. Whenever we added a new type of object, the Level struct was given a new array field for that object. Each of the game's stages is created with a function that populates the struct with the needed objects. The only dynamic allocations in the game are for a handful of assets, which are held for the whole runtime.","Since Raylib was the only external library we used, we had to roll our own physics and collision resolution, which are very difficult to get right. Even now there are jarring, but thankfully scarce physics bugs.",,You can do a lot in plain C.,"Bipolar Lizard, perhaps","c, raylib","https://polar-lizard.vercel.app, https://github.com/GavenB141/HackKU25"
HackKU25,Moodi,https://devpost.com/software/moodi-57qipx,False,"The inspiration for this app came from journaling and emotion tracking apps which were overwhelming for the user. Apps such as Moodwave and Apple's journaling app, which ui was both frustrating, overly complicated, and difficult to look at for a space that was meant to be calming.","Moodi is both an emotion track and journaling app. The emotion tracker color codes your emotions throughout the month, to allow the user to reflect on how they have been feeling in one simple space. You can see this tracker separated by the separate months or weeks. Along with this, it has a journaling function. This records each entry that the user enters, both allowing them to enter multiple entries and letting them look back on their entries for later reflection. Where our app differs from other apps of the same kind, is it's user friendly ui/ux. By simplifying the app down to a single page, as well as utilizing calming colors and eye-pleasing fonts. On a device which encourages distractions, we created an app that puts our users first to have a place to vent and reflect.","This app was made on Flutter, utilizing the language dart. Every member was completely new to this language and had to learn it from scratch during the hackathon. To do so, we practiced first on small individual projects to get familiar with dart. After learning the basics, we honed in our knowledge, along with using tutorials for the more unique parts of the ux, in order to put together our final project.","The hardest challenge was getting familiar with a completely new language, enough to create an app that was both appealing and ran without errors. Although we were able to overcome this by learning individually, we were then faced with the challenge of piecing our work together. To do this, we used the help of AI to help put together the calendar alongside the journaling aspect of the app.","As a group, we are proud of both how much we learned and the fact that we never gave up when faced with so many challenges. We took on a large task trying to create an app from scratch on a language none of us have ever heard of. It was difficult, yet we all walked away knowing we had learned so much from the experience. Even if we don't use dart itself again in the future, we now know that we are capable of exploring more languages. Not only this, but we created a finished product that we were proud of. We refined it again and again, leaving it without an error in sight.","This project was nothing but learning. Each part of our work was created through us learning as much about dart as we could in this short amount of time. From the simple basics of dart, to the unique ui and ux that we were able to create, it was all a learning curve for our team to eventually overcome.","We plan to streamline things further in order to make for the best experience for our users, assuring that we always keep things simple while still providing a unique experience. A big part of this is adding more to the ui in order to make it more concise and a better tool for reflection. One way to do this is through adding a gradient to the top of the ui to get a more comprehensive view of how the user has been feeling. Along with this, our team wishes to use the skills we learned from creating this app in order to create other apps.","dart, emulator, flutter, vscode",https://github.com/f1nleyb/Moodi
HackKU25,AI Driven Text-Based RPG Discord Bot,https://devpost.com/software/ai-driven-text-based-rpg-discord-bot,False,"Our idea for the project mainly came from the decorations for HackKU, and a collective passion for high fantasy and similar settings.","The bot functions like a classic text-based RPG, letting the player chose their own path. Currently, there's a limitation to the current functions of the bot, as we had plans to implement a much more in-depth system but suffered a lack of time. However, the bot will create vibrant descriptions and stories for each scene that the player interacts with. There is a bit of player data functionality which allows for storage of important game elements, with room for improvement in the future. 
A player is anyone who goes to enters the command ""!begin"" in a discord channel that is shared by the bot. From there, the program will begin listening and responding to user input.","The project was constructed entirely from python with API calls to a variety of sources, each of which providing a different part to our bot's functionality.","Time was our biggest challenge, as we had bigger plans for what we wanted our bot to be able to handle. Unfortunately, as we ran short on time, some parts and files of our code don't impact the experience as much as we hoped for.",The prompt engineering along with how our code handles it on the backend is something we're all proud of. Being able to see our efforts be reactive in real-time was also great!,"One of the primary skills our team gained was how to develop and bugfix a discord bot. Additionally, we learned how to use ChatGPT's newer Assistant feature to drive our story development.","The next step would definitely be implementing more functions to make the experience more interactive, allowing for better-structured events within the game.","discord, openai, python",https://github.com/aidanp12/Steve
HackKU25,LinguaSwap,https://devpost.com/software/linguaswap-1w5pef,False,"As someone who loves languages and speaks four of them, I know firsthand how overwhelming learning a new one can be. From grammar drills to countless flashcards, it’s easy to feel discouraged. But what if learning could feel natural, like how we picked up our first language?",LinguaSwap is a Chrome extension designed to help learn new languages by replacing words on web pages with their translations. Key features include:,"Initial planning and setup: we didn't have ""good"" ideas for the theme tracks so we decided to do whatever we thought best and LinguaSwap was created. we first planned and set up a structured project environment using Node.js and npm for dependency management. We divided the project into key components: the Popup UI for user interaction, the Content Script for core translation functionality, and the Background Script for managing extension state and tasks. Directory Structure:",,"It works! although it's not fully complete and still has a few bugs and some optimization issues, we successfully built a working extension.",,"Probably going to try and improve it further, fix the bugs, improve translation quality, increase optimization, and maybe publish it","ai, chatgpt, chrome-devloper-tools, css, font-awesome, html, javascript, mymemory, node.js, webpack, xlsx",https://github.com/xNIGMAx/LinguaSwap
HackKU25,Recall,https://devpost.com/software/recall-1hf7rq,False,"Two of our team members have grandparents who were diagnosed with Alzheimer’s, and we've seen firsthand how difficult memory loss can be. This can be difficult for those affected, and for their loved ones as well. This personal connection inspired us to build a tool that helps preserve meaningful moments and empower those going through similar experiences to relive and reinforce their special memories.",,,,,,,"gemini, supabase, svelte",https://github.com/yephrem22/Ku_hack_2025.git
HackKU25,Price Aware+,https://devpost.com/software/price-aware,False,"When I visit the doctor, it's hard to know how much the procedures will cost. It also depends on the insurance.","PriceAware+ is a website to make cost transparency, having a predictive model to see historical data and forecasting to see what's the best plan for your health insurance, as well as the prediction of the cost of procedures","I used the Next.js framework. I use front-end and back-end typescripts, and I used Supabase, Pinecone, and OpenAI to have a database for all the data that I store from healthcare data. Pinecone effectively filters out the key components, such as only generating the relevant questions, and OpenAI queries Pinecone.","The challenge was to geom_line plot the hospital zip code, which was not retrieved correctly.","I am proud of learning how Supabase works, how Pinecone vectorizes the key points(gives the most relevant info)",I learned full stack and how to structure the code learn a lot about typescript,"Keep working on PriceAware+; it's just a starting point, so I will make it real.","openai, pinecone, supabase, typescript",https://github.com/Ed0827/PriceAware
HackKU25,MOOD-AI,https://devpost.com/software/mood-ai-cnlgyq,False,"      The idea for MoodAI was born out of a personal journey to understand how our emotions shape our daily lives, especially in the context of health and well-being. I was inspired by the countless times I noticed how a tough day could leave me reaching for comfort food or how a burst of energy begged for something vibrant and fresh. I wondered: what if there was a way to align our moods with intentional, health-focused choices—be it food, activities, or simple moments of reflection? The growing awareness of mental health and its deep connection to physical wellness fueled my desire to create a tool that empowers people to nurture both, seamlessly and playfully. Building MoodAI taught me a ton about the interplay between technology and human experience. I dove into the psychology of emotions, learning how moods like happiness, stress, or sadness influence our decisions—and how small interventions can shift them positively. On the technical side, I sharpened my skills in JavaScript, CSS, and HTML, grappling with dynamic UI design and real-time interactivity. Integrating external APIs (like Unsplash for images) opened my eyes to the power of third-party tools, while crafting games like Tic-Tac-Toe and Snake reminded me how coding can be both functional and fun. Most importantly, I learned patience—debugging is a marathon, not a sprint! MoodAI is a web-based platform designed to respond to your mood with tailored health and well-being recommendations. I started with a clean HTML structure, styling it with CSS to create a visually soothing, glassmorphism-inspired interface that feels modern yet calming. JavaScript powers the core functionality: mood buttons trigger dynamic content updates—recipes from a custom dataset, motivational quotes, and game suggestions. I built four mini-games (Tic-Tac-Toe, Memory Match, Snake, and Rock Paper Scissors) to offer playful stress relief, each coded from scratch with event listeners and state management. The food recommendation page connects moods to recipes, styled with a responsive grid layout. Challenges like modal management and game logic were tackled with modular functions and persistent testing. No project comes without hurdles! One big challenge was ensuring the games worked smoothly within the modal system—early versions of Tic-Tac-Toe and Snake had buggy resets and event overlaps, which I fixed by refining initialization logic. Integrating the mood-based recipe system meant designing a robust dataset that felt personal yet practical, which took multiple iterations. Browser compatibility threw curveballs, especially with CSS animations and grid layouts on mobile devices. Time management was another beast—balancing feature richness with polish was tough, but I prioritized user experience over perfection. Finally, debugging the Chess game (initially planned) proved too ambitious, so I simplified it to focus on core functionality.",,,,,,,"canvas-api, chess.js, chessboard.js, cloudflare-(implied), css, github-pages-(assumed), google-fonts, html, javascript, local-storage, none, spotify-api-(partial), static-data, unsplash-api, web-browser, youtube-api-(implied)",https://github.com/bhavanagoud111/MOODAI
HackKU25,EZRead,https://devpost.com/software/ezread-kc4ef2,False,"EZRead started from a seminar where I kept rereading a paragraph and still couldn’t understand it. It made me think: if I’m having this much trouble, it’s probably even harder for people who process information differently. There aren’t many tools that make online reading more accessible in a practical way, so Emma and I decided to build something lightweight but useful. We ran a quick survey to understand what features people actually wanted, and used that feedback to shape EZRead into a tool that simplifies, reads, and saves content to better support different reading needs.",EZRead is a Chrome extension that:,"We built EZRead as a Chrome extension using JavaScript, HTML, and CSS for the frontend. For the backend, we used Python with Flask to handle API routes for simplification, speech generation, and saving content to MongoDB. The core of our simplification feature relies on the Gemini API, which rewrites selected text based on the user's preferred accessibility level. We designed prompts that adjust the complexity of the output to support different reading styles and needs. To make content audible, we used Google Cloud’s Text-to-Speech API with SSML markers to sync spoken words with real-time highlighting. User data is saved using session IDs, allowing personalized history without requiring logins.","Syncing the text-to-speech with real-time word highlighting was one of the harder parts. Getting the SSML markers to work right with Google’s Text-to-Speech API took some trial and error. We also had some early issues with MongoDB because our IPs weren’t added to the network access list, which blocked our database connection until we fixed it. On the frontend, we had to make a lot of small design tweaks so the toolbar didn’t break websites or feel intrusive. Using the Gemini API was mostly smooth, but we still had to test different models and prompts to make the simplifications sound clear and helpful. Making everything work together in a way that felt clean and easy to use took some time.","We’re proud that we built a fully working Chrome extension that simplifies, reads, and saves web content in a more accessible way. It connects to a backend, handles API calls, syncs audio with highlighted text, and stores data in MongoDB. It was our first time building a Chrome extension and working with MongoDB, so getting everything to talk to each other smoothly felt like a big win. We also made the UI clean and lightweight so it works across different sites without being disruptive. Most of all, we’re proud that we created something that could actually make reading online a little easier for people who need extra support.","We learned how to build a full-stack Chrome extension from scratch and connect it to external services like the Gemini API and MongoDB. It was our first time using both, so we had to figure out routing, and how to handle API requests between the extension and the backend. We also got more comfortable debugging across different layers of the project from frontend to backend to browser permissions. On top of that, designing a clean UI that works across all kinds of websites helped us think more about accessibility and user experience. Overall, this project helped us improve at breaking down problems and connecting different tools and technologies into a single, working product.","We’re planning to keep working on EZRead after the hackathon. Some of the next features we want to build include user accounts for saving personalized settings, a pause button for speech, and a back button that lets you return to the original text after simplifying. We also want to add an option to simplify text into bullet points, which could be helpful for studying or quick reference. Beyond that, we’re looking into AI-powered summaries, more font options, and a focus mode for distraction-free reading. We’d also love to add collaboration tools for sharing simplified content and expand to multiple languages to support a wider audience.","css, flask, gemini, google-cloud, html, javascript, mongodb, python",https://github.com/sadorno1/EZread
HackKU25,Gnomtastic Gnomular Quest: Those Who Gnome,https://devpost.com/software/gnomtastic-gnomular-quest-those-who-gnome,False,"We started by asking ourselves: what makes a game so fun you just have to play it “one more time”? We realized it’s not about flashy graphics or complex mechanics, it’s the little things: a quick learning curve, levels that never feel quite the same, the thrill of beating the clock, and a steady climb in challenge. We wanted to pack all of that into a tiny, tunnel-crawling gnome adventure. With some help from simplex noise and a lot of coffee-fueled brainstorming, we built something we hope feels as fun to replay as it was to create.","In this game, you play as a determined little gnome with a big sense of adventure, diving headfirst into a maze of mysterious tunnels. Your job? Dodge traps, outsmart enemies, and scoop up as much treasure as your tiny arms can carry. But don’t get too comfortable—the deeper you go, the trickier things get. The tunnels shift, the challenges grow, and only the craftiest gnomes make it out rich.","We built the game using JavaScript and HTML5 Canvas, but the real magic (and chaos) happened behind the scenes. We challenged ourselves to use simplex noise to generate procedurally crafted tunnel level, so no two runs are ever the same. That meant writing a custom level generator from scratch, tweaking noise parameters until it felt just right. On top of that, we built our own physics engine to handle movement and collisions. It was equal parts math, madness, and debugging marathons, but 100% worth it.","One of the biggest hurdles was building and fine-tuning our custom physics engine. Getting a gnome to move smoothly through tight, twisty tunnels sounds simple until you’re knee-deep in collision logic, edge cases, and velocity bugs that send your poor gnome flying into the void. On top of that, integrating all our custom utilities with procedural generation meant constantly verifying that the randomly created levels were not only playable, but fun. Making sure every level had a clear path (and didn’t trap the player in a gnome-sized dead end) became a puzzle in itself. It was a messy, head-scratching process, but incredibly rewarding when it all came together.","We built an entire physics-driven, procedurally generated tunnel game in just 36 hours. From designing a custom level generator to building our own physics engine from scratch, we pushed ourselves on the technical front. And while the code was coming together, our art team dove into uncharted territory creating pixel art, even though none of us had ever done it before. Between the bouncing gnome sprites and the shifting tunnel tiles, every piece was handcrafted with love (and a sizeable amount of sleep deprivation). It was a whirlwind, but seeing it all come to life was absolutely worth it.","TEAMWORK IS ESSENTIAL. It’s the glue that holds everything together when you're racing against the clock. From the coding team battling through physics bugs to the art team figuring out pixel art for the first time, every piece of the puzzle relied on collaboration, communication, and a whole lot of trust. We quickly learned that no one can do it all alone, and when everyone pitches in with their unique strengths, magic happens.","While that's a wrap onGGQ:TWG, our game development journey neither starts nor ends here! If you enjoyed with our quirky gnome-filled adventure, we invite you to check out our passion project of the past year:Chicken Society, an MMO that’s just as offbeat, fun, and full of surprises. Check out the demo atchickensociety.net, or explore the code behind the chaos on ourGitHub repo.","github, howler, html5, javascript, simplex-noise","https://bwblackbird.github.io/thegnomethree/, https://github.com/bwblackbird/thegnomethree"
HackKU25,Goose Guide,https://devpost.com/software/ar-goose-chase,False,"We are big fans of Niantic's apps and how they leverage technology to amplify in-person experiences. We admire how they blend technology with ways that actively engage their users by leveraging location data, geospatial technology, and AR/VR!",Our project lets you go on a real goose chase! You follow a goose while going on a scavenger hunt 🪿 The locations specified in the hunt can either be a custom user-created course or generated with mapping data!,We learned Blender to create the graphics for this project and then completed our first-ever AR project using Niantic Studio!,"This was our first time building things related to AR/VR, so getting used to how graphics interact with 3D space and our phone cameras was a challenge. Learning how to use Blender was also a bit of a learning curve, and it took a while to remember all the shortcuts and best practices to create 3D models.","We have only done software projects that revolve around things like full-stack development of web/mobile apps, so it was fun to learn how to create an AR hack this weekend! It was also fun to tap into our creative sides a bit with the design of the app and the use of Blender!","We learned about AR, Blender, and geospatial technologies.",We want to expand its uses so that the goose helps users go on more adventures (in safe ways!) with integration of Niantic's VPS Coverage API to integrate real-world map locations.,"blender, niantic",
HackKU25,MyFolio,https://devpost.com/software/myfolio-cqj5lk,False,"MyFolio is inspired by the challenge many professionals face when creating an online portfolio. The traditional process of building a portfolio website requires design skills, coding knowledge, and significant time investment. Our project aims to simplify this process by automating portfolio creation from existing resume data.",MyFolio is a web application that:,,,,,,"convex, nextjs, react, shadcn, tailwind",https://myfolio-orpin.vercel.app/
HackKU25,BloodLens,https://devpost.com/software/bloodlens,False,"🩸 Inspiration
We were tired of looking at our blood test results and feeling like we were trying to decode ancient runes. What’s a “CRP”? Why is my HDL being so passive-aggressive? We thought: what if your blood work could talk back... with AI? And thus, BloodLens was born. A project to turn cold, clinical numbers into hot, spicy, personalized health feedback with the help of a digital doctor who never sleeps (aka Gemini). 🧠 What It Does
BloodLens takes your blood test data, mixes it with some age, height, and weight, then hits it with the full power of Google's Gemini AI to generate: Easy-to-read explanations of your biomarkers Personalized health advice A calculated Phenotypic Age to guess how old your blood thinks you are Surprisingly good vibes for a medical app 🛠 How We Built It
Backend: Flask, with some light Python sorcery Frontend: HTML, CSS, and dark mode gradients that go hard AI: Google Gemini API because we like our responses with a side of intelligence Blood Math™: Phenotypic age formulas and a few too many floats Deployment: Render (and yes, we read the docs. Mostly.) 🧱 Challenges We Ran Into
Gemini models throwing mysterious errors like a moody oracle Styling HTML to look “medical but also cool” without summoning a CSS demon Render cold starts… or as we call them, the 30-second naps Making our app fast, even when the AI thinks slow Turning 14 confusing inputs into a user-friendly UI (we may have cried a little) 🏆 Accomplishments That We’re Proud Of
It works! You can paste blood values and actually get useful, non-scary info Built a fully functional AI-powered health tool from scratch Designed a UI that looks more like Iron Man's J.A.R.V.I.S. than WebMD Actually learned what half the blood markers mean. Maybe. 🧬 What We Learned
Prompt engineering is a legitimate superpower Gemini is smart, but you gotta speak its language Frontend design is... a rabbit hole. We went in. We haven’t come back. Deployment needs patience, caffeine, and occasionally a sacrifice People will definitely use something if it makes them feel healthier and cooler 🔮 What’s Next for BloodLens
Let users save their blood results over time Add visual charts and trend tracking Introduce a gamified “biological age battle mode” Integrate wearables and continuous health data Take over WebMD (respectfully)",,,,,,,"flask, gemini, python, render","https://bloodiq.onrender.com, https://github.com/lntegrals/BloodIQ.git"
HackKU25,Budget Meal Maker,https://devpost.com/software/budget-meal-maker,False,One of our team members was doing community service with the KU Honors Program and realized that she could make a difference in Lawrence by promoting healthy meals on a budget.,This app works by uploading images of coupons to Google Gemini and in return gets several delicious meals for you to make!,"We used Flask as a building block and used HTTP requests to perform actions regarding uploading images and creating the recipes with calls to the Google Gemini API. We then used JavaScript to display the recipes, along with CSS to make our website more pleasing to the eye.","We had issues with HTTP request priority calling the wrong functions, creating a website that would function well on a phone (and styling form inputs), as well as general Gemini API difficulties.",We got this done in less than 24 hours because one of our professors made a super hard lab due this Sunday >:|,"We learned how to call Gemini API from python, how to style with phones in mind, and using form requests in HTML.",We would want to make it publicly accessible and more useful by giving item prices as well.,"css, flask, gemini, html, javascript, python",https://github.com/kucaleb/HackKu_2025
HackKU25,Hack it Up,https://devpost.com/software/hack-it-up,False," Logo with text Logo without text Logo on sunset background Hack It UpThe best way to send problems to your medical team Every time I go to the doctor I have to do this song and dance for my symptoms. ""Whats going on, how bad is it, how long has this been bothering you?"" I can never remember what was wrong with me especially because this appointment was scheduled weeks ago! Not to mention every office or hospital I go do has a different portal I need to use. To remedy this common problem, we created Hack It Up. This app takes in an issue from the patient, how severe it is, and when they noticed it. You may think that this sounds like a basic symptom tracker. Well that's where you're mistaken. What sets Hack It Up apart is that we send a report of your symptoms directly to your provider. Using Google Gemini you can create a report that will automatically be sent to you and your provider. You can also chat directly to your provider using an instant messaging system. We ran into a few issues hosting the site and using the API key without leaking the data. We wound up just recommending that users acquire their own key. First finding a place to host was difficult due to Vercel not liking our imports and API key. We eventually decided to host the site on one of the group member's personal servers due to time crunch and lack of resources. For the API we needed to find a way to have the API key in Github without leaking the data for all the world to see. We eventually used Github secrets to remedy this.
The hosting issue really threw a wrench in our plans. On top of learning how API's work on our feet, I feel if that had gone smoother we would've had a few more of the wanted features. We learned a lot about API's and creating databases. Mainly how to connect the data to the frontend and then to the API. 📽️Watch the demo Health & Well-being (Theme Track), Best Domain Name, Best use of Gemini AI, Patient Safety and Technology Challenge Image Credits:BackgroundLogo was created by Maddi Clark",,,,,,,"css, flask, gemini, html, http, oracle, python, sql, sqlite, uv, websockets","http://hackitup.us, https://github.com/EGHageman/hackitup"
HackKU25,Git Guru,https://devpost.com/software/git-guru,False,"Managing pull requests from the command line can be tedious, especially when juggling code reviews, formatting standards, and commit history. We wanted a CLI tool that simplifies this workflow, integrates AI for better context, and enhances productivity without leaving the terminal. Thus,Git Guruwas born—a fast, smart, and minimal PR assistant.",Git Guruis a CLI interface that streamlines GitHub pull request management. It lets users:,,,,,,"electron, gemini, linter, python, typescript",https://github.com/ChuckGills/Hack_Ku_25
HackKU25,Stravive,https://devpost.com/software/stravive,False,we were inspired to try and make an singular platform to try and help make healthier choices.,,We utilized the power and flexibility of python to access strava's api along with Google's Gemini. Along with this we are using a mysql database and a react front end for. better integration.,in the beginning we had issues trying to get the strava api working in our python script. setting up a local database and connecting it to our python script and front end.,anytime you complete a run with strava it quickly added to our database almost in real time. along with some small ads for possible marketing.,"we learned how to make a local database, and a well integrated React app with API.",,"api, mysql, python, react",https://github.com/Foley-N/HackKU-2025
HackKU25,CineME: Personalized Film Trivia,https://devpost.com/software/cineme-trivia,False,"The project takes user's Letterboxd accounts and finds the intersecting films between users and a given Letterboxd list. Questions are then outputted based on the number of questions that the host wants to create and can be played in a Kahoot type style. The handling of Letterboxd's API is done with the letterboxdpy library to ready user profiles and lists. Movies are checked against a database created for this project on which is hosted on Supabase. If no questions for the given movie exist in the database, Gemini is prompted to generate questions in the categories of Plot, Cast, Crew, Cinematography, and Behind the Scenes. These questions are then stored in the database. The Python backend is connected to the React frontend using FastAPI. The FastAPI backend is hosted on Render with the React frontend being hosted on GitHub pages. This allows for the site to be fully online and independent of a host machine, though computation on Render may take longer. Backend support currently allows for user-inputted questions, but it is not yet connected to the website view. Gemini is often accurate when generating questions but can still make mistakes. This is why a user voting system has been implemented so that questions with enough dislikes will be under admin review, whereas questions that are liked will take many more dislikes to reach that same review point. Users will also be able to vote on user-submitted questions in case of inaccuracies by the community. The website is still in a very early state, so there may be missing parts or it may be unplayable. The testing.py file on the GitHub repo is a better example of what the final idea should be. This was my first time working with any online formats for server hosting, so the largest challenges came with connecting it to the frontend to allow for a fully online experience. That being said, I went into this weekend with no knowledge of Supabase, React, FastAPI, and Render, and have left with being much more comfortable in all of them, so I would be happy to use them again in future projects.",,,,,,,"fastapi, github, python, react, render, supabase","https://aidenap21.github.io/HackKU2025/, https://github.com/aidenap21/HackKU2025"
HackKU25,Recipe Arsenal,https://devpost.com/software/recipe-arsenal,False,Website Interface Logo First Support Ticket of the entire Hackathon! Recipe Arsenal helps you whip up healthy meals using what’s already in your kitchen.,,,,,,,"api, gemini, python, spoonacular","https://gunners-hackku25-7lnismjywidypgyksz577q.streamlit.app, https://github.com/FruitGuesser/gunners-hackku25/tree/main"
HackKU25,JARVIS,https://devpost.com/software/jarvis-5u2lsg,False,"My uncle's recent heart condition crisis was preventable. Ignoring early symptoms led to 30 critical days in ICU - a stark reminder of how symptom neglect costs lives. This personal tragedy, combined with America's surgeon shortage crisis and the communication barriers between patients and doctors, inspired my solution.","What it does
This is a healthcare conversation assistant with three core modules: Patient Interview System: AI-powered conversational interface that collects comprehensive symptom data, medical history, and lifestyle factors.Medical Assessment Engine: Analyzes collected data to generate priority scores (0-100) and classify cases into Critical, Urgent, Standard, or Routine categories.Doctor Dashboard: Presents prioritized patient queue with detailed symptom reports, possible condition predictions, and appointment scheduling tools.","My system architecture consists of:
Frontend: Responsive HTML/CSS/JS using Bootstrap framework with separate patient chat and doctor dashboard interfaces
Backend: Python Flask server handling REST API endpoints with session management
Conversation Manager: Python module for structured medical interviews using OpenAI's ChatGPT API
Data Processing Pipeline: Extracts symptom severity, duration, and characteristics from natural language
Storage Layer: JSON-based patient records and assessment data with filesystem persistence
Authentication System: Role-based access control separating patient and provider views
Prediction Module: Generates possible conditions with confidence levels based on reported symptoms","1) Maintaining conversation context across HTTP requests in stateless web environments
2) Balancing thoroughness of assessment with user engagement
3) Creating an ethical framework for medical predictions without diagnostic claims
4) Optimizing API costs while ensuring comprehensive symptom collection","1) A conversation state management system that handles complex medical interviews
2) A symptom extraction pipeline with duration and severity classification
3) Priority indicators in the doctor dashboard
4) Disease prediction with appropriate medical disclaimers","1) Implementing stateful conversation tracking in Flask using secure session management and JSON serialization
2) Designing extraction algorithms that reliably convert natural language medical descriptions into structured data points
3) Optimizing ChatGPT prompt engineering techniques for more effective medical information collection
4) Building priority scoring algorithms that balance multiple factors including symptom severity, duration, and risk factors
5) Creating secure authentication workflows that protect sensitive patient information while remaining accessible","Our technical roadmap includes: 1) Make app compatible with real time data about patients collected by applications like google fit
2) Development of a React Native mobile application
3) Machine learning model for improved symptom severity classification
4) Expansion of the medical knowledge base through partnership with healthcare institutions
5) Implementation of voice input for accessibility","architecture, bootstrap, chatgpt-api, data, flask, git, html, javascript/html/css, json-data-storage, openai, python, responsive-web-design, restful, session-management",https://github.com/pranavitf/healthcare-app
HackKU25,TrustRipple,https://devpost.com/software/trustripple,False,"I came into my first hackathon wanting to build a basic cybersecurity toolkit web app. But once I realized how quickly I was progressing, I increased my ambitions and threw myself into the deep end of learning some blockchain security! I chose this topic because it seemed interesting to me and I've always wanted to learn more about it. That journey led me to explore the XRP Ledger and its security pitfalls and inspired me to create TrustRipple, a tool that empowers users to protect their wallets while learning along the way.","TrustRipple is a web-based wallet safety scanner for the XRP Ledger. It lets users generate or import XRPL testnet wallets, scan for suspicious activity, take phishing and hygiene quizzes, encrypt their seed phrases, and track wallet health in a secure, educational dashboard.",,This was my first time making a webapp so naturally this project was going to include a lot of trial and error to get it to an end product I was happy with. I am most familiar with python so I had it do most of the heavy lifting on the backend but teaching myself how to integrate JS + HTML/CSS + Python was where I struggled. Thankfully the xrpl-py library was easy to use and I found plenty of online resources to learn more about blockchains and blockchain security.,,,,"css, flask, html, jninja2, python, xrpl-py",https://github.com/Jordin-Myers/hack-ku-2025
HackKU25,Jarvis,https://devpost.com/software/jarvis-hn3i0y,False,Inspired by Jarvis' ability to autonomously carry out actions inIron Man.,"Jarvis is a web extension that takes in a goal in the form of natural language, and will automatically interact with buttons, links, input fields, and more until Jarvis achieved the task for the user.",We used bootstrap and raw HTML and CSS to style our web extension.,"Linking websockets, deploying our back-end flask server to Heroku.","This was the first hackathon for Evan, and all three of us learned an immensely valuable amount of information throughout the process of building Jarvis.","As a team, we had very little experience with web development, network requests, and JavaScript. Throughout completing this project",Jarvis will be released upon the public soon.,"bootstrap, css, flask, heroku, html, javascript, openai, python",https://github.com/jarsnow/HackKU25
HackKU25,The Queens Gamble,https://devpost.com/software/the-queens-gamble,False,"   A battle of strategy and luck — capture your opponent’s piecesonlyif you win in blackjack.It’s checkmate meets 21. We wanted to reimagine the classic game of chess by injecting it with chaos, risk, and Vegas-style tension. The result?The Queen's Gamble— a one-of-a-kind mashup that demands not just smart moves but bold bets. No capture is guaranteed. Every move could turn the tables. The Queen's Gamble is a chess game… with a twist: Our stack:",,,,,,,"pygame, python",https://github.com/AlexDoehring/The-Queens-Gamble
HackKU25,HealthWise,https://devpost.com/software/healthwise-8m7rtf,False,"Access to affordable, secure, and intelligent healthcare is still a challenge for many. We were inspired to create HealthWise as a decentralized healthcare companion that not only connects patients with doctors but also leverages AI and blockchain to make medical assistance more accessible, trustworthy, and proactive. From smart diagnosis to mental health checks — we wanted to build a Web3-powered, AI-integrated solution that puts the patient first.","HealthWise is a full-stack healthcare web application that empowers users to: 🔗 Book appointments with certified doctors based on specialization and availability. 🤖 Chat with an AI medical companion built using Gemini API, capable of answering health-related queries. 🩺 Take diabetes and depression screenings via AI classifiers built into the web app. 🧠 Get follow-up support, such as explaining previous prescriptions or tracking symptoms. 🔐 Securely store health records and appointment data using Hyperledger Fabric, ensuring data privacy and trust.","Using Hyperledger Fabric, Express, React, Tailwindcss, MongoDb, Google gemini api","Implementing Hyperledger Fabric for healthcare data required careful design For some of us, it was our first time working with React. Getting the Gemini LLM to respond accurately in a healthcare context was tricky. We had to experiment a lot with prompt engineering to reduce hallucinations and maintain clarity in answers. Integrating our diabetes and depression classifiers into the web app and connecting them with user input forms was technically challenging.",We could successfully complete the project within time as per our planning.,Gained deep insights into prompt engineering for healthcare LLMs. Learned how to architect blockchain-based applications for sensitive use-cases. Learned to build and deploy hybrid AI + Web3 solutions that can scale and adapt to real-world healthcare needs.,"Integrate real-time wearable health data for continuous monitoring. Expand multilingual AI chatbot support to break accessibility barriers. Add chronic disease prediction modules (e.g., heart/liver/kidney conditions). Onboard real healthcare providers and enable telemedicine integrations.","express.js, gemini, hyperledger, mongodb, react, tailwind",https://github.com/rudro12356/HEALTHWISE-HACKKU25
HackKU25,PillowPath,https://devpost.com/software/pillowpath,False,"As college students with busy schedules, we can get stressed out and have trouble taking care of our everyday mental well being. In addition, with our busy schedules, we often have problems when it comes to being organized and staying on top of our sleep schedule. This is what led us to create PillowPath. This project uses personal journal entries and Google Gemini to determine the best course of action to succeed and feel energized in their everyday life.","PillowPath takes personal journal entries that are inputted by the user. These entries are fed into our AI model Pluma the Pillow, which is powered by Google Gemini. Pluma then analyzes the response from the user on how to confront or embrace their feelings while also giving suggestions to create reminders in the future to help set them up for success. Users can create reminders on the app using our calendar feature and also receive notifications throughout the day on their mobile device. This app provides a robust UI experience to help ease the mind while the user chronicles their daily journal.",We built this website/mobile app using React Native and Expo frameworks so that it could be compatible on the web and on a mobile device. We used TypeScript and JavaScript for the full stack development of our project.,"We had some troubles integrating the Expo environment on our personal websites due to the specified packages that were included that were not compatible with our devices. To work around this, we edited these files to include the proper dependencies so that PillowPath could run on the web as well as iOS.","We are proud of the inclusion of the Google Gemini API as a personal ""life coach"", and we also liked how we configured the UI for this project. The compatibility of PillowPath being able to be viewed on a web browser and a mobile app is also something that we thought was essential to our project, and we are excited that this idea came to fruition through the use of React Native.",Firstly we learned what it's like working as a team to push code together and access the program through GitHub as navigating that is something we are proud of. We are also proud to have learned how to implement Gemini API in our program as well as use React Native and Expo to build an app like this.,"We would like to scale our application by possibly including a database to store all the journal entries, reminders, and possibly more features. We would also like to directly import reminders generated by Google Gemini into the calendar portion without having to add these reminders manually.","gemini, javascript, npm, react-native, typescript",https://github.com/Aiman-Boullaouz/PillowPath
HackKU25,Aptly,https://devpost.com/software/aptly-gqwsv8,False,"Splash Page Pick your top three Priorities Like So! Scroll through a list of curated apartments that fit! Aptly is a web app that simplifies apartment hunting by combining modern matchmaking algorithms with real-time rental data. Inspired by dating apps, Aptly helps users find apartments tailored to their preferences, making the search process efficient and intuitive. The idea for Aptly came from Hitarth’s personal need to find an apartment. The concept of using matchmaking algorithms—commonly seen in dating apps—to match users with apartments felt like a natural fit. The goal was to create a tool that could help anyone find their ideal apartment in any zip code across the country. Aptly is a simple yet effective demo of a rudimentary matching algorithm. Users input: The app then generates a short list of apartments using real-time rental APIs. Users can like or dislike listings, which further refines the recommendations down to the top two or three apartments. Key features include: Aptly uses a modern tech stack: We’re proud of how close we got to delivering an MVP that aligns with our original vision. The app successfully integrates real rental data with a functional recommendation system, providing users with a streamlined apartment hunting experience. Each team member gained valuable insights: The roadmap includes exciting features and expansions: Aptly has the potential to grow into a comprehensive platform that revolutionizes how people search for homes.",,,,,,,"mongodb, next.js, python, tailwindcss",https://github.com/brohudev/hackku25
HackKU25,Marcotics Marcollectibles,https://devpost.com/software/marcotics-marcollectibles,False,"This web app was inspired by one of our team member's frustration while trying to find a specific collectible, along with the success of a similar platform, GasBuddy. Our current web app focuses on reporting both the availability of McDonald’s Happy Meal toys and the prices of common grocery store items across locations in Lawrence, Kansas. Ultimately, our goal is to expand the platform into a robust, community-driven resource for tracking the availability and pricing of essential goods.","Helps users locate specific collectibles available at different McDonald’s locations in Lawrence, KS. It also allows users to compare the prices of grocery items across various stores in the area, making it easier to find what they need at the best value.","We built the web app using TypeScript, React, and Next.js.",Integrating Leaflet with Next.js,,,,"leaflet.js, next.js, nominatim, react, typescript",https://github.com/Elizabeth-327/MarcoticsMarcollectibles
HackKU25,Schedule Builder,https://devpost.com/software/schedule-builder-vk86eo,False,Our inspiration is that the KU schedule builder is currently down because they are switching to a new software for next year. This made us want to try and make a schedule builder of our own.,"Schedule Builder allows users to instantly input class information—like title, location, time, duration, teacher, and day—and visually displays a clean, formatted weekly schedule. It handles different class lengths (50, 75, 110 minutes) and automatically places courses into the correct time slots across the week.","We used Python to build the backend, with two main classes: Schedule and Sorter. The Schedule class organizes the weekly layout and time slots, while Sorter stores each class's specific data. An Executive file handles all user input and schedule printing. We carefully formatted the output to mimic a real-world timetable with column alignment and automatic row handling for multi-slot classes.","One challenge was properly placing courses that spanned multiple time slots based on their duration. It was also tricky to dynamically adjust the visual layout to make everything readable, especially when multiple courses were stacked in the same hour range across different days.",We're proud that our schedule builder can accurately handle courses of varying durations and still maintain a clean and consistent layout. It feels rewarding to have recreated a simplified version of KU's tool that’s flexible and functional from just a terminal interface.,"We learned a lot about time parsing, data formatting, and how to cleanly separate logic using object-oriented programming. We also gained experience working with user input, input validation, and aligning output in a readable table format.","Next, we want to add features like conflict detection, color-coded courses, and even export options to PDF or image formats. We'd also love to build a simple web interface to make it more user-friendly and accessible across devices.",python,https://github.com/CooperSchmid/HackProject.git
HackKU25,Repo Runner,https://devpost.com/software/repo-runner,False,We hated downloading 1 million different dependencies and files just for a GitHub repo to not have the functionality that we were looking for,"Repo Runner will take a link to any GitHub repo. The software will then analyze and download any dependencies needed to run that specific repository. Next, it will run the repo in the browser using a docker image so you can use the software without needing to run it on your local machine.","We used a simple HTML frontend, flask to connect to our python backend, and docker to host the repo that shows up on your screen in the browser",We ran into most of our problems while trying to connect our various backend components together.,Not crying,How to hold our tears in,"Currently, Repo Runner only works well with repositories that have only a few simple dependencies. We'd like to expand its capabilities to be able to handle any repo's dependencies.","css3, docker, flask, html, javascript, python",https://github.com/jaxonavena/HackKU2025
HackKU25,TransparentCare,https://devpost.com/software/transparentcare,False,"Medical bills can often be confusing, stressful, and unexpectedly high. We’ve personally seen family and friends struggle to understand why they were charged so much after healthcare procedures, leaving them feeling overwhelmed and helpless. This inspired us to createTransparentCare, a straightforward app designed to give users clarity and control over their healthcare expenses.",,,,,,,"expo.io, gpt-4, javascript, react-native",https://github.com/ManuelBCabral/KUHACK25
HackKU25,FitTrack,https://devpost.com/software/fittrack-o7le3n,False,"We were inspired by the Health and Wellness prompt and our own experiences dealing with annoying workout apps to track workouts, so we decided to create our own.","By using Firebase to allow for user authentication and databases, this web application allows users to create their own accounts where they can build and do workouts that are then kept track of in a log so they can review their progress.","This was built using Firebase for the user authentication, hosting, and database as well as HTML, CSS, and Javascript.","We attempted to implement an AI chatbot that could recommend workouts and changes to the user, but it only works on the local host. Other challenges included user authentication on Firebase.",We are proud that we even got a project since it is our first Hackathon. We are also proud that everything flows smoothly and is easy to navigate.,"We learned a lot about how to use Firebase, as well as general rules for using HTML and Javascript together that we had never known before, like Global Scope.",We hope to further work with FitTrack to fully implement the AI chatbot.,"css, firebase, html5, javascript","https://worko-1e16b.web.app/home.html, https://github.com/ash417-c/new-hackku2025/tree/main"
HackKU25,nAIvAIgAItor,https://devpost.com/software/naivaigaitor,False, AI used AI to mAIke this prAIjAIct.,,,,,,,ai,"http://theycallmezeal.me/pf/naivaigaitor/, https://github.com/theycallmezeal/theycallmezeal.github.io/tree/main/pf/naivaigaitor"
HackKU25,Rusty Zelda,https://devpost.com/software/rusty-zelda,False,Zelda. Rust.,"Currently, only a section of Dungeon 1 is built. Simple graphics, movement, combat/enemies, and inventory are all implemented.",All code is written in Rust. Sdl2 crate is used for graphics and keyboard input.,"Both group members are new to Rust- one began learning Rust a couple weeks before the hackathon, and the other began on the 1st day of the hackathon. Rust is hard.","Managing the codebase, compiling across both Windows and Linux, and getting a working game loop",We learned a lot about both Rust development and game development in general!,"Future plans- get Rusty Zelda playable on the PS Vita, implement overworld, implement sprites and sound","rust, sdl2",https://github.com/3yassu/Rusty-Zelda
HackKU25,Poker?,https://devpost.com/software/poker-2yr4gf,False,Lovecraftian lore and gambling.,"It is a game of Texas Hold'em, but with an eldritch twist. As the game progresses, the Player and the four NPCs gain Insanity, causing odd events to happen.",Using python with assistance from Google Gemini.,"Trying to get the events to trigger properly, and ultimately making the program in a single file when it should be multiple.",,The capabilities of AI as a programming assistant and helper.,More events and adding RPG style randomized story events.,"google-gemini, python, visual-studio-code",https://github.com/NickHage/HackKU/blob/main/main.py
HackKU25,CalorieCounter,https://devpost.com/software/calorie-counter-0x1b8j,False,"We were inspired by the Health and Well-Being theme for this year's HackKu and decided to try to improve people's well-being with our project. After a lot of brainstorming, we decided that the thing we could make to impact as many people as possible was a calorie counter program, as that is an integral part of one's health. We hoped that by making this process easier, it would allow more people to take control of their health and meet their personal goals, whatever they may be.","This script, at this point in development, tracks caloric intake and caloric burn by allowing the user to input data about the food they eat and the length of time they exercise, culminating in a graphical display of this data so the user can visualize their daily caloric balance, as well as track how this data changes over time.","We built this script in MATLAB because we knew for this project we would mainly be manipulating data, which is something MATLAB is good at.","We had initially wanted write an app that would track GPS data from a phone's GPS sensor and collect caloric burn information from how far the user walked each day, but upon further examination of MATLAB's capabilities, we decided that MATLAB either isn't able to do it in a way that could be programmed in 36 hours or can't do it at all. 
We had also wanted to make the caloric intake calculator operate by allowing the user to scan the barcode of the food item they were eating and pull nutrition information about that item from a barcode database using an API, but this proved to be much more difficult than we had initially thought, forcing us to shelve this feature for later development. 
A common challenge we found ourselves repeatedly facing was trying to design a script that was too ambitious. Our initial brainstorm page contained over ten features we wanted our app to include, and we ended up with a script that performs three of those tasks.","This was our first hackathon and we're proud of ourselves for creating something that we feel is worth submitting, even though it isn't as flashy as we'd hoped it would be. 
We're also proud of creating this script without the help of AI or other help sources - this is something we made on our own, learning everything we needed to know along the way. Tangential to this, we are also proud of ourselves for getting out of our comfort zone in MATLAB and trying to program functions that we've never tried to do before, such as communicating with an API.","We learned that writing an effective and complete script is much more difficult than it seems, but we also learned that there is a lot more to coding than we had thought. Before this competition, none of us knew what an API was or what it stood for, or even the difference between a program and a script. We also went to a couple of the workshops and were really impressed with all the things that we saw that could be done through coding.","CalorieCounter will become a passion project for this team, updating and improving it in our spare time until it has all the functionalities we had originally hoped for. We hope to eventually learn how to get around Android Studio well enough to launch it as an app on the Google Play Store, meeting our initial goal of providing a resource for people to take control of their health and well-being.",matlab,https://github.com/JacksonS501/CalorieCounter
HackKU25,Cyber Office Chronicles,https://devpost.com/software/cyber-office-chronicles,False,"With cybersecurity becoming more relevant in everyone’s daily lives—from email scams to suspicious app permissions—I wanted to create something fun, educational, and accessible. I was inspired to make a game that not only engages casual users who might not know much about cybersecurity, but also gives security professionals a chance to smile (or cringe) at all-too-real scenarios. My goal was to bring awareness to common threats through humor, gameplay, and storytelling—and to do it all in a way that doesn’t feel like homework.","Cyber Office Chroniclesis a collection of distinct, bite-sized cybersecurity-themed games: Each mini-game focuses on a different area of cybersecurity, from phishing and device security to human manipulation and stealth threats. The variety keeps things fresh, replayable, and engaging for different types of players.",,,,,"I’d love to expand the game with more scenarios, a scoring leaderboard, and new mini-games like: I also want to polish the art, add sound effects, and maybe port the game to web or mobile to reach more people. Cybersecurity deserves a bigger spotlight—and if it can be fun along the way, even better.","gdscript, godot, json","https://github.com/manvirk21/Cyber-Office-Chronicles, https://manvirk21.itch.io/cyber-office-chronicles"
HackKU25,Flood Escape,https://devpost.com/software/flood-escape,False,Platformers,,Unity,All of them,Getting our game out there,Game Development,More Game Developmment,"aseprite, bandlab, c#, devops, pixilart.com, unity",https://github.com/SirMixaWatts/Flood_Escape
HackKU25,Bee tamagotchi,https://devpost.com/software/bee-tamagotchi,False,The minecraft movie comes out this weekend,"chills, exists bzzzzes",winGDI and other windows internals and arcade phyiscs,Actually making ms internals work,made microsoft internals work,winGDI32 and other internal windows APIs,"idk make it funnier, more features, etc","box2d, tkinter, wingdi",
HackKU25,Media Oracle,https://devpost.com/software/movie-oracle,False,"Our team was tired of jumping between sites just to get basic info on what to watch next. Normal browsing yields too many ads, too little personality, and no easy way to track it all. So we set out to build something that made the experience quicker, smarter, and more enjoyable.","Instead of digging through cluttered search results or endless ratings, we built something quick, witty, and tailored to what you actually care about. By entering the title of a piece of media into the search bar, many results are returned to you including a personalized remark generated by public opinion gathered by AI. With our website, you can also track what you've watched or plan to watch and what you thought about it.",,All of us were working with at least two tools we had never before used and that put extra difficulty into the development process. After no little amount of work we were able to learn and adapt to the new environment we were working in.,,,,"atlas, bash, discord, docker, flask, gemini, git, github, godaddy, google, google-cloud, html, javascript, json, mongodb, node.js, npm, postman, python, react, zsh","http://mediaoracle.us, https://github.com/HackKU2025/MovieOracle"
HackKU25,PocketZen WebXR,https://devpost.com/software/pocketzen-webxr,False,"SCAN QR TO PLAY! So zen! Project: PocketZen WebXRI created a pocket zen garden where users can collect coins while listening to zen music, offering a relaxing and immersive experience. I wanted to incorporate theNiantic Sponsor Track, creating a WebXR experience that utilizes Niantic Studio’s beta visual editor to transform the world around users. Inspired by the calming, mindful practice of zen gardens, I wanted to create an interactive experience where users could enjoy a relaxing virtual environment while collecting virtual items (coins) and listening to zen music. The goal was to provide a space for people to unwind and be present, much like the real-life practice of arranging stones and sand in a zen garden. PocketZen WebXR is an interactive virtual zen garden where users can explore, collect coins, and enjoy a peaceful atmosphere with zen music. The coins are scattered throughout the scene, and players can collect them as they explore the environment. Once they collect 10 coins, they are presented with a ""You Win!"" message, creating a sense of accomplishment. Additionally, this experience is designed to be immersive by utilizing Niantic Studio's beta visual editor to transform the environment and integrate it into the real world using WebXR. We built PocketZen using8thWall's WebXR platformandNiantic's Studiofor visual editing. The experience was structured to integrate dynamic elements like coin collection and interactive zen music. The visual editor allowed us to enhance the spatial design, turning the user's physical surroundings into part of the zen garden. We also integrated event listeners to track coin collection and display feedback to users, enhancing their interactive experience. One of the biggest challenges wasgetting precise UI positioningfor the score and messages. The UI needed to appear at the bottom center of the screen to be visible and easy to read during the experience. Also, maintaining smooth performance on different devices required optimization of resources, particularly for visual elements and interactive assets in the virtual garden. SCAN IMAGE TO PLAY Let me know if you'd like to refine any section or if you need any additional information!",,,,,,,"8thwall, models, typescript, webxr",https://www.8thwall.com/brettbalquist/myfirstproj
HackKU25,FormulaBot,https://devpost.com/software/formulabot,False,We were inspired by the hit Netflix docuseries: Drive to Survive! The show travels with the teams and their respective members around the world and delivers exciting racing content with a dramatic story.,"This bot will tell any user driver statistics, team statistics, and fun responses for certain user-initiated messages.",This was built with Python and SQLite,"Outputting database info in a neat format. Also, initially getting the bot to work congruently with discord proved to be challenging.",A bot that delivers what we initially set out to build.,We learned more about SQLite and how web scraping works.,"We will further develop the bot to provide statistics from previous racing seasons. We also plan on developing autonomous functions, so that the user does not always have to prompt the bot for a response.","python, sqlite",https://github.com/nfahmad/FormulaBot
HackKU25,MedTrack,https://devpost.com/software/medtrack-76o2rg,False,Many seniors struggle with medication compliance. MedTrack hopes to help seniors take their medication accordingly by creating an easy-to-use schedule with photo verification to help them take the right medications at the right time.,"Users are shown the list of medications they are required to take in a day in a to-do list type of format. When they want to take a medication, they choose the medication and take a picture of the prescription bottle to verify it is the correct one.",We built it using a Node.js backend and React Native Frontend. Our text cognition is done by a Python script that is spawned by the Node.js backend We created a simple database with Supabase for some of the required information.,"Getting the Python environment to work with the Node.js backend was challenging at first.
React Native was fairly new to us, so that took awhile to get set up as well.","Getting the text recognition to work, as well as getting the backend/frontend to communicate mostly without flaw.","How to build an app with React Native, and how text recognition works with Python libraries.","UI Accessibility- MedTrack is designed to be used by people who may have a hard time reading prescriptions, so have more accessibility features(text to speech for instructions/prescription names) would be useful.
Connection with medical staff-Having an interface where pharmacists can easily add, remove, or update prescriptions could also be useful.","easyocr, expo.io, node.js, python, react-native",https://github.com/maelikax/HackKU2025Project.git
HackNC 2024,Unplugged,https://devpost.com/software/unplugged-githlb,True,"With the typical social media user spending an average of 2.4 hours a day—and teenagers nearly doubling that at 4.8 hours—new generations dedicate a significant portion of their time to consuming random content, much of which has little impact on their lives. Many of us have tried to quit social media entirely, only to experienceFOMO,  or thefear of missing out. After deleting our apps, we’re left wondering what our friends, family, and community are up to. This problem can especially be seen in college students. We rely heavily on social media to receive updates and news on local events or clubs and organizations that relate to us. That’s where Unplugged comes in! Unplugged offers a refreshing alternative to purging oneself from mainstream social media by allowing you to stay connected with loved ones and accounts that pertain to your everyday life while filtering all other media.","One of the main reasons social media is so addicting is because the platforms contain algorithms specifically intended on showing you new, irrelevant content to keep you on the app for longer. To tackle this issue, Unplugged consolidates the user's feed from various linked social media platforms (currently Facebook and Instagram) and filters out all mainstream media posts while keeping material from friends, followers, and select community accounts.","We built this app using Swift and SwiftUI for native iOS devices. To combine social media feeds, we needed to have access to the latest feed for each user on each platform. Since Instagram and Facebook do not have APIs to get this data directly, we needed to reverse-engineer these APIs and endpoints using the user's cookies, generated from logging into each social media account for the first time.","APIs:The most challenging piece of this project was accessing and ingesting network requests that were sent to Instagram and Facebook. The necessary APIs used aren't publicly available, so we had to examine the network requests and replicate the appropriate request bodies, which required us to essentially manually generate/mimic the actual browser headers. Parsing through the response data also proved to be a tricky task. Swift:Another challenge we ran into was the learning curve for the framework we used. Only one of our team members initially had experience with mobile development, so there was a great deal of overhead when it came to the rest of the team navigating Swift.",,,"Looking ahead, we plan to expand our social media integrations to include Instagram Stories and TikTok, along with broader multi-platform support. Additionally, for Unplugged to be an all-in-one solution for managing mainstream social media, enabling users to comment, direct message, and post directly through our platform are all crucial features that we would like to implement.",swift,"https://github.com/ajaygandecha/Unplugged, https://www.canva.com/design/DAGVY7n5IGE/Myb_JCs_W-a7nu1g9BhMng/edit?utm_content=DAGVY7n5IGE&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
HackNC 2024,Car Thang,https://devpost.com/software/car-thang,True,"We thought this would be a great project to grind out for a Hackathon, since we were going to do it anyway and it has a real use case for us today. It is also fun to do things we have never tried before.","We have put a custom distribution of NixOS on the Spotify Car Thing, which runs a custom Rust app that emulates the original Car Thing UI (with some tweaks), and allows for seeking, shuffling music, liking songs, playing/pausing, and skipping to next and previous tracks.","Joey worked on actually hacking the Thing by entering the bootloader, booting an in-memory Linux kernel via U-Boot commands, then dissecting the boot chain of trust and constructing his own kernel and customized Linux environment using Wayland. While he was doing that, Ben wrote the Rust app using the Slint GUI crate and the RSpotify crate as a wrapper for the Spotify Web API. The app takes data on the currently playing song from the Spotify API and uses that to update the Slint GUI, which has buttons which give callbacks that we use to provide the features listed above.","Ben hadn't done much Rust programming before, so making a multi-threaded async app felt like running full tilt into a brick wall (the borrow checker). Slint specifically was a challenge because the documentation wassparsein many places, forcing him to trawl GitHub issues, where the usual answer was""Here's a workaround, we haven't implemented that yet."". The biggest challenge was definitely the Rust borrow checker, though.","The Car Thang has a completely custom build of Linux on it, designed specifically for the device (and it runs Doom. All good things run Doom). The Car Thing originally ran a fork of Android, which made getting full Linux on it quite a challenge. ARM embedded devices tend to have very custom boot chains, so getting the full kernel to boot actually requires two steps across two partitions. Debugging problems on the Car Thing was also quite difficult given its general lack of... keyboards.","Ben learned a lot about Rust and async development, as well as the crates Tokio, RSpotify, and Slint. Joey learned a ton about building custom kernels and boot chains for embedded devices, and is very excited to continue working on embedded applications in the future.","Currently the Car Thang only supports the most rudimentary Spotify controls, so we would like to add support for things like Volume control, Queue editing, Playlist/Library searching, and even new features like adding to any playlist instead of just ""Liked Songs."" Adding some on-screen controls to Doom would also be nice. Other than that though, Joey plans to use it in his car every day, and we will create a writeup and some install scripts for the dozens of us that actually own one of these things.","bash, hacking, linux, nix, nixos, rspotify, rust, slint, tokio",https://github.com/BounceU/car_thang
HackNC 2024,fairLends,https://devpost.com/software/new-ycwrgv,True,"Even today, discriminatory lending practices persist, leading to unequal access to financial opportunities and leaving certain communities at a disadvantage to home ownership and economic mobility. By shining a light on these inequities, we aim to build a future where financial access is fair, transparent, and inclusive for all.","fairLends analyzes a bank's lending history for potential discrimination using a logistic regression model. Leveraging GPT-3.5, it then generates a polished, well formatted report that details the results of the analysis and its interpretation, data visualizations, and actionable recommendations on how to improve.","fairLends was built using Python and a suite of its libraries, including scikit-learn, statsmodel, and pandas for preprocessing and data analysis, matplotlib for data visualization, and GPT-3.5 for generating detailed reports. The frontend interface was developed with Python’s Tkinter library, enabling the creation a simple, user-friendly UI.","We used a public dataset provided by the HMDA (Home Mortgage Disclosure Act), which had a lot of issues, including missing entries, as well as many irrelevant and confusing data fields. Because the dataset was so large and contained many unexpected problems, preprocessing the dataset into something useable caused quite the headache. Another major issue we faced was optimizing GPT -3.5's report generation, as it often times did not do what we specified of it. Lots of prompt tweaking was needed, and many credits were spent.","Firstly, we are proud of creating an application that is fully functional, especially since this is the first hackathon for all of our team members. We’re particularly proud of overcoming the challenges of working with a massive and complex dataset, transforming it into actionable insights that address real-world issues in lending practices. Additionally, we are proud of how much we all grew as programmers through this experience.","We learned how to work with a variety of data analysis libraries in Python, and learned how to preprocess a dataset into something useable for analysis.","We hope to partner with banks to gain access to more comprehensive datasets, enriching our analysis and providing more thorough insights into lending practices.","ai, api, gpt-3.5, matplotlib, python, scikit-learn, statsmodels",https://github.com/aerocollin/fairLends.git
HackNC 2024,LocalVoice,https://devpost.com/software/local-elections-matter,True,"We felt that we did not have access to resources that allowed us to vote in local elections in an informed manner. Thus, we determined that producing a website that connects voters with their local candidates and allows voters to reach out easily to them about important social and political topics would be helpful, since voting is such an important aspect of citizenship. Further, voters might not always be able to come up with questions to ask their local candidates, so we wanted to provide sample questions that allow voters to gain a holistic understanding of their candidates' policies.","Our website takes a citizen's address as an input and finds the candidates running for office in their local election. Then, it provides the citizen with basic information about the candidate (such as their party affiliation, website, and photo). Finally, it provides the citizen with the option to generate an email template which asks the candidate about their stances on important social and political issues (from which the voter can select the most important ones to them) such that the voter can vote confidently. If these general issues do not resolve all the questions a voter might have, it provides voters with the option to enter a specific issue they would like to ask about. Questions about the specific issue will be integrated into the email template.","We developed the code in JavaScript, HTML, and CSS. It makes API calls to Google's Civic Information API to determine the relevant candidates based on a voter's address. In order to generate a custom question about a specific issue, it makes an API call to OpenAI's ChatGPT 4-o mini model. The functions of the site are coded in JavaScript, and the UI in HTML and CSS.","One challenge we ran into was that Google's Civic Information API does not have all of the information we want it to have (i.e. it does not have candidate information for the state of North Carolina). For the majority of states, it does have candidate information, so we decided that given the time constraints of HackNC to stick with the API. However, given more time, we might attempt to find a more optimal API which has information on every election throughout the country. Another challenge we ran into was that all of us are new to styling with CSS and HTML for UI design. Coding the functions and API calls already consumed much time, so not knowing how to style well only slowed our project down even more. Finally, we all lacked experience collaborating on a project in GitHub. Learning to push, pull, and commit effectively was initially a challenge, but we quickly got the hang of it and learned a useful skill.","Despite the aforementioned difficulties we encountered, our website is highly functional and accomplishes our primary goal: making voting in local elections more accessible. We are proud of successfully integrating API calls with displaying useful information to voters and ultimately compiling a comprehensive email that can easily be sent to candidates.","We developed collaborative skills including specialization and GitHub-specific collaborative skills. We also developed a basic understanding of styling in CSS. Finally, we became adept at parsing and compiling information from API calls in a focused, useful way.","With more time to work on LocalVoice, we would love to make our UI cleaner, find a more comprehensive API that has candidate information, and fine-tune our prompts for OpenAI's API to make our template email even better.","css, google-civic-information, html, javascript, openai",https://github.com/ZaraTek/LocalVoice
HackNC 2024,Magic Missiles,https://devpost.com/software/magic-missiles,True,"I saw someone online reminiscing about an old game from the early days of the App Store where friends could ""launch missiles"" at each others real-time location, forcing each other to get up and run if they didn't want to lose the game. All that existed of this decades old game was a single screenshot, but the simple idea of forcing my friends to drop their backpacks and sprint across the quad to last another day sounded fun enough to design a whole game around. Finding groupmates was easy as everyone immediately understood the concept and was as excited to bring it to life as I was. As far as I know, the original concept was not specific to college students, but a college campus felt like the perfect place for this game.","Wizards can join a game lobby with their friends, where they'll be given 3 lives and tasked with being the last one standing! Gameplay consists of daily 8 hour ""action phases"" from 10 AM to 6 PM, with the rest of the day left to the ""planning phase"".  In the planning phase, wizards can go into the app and select a target impact zone for their magic missile, as well as position a warding spell to guarantee a safe space somewhere during the day. Once the planning phase is over, wizards can launch their missiles when they think their unsuspecting friends have walked into danger. If you are unlucky enough to be caught in the blast radius, you'll get a notification, a map of the impact zone, and a timer: you have 1 minute to get out of the spell's range or you're toast! Since magic missiles and warding spells affect all wizards equally regardless of who cast them, wizards are incentivized to be strategic and not pick locations that are too obvious or else their spells will be cancelled. The result is an engaging strategic challenge, where wizards try and outmaneuver their friends, predicting their moves and avoiding their devious traps.","We built our frontend in Swift, and our backend with TypeScript express and Prisma, with a Postgresql database. Our database stores information about each game on the server, each wizard in those games, and that wizard's name, life total, and spell targets, and we check client-side if a wizard is in the blast radius when a magic missile is launched, so we do not store anyone's real time location in our database.","While our group had Swift experience, none of us had ever used notifications before, so we had to learn how to send those based on updates from the database. Overall, the hardest challenges we faced were interfacing our database with our frontend. We had separate teams working on each part of the application, and even though we were communicating well throughout the experience, our main backend developer had no Swift experience, and most of our frontend developers didn't know much about backend dev, so it took extra effort to make everything work together smoothly.","We managed to conquer Apple's convoluted push notification system, and everything worked perfectly the first time we tested it (a cause for great celebration). Our workflow avoided the worst pitfalls of rushed hackathon collaboration, and we made it to the end with (relatively) few merge conflicts. We had a ton of fun!",,,"postgresql, postman, prisma, swift, typescript",https://github.com/rydklein/missile-app
HackNC 2024,Give Me Some Space!,https://devpost.com/software/give-me-some-space,True,"Our team was inspired by the wonders of the cosmos and a desire to make astronomy accessible to everyone. Recognizing that many are fascinated by space yet often feel distant from understanding it, we wanted to create an immersive, educational tool that simplifies the complex structure of our solar system. By presenting a 3D model of the planets, orbits, and nearby stars, we aim to spark curiosity and foster a deeper appreciation for astronomy.",,,,,,,"css, generativeai, html, javascript, openai, webgl",https://github.com/64nilneb/SolarSystemHackNC2024.git
HackNC 2024,EchoAlert,https://devpost.com/software/echoalert-vo8tgy,True,"The idea for Echo Alert was born from the challenges our community faced during Hurricane Helene. When the hurricane hit, many were left stranded without transportation and with limited or no internet access. Vital information like road closures, shelter locations, and available resources became difficult to share and access, leaving people feeling isolated and unsupported. Echo Alert aims to solve this problem by enabling Bluetooth-based communication that doesn’t rely on WiFi or cellular service. With Echo Alert, users can receive critical updates, mark hazards, and find resources on a live map, ensuring that even in the midst of a disaster, people stay informed and connected to the information they need to navigate safely","EcoAlert is a Bluetooth-based emergency app for disaster scenarios, enabling users to share location-based alerts like ""Road closed"" or ""Medical help needed"" offline. It includes an SOS feature to broadcast distress signals within range. With categorized messages and a map showing Bluetooth coverage, EcoAlert keeps users informed of nearby hazards and resources.","EcoAlert was built using SwiftUI for a clean, intuitive interface and SwiftData for offline message storage, ensuring critical messages are always accessible. The app’s core communication relies on CoreBluetooth, allowing devices within range to connect even without cellular or Wi-Fi networks, making it ideal for disaster-prone environments. We leveraged MapKit to provide a map interface, displaying user location, nearby message drop points, and a Bluetooth range indicator to help users understand the reach of their messages. EcoAlert also includes a Proximity SOS feature, which uses CoreBluetooth to broadcast urgent alerts to nearby devices, including location and distress details. The app can operate offline via Bluetooth and online to pull a history of messages from a public API endpoint, ensuring flexibility and reliability in diverse connectivity scenarios. 
The backend for the online storage, was made using Terraform scripts. These scripts created the AWS RDS instance of a PostregresSQL database and ensured that our database was set up properly. We also have a public REST API built using FastAPI, Python, and SQLAlchemy as our ORM, that is hosted on an EC2 server that was also created through terraform scripts that interacts with out public database to create messages, update message statuses, and delete messages. We also created security measures, like a VPC with attached subnets and attached security groups, that was also configured through Terraform and through use of the AWS Management Console.",,"We’re really proud of the technical achievements we accomplished with EcoAlert. Combining these technologies in such a short time was challenging, but we’re proud of the final product and how each component enhances EcoAlert’s functionality,","Implementing SwiftData for local storage showed us the value of data persistence, especially in situations with unreliable connectivity. Using MapKit helped us learn Apple’s tools for visualizing locations and Bluetooth range, which will be useful for future location-based projects. Setting up AWS with Terraform gave us hands-on experience with cloud infrastructure, teaching us about scalability, reliability, and backend structuring.
On the teamwork side, we learned to handle branch merging conflicts and saw just how important clear communication is for complex projects! Going through the entire development cycle—from brainstorming and problem-solving to integrating and debugging—showed us the full process behind creating a functional app.",,"amazon-rds-relational-database-service, amazon-web-services, corebluetooth, ec2, mapkit, postgresql, python, sqlalchemy, swift, swiftdata, swiftui, terraform","https://github.com/skanta04/HackNC2024/tree/main, https://www.figma.com/design/s9mydq5NJO0ZWjGJTTKwvB/Untitled?node-id=40-57&node-type=frame&t=QhE9cPMctblfsbzr-0"
HackNC 2024,FLUNC - Financial Literacy at UNC,https://devpost.com/software/flunc-financial-literacy-at-unc,True,"Financial literacy is essential for building a good life in our college career but it’s often overlooked in university and high school curriculum. This leads to many students facing challenges in managing their student loans, budgeting each month, and planning for their financial future. Therefore we created FLUNC ! - A financial literacy webpage that helps students find the right resources and tools to navigate their finances confidently here at UNC. Our goal is to demystify personal finance and make financial skills accessible to everyone!","FLUNC provides an all-in-one platform for UNC students to gain financial literacy! The platform has main tools like a Student Loan Calculator, chatbot advisor, resources page tailored to UNC students and a cheaper alternative product finder. Each feature is designed to meet the specific needs of college students, offering practical resources and personalized advice to help them make informed financial decisions. UNC already has some great resources, such as a Net Price Calculator and a Loan Fee Calculator that help students determine the size and type of loan that is right for them. However, there was not a tool present that helped students set a plan for how they would pay these loans off (which is the first step to financial health in the future). This is where FLUNC comes in.","We developed FLUNC using HTML, CSS, and JavaScript for the front end, alongside Python and Flask for the backend functionality. We integrated a main Student loan calculator and a skilled financial chatbot using API calls to Open AI. We also pulled in resources from reliable financial literacy sites and tools to ensure students have access to accurate information.","One of our biggest challenges was making the platform intuitive and engaging. Financial topics can be dense and overwhelming, so we needed to ensure our design was user-friendly and visually appealing. Additionally, implementing the calculators accurately and testing them for various inputs to ensure precision was time-consuming.","We’re proud to have created a comprehensive tool that fills a critical financial literacy gap for UNC students. From the functional calculators to the interactive chatbot, each feature contributes to our mission of enhancing financial literacy. We’re also proud of the platform's clean, accessible design, which makes it easy for any student, regardless of their background in finance, to use and benefit from.",Building FLUNC taught us a lot about the technical aspects of creating financial tools and the importance of user-centric design in educational platforms. We also gained insights into the financial challenges students face and how crucial accessible financial literacy is for young adults.,"We aim to expand FLUNC’s features, incorporating more personalized content, such as tailored financial advice based on user inputs. Additionally, we hope to collaborate with financial experts at UNC to offer virtual workshops and more interactive tools. We also plan to gather user feedback to improve and adapt FLUNC to meet students' evolving financial needs.","css, flask, html, javascript, python",https://github.com/Ethanxu1/HackNC2024
HackNC 2024,Aurora,https://devpost.com/software/finance-nuk9id,True,"Financial literacy is a pervasive issue in the United States. Studies show that the average American loses over $1500 a year due to financial illiteracy, and nearly 50% do not have retirement savings. These gaps in understanding lead to poor financial decisions, high debt levels, and ultimately poor economic mobility. Furthermore, these issues disproportionately affect low-income communities and other minority groups. Traditional methods of learning personal finance, like watching online videos, attending classes, or reading wiki-sites like Investopedia, are helpful but often lack the personalized guidance that many people need to truly understand and apply financial concepts to their unique situations. Additionally, limited access to resources, language barriers, and complicated interfaces can feel inaccessible, overwhelming, or simply out of reach. This is where Aurora AI comes in. Our vision was to create a tool that doesn’t just provide information but feels like a trusted companion in the financial journey. By using hologram technology and LLM processing, we’ve built an interactive experience that guides users step-by-step, answering questions in real time, and adapting to their language preferences. Aurora AI aims to bridge the financial literacy gap by making complex information easy to understand and accessible anywhere at any time. We’re especially passionate about reaching communities that have been underserved by the financial system—helping people gain control over their finances, avoid costly mistakes, and build a foundation for financial success.","Aurora AI is a tool built to provide equitable financial services across social economic statuses. Designed with an intuitive, user-friendly interface, Aurora AI empowers users to access critical financial insights quickly and effortlessly, regardless of their financial knowledge or background. Our innovative hologram technology creates an immersive, human-like interaction that feels as though you’re engaging with a financial advisor, not just a chatbot. Users can ask questions about budgeting, savings, investments, loans, or any other financial topic, and Aurora AI responds with clear, relevant, and actionable information tailored to their personalized needs. We plan on expanding our hologram technology to support all languages. Aurora AI can detect the language it’s being spoken to in and respond accordingly, making the experience feel even more personalized and accessible. By recognizing and adapting to each user’s language, Aurora ensures that everyone receives guidance in a way that feels natural and intuitive. This feature is especially valuable for reaching people from diverse backgrounds, enabling them to connect with financial resources in their native language without any barriers. By breaking down language barriers, we aim to make financial knowledge and tools accessible to everyone, supporting communities that have historically been underserved due to linguistic limitations.","Creating a solution that is personalized to cultural preferences, speaks any language, and is deeply knowledgeable required a robust and dynamic question and answer service. We created this by building an agent that uses chain of thought (CoT) prompting and retrieval augmented generation (RAG). Our RAG system is powered by LangChain and LanceDB, an open source vector database, which we populated with content scraped from Investopedia and Capital One. We then used FastAPI to serve the generation endpoint to our frontend. We used Docker to containerize our application and allow for any one to use our framework (pmoharana/rag-app). Terraform was used to generate the AWS resources required to run our API such as the underlying virutal machines, load balances, and container service. Our Infrastructure as Code implementation allows to deploy, update, and destroy our infrastructure as needed. The hologram interface was designed by building a basic cardboard frame with a cutout at the top for a screen, like a phone or tablet. Inside, we placed a clear piece of glass at a 45-degree angle. This simple setup allows the screen’s image to reflect off the glass, creating the 3D floating effect.","We ran into a few challenges along the way. The first big hurdle was getting our multi-language feature to respond at an appropriate language level for the user. During testing, we noticed that our model would often use language that was more complex than what the average speaker might use, which wasn’t ideal. To tackle this, we decided to first convert everything into romanized English and then translate it back into the target language. This approach helped us keep the responses more natural and at a level that would be easy for users to understand.","We’re incredibly proud of our multi-language feature, which can accurately converse in various languages. Testing our model in languages we already know beyond English allowed us to gauge its accuracy firsthand, confirming that it communicates effectively and naturally across linguistic barriers. Seeing it respond naturally in multiple languages is such a rewarding experience and gets us even more excited about making financial knowledge accessible for everyone, no matter where they’re from or what language they speak.","Throughout this project, we learned that it is much more difficult to build a multi-lingual assistant and even harder to create a hologram that we can talk to. Finding a solution that allows us to help different groups of people and create a product that would create a change in the world was difficult. We got experience creating a physical MVP, a RAG agent, deployment of a product into a cloud environment, and tying everything together in a clean frontend.","In the future we aim to having different models that will be more personalized for each community. Avatars that reflect users’ own communities—not just in appearance, but in voice, accent, and even local dialects. This level of customization will make interactions feel more relatable, authentic, and inclusive, which will help users feel understood and connected to the information they receive. To achieve this, we plan to leverage local language models (LLMs) to fine-tune responses based on region-specific data, making our guidance more relevant and accurate. By aligning with cultural and linguistic nuances, these models will ensure that financial advice resonates personally with each user.","amazon-web-services, cloud, fastapi, javascript, python, terraform","https://hub.docker.com/repository/docker/pmoharana/rag-app/general, https://github.com/aniruddhdoki/hacknc2024"
HackNC 2024,Pragma,https://devpost.com/software/pragma-hx4nk6,True,"In today’s data collection standards, research organizations frequently rely on manual data collection methods, often hiring multiple data collectors to access reliable and scalable data. This approach is both financially burdensome and time-consuming, significantly slowing down the research process. There have been attempts at automating this process by large language models, however the outputs that these models provide are not very reliable. Trying to solve this problem, we were able to create a maximally accurate automated data collector by grounding the outputs of large language models to the factual information scrapable from the internet. We have used Retrieval Augmented Generation in order to ground the data we can scrape from the web and create a similarity score between the data points that come as inputs and the organizations from which we try to collect data. This tool enables: Our solution emerged from recognizing the urgent need for a pragmatic tool that could bridge the gap between manual reliability and automated efficiency, particularly in data-intensive fields like finance and education.","Pragma automates the data collection process by gathering accurate and relevant information from verified online sources. Users can log into their accounts, enter organizations or topics they wish to research, and specify the questions they want answered. Pragma then compiles these inputs into structured search queries, retrieves reliable content from the web, processes it, and presents the information in a user-friendly interface. The platform also stores data for users, ensuring continuity and ease of access in future sessions. By providing data and insights in a convenient manner, Pragma helps communities leverage a data-driven approach to better outcomes in areas such as public health, urban planning, and resource management. In times of crisis, Pragma is capable of quickly gathering critical information, coordinating resources, and providing support to ensure that community is well-supported.","Pragma’s frontend is built on React and Tailwind while backend utilizes a Node.js/Express server. Our FastAPI applications handle the entire data retrieval and structuring pipeline: Apart from the code, we have utilized tools like Figma, Terraform, or certain Databricks services to ensure the functionality of our UX and DevOps.","One of the key challenges was minimizing hallucinations that can arise with LLMs, especially considering the large amounts of data they are pre-trained on. Additionally, managing how we wanted to structure the web-scraped content required extensive testing and validation. Implementing a reliable vector database and efficiently querying it to support real-time data requests was also technically challenging, but Pinecone helped us a ton on that end. Similarly, ensuring performance across various API integrations while building and deploying our own API app was a challenge we had to spend time towards learning to tackle.","We’re proud to have created an automated data collection platform through a fully working RAG process from contextual web scraping and embeddings to retrieval and completions. Our successful integration of multiple APIs, including but not limited to Google Search API and OpenAI's Vector Embedding and Completion API, coupled with our robust data processing pipeline makes us proud.","We gained valuable insights into how language can be represented as a set of relations of different vectors. We learnred techniques such as embedding and querying, web scraping, and managing data storage for quick access. Additionally, we deepened our understanding of APIs like Google Search, Pinecone, and OpenAI’s suite, as well as the backend workflow required to manage large volumes of structured data.","Our next steps include refining our data validation processes by including an option to embed a human-in-the-loop review system for further accuracy. Right now, the model is prompt engineered to provide accurate and reliable information for education and finance, we aim to expand into more fields like healthcare, law, and other data-intensive industries. In addition to that, we want to incorporate the option to download multiple types file extensions beyond CSV, like JSON, plain text, excel, and more.","beautiful-soup, databricks, express.js, fastapi, langchain, mongodb, nltk, node.js, openai, pandas, pinecone, pyspark, python, react, tailwind","https://github.com/alpnix/Pragma, http://getpragma.tech"
HackNC 2024,AI-powered Investment Portfolio Creator,https://devpost.com/software/ai-powered-investment-portfolio-creator,True,"We know investing can be really daunting for new investors, so we wanted to make a quick and easy way for beginner investors to start by giving them a personalized portfolio.",The app generates a tailored portfolio based off of user responses to the given prompts.,We built it using react for the front end and flask for the back end of the web app. We also used Yahoo finance for real time and historical data from the stock market.,"Connecting front and back end, deploying the web app","Integrated an LLM,",Having a clear initial scope for a project helped save time. Writing tests. Learned to find resources online.,,"api-ninjas, gemini-ai, numpy, pandas, python, react, tailwind, typescript, yahoo-finance",https://github.com/ColeWhaley1/hackNC24
HackNC 2024,UnityConnect,https://devpost.com/software/unityconnect,True,"The inspiration for UnityConnect came from hearing about the significant struggles that refugees and immigrants face as they attempt to adapt to life in a new country. We were deeply affected by stories of how language barriers, limited resource access, and cultural unfamiliarity can hinder one’s ability to thrive and feel safe. The more we researched, the clearer it became that language proficiency isn't just a convenience—it’s a determinant of health and well-being. With UnityConnect, we set out to create a comprehensive solution that empowers individuals to overcome these barriers and feel welcomed in their new communities.","UnityConnect is a digital platform designed to support refugees and immigrants as they integrate into their new surroundings. The app provides: Real-Time Translation for Conversations: Seamless communication with automatic in-app translation, enabling cross-language conversations. Resource Directory with Maps: A guide to essential services like healthcare centers and shelters, complete with location details and descriptions. Cultural and Community Events: A curated list of local events that foster cultural exchange and social connections. Support Chat with Volunteer Mentors: A network of mentors who offer guidance on navigating daily life, from transportation to job opportunities. Emergency and Legal Support Information: Quick access to contacts for crisis and legal aid, ensuring users have the information they need in critical situations. Language Exchange Pairing: A feature that connects users interested in learning each other’s languages, encouraging mutual language practice and cultural understanding. Transportation Resources: Information on local transit systems, including schedules and route guidance. AI Chat Assistant: A smart, multilingual assistant that helps users navigate the app and access relevant resources.","Our development process involved leveraging various technologies: Real-Time Translation: We integrated translation APIs to enable Mapping and Resource Directory: Using geolocation services and mapping tools, we created an intuitive directory for essential resources. Events and Language Exchange: We designed a database-driven system to handle events and user matching for language practice. Volunteer Mentor Network: Collaborations with local organizations enabled us to establish a mentor support feature, integrated into our messaging framework. AI Chat Assistant: We built our AI Assistant with natural language processing capabilities to offer multilingual support, ensuring accessibility for users with varying language needs.","Developing UnityConnect posed several challenges. One major hurdle was achieving accurate and culturally aware translations for our real-time communication feature. Another challenge was ensuring that our resource directory remained up-to-date and comprehensive, which required extensive partnerships with local service providers. Designing an AI Chat Assistant capable of understanding and responding in multiple languages, while maintaining accuracy and cultural sensitivity, also proved difficult. Finally, balancing the platform’s complexity with a user-friendly design was an ongoing challenge that required careful planning and feedback.","We’re incredibly proud of creating a platform that addresses real-world needs and has the potential to make a meaningful impact. Successfully integrating real-time translation and launching an AI-powered multilingual assistant were significant technical achievements. Additionally, building a user-centric interface that facilitates access to critical resources and community support is a major accomplishment. We’re also proud of our collaboration with local organizations to ensure our resource directory and mentor network are robust and reliable.",This project has been a tremendous learning experience for our team. We gained insights into the importance of cultural competence in design and technology. Working with translation and language support technologies deepened our understanding of the complexities of multilingual communication. We also learned the value of partnerships in building a resource-rich platform and the importance of iterative user testing to refine our design.,"Looking forward, we plan to expand UnityConnect’s features. Future updates include: Resource Verification: Partnering with more organizations to verify and expand our directory. Community Feedback Mechanism: Enabling users to review resources and services to improve reliability. Offline Access: Developing offline capabilities for accessing crucial information without internet connectivity. Scalability: Expanding to support more regions and languages as our user base grows.","apenai-realtime-api, chatgpt, database-management, firebase, firestone, ios, javascript, mapkit, node.js, open-ai-api, openai, railway, swift, swiftui, xcode","https://github.com/Ialzouby/UnityyConnect.git, https://github.com/Ialzouby/UnityConnector.git, https://unityyconnect.up.railway.app/"
HackNC 2024,Smart Kitchen,https://devpost.com/software/smart-refridge,True,"This project was inspired by the common problem of managing food inventory effectively at home. Many people, including us, often struggle with remembering what is stored in the fridge, leading to food waste due to spoilage or the inconvenience of last-minute grocery runs when items run out unexpectedly. We wanted to create an intelligent, user-friendly solution that minimizes waste, helps users make the most of their groceries, and streamlines meal preparation.","Our AI-powered app, Smart Kitchen, revolutionizes fridge management by offering comprehensive inventory tracking and personalized meal planning. The app scans and categorizes items using photo-based receipt recognition and voice input, making it easy to upload new purchases. Users receive proactive alerts for items approaching their expiration dates and low-stock notifications to restock essentials. The app uses the stored inventory and individual dietary preferences to generate balanced, healthy meal plans for breakfast, lunch, and dinner, with customization options to accommodate various dietary needs and cuisines. Additionally, Smart Kitchen features a personalized dashboard that tracks weekly nutritional intake, helping users ensure a balanced diet while providing data to refine future meal recommendations. The dashboard can offer insights into nutrient consumption trends, caloric intake, and even suggest adjustments for optimal health.","Smart Kitchen was built using the Flask framework for backend development, while the frontend leverages HTML, CSS, and JavaScript for a responsive user interface. Integration with OpenAI’s API, specifically the gpt-4o-mini model, is a cornerstone of the project, enhancing functionality across multiple features. The application processes user data through machine learning algorithms to analyze inventory patterns, optimize recipe suggestions, and generate personalized insights. We designed the app's data architecture around JSON structures that represent user inventory and preferences, ensuring efficient data retrieval and interaction. Real-time updates and streamlined data flow were prioritized to keep the app highly responsive and user-centric.","We encountered several challenges during development, particularly in defining the scope of data needed for the JSON files and integrating them efficiently into the app's core functions. This required multiple revisions and rigorous testing to find the optimal structure. Additionally, we spent considerable time refining the visual and interactive components to create an intuitive user experience. Ensuring smooth integration with the OpenAI API for real-time recipe generation and handling complex user inputs posed its own set of technical hurdles.","One of our proudest achievements is creating a robust solution to a real-life problem that enhances daily kitchen management. We developed a feature-rich application that seamlessly integrates AI to provide intelligent food management and meal planning. Despite ongoing development, the current version is fully functional, capable of tracking items, alerting users, and generating meal plans effectively. We’re especially proud of the personalized nutrition dashboard, which adds an extra layer of depth by helping users track their diet and optimize their meal choices.","Throughout this project, we learned valuable lessons in API integration, data structure design, and user-centric development. We gained a deeper understanding of working with machine learning models for natural language processing and how to build seamless interactions between the backend and frontend. Managing project scope and aligning various technical components to work in harmony was another significant learning point.","Looking forward, we plan to develop an iOS application to make Smart Kitchen even more accessible and user-friendly. Our vision also includes connecting the app to smart refrigerators, allowing the system to automatically update inventory as items are added or removed. This integration will enhance the app’s capabilities, offering users a fully automated, hands-off experience. Additional features like enhanced meal suggestions based on dietary trends, AI-driven shopping lists, and deeper analysis of nutritional intake are also on our roadmap.","css, flask, gpt4, html, javascript, json, openai",https://github.com/Lydiiiiia27/HACKNC_food
HackNC 2024,Carolina Radio,https://devpost.com/software/carolina-radio,True,"The radio has always been a great way to be exposed to new music, especially with student radio stations such as WXYC. Carolina Radio takes this to the next level, serving as a platform for jam sessions wherever you are!",Carolina Radio takes in song submissions from across campus and creates a queue that anyone can listen in on. Our song selection algorithm was carefully tuned to ensure a balanced campus-wide listening experience.,"We built the frontend using React, hosting it on GitHub Pages for easy global distribution. The backend was built using Python and FastAPI combined with a NoSQL database using MongoDB Atlas. All of the management of user sessions and the connections with the Spotify API for playing songs is managed entirely by the backend, maintaining user privacy and adhering to industry standards for cross-application integrations.","The Spotify API was challenging to manage, as we had to manually control the user's queue and track our own record of songs that had been requested. In addition, we faced many difficulties with managing the song queue and properly syncing it across all connected users. Finally, the Spotify documentation was spotty in some points, which led us to devise our own solutions and unique debugging workarounds as we hunted down a path to our MVP.","A big accomplishment we are proud of is the management of our own authorization session system, managing the OAuth login using Spotify from end to end. We made an effort to follow industry best practices and maintain a sleek, professional workflow throughout our 24 hours of hacking in order to ensure the excellence of our app and the quality of our codebase.","We learned a lot more about the full stack web development process, and each used some new technologies that we hadn't used before. We also learned more about the fundamentals of user authentication and session handling.","One big goal we have is to formally lock it down to Tar Heels, as well as to expand variations to other campuses and communities, allowing others to experience the shared music queue as well. We're inspired by platforms like YikYak, which leverage close-knit, geographically-defined communities to build a stronger sense of cohesion. Additionally, we would like to provide a spin-off feature that is designed for managing a queue for a party, using the same technology with a single player as an alternative to Spotify Jams.","cloudflare, fastapi, mongodb, mui, oci, python, react, spotify",https://github.com/nolanwelch/hacknc2024
HackNC 2024,SpotME.tech,https://devpost.com/software/tba-k1bvjx,True,"When brainstorming ideas for HackNC 2024, we tried to come up with the most unique and impactful projects. Bouncing between ideas, we landed on a money transfer app, but we struggling to find ways this app could be unique among many other similar money management apps. This lead to us thinking of unique methods of signing in, with facial recognition standing out among these. With the limited time of the hackathon, complexity of the project, and our lack of experience with image recognition software, we decided to develop a standalone app incorporating facial recognition. And with that, SpotME was born.","Have you ever come home from a party only to realize you forgot to get that person's phone number? Trying to figure out who is in those pictures you took? Well, we have the solution! Spot: Matching Engine is a powerful tool that allows you to upload a picture of yourself to a database, along with any information you would like to share, ranging from social, to contact information. With the tool, all information is completly optional to opt into providing. By uploading your information, you allow people to contact you after having met you. Additioanally, you can use it to talk to people you haven't gotten the contact information of yet. The public access database is a great solution to never losing someone that you met-- as long as your have a photo! Want to see for yourself? Check out spotme.tech! Note that due to restrictions of our fly.io backend deployment, this version is slower and more unreliable than a development build (instructions for this in our GitHub repository README).","SpotME was built using Node.js with JavaScript in a full-stack application. The client side of the app is built with React.js, which allowed us to have better control over reactive state management. We also used tailwindcss to help organize css and framer-motion to help make smooth animations for a better User Experience. On the server side, we used a Node.js server with express.js to serve an API which helps to interface with the app's persistent storage for user data. We used Sqlite for most data storage with SQL queries and JSON storage for more complex detection files. Our facial recognition software was built with the help of the face-api.js library, built entirely server side. Finally, we were able to utilize the domain name provided by Free.Tech & MLH to host our application on a custom domain (spotme.tech)! At some points in the project we did use AI to supplement our coding. ChatGPT & Claude were used to help us learn unfamiliar concepts quicker and debug through tough obstacles. With the exception of the animation on our Spot avatar (to help quickly create a cleaner svg), all code was written during this hackathon ourselves.","One of the biggest challenges we ran into was the deployment process. We tried to set this up early on and had issues with connecting all the pieces of our application together. Another large challenge that we were faced with was learning how to use a facial recognition library within the span of this hackathon. There were many errors that we made when attempting this, such as management of asynchronous functions in JavaScript.",Perhaps the largest challenge was reducing the time to search for other people in the application. Our initial algorithm consisted of inefficiently running facial detection algorithms for each image it came across when trying to make a match. We were able to significantly reduce this time by caching files on our server when each new image is uploaded. Another big accomplishment of this project was the amount of polish we were able to put in while still focusing on making sure core features worked well. The UI of our application turned out well and works fluidly with the server-side of our application.,"As mentioned earlier, this project reinforced the idea of getting development done before worrying about deployment. While we were able to get deployment working in the end (with some drawbacks on performance & reliability), we learned to know when to focus on the main parts of the app early on. In terms of technical skills, this project helped us both learn a lot more about tools in the many frameworks and libraries we used, listed below. In particular, we learned a lot about facial recognition, how to build a backend server with an API and database, a better understanding of the syntax of tailwindcss.","If we are to continue development on this application, there are still many things we would add. Some of these include a user login feature with better user verification, faster image recognition algorithms, and improving our deployed production product.","express.js, face-api.js, fly.io, framer-motion, javascript, node.js, react, sqlite, tailwindcss, vercel","https://www.spotme.tech/, https://github.com/cfields7/SpotME"
HackNC 2024,Cat-egorize,https://devpost.com/software/cat-egorize,True,"Many people have had their cats simply disappear from their homes due to how independent and cunning they can be. This project was made to help others who may be or have been, in similar situations where their furry friends have gone missing. It is also meant to be an entertaining way to find local cats in your area, stray or otherwise.","Users would be able to login and add their cats, making it easier for people to identify cats that belong to them. They would also be able to upload photos of local cat sightings, which would then use image classification to identify the breed, coat pattern, and age of the cat, and then use a vector similarity search to identify a registered cat that looked as close to the cat in the photo as possible. They'd also be able to see cats in their community and where they've been previously spotted by others.","Our initial goal was to simplify the code needed to communicate between the interface and our backend, so we deployed a self-hosted instance of Supabase on a virtual private server using docker and terraform. We deployed additional containers for weaviate, an open-source vector database that stores both objects and vectors, for our image similarity search and a custom Python backend made using FastAPI that utilized the JWT from supabase for authorization and served as the in-between for the front end and our AI-powered tools. Our front end was written entirely using SwiftUI for iOS devices, with the plan originally being to connect the two once both were ready for integration, but that proved to be much more difficult than anticipated.","We had to construct our Terraform file to replicate an existing highly complicated stack which took far longer than expected. Still, we wanted to be able to use our own instance of the application since it was open-source and played a large role in our application since it handled image storage, authentication, and the storage of other data. When finally integrating parts of our application, we ran into a slew of problems. There was far more difficulty involved in integrating superbase in Swift than initially anticipated, exacerbated by Swift's very strict typing. This made us unable to complete our project to our satisfaction in the time provided.","This was the first hackathon of, and the first fully interactive interface created by, one of our members. So we're very proud of what we accomplished in such a short time. We knew very well that it would be a daunting task that may not be completed, but it was a challenge we simply couldn't turn down. We're also very proud of the lengths we went to for the actual deployment of our project. We gained a lot of experience working on an actual server for deployment, having to understand the importance of all of the application's components on a deeper level, especially networking and environment isolation, instead of abstracting or concealing the inner workings like using the cloud does. We even secured our API and our admin portal behind an HTTPS reverse proxy!","We learned... a lot. A lot of time was spent looking at various options for structuring a user's mobile experience, especially in the hierarchy between views and what is most convenient for a user vs. what is most convenient for a developer. We also learned that an insane amount goes into securing communications between devices across platforms for what may seem like a relatively simple task.","Completion. Though we weren't able to fully realize our dream for the application, we are determined to see it complete, no matter what it takes to do so. This will largely be an effort to flesh out full-scale communication between our backend and frontend.","caddy, docker, openai, python, supabase, swift, terraform, weaviate",https://github.com/PR0C355/Cat-egorize
HackNC 2024,ClearView,https://devpost.com/software/clearview,False,"Did you know that over 2.2 billion people around the world face visual impairments that impact their ability to use the web? Traditional websites are often challenging to navigate for users with visual impairments, making it difficult for billions to access the information and resources they need. ClearView is here to change that. By recognizing the unique needs of visually impaired users, ClearView tailors a browsing experience that's both accessible and enjoyable. With ClearView, we’re transforming the web into a place where everyone can feel included, empowered, and connected.","ClearView is a powerful Chrome extension designed to make the web accessible and comfortable for everyone. We know that each user’s needs are unique, so ClearView offers customized modes for a range of visual and cognitive challenges, including With ClearView, users can easily adjust their browsing experience to suit their needs by making online content clearer, more readable, and less straining.","Accessibility and inclusivity are at the very core of ClearView. We started by researching the special needs of users who had different visual and cognitive challenges. Then, we drew insight from these findings, to create special viewing modes crafted with each of those conditions in mind. We developed a friendly Chrome extension using web technologies, like HTML, CSS, and JavaScript, that interfaces directly with the browser to change what is displayed on a webpage in real-time. Each mode was made with quick, easy user choices for customizing users' browsing experiences with a simple click. Our team thoroughly tested the extension in order to make sure it meets the highest standards of accessibility and enhances user experience. That's why ClearView will work to make browsing the web smoother and more comfortable for everyone through considerate design and a commitment to accessibility.","One of the main challenges was defining particular accessibility modes that would serve very different needs improving readability for users. We diligently balanced the design adjustments of each mode carefully, so that each mode would serve its purpose without compromising overall usability. Technical problems also arose, such as ensuring ClearView was compatible with different layouts and styles on any website. These changes had to be done across web pages with many tests and optimizations. We also paid much attention to maintaining the performance and resource usage as low as possible. This would keep it from slowing down browsing. In this, we smoothed away many errors in ClearView to make it robust and easy for those most in need of web browsing access.","We take pride in developing ClearView into a fully featured accessibility tool and changing how users of diverse needs browse the web. ClearView has various modes for color blindness, dyslexia, ADHD, eye strain, and low vision to be inclusive and accessible for all. We hope to continue optimizing our extension in the future.","In the process of working on ClearView, we learned a lot about the specific challenges that people with various accessibility needs face. We also realized that even small changes can make huge differences to the user experience in things like text contrast or font spacing. We know that these differences may seem minimal to us, but in reality are game-changers for others.","We're not done yet. Our further development plans will continue to expand the accessibility options for users. We see ClearView integrated with other browsers and mobile platforms to expand its reach. Also, we are thinking about adding new features that enable users to save and share their customized settings to make ClearView more versatile.","css, html, javascript",https://github.com/terryzg/ClearView
HackNC 2024,PerCent,https://devpost.com/software/percent,False,"As international students, we quickly realized how challenging it can be to manage finances independently, especially when we were used to having our parents oversee it. After talking with peers, we found that this struggle was common among local students moving out and young professionals starting corporate jobs. To help everyone achieve their financial goals, we createdPerCent.",,,,,,,"express.js, postgresql, react, typescript",https://github.com/adityapai18/HackNC24
HackNC 2024,Desing Your Dream Home,https://devpost.com/software/desing-your-dream-home,False,,,,,,,,"adobe-express, css, html, javascript, python",
HackNC 2024,Rendezvous,https://devpost.com/software/rendezvous-hd6es7,False,"Rendezvous Logo Figma Wireframe In the wake of natural disasters, every second counts. Rendezvous is a community-driven mobile app designed to streamline disaster relief efforts. It connects those in need with those who can help through a dynamic feed of updates and news, a page for hosting donation drives, and a dedicated space for exchanging supplies—all for free. With a user-friendly interface and real-time notifications, Rendezvous empowers communities to come together and make a meaningful impact when it matters most. Displays a feed of public posts from other users and officials. Official news would be pinned up top for emphasis, showing major weather updates and other related news. Users can post questions they may have, advice they want to offer, or other relevant conversations. Allows users to host donation drives and fundraisers directly in the app. Donation drives would include drop-off addresses, that when pressed will direct the user to a separate maps app. It would also show a countdown of how many days the user has left to drop off items before they are collected. Users are also able to host fundraisers in the app that will allow other users to donate in the app instead of going to a third-party application. Another forum page specifically for users to post requests for supplies that they need or supplies they have to give away. All exchanges will be for free. Displays user's notifications including any messages for offers, updates from fundraisers or donations that they supported, important weather updates, or updates from any posts that they watched. User profile page, settings After the recent tragedies from Hurricane Helene in Western NC, affecting people we know personally, we were motivated to make this app to provide direct access to help. The app is for everyone, but two main user groups were in mind: people who need help and people who can help. Time! We had a lot of goals and wants for this project but with such little time, we could really only implement the basics of the app. We spent a good chunk of time trying to come up with ideas- some of them were too ambitious for 24 hours while some of them we thought were too simple. So, we are really proud of the idea we came up with! It's relevant to recent events and foreseeable future natural disasters, as well as a good way to connect the community and build compassion. We learned more about how to use React Native and that we need better time management. We would add functionality, we created the basic framework of the app with ideas of what each page would do, so they mostly have basic functionality of being able to navigate to them and pressing a couple of buttons. We would also like to add the feature of being able to donate directly in the app instead of being sent to a third-party app. Additionally, we created this app with mostly hurricanes in mind, since that is what we are mostly experiencing in North Carolina. In future iterations, we would consider other natural disasters and how our app can be updated to include those.",,,,,,,"expo.io, figma, nginx, postgresql, react-native, visual-studio","https://github.com/eddiefiv/rendezvous/, https://www.figma.com/design/Wk2EpAUIymPT55HwDSPwhw/Rendezvous?node-id=1-2&t=cb3V1qyAMdMTjPeA-1"
HackNC 2024,Linear,https://devpost.com/software/linear,False,"We realized how distracting some sites can be, feeling like hours passed and you know you should be finishing that task. we wanted to build something that was convientient and effective and let you know your metrics as well.","The Chrome extension is created to assist users in overseeing their time spent on different websites by monitoring usage and setting time restrictions. A background script is utilized to track active tabs, measure time spent on each site, and categorize websites as either productive or distracting with the help of a TensorFlow.js model. If a user goes over the set time limit for a website, a ""Time's Up"" message will appear with a 5-second delay, giving users the option to either keep browsing or get blocked from the site. The extension comes with a user interface for adding websites, setting time restrictions, and unblocking sites when requested by the user. The project focuses on empowering users to control and be mindful of their online behavior in order to improve concentration and efficiency.","The purpose of the Chrome extension is to assist users in monitoring website usage and setting time restrictions to manage their time effectively. It includes a simple interface created using HTML and CSS, enabling users to input websites and establish restrictions. The script in the background watches open tabs, logs how long is spent, and utilizes TensorFlow.js to categorize websites as either productive or distracting. After surpassing the time limit, a ""Time's Up"" notification will pop up with a 5-second delay, offering users the choice to either extend their session or have access to the site restricted. The project highlights the importance of improving concentration and efficiency in online tasks.","We ran into a few challenges, the first one was getting TensorFlow to work effectively. it kept throwing errors and would not let us sumbit the information, we did some working around this and managed to get it finished. Another issue we had was the popup coming on to early. we managed to find the way around this by adding some test blocks and getting it to work.","Me and my team managed to finish this extremely late in the night, even around 5 am we all where working tirelessly to make sure this worked all the way through.","As Cliche as this may sound, we learned that anything we want to do we can do if we put our mind to it. no matter how fast it may need to be done, if theres enough will power it can be done.","Whats next is hopefully deploying this project, adding a real logo and making sure its even mroe fleshed out, so everyone around the world can use it.","css, html, javascript, pandas, python, tensorflow",https://github.com/sanskardeva/Linear2024
HackNC 2024,Heels & Meals,https://devpost.com/software/heels-meals,False,"At UNC, our development team noticed a growing issue among students: too much food going to waste, and too many students missing out on affordable, fresh meals. This summer, our team first encountered an app called Too Good To Go while studying abroad in Copenhagen, Denmark. This application allows people in big cities to purchase food for cheaper. Inspired, we came together to build something tailored for our community: an application that connects UNC students with local restaurants and stores to find discounted meals from surplus food that would otherwise go to waste. As a part of our project, we created an ios app and a website platform.",,,,,,,"css, html, json, swift","https://github.com/natamm4/HackNC, https://youtu.be/rqhi6yFG3XM, https://youtu.be/ckfluGhZa38"
HackNC 2024,FinWise AI,https://devpost.com/software/finwise-ai,False," FinWise AI is a smart financial assistant designed to help users track their net worth, manage expenses, monitor market trends, and reach savings goals. With a clear dashboard showing assets, liabilities, budget breakdowns, and portfolio growth, FinWise AI empowers users to make informed financial decisions and build a secure future.",,,,,,,python,https://github.com/tejdeeppathipati/FinWise-AI
HackNC 2024,MoonWallet,https://devpost.com/software/moonwallet,False,"In today's digital age, managing personal finances can be both complex and time-consuming. We wanted to create a tool that not only provides users with a consolidated view of their finances but also empowers them with insights and guidance tailored to their individual needs. By integrating a chatbot powered by ChatGPT, we aimed to make financial advice more accessible, intuitive, and interactive so users can better understand their spending habits and make informed decisions with confidence.","MoonWallet is a comprehensive mobile application that consolidates all your financial accounts into a single, intuitive interface. By leveraging Plaid's secure API, MoonWallet connects to users’ banks and financial institutions to fetch real-time account data and transaction histories. Users can view recent transactions and get an organized breakdown of their spending by category. MoonWallet also features an integrated chatbot powered by ChatGPT, allowing users to interact with their financial data conversationally. This chatbot provides users with personalized, context-aware financial guidance that considers their current financial status, helping them make informed decisions, plan budgets, and better understand their spending patterns.","We built MoonWallet by using Swift and SwiftUI for our frontend mobile application, and for our backend, we utilized Firebase. The frontend interface is designed with SwiftUI to provide a seamless and interactive user experience. On the backend, we integrated Plaid's API within a Firebase cloud function to securely handle authentication and data retrieval. Firebase Firestore serves as our real-time database, storing user data, accounts, and transaction details. This architecture allows for efficient data synchronization between the app and the backend, ensuring that users have up-to-date financial information at their fingertips.","One of the primary challenges we faced was integrating Plaid's SDK with SwiftUI, as the SDK is primarily built for UIKit. Adapting it to work seamlessly with SwiftUI required creative solutions and a deep understanding of both frameworks. Furthermore, handling sensitive financial data necessitated a strong emphasis on security and privacy, and we had to work diligently to ensure that our API keys were stored securely and that no sensitive data was exposed on our front-end application.","We're proud to have successfully integrated Plaid's financial data aggregation into a SwiftUI application, bridging the gap between modern UI design and complex backend services. Achieving secure and efficient data synchronization between the user's device and our backend was a significant milestone, and we also take pride in creating an intuitive and aesthetically pleasing user interface that makes financial management accessible and engaging.","Throughout the development of MoonWallet, we gained valuable insights into financial technology integrations and the importance of data security. We learned how to navigate the complexities of third-party APIs, particularly in handling OAuth authentication flows and managing API tokens securely. Our experience with SwiftUI deepened, especially in integrating it with UIKit components and managing state effectively. We also recognized the critical role of user experience design in fintech applications, ensuring that powerful features remain user-friendly and accessible.","In the future, we hope to enhance MoonWallet by integrating more in-depth financial analytics and dashboards so that the user can gain more comprehensive insights into their financial health. This includes advanced budgeting tools, spending trend analyses, and personalized financial recommendations based on individual habits and goals. Additionally, we would like to deploy custom machine-learning models to detect unusual spending patterns or potential fraud. These models would analyze transaction histories in real time to identify anomalies and alert users immediately, helping to ensure the safety and security of their finances. Our ultimate goal is to make MoonWallet a holistic financial platform that not only simplifies money management but also empowers users to make informed decisions for a secure financial future. We are committed to continuous improvement and look forward to incorporating user feedback to refine and expand MoonWallet's features.","firebase, github, python, swift, swiftui",https://github.com/ege96/MoonWallet
HackNC 2024,HACKNC Initial D Inspired Racing Game,https://devpost.com/software/hacknc-initial-d-inspired-racing-game,False,"This game was inspired by Initial D and 90s Japanese street racing, specifically an old PS2 Game called Tokyo Extreme Racer 3.","As this was our teams first experience with unity and C#, its a crude version of what street races were like back in the day, where the farther you can gap your opponent by the more you win by.","We built this almost purely using Unitys engine and C#. As this was our first experience using Unity and C# we needed a lot of youtube tutorials, googling, stack overflow, etc. We also utilized some free assets we were able to find on unitys asset store.","The initial learning curve was extremely steep, as we could barely find our way through the engine. However, we initially hunkered down and decided to grind out some of the initial tutorials. After that, we began map building, asset hunting and getting our first prototype, where we ran into more issues in getting the wheels to not fall off the cars, getting the game logic to run properly, and literally figuring out how to move some stuff into folders without it breaking.",The biggest accomplishment for us is tackling something that was completely foreign to us and still being able to produce something that runs and looks pretty nice.,Cameras must be fixed before adding a child asset >:( (took 3 hours),,"c#, chatgpt, stackoverflow, unity, youtube",https://github.com/Rajin6767/HACKNC2024-Initial-D-Racing/releases/tag/builds
HackNC 2024,School Safety NYC,https://devpost.com/software/school-safety-nyc,False,I was born in NYC and lived there until I was 7. I knew what it was like going to school there. I also know that NYC has a high crime rate relative to other cities. I made this website in hopes of showing people some essential historical information about the safety of their schools. Were their crimes committed there? How many? I answer these questions with School Safety NYC.,Provides a map and data table that depicts numbers and categories of crimes committed near/at the school over the past several years. Also contains resources for parents and staff that allow them to report school safety concerns and improve school safety in NYC schools respectively.,"HTML, CSS, Vanilla JS, Leaflet, NYC Open Data datasets.","This was one of my first times working with Leaflet! I wasn't very familiar with tile layers, adding circles/markers, popups, and just the overall groove of creating maps. It was a tough process for me to map points indicating school locations from the NYC Open Data school crimes API to Leaflet as well as attaching information to those map points, as I'm not great at JS nor do I have much experience using APIs and Leaflet. The development process was a LOT of debugging, reading documentation, Stack Overflow, learning about APIs, and browsing through data. It was a great opportunity at challenging myself.",,,"To provide more information (in terms of quantity of information as well as specificity of info) regarding school safety, and to add another 500-1000 schools to the map.","css, html, javascript, leaflet.js, nyc-open-data","https://github.com/Jusmanov/school-safety-nyc/tree/main, https://schoolsafetynyc.vercel.app/"
HackNC 2024,Rams-Rocket,https://devpost.com/software/rams-rocket,False,"Check out our newest version of the game onlink! This project,Rams-Rocket, is essentially inspired by the recent heated tech topic about SpaceX's rocket launches and Boeing's failure on NASA's next-gen rocket development. We deeply felt that an easy-accessible educational game is needed as an educational tool for people to gain a brief understanding about rocket and the technologies behind it. After deep review of the existing products on the market, we felt that an educational simulator game is a more favorable way to attract users, as the interactive element was proven to be beneficial on enforcing learning experiences, hence the Rams-Rocket project came alive. We split the game to two modes: theGamemode and theLearnmode. TheGamemode contains two scenes: theRocket Customizationand theRocket Launch. TheLearnmode provides information about the three parts of a rocket that were used in the demo: thePayload, theBody, and thePropulsion. We also include important physics theorems and formula in theLearnmode to provide educational information. Overall, We created8 scenesin Unity: StartGameSceneThe scene is the introduction to the Game, with a button that will lead to the next scene.ChooseModeSceneThe scene is the site for the player to choose between the two modes. There're two buttons that will lead to the corresponding scenes for the two modes.LearnModeSceneThe scene is the mother to three children scene, the players are able to choose what part of the rocket they want to learn about by clicking the corresponding buttons.BodySceneThe scene containing the information for the body part of the rocket.PayloadSceneThe scene containing the information for the payload part of the rocket.PropulsionSceneThe scene containing the information for the propulsion part of the rocket.BuildRocketSceneThe scene that the players will first enter when they choose theGamemode, and they can customize the rocket according to the customization panel. Each click on the buttons will swap the prefabs between a collected list of the prefabs for the rocket parts. The behavior is controlled by the scriptRocketPartManager. On the other side, the rocket status panel will show the statistics for the rocket that will affect the launch of the rocket, which includes the thrust and the weight of the rocket. The launch button will lead the player into the next scene.LaunchSceneThe scene that the rocket will actually launch. This scene includes the rocket GameObject, a VFX particle that performs as the flame from the rocket's engine, the changing sprites of the background image. The effect is acheived by multiple scripts, which will be explained in the following section. The biggest challenge of doing this project lies within ourselves:All members of our team were focused on Back-end Development, and we basically knew nothing about Full-stack Development. During our start-up team meeting on Friday, after brainstorm we proposed two methods of building thisFull Stack Project: 1) Use full-stack frameworks such asReactorTailwind CSS; 2) Use game engine such asUnity. After in-depth consideration we decided to use the later, giving the fact that we have to learn all the basic functions for either choice, we decided to use a more industry-specific tool for our development. After studying the basics of Unity through tutorials, we started development on Saturday. It turns out that Unity was especially helpful for our development and we were able to apply much of its unique features to help up build the game project. We utilized thePrefabfeature from Unity to create undestroyable GameObjects that can persist as our UI and rocket model. We converted much of thePrefabsintoScriptable GameObjectsso that they can be applied into ourC# Scripts. The uniqueScene Controllhelped us a lot to regulate and assort different assets that were used in the project. We were also able to apply theVFX ObjectsandParticle rendererfrom Unity to create the visual effects that improve the player experience, such as the flame object from our Rocket launch. In addition, Unity Engine was helpful in its building options, we were not only able to build the game in the traditional executable .exe format, but also a WebGL format so that it can operate on a website. We created multipleC# Scriptsfor the Unity project, which will be illustrated in the following list: A simple helper script that takes in a string as parameter and utilizes Unity's built-insceneLoadfeature to help us switch between scenes. This helper script was mainly used on the buttons we implemented for scene swapping. A simpler helper script that utilizes Unity's built-inDontDestroyOnLoadfeature to keep the GameObject undestroyed when accross to different scenes. This script helped us to assemble the rocket from different options of rocket parts. It takes inScriptable GameObjectinto three distinctive lists that contain all the prefabs we designed for the rocket. It also contains functions that helped to display the basic statistics of the customized rocket: weight and thrust. This script defines theScriptable GameObjectthat we will use to assemble the rocket, the attributes of the rocket are defined in the file, namely the statistics of each part, including weight, name, thrust (propulsion part), and the prefabs created. This script is the very core of the game which controlls all thereal-time calculationfor the rocket's motion in the Launch scene. All the math functions involved will be explained in the following section. In addition, multiple Update functions were used to update the real-time mass of the rocket, as well as the velocity and acceleration so that they can be displayed for the players. We used theFindComponentandTagfeature in Unity to help us track variables across different scripts, which helped us to better regulate the game rendering process. This script primarily helped us to controll camera motion during Rocket Launch. Theintegration of camera following motion and changing mesh of the quad backgroundhelped us to achieve a visual effect of the Rocket Launch that as if the scene is built in 3D. The lack of assets that we need to use in the Unity project was yet another big challenge for us to resolve. We lacked the time to inidividually design different styles of rocket parts as well as other visual elements in the game. We came up with the idea of utilizinggenerative AIto produce most of the art for our game. The main tool that we used wasAdobe Express, the powerful generative tool provided by Adobe. The chief advantage ofAdobe Expresswas its ability to select through different artistic styles during generation, through which we were able to finish a Game project with an unified artistic style. The result was magnificent. Through the help ofAdobe Express, we were able to generate 4 sets of Rocket design, sum to 12 distinctive assets, as well as all the background images for our game scenes. There are five main functions for mathematical calculation of the rocketcalculateNetPropulsion, ConsumeFuel calculateAcceleration, updateHeight,andgetGravity. Before the launch, the rocket can be assembled on the player's preference that changes the weight and the propulsion of the rocket. We calculate the gravitational force using G = $\frac{Mm}{r^2}$ where total weight of the rocket depends on the player chosen payload and body before the launch. The radius in the formula is calculated as the combined distance of the radius of the earth (constant) and the distance traveled of the rocket after the launch, which is changing. We use theupdateHeightfunction for the distance traveled. To reflect the real-world scenario, we use thecalculateNetPropulsionto subtract the gravitational force derived fromgetGravityfrom thetotalPropulsion. To show how the rocket is accelerating, we use thecalculateAccelerationfunction to reflect the changing acceleration of the rocket while it is traveling into the space. We also have theConsumeFuelfunction to calculate how many fuel has been consumed during the travel. Here, it is significant because the fuel consumption rate reflects the total weight of the rocket that reflects the gravitational force between the rocket body and the earth. We aimed to make our game accessible to a broad audience by hosting it on a website. To achieve this, we chose to build the game usingWebGL, allowing seamless display of game elements across web platforms. Unity served as our development engine, providing flexibility for building and deploying the game in multiple formats should we decide to explore other hosting options in the future. For deployment, we created astatic websiteincorporating theWebGLelement to showcase the game. GitHub Pages was selected as our hosting service, offering a cost-effective solution for static sites. Additionally, we purchased a custom domain andconfigured DNSrecords to direct traffic to GitHub’s servers, ensuring our website was accessible through our chosen domain. Initially, we launched our game on a.tech domain, but it was mistakenly flagged as malware by the school's network, likely due to the absence of anSSL security certificate. To resolve this, we secured a new .site domain, where we will implement HTTPS to ensure secure, trusted access to our website and enhance user confidence . There are two categories that we would like to improve: Graphic and User Interface for more interactions and complex physics and mathematics to power the rocket for more functions. We would like to improve more on Learn part of the game for educational interactions with interactive labels for each part of the rocket. In the future, we would also like to include 3D models for parts as well. We also would like to add a graphical feature to mimic parameters likecurrent velocity, current acceleration,andremaining fuel. For more mathematical calculations, we would like to includeAerodynamic Dragto experience air resistance,Altitude-Based Air Densityto simulate the Earth's atmosphere, andRocket Staging. ""Game Dev"":Lingxiao & YuyangThe two individuals primarily focused on Unity development, including the coding for most C# scripts, scene designs, and engine build.""Chief Scientist"":Wei PhyoThe individual primarily focused on building theLearnmode, including finding the background physics concepts and theories, as well as developing the math functions for us to calculate rocket velocity.""Web Dev"":BenThe individual primarily focused on setting up the configuration of the website access of our game, including DNS configuration, WebGL loading, as well as generating the visual elements that were used in our project.",,,,,,,"c#, git, hlsl, shaderlab, unity","https://github.com/Gimmick12-DYY/Rams-Rocket, https://ramsrocket.site"
HackNC 2024,RubberDucky,https://devpost.com/software/rubberducky-1967rg,False,The rubber ducky method proposed to new programmers,"Takes the code users put in and provides feedback on syntax, runtime, and lint errors.","Python programming for the backend, HTML for the webpage, ChatGPT helped filled in blanks and bridge gaps between pieces of code and languages.","Bridging the gap between languages, proper spacing of the webpage features, and exhaustion.","Completing the primary application, quiz functionality, and the duck model. Learning and implementing flask, HTML, and 3d rendering.","Flask, HTML, and 3d rendering.","Cleaning up the webpage and quiz functions a bit, properly implementing the duck model, providing graphic improvements to the webpage.","css, flask, gemeni, html, javascript, python",https://github.com/AndrewCart5/HACKNC2024
HackNC 2024,CD,https://devpost.com/software/cd-gv3w2b,False,"Website screanshot Navigating dining hall menus and calculating nutrition can be overwhelming for students, especially those at UNC. That's why we created an innovative nutrition calculator that seamlessly integrates with UNC’s dining hall menu. Our tool simplifies the process, letting students know which food combinations meet their specific caloric intake and dietary needs. Whether you're an athlete optimizing performance, someone aiming to lose or gain weight, or managing dietary restrictions, our tool customizes recommendations to fit your goals. Unlike other nutrition calculators, this one automates the process, offering a personalized experience tailored for UNC students. Simply connect it to your Google account, fill out a quick form, and let it do the rest—saving time and effort while empowering healthier choices on campus.",,,,,,,"python, react, sql",https://github.com/celestialdust/unc-dining-app
HackNC 2024,SPay,https://devpost.com/software/spay-cf4hri,False,"SPay started from a simple idea during our Capital One hackathon brainstorming session: what if we could turn underperforming stocks into something useful, like paying for everyday purchases? This question sparked our curiosity, even though most of us had limited experience with finance. Ahmad, our finance minor, stepped up and guided us through the basics, patiently explaining stock concepts and strategies to help us all get on board. His insights were key to bringing SPay to life.",,,,,,,"fastapi, gemini, nessie, node.js, python, react, scikit-learn, tensorflow",https://github.com/dhruvadrew/Spay
HackNC 2024,SnapSuggest,https://devpost.com/software/snapsuggest,False,"As Computer Science students, we value our desk setups and how our rooms look, what better way to decorate our setups or rooms than to use technology!","You snap a picture of your desk, patio, living room, bathroom, any room you wish to decorate and with the use of Clarifai to analyze the image, our app tries to find things to recommend for you to add to this space. After Clarifai processes your image and we build your recommendations, we use puppeteer to scrape the Lowes website to provide you with a great selection of high quality, long lasting products to improve your home.","For this we use a nodeJS server, EJS for the pages and we used the clarifai api to process the images and puppeteer for the web scraping to find the products.
Frontend: EJS Backend: Node.js APIs: Clarifai","We had a few more plans for this project which we didn't have time to accomplish due to time constraints, this was also our first time using the Clarifai api and a web scraper, so figuring how to use these tools was a great challenge, we also believe our app could've been optimized to run faster and display better results so we plan on working on these improvements for the future.","We all were able to learn something, for some of the team members this was their first hackathon ever, and for others it was learning new technology like puppeteer and the clarifai.","This was a new experience for some of the team members, this being their first ever hackathon, learning how to rapidly build an app within 24 hours, and for others learning new tech, integrating a new api they have never used or using a web scraper dependency.","As mentioned previously we had a lot more planned for this project. We wanted to use their color palette recognition to be able to provide recommendations based on the color flow of the image, our main constraint here is we couldn't figure out how to extract the color from the products on the Lowes sight to be able to compare these to the color theme of the image. Next we were thinking of adding another AI layer to analyze the products on the image to better recommend products, basically let AI recommend what best would fit your space, but again due to time constraint we decided not to add another layer of complexity to our application.","clarifai, css3, ejs, html5, javascript, node.js",https://github.com/ebravo04/UNCHACKNC2024
HackNC 2024,Local Pioneer,https://devpost.com/software/local-pioneer,False,"Local Pioneer and Developer Names   Technology and Development  Our team dedicated a substantial amount of time reflecting on what truly mattered to us and how we could showcase it in our hackathon project. Ultimately, we decided to build acommunity-focused applicationaimed at empowering individuals to engage in skill-enhancing volunteer work and participate in community improvement events. Our web app enables bothindividuals and organizationsto create profiles, facilitating connections with like-minded people and shared goals. Through our platform, users can: We followed a structured development process involving design, testing, and implementation. Key tools and technologies used include: Building this application presented several challenges: We're extremely proud of our platform’smission to foster communitythrough technology. Our app is the culmination of countless hours of hard work, and we’re excited about its potential to drive positive change and encourage social engagement. Throughout this project, our team gained proficiency in: Though our project has made significant strides, we plan to implement further enhancements to fully realize our vision. Future developments include:",,,,,,,"bootstrap, css, flask, github, html, javascript, leaflet.js, python",https://github.com/AlvajoyAsante/LocalPioneer
HackNC 2024,Nail it with Lowe's!,https://devpost.com/software/nail-it-with-lowe-s,False,"Amishi's dad is always up to new home DIY projects. Whether it's minor tweaking or building an entirely new space, planning out a DIY can be pretty difficult. Finding the right materials and staying within budget can be overwhelming, especially for first-time DIYers. With Nail It with Lowe's, we wanted to make a program that can simplify that process.","Nail It with Lowe's takes in a user's dream DIY project--say a bookshelf--and provides the user a list of materials from Lowe's, their costs, and a step-by-step tutorial to accomplish their goal.","Our backend coding, which involved querying Chat GPT and scraping data from Lowe's API, was initially done in Python. We converted our code to JavaScript. Our frontend coding was done in React.","This project was a steep learning curve for all of us. Three of us are COMP110 students, and one of us had never coded. For all of us, this was also our first hackathon. We came in without any conception of how to collaborate on a coding project--struggling to set up React, commit to GitHub, install different packages. Things got to a slow start as everyone acclimated to the world of coding. We taught ourselves everything from scratch--from using APIs and integrating Chat GPT into our program to creating a domain and coding in JavaScript. Going to workshops helped demystify abstract concepts and the mentors were extremely helpful and patient in fixing our devices, pointing out what had gone wrong, and explaining how everything fit together.",Our website turned out stunning thanks primarily to Reiyah--the logo on the website links to the Lowe's site. Amishi's web-scraping of the Lowe's site was critical to our project and the integration of Chat GPT to our functionality also helped.,"We developed a foundational understanding of API's, how they work, and how to use them. We learned about how Github and collaboration works. We also practiced coding in JavaScript and Python as well as using the openai package.",We would like to learn to create a more sophisticated AI model for this purpose rather than relying on pre-existing models for our information. We would also like to have a more interactive user experience where the user can specify more things about their DIY project so that we can offer them more accurate estimations and tutorials.,"github, javascript, openai, python, react, vscode","http://nailitwithlowes.tech, https://github.com/amishmish/HackNC"
HackNC 2024,HeelEats App,https://devpost.com/software/heeleats-app,False,"Have you ever wandered around UNC Chapel Hill feeling hungry yet unable to decide where to eat? Worry no more, HeelEats is here to help!We realized that there are so many restaurants on and around campus, and the sheer number may make it a struggle for students to make a selection. HeelEats was created to streamline the search process and help users find highly rated restaurants in the area!","The home screen of the app prompts the user to select a cuisine type from a picker menu. After selecting a cuisine type, the user can see the our recommendations!","To get a prototype of the app working before we started coding, we used Figma to develop a design for our Views. Once created, we went into XCode and recreated the interface.","We are still new to learning the Swift/SwiftUI programming languages which made it a bit difficult building some aspects of our app. For example, when we were working on our reviews view, we didn't know how to fill only half of a star. After a lot of searching for documentation online, we found the methods we needed to help finish our app!",,,,"figma, swiftui, xcode, xfsymbols",https://github.com/amazinyang/Heel-Eats
HackNC 2024,ShapeShift Prosthetics,https://devpost.com/software/shapeshift-prosthetics,False,"Milo is a boisterous 7 year old boy with a congenital limb difference, meaning he was born without his left hand.  We first met Milo through the Helping Hand Project at UNC, a club which designs and 3D prints prosthetic-like devices for children like Milo.  Speaking to him, we learned that he was interested in 3D printing and had recently bought a 3D printer.  He was incredibly excited to use his new printer to print his own prosthetics, just like the one HHP made for him. This project was inspired by him, and tailored to individuals with transradial limb differences. Children are in special need of inexpensive and easy to produce prosthetics, as their continuous and rapid growth makes it a challenge for them to use conventional prosthetics, which can cost upwards of $15,000.  This is the mission of the Helping Hand Project, however the intense and complex computer design needed to create each prosthetic means the team can produce just 2-3 prosthetics per year. This project intends to bring custom prosthetic design to the home of thousands of people with transradial amputations and congenital malformations.  Users simply upload a scan of their limb, which can be easily made using popular phone apps, and input 4 measurements from their limbs. Within one minute, they have a downloadable STL file that is specially designed to be 3D printed on consumer printers.","Users seeking an arm prosthesis can easily upload a scan of their arm and provide key measurements of their current and desired limb dimensions. Based on these inputs, the platform will design and generate a customized arm and hand prosthetic file, ready for the user to 3D print.","ShapeShift was developed using a robust technology stack, including Python, NiceGUI, and OpenSCAD. The frontend leverages NiceGUI's Python-based UI framework, enabling seamless integration of Python functions directly into the user interface, creating a responsive and intuitive web experience. The backend is powered by Python, which orchestrates the generation of 3D models by interfacing with OpenSCAD. This architecture allows for efficient processing of user inputs to create customized prosthetic designs that are readily accessible for 3D printing.",We encountered several challenges during the development of ShapeShift:,"We're thrilled to have brought ShapeShift to life with just two people, 25 hours (thanks to daylight savings time 😉), and a whole lot of determination! From the ground up, we built the entire frontend and backend of the website, tackling each line of code from scratch. We’re especially proud of mastering NiceGUI in record time—even when the documentation left a lot to be desired! This project has been an exciting, fast-paced journey, and we're proud of what we accomplished in such a short period.","Along the way, we picked up some valuable lessons (and a few battle scars):","Future developments will include a comprehensive resources section, where users can browse and download a variety of specialized hand attachment files tailored to their specific needs. This feature will offer free access to attachments designed to enhance functionality in various activities—whether it's a grip designed for cycling, a bow holder for playing the violin, or other customized tools to support daily tasks and hobbies. By expanding the range of available attachments, we aim to empower users to further personalize their prosthetic experience, enabling greater independence and engagement in diverse activities.","css, nicegui, openscad, python",https://github.com/francinew6/shapeshift
HackNC 2024,Glutten,https://devpost.com/software/glutten,False,"One of our team members has Celiac Disease and stated that it is difficult to find products and restaurants that specify if an item is gluten-free. Not knowing whether a product is gluten-free can be life-threatening for people who have gluten dietary restrictions. Based on our research, there is no current system that searches both restaurant menu items and grocery store products for ingredients that contain gluten.","This application connects to a database built from USDA data that has ingredient information of over a million branded products and from uploaded menu items from different restaurants (specifically Red Robin, Daniel's, and Rock 'N' Roll Sushi). The user can search by product, meaning they can enter a specific keyword to find all products in the database that match and can select a specific item to see if the product is gluten-free, may contain gluten, or is not gluten free. The user can also search by restaurant, where they enter a specific restaurant and the app then returns menu items that are gluten-free or have a gluten-free option.","Within Eclipse, the JBDC driver was used to connect Java to pgAdmin4 in order to send SQL queries to the PostgreSQL database. This database was created using pgAdmin4 with .csv files extracted from public files on websites such as Nutritionix and USDA FoodData Central. The front-end is mocked up on Figma so for this version, users can use the application through the terminal. Through this, users can choose if they want to search by keyword and search by restaurant. From USDA FoodData Central, we downloaded two databases and combined them using query calls in the PostgreSQL database manager. One database had the name and brand of the food along with an ID, and the other had the same ID but with an ingredient list of each product. Merging them gave us access to a single table with everything we needed for our system.","We originally built a React Native interface for our front-end, however, due lack of time, we had trouble creating our own API to connect it to our database. This forced us to pivot, instead creating a terminal-based project with a front-end mocked up with Figma. We also planned to use public Nutritionix and USDA FoodData Central APIs to search for ingredient data. We did a lot of research on incorporating RESTful API calls, even attending the REST workshop; however, we found that neither API returned the data we needed. We were only able to access nutrition information. Instead, we created a database and downloaded the data to a csv file that we uploaded to pgAdmin4.","We are proud of our ability to adapt and restructure our software architecture after struggling to integrate the front-end and back-end. Furthermore, our product is currently usable for this dataset and can have a tangible impact in its' current iteration. It can be used to determine if an item if gluten-free since it uses a real-world dataset pulled from a reliable government source. This is extremely beneficial to the Celiac Disease or gluten allergy community since it has greater capabilities than other software to the best of our knowledge.","We learned how to use Java in order to run SQL queries to pgAdmin4, specifically through a JBDC driver. We also learned about integrating APIs, RESTful APIs, creating HTTP requests, Java libraries such as Spring Boot, and how these can all be integrated with a backend in Java and a react native front-end.","In the future, we plan to integrate our front-end application, creating our own API to communicate between React Native and our database, so that Glutten does not just run in the terminal. By completing more research, we also hope find applicable public RESTful APIs so we can access real-time updated data. We also hope to implement more restaurant data, with features to search by an item, return gluten-free restaurants, etc. Additionally, we plan to include other data such as makeup or hygiene products to determine if they are also gluten-free.","eclipse, java, postgresql, sql",https://www.figma.com/design/dNBL64IM0YbTg3IGyOFeuo/glutten?node-id=0-1&t=G52IyFNZ96QOkvqz-1
HackNC 2024,Lifeline,https://devpost.com/software/lifeline-706hkb,False,"Around the world, journalists work tirelessly as part of the free press in order to keep us, and our communities informed of significant and life-changing events. However, the unfortunate reality of the world is that there are many bad actors who would like to silence individuals attempting to exercise their right to free speech. Recently we’ve seen a rise in violence targeting journalists and the press worldwide, from dozens of journalist casualties in Ukraine, to hundreds of journalists killed in the Israel-Hamas war. Lifeline seeks to ensure that no silencing, even death, can prevent an individual from saying what needs to be said.","Lifeline provides an easy, free, and open-source deadman’s switch to anyone, regardless of technical experience. Lifeline allows anyone to create an account and upload a text payload, which could be anything such as a short message, a last will, an expose, or even decryption keys to their casework. Users can also designate specific intended recipients for the payload. After account creation and setup, Lifeline’s servers will regularly ping users with email reminders to login and reset their switch countdowns. If Lifeline detects that a user has failed to login within the last 7 days, then the server will blast all designated recipients with the payload the user specified.","We built Lifeline using Angular for the frontend, Express for our server framework, and Mongoose and MongoDB Atlas for our backend database. First, we storyboarded our project and compiled a list of tasks to be completed for the first initial stage of setting up a frontend and backend. For the first stage we focused on allowing User Logins and account creation. In our second development phase we implemented the majority of the endpoint APIs for our backend, allowing the frontend to communicate with the backend database.","We encountered significant issues with implementing file uploading. Some of our dependency conflicts for file uploading functionalities could not be resolved within the Hackathon timeframe, so we slated it for a future feature release.",We are incredibly proud to have been able to bring Lifeline into reality. Now there is an open source and free deadman’s switch option for civilians and journalists. We successfully implemented login/authorization capabilities and also the ability to send out regular and blast emails.,,"In terms of immediate goals, we first and foremost want to deploy Lifeline to AWS and make it available for users around the world. For immediate goals, we will implement at-rest encryption for the database, ensuring client data integrity even in the event of a total breach of our systems. We will also enable file-uploading in order to increase the variety of information that can be stored on our servers. Both of these functionalities were things that we really wanted to have implemented but we did not have the time to do so. Our most lofty goal is to make Lifeline and any forks of this project so prolific that it becomes impossible to silence civilians and journalists attempting to exercise their free speech. In short, we wish for Lifeline itself to become obsolete.","angular.js, express.js, javascript, mongodb, mongoose",https://github.com/aayzhao/deadmans-switch
HackNC 2024,Professor X: Virtual Mental Health Support,https://devpost.com/software/professor-x-virtual-mental-health-support,False,"The increasing prevalence of mental health challenges, compounded by issues like limited access to resources, social isolation, and stigma, inspired us to developProfessor X. We wanted to create an accessible and empathetic mental health support system, available 24/7, to bridge these gaps and provide a safe, inclusive space for diverse communities.",,,,,,,"heygen, livekit, nextjs, node.js, openai, realtime-api",
HackNC 2024,SpaceDuck,https://devpost.com/software/spaceduck-cw7voz,False,,,,,,,,"axios, css, express.js, expressratelimiter, html, javascript, llama, node.js, ollama, python, squlite3",https://github.com/Barthmalemew/SpaceDuck
HackNC 2024,Apple Planet,https://devpost.com/software/apple-planet,False,"Games like Superliminal that mess with your expectations while playing, while taking cues from story games for how to pace our game.",This is a game about figuring out the workings of a world called Apple Planet and interacting with the civilians in it.,"Entirely in Godot, with assets made in LibreSprite and Blender. The dialogue system is not ours, as it is the Dialogic addon and we do not claim ownership over it.",Main thing we ran into was figuring out git and how to merge our changes when we made sweeping changes to the game.,"Transitioning from 3rd person to 1st person in the middle of development, figuring out git/github.","How to use Github Desktop, 2d elements within 3d scenes in Godot","Maybe a 3rd phase or a sequel? At minimum, we want to add better graphics.","blender, dialogic, gdscript, godot, libresprite",
HackNC 2024,Tar Heel Trade,https://devpost.com/software/tar-heel-trade,False,We were inspired by Facebook marketplace and thought it would be awesome to have a campus specific marketplace so students can trade supplies more easily. We wanted to encourage people to communicate in a way that promotes campus life and safety. Being able to buy and trade used goods allows for a more sustainable approach to living.,"The website allows the user to create a profile, write posts, view others posts, and chat with sellers and buyers within the application.",We built the application using VSCode and we used GitHub for version control. We used Next.js and Firebase in this project.,"One of the most challenging parts was getting the messaging part of the application running. Specifically, keeping the messages in real time and saving individual chat rooms.",We are proud of the messaging feature we added for in-app communication between users as this is a tough challenge as the number of users increases.,We learned a variety of things. We learned how to build a scalable database to safely store emails and passwords. We also learned how to integrate CSS with React to create the user interface.,Refining the UI a bit more and scaling it so it can be available for campus wide use and adding more features like community carpooling and tutoring.,"css, firebase, html, javascript, next, node.js, react, tillwind",https://github.com/mehta-aditya/uncmarketplace
HackNC 2024,Project Secure,https://devpost.com/software/project-secure,False,Logo Figma,,,,,,,"figma, github, java, visual-studio","https://github.com/spanja05/HackNC24.git, https://www.figma.com/proto/ydH8MV2CRbsh9ANxqeuujk/Untitled?node-id=0-1&t=2jKi6sC5KZzLQ4gC-1"
HackNC 2024,EconomicSpaceGame,https://devpost.com/software/economicspacegame,False,"The project is a space game about resource trading. The resources fluctuate based on a randomly rotating stock market. The resources have a ""elasticity"" that effects how much their value relatively changes in a year, naturally going higher or lower than another resource might in that year. The price the resource was sold at in the last year will be different from the current year and the next year. Resources can be bought and sold at the nearby planets. The plans for the future of the game are to add a local planet market resource value that scales the global resource values based on the demand and supply of the planet. We want to add factions that also influence the economy based on the views of the faction. There would be a upgradable development level on the planets that would allow the player to directly infleunce resource values on the developed planet. The ship should have upgrades that visually change it and increase the total resources it can hold, and along with that we would also add a cap to the resources the ship can hold at one time.",,,,,,,"aseprite, c#, unity",https://github.com/Buttertails/HackNC
HackNC 2024,HandShake,https://devpost.com/software/handshake-py907x,False,"We wanted to build an app that could bring people together, and connect with those that would otherwise be a missed connection. Handshake is mainly inspired by Nintendo's StreetPass system, which similarly utilized WiFi networks to connect Nintendo DS users.","HandShakeis a mobile app built for iOS that allows for users to meet new people utilizing Bluetooth. It finds other users based on proximity and runs in the background and collectsPassbys.Passbytokens are logs of other users that happen to come within proximity of the user. At the user's discretion, they can access a list of these Passby's and view that user's Handshake Card. 
These Handshake cards are customizable public facing profiles that list a user's name, a short biography, their interests, and some of their favorite songs. 
If the user finds a card they like, they can request to connect with them by requesting a ""Handshake"", a mutual agreement between the user and the recipient to share more contact details, such as phone number and social media handles.","The frontend of the app is made withSwiftUIand with Spencer'sSpenceKit, a UI prototyping framework he designed himself. The UI graphics were created using Adobe Creative Suite. 
We built the backend usingFlaskwritten inPythonand utilizing Google'sFirebaseplatform for device authentication. This backend also works as a middleman between user devices that handles storing and sending user credentials in aSQLdatabase. 
The method of communication between iOS devices utilizesBluetooth Low Energy (BLE)technology to detect other app users. This protocol is optimized to just catch each other and generate Passbys",Adopting Firebase into the project was challenging as it as completely new territory for the team. Integrating the backend server and connecting with the frontend of the mobile app took time to merge cohesively together. Designing the back and forth communication between server and device proved to be a complex and multifaceted task involving the entire team to implement.,"We’re proud to have built a fully functional app in just 24 hours. The user interface is polished, and getting the Bluetooth Low Energy (BLE) functionality to work immediately was a significant achievement. It was also rewarding to explore new technologies like Firebase to enhance our app development process.","Individually, We each learned the importance of our roles on the team. Spencer deepened his love forUI Designworking on the frontend. Dylan learned new skills working innetwork communicationwith cloud-based servers like Firebase. Erdem and Alex both developed skills working together designing the backend.","The future for connecting users always has plenty of potential. A cross-platform mobile app would entail adding Android support. The sharable profile data can be fleshed out by adding more user details to share between users, such as Age, Favorite Foods, Zodiac Sign, etc. Account Integration can expanded with other services such as Spotify, LinkedIn, or Facebook.
Exploring other fields, our app has potential in the professional world as a means to connect job seekers and employers at career fairs or conferences. The Handshake protocol can be used in food service as a means to check in customers for mobile orders at restaurants.","ble, bluetooth, firebase, flask, hashlib, javascript, python, spencekit.swift, sqlalchemy, swift, swiftui","https://github.com/easyink/HackNC-HandShake, https://github.com/steadman1/Handshake"
HackNC 2024,ReadyNow Hurricane Readiness,https://devpost.com/software/readynow-hurricane-readiness,False,"We were inspired by the devastation and damage done by hurricanes during the 2024 hurricane season. We also found that many of the hurricane readiness websites were complex and had a lot of navigation and pages. We thought that for those dealing with a hurricane crisis, there should be a more straightforward website that directs them to the resources they need the most. We wanted there to be a simple and easy-to-use source for various types of hurricane readiness and relief information.","This website provides various resources for anyone who is facing a hurricane crisis. It is split into three sections: before, during, and after a hurricane lands. In the before section, there is a pre-hurricane checklist, hurricane tracker, emergency plan generator that allows the user to write and download a pdf file of an emergency plan, and a local weather forecast with severe weather alerts. In the during section, there is crucial info for what to do if you are in the midst of a hurricane. In the after section, there is a post-hurricane safety checklist as well as a news feed of hurricane news.","We built this app using Node.js and React. For the UI elements, we used the ChakraUI library. We used the national weather service API for forecasts and alerts and embedded ABC newsfeed and a hurricane tracker app. We implemented location-finding technology to get the local user data and the React-pfd package so the user can download their emergency plan.","For most of our users, this was the first type developing a webapp and the first time working on a project in a group. We had to learn a lot of Javascript and React on the fly as well as understanding how to use git and work collaboratively.","We are proud of making a completed project that looks good and that will be genuinely useful. We are also happy to have functionality for local user data and for allowing the user to generate and download their own emergency plan. Finally, we are proud of being able to implement so many useful resources in our project.","We learned how to make webapps, how to work collaboratively, how to use git, and how to create a complete project with simple navigation.","In the future, we plan to refine the UI and implement location-based features that allow users to find local storm shelters and receive notifications about power outages and road closures in their area.","axios, chakra, javascript, national-weather, node.js, react, react-pdf",https://github.com/tcbarzyk/hacknc-2024.git
HackNC 2024,EasyBreathe,https://devpost.com/software/easybreathe,False,"The inspiration forEasyBreathecame with a will to demonstrate how novel machine learning algorithms can have practical purposes in our development of environmental technology. Furthermore, it stems from a pressing need to address air pollution’s health and environmental impacts in North Carolina, a state with complex pollution sources and varied air quality. Recognizing that advanced machine learning could offer new insights, we aimed to create a project that demonstrates that environmental health is no exception to the AI/ML revolution we're currently in.","Trained on data collected from 20 different cities in North Carolina, including 3 of those in the RTP, EasyBreathe makes accurate predictions on the AQI and it predicts the Air Quality concern levels
based on the U.S. Air Quality Index (https://www.airnow.gov/aqi/aqi-basics/). Mainly geared towards bettering the health awareness of UNC Chapel Hill's students, but applicable to individuals all over North Carolina, this web app utilizes extreme gradient boosting to make intelligent predictions and it even has capabilities to contact users through email!","We built EasyBreathe using the XGBoost machine learning algorithm for accurate air quality predictions. The frontend was developed with React.js, while Flask served as the backend web framework to seamlessly connect the different components. We used a MySQL database to handle our data storage. To enhance the data visualization, we incorporated the Google Earth API, enabling live satellite imagery for real-time environmental insights.","Finding the data to train the model was not very difficult. However, finding predictive data to make our project functional was a huge challenge. Additionally, making sure the database was having cells and values inserted into it was challenging as we ran into issues with merging the frontend and our backend. In terms of developing the machine learning model, although the AQI concern levels seem to be a Classifier task, we found that the Regressor class seemed to produce more accurate results. Since we're unsure of why this is, we hope to uncover the true nuances of the algorithms that worked for us in this project.","We were able to integrate a frontend with Google Maps that was connected to a Flask framework which also had an ML model that accurately predicts AQI. This was a challenging task since AQI can seem random and is reliant on a few parameters that we found difficult to gather data for. Eventually, perseverence taught us to have grit and patience and it all worked out in the end. Furthermore, we'd love to give a huge shoutout to our frontend developer as our desgins look great for what we had to work with!","Our team learned that one of the biggest motivations to get a product done is for there to be an important use case. The prospect of being able to benefit others with our knowledge was very exciting and drove us to complete the project, despite the many setbacks and hurdles. Knowing that our product could help someone confidently plan their day or protect their well-being made us understand the responsibility behind each feature we built. It taught us that the product’s purpose goes beyond technical accuracy; it's about reassurance, safety, and the chance to live life a little easier. Developing technology that can give people control over something as unpredictable as air quality has shown us how powerful it is to build tools help others live better, healthier lives. 
Our team was often times not working on the same thing, we all had a role and were expected to complete the role by each other. We had to then take everything we did and merge it together. Through this we learned how to collaborate together, but also play our part in a bigger whole. We are all in this together, our product is a combination of all our efforts.","Next for EasyBreathe is a series of expansions aimed at broadening its impact and accuracy. By incorporating more parameters, EasyBreathe will deliver increasingly accurate predictions, proving itself to be a it a valuable tool and a proof-of-concept for environmental monitoring and action.","flask, google-earth-engine, mysql, react.js, xgboost",https://github.com/ArshNoman/EasyBreathe
HackNC 2024,Astral Journey,https://devpost.com/software/astral-journery,False,"Starting screen Level 3 based on the Full Moon, Supermoon (Beaver moon) End screen Level 1 based on the New Moon Level 2 based on the Taurids Meteor Shower Level 4 based on Leonoids Meteoir Shower We were inspired by the creation of a website from the Web Developer workshop and originally we were going to create a mobile app that would scan the barcode of food items and suggest local and eco-friendly alternatives that have a lower carbon footprint. However, after many hours and failed attempts, our inexperience made it very difficult to try and interact with the website and incorporate the appropriate APIs to make it communicate from front to back smoothly. We then scrapped the idea and learned about pygame from the YouTube video “Getting Started in Pygame in 10 Minutes” by Coding with Russ. We wanted our game to have unique input from our team so we designed a game that had four levels, each created by one team member. The original idea was to have it reset back at the first level when any level was lost, but we decided to make it points-based, so you could lose a level and proceed, but would miss out on points in turn. We built it using pygame and python, only using ChatGPT to tweak minor errors when the game wasn’t running or to make the code more concise, to make the game lag less. There are 4 levels that continue into the next and calculate the total score based on each of the level’s scores. This allows players to compete with each other.New Moon Exploration:The goal of it is to collect the moon debris and explore the moon. The level moves as you go through with randomly spawning moon debris and platforms. The inspiration was the American moon expedition. The whole idea was to collect the moon debris to then study back on earth. Taurus Meteor Shower Game:This game makes the player collect the 2 meteoroid streams using a telescope. The games periodically dims cyclically based on the moon cycle, and as the moon light brightens the meteoroids dim. The game also uses a Q learning table with 3 parameters: the learning rate, the discount factor, and the exploration rate in order to adaptively change the difficulty of the game using reinforcement learning based on the player’s performance. As the player performs better the meteoroids come down faster and more of them will be spawned. Beaver Trap Maze:The maze is a procedurally generated maze game that consists of a beaver sprite as the player and logs representing the walls. It is based on the beaver moon, which is the name given to the full moon that occurs in November. We chose this astral event for the maze game as it is November currently. Earlier in history, the Native Americans and American colonists trapped beavers during this time due to beavers being the most active at this time of year as they prepare for winter, and this maze represents the “trapped” beaver. Asteroid Bullet Hell:This game functions as an increasingly difficult bullet hell where you accumulate more points the longer you are able to stay alive. It is representative of the Taurid’s meteor shower and how a meteor has more debris attracted to it as it gets larger. We built it using Python and the pygame extension built into VS Studio. This allowed us to tweak our game and not have to write such complex functions as they were already defined in the pygame extension. We built each level separately by a team member, then combined all their functionality under a main() function that runs the entire game. The Q learning table used the python’s numpy library to operate. Our website idea didn’t work out, and we were unfamiliar with the python engine. Additionally, our games had many errors and it was difficult to seam all of the different levels into one complete game. Taurids meteor shower: Implementing the adaptive difficulty was probably the hardest aspect since we needed to determine the influencing factors and change the output. New moon exploration: In this game the 2 big challenges we ran into was the jumping mechanic and the randomly generated rocks scattered throughout the map. The rock's generation code would cause the collision to mess up causing the player to be able to pass through the platforms this alongside the jumping physics which wouldn't allow you jump like you were needing to collect the rocks took a while to fix but with some help with online tutorials we managed to get everything to function correctly. For theBeaver Trap Maze Game, we ran into issues for making the logs in a uniformed maze-like appearance as the walls, but through the use of 2D arrays, we figured out the issue. For theAsteroid Bullet Hellit was hard to program the bullets to come from all sides, so I then decided to have them target the player so it's more of a running game than dodging. Additionally, it was difficult to integrate moving from one level to the next after either a loss or completion. But this was eventually resolved. We’re proud that we were able to create a solid and working project for our first Hackathon with the little time and experience we had. The experience was valuable and the fact we were able to learn more about November astrological events is a good bonus from the whole experience. We’re proud that our game reflects the November 2024 astral calendar, and although a very beginner project, still represents the theme of this year’s HackNC, which is space. We learned that for the next Hackathon we attend, we need to be more versed in the type of coding we are interested in for our project as well as certain platforms like React. We learned a lot of shortcuts and the overall functionality of python, as well as the mechanisms behind game development as this is our first game we created. The struggles we went through with the jumping physics and procedurally generated maze also helped us learn new things about python that we were quite not familiar with before. Overall, Astral Journey was created as an experience for people to learn about the events in space and will be a great learning experience for our team and we hope that our game can help people become interested in space and the cosmos.",,,,,,,"numpy, pygame, python",https://github.com/Jimngn/HackNC2024/tree/main
HackNC 2024,StarDaze,https://devpost.com/software/stardaze,False,"One of my friends is deeply passionate about astronomy, and I’ve always found it fascinating, too. Yet, the vastness of space and the complexity of it all felt overwhelming. I didn’t know much about constellations or where to find them, and I knew stars had different life stages but didn’t understand their distances from the Sun. Even stargazing felt challenging—where would I go for the best view? I’d heard that factors like light pollution could affect visibility, but I wasn’t sure how to navigate them. Astronomy seemed intimidating, so we’ve worked to make it a more welcoming hobby. By gathering essential information in one place and making it engaging and easy to understand, we aim to remove the barriers that keep people from exploring the wonders of space. Our goal is to make the awe and joy of discovering the cosmos accessible to everyone.","StarDaze features an interactive celestial map, immersing users in a 3D view of stars and constellations, positioned as they would appear from their actual location using geocoordinates. This lets users explore their favorite stars and constellations in a realistic simulation of the night sky. Additionally StarDaze displays ideal stargazing spots within a 50-mile radius of the user’s location, helping them find the best nearby places to experience the night sky.","Stardaze was developed using React, d3.js, Vite, and Node.js, with MongoDB as the database. For external data, the team integrated Mapbox and OpenWeather APIs, and they used Claude to generate constellation data for the star map.","Our team encountered challenges with MongoDB, as none of us had prior experience with it. Although two team members focused on it from the beginning to the end, we couldn’t successfully implement it. We also struggled to find light pollution data in a compatible format, so we attempted to calculate our own. Unfortunately, we couldn’t get it functioning in production.","The team is proud of the website's visually appealing and inviting UI and is pleased with the outcome of the celestial map. Additionally, they have successfully presigned 24 users, marking an exciting start for the platform.","Our team gained valuable experience with MongoDB, even though we couldn’t fully integrate it with all the features we envisioned. Two of our members also had their first exposure to React and JavaScript, learning a great deal along the way. Additionally, we discovered just how powerful d3.js can be and explored many of its capabilities.","We plan to add more gamification elements to Stardaze, including a badge and point system to enhance user engagement. Additionally, we aim to complete some unfinished features, like the light pollution index, which will assign a light pollution score to different areas. Enhancing interaction with our database is also a priority to improve the overall functionality and user experience. We would also like to add additional features such as a location finder of the International Space Station and a countdown to any comets and asteroids.","css, d3.js, express.js, html, javascript, mapbox, mongodb, node.js, openweather, react, vite",https://github.com/AChakka/HackNc2024
HackNC 2024,FindMe,https://devpost.com/software/findme-xyw5zj,False,"It can be difficult to meet new people, especially when everyone is nose down in a mobile device often in public areas. If there were an easy way to chat with a stranger, but to know there was something in common, it would make impromptu friendships and connections natural in a modern environment.","Users of the app, may turn on the ability to see and be seen in public areas. When two people arrive in relative close proximity of each other, and also have a minimum threshold of similar interests, a ""match"" occurs. For example, if two people are into Pokemon Go, and speaking Japanese, they will be both be shown each other's profile and a chat dialog becomes available.","We used react and react native to build the mobile app, which can be built for iOS, Android, and Web. We have heroku deploying our front end for web as well as the backend. Currently, we are using localhost for testing, but we were able to get auto deploys working for cross team testing.
We built the backend to be able to keep track of user positions and manage match conditions. We weren't able to fully test it all in time, but the main code is there.","We added 2 new team mates last minute and started off in the main gym nose down too early. Later we found a classroom with a white board to help sort out tasks and divide roles, but also we narrowed our scope to the basic original idea, as temptations of feature creep set in.","We are proud of utilizing a wide skillset in our team. We had two backend engineers working together, a main front end engineer, a devOps engineer working on deployment, and and UX designer working on the Adobe, and Figma designs for logos and wireframes. We also love the mission of the idea.We also purchased findmeapp.tech.","We learned that we should have things like react native already setup on our systems ahead of time as pc configurations in the heat of the moment can be a challenge, especially when mixing systems and technologies. In the future, we will have standard operating procedures for each role, and docker images for dev environments ready to go.","We would like to produce a version ready for private release for early release. Note:
Backend is currently locally hosted so most features will not work in the deployed version. You can build the project yourself locally for full testing.
Also adding the domain findmeapp.tech. It may take a day for the name servers to update.","heroku, javascript, mongodb, node.js","https://github.com/hacknc-24/findme, https://github.com/hacknc-24/findme-backend, https://findme-63aedeb3313d.herokuapp.com/"
HackNC 2024,AccountaBuddy,https://devpost.com/software/nudge-4groy0,False,"As college students, we know how hard it can be to stay motivated, especially when tackling long to-do lists alone. That’s why we created AccountaBuddy, an AI-powered accountability assistant that gives you the push you need to get things done. Instead of relying solely on willpower, AccountaBuddy brings motivation to you—through scheduled calls and personalized guidance, the assistant helps you tackle your tasks, prioritize, and organize your thoughts into actionable steps. With AccountaBuddy, staying on track feels less like a solo battle and more like having a supportive partner by your side.","AccountaBuddy is a collaborative accountability assistant that goes beyond simple to-do lists. After creating an account and adding tasks, users schedule check-in times for each item on their list. At the designated time, AccountaBuddy will call and check on their progress. If a user has completed a task, the assistant celebrates their accomplishment. If not, AccountaBuddy dives in to understand any obstacles, helping users prioritize and reorganize their tasks to move forward. The assistant can even assist in breaking down vague goals into clear, manageable action items, making it easier for users to tackle what needs to be done.","With just the two of us, we used JavaScript, React, and Next.js to create a simple yet dynamic user interface. We chose Prisma to manage our user and task database, ensuring that user data was both accessible and well-organized. For the voice calls, we integrated bland.ai, allowing AccountaBuddy to call users at scheduled times. ChatGPT powers the conversational aspect, enabling AccountaBuddy to provide personalized guidance based on each user’s unique needs. By leveraging these tools, we created an app that helps users organize, prioritize, and take action on their tasks with AI-driven support.","As a two-person team, handling the technical and logistical complexities of this project was challenging. React’s component lifecycle required careful management, especially when implementing real-time updates for new lists and data changes. Coordinating between multiple APIs (ChatGPT, bland.ai, and our own backend) added another layer of complexity, especially for real-time interactions. Managing background processes for the scheduled calls also tested our problem-solving skills, but we found effective solutions that kept the app running smoothly.","We’re proud to have taken on a project of this scale with just two people. Integrating real-time AI-driven calls was a new challenge for us, and seeing it come together was incredibly rewarding. Getting bland.ai to work seamlessly with ChatGPT and our backend created a truly interactive experience, allowing users to feel supported at every step. The fact that AccountaBuddy can actively engage with users, helping them prioritize and organize their tasks, makes the app more than just a reminder tool. It’s an actual accountability partner, and we’re thrilled with how it turned out.","This project taught us a lot about working with React and the intricacies of managing state and component lifecycle in a dynamic app. We learned how to handle asynchronous processes with multiple APIs, ensuring smooth real-time experiences for users. Working with ChatGPT and bland.ai gave us insight into managing API limitations and creating a natural, engaging user experience.","Our next steps for AccountaBuddy include integrating Stripe so we can offer it as a subscription-based service for ongoing, personalized support. We also plan to expand the assistant’s capabilities with real-time voice interactions powered by ChatGPT, enabling users to have live conversations with their AI assistant. This will make it possible for users and the assistant to collaborate on organizing and refining tasks, discussing priorities, and making real-time changes to their to-do list. We envision AccountaBuddy as a comprehensive productivity tool that not only helps users stay accountable but also actively supports them in achieving their goals.","bland.ai, chatgpt, next.js, prisma, react, typescript",https://github.com/JakeBoggs/HackNC2024
HackNC 2024,StudyHeels,https://devpost.com/software/studyheels,False,"This project was inspired by our experience as college students, where lectures have over 300 people in them and it is hard to form study groups.",,,,"We accomplished creating a server that could allow users to search for courses, find other profiles, and allow users to log in/create an account for StudyHeels. We are proud of having a unique idea and creating something that could be used by UNC students across campus, and enrich their academics. Creating the actual server that takes data stored in the database is an accomplishment that we all are proud of.","We learned react, fastAPIs, Supabase database, and much more relating to front end and backend development. We are very proud of ourselves for learning so much in such a short period of time. Although we could've added more to our project, we are proud of all the new material we learned and progress we made.","StudyHeels can be further developed much more, such as the frontend development implementing more user friendly interfaces, and the functionality being more closely to what was intended.","fastapi, javascript, python, react, supabase, tsx, typescript",https://github.com/AlexShen12/HACKNC2024
HackNC 2024,Satellyte,https://devpost.com/software/satellyte,False,Stargazing,Simulates the orbit of satellites.,,,,,"Addition of visual orbit paths, and deployment.","html5, javascript",https://github.com/danbogdan159/Satellyte.git
HackNC 2024,DripCheck,https://devpost.com/software/dripcheck-jb0e4n,False,"We wanted to create a platform combining social media interaction with fashion technology. The idea of DripCheck emerged from our love for expressing individuality through style and sharing it with friends, while also exploring how AI can enhance fashion insights.","DripCheck features a camera interface, similar to Snapchat, allowing users to capture moments. Swiping right brings users to the “Stories” section, where they can view photos shared by their friends. Swiping left leads to the “Gallery”, where users can browse their own saved photos. After taking a picture, DripCheck runs an AI-powered analysis of the outfit, providing instant feedback and insights about the fit, trends, and style breakdown.","DripCheck was built using a combination of front-end, back-end, and AI Vision Processing interfaces. We developed the camera and gallery interfaces using Swift IOS Development, XCode. The backend infrastructure was powered by Swift as well and was created to be integrated with a machine-learning model built with TensorFlow for outfit analysis.","One of the biggest challenges was training the machine learning model to accurately identify different clothing items and evaluate styles. We had to collect a substantial dataset and fine-tune our model to recognize various types of fashion trends. Although we made significant progress in training the Neural Network, we ultimately decided to pivot to using OpenAI vision processing. Given more time and resources, we would continue to build out machine learning to increase the specificity of the app and gear it towards more fashion-forward trends.","We’re proud of building an app that combines fashion, technology, and social connectivity into a single platform. The successful integration of real-time AI analysis and a user-friendly interface is a major achievement for our team.","Throughout this project, we deepened our understanding of machine learning model training and image recognition algorithms. We also gained valuable experience in optimizing app performance for real-time functionalities and improving user experience design to make navigation intuitive and enjoyable.","Looking forward, we plan to enhance DripCheck by introducing more advanced outfit recommendations and trend analysis. We aim to incorporate personalized style suggestions based on user preferences and previous analyses. Additionally, expanding the social features to include direct interactions, such as fashion polls and collaborative galleries, is on our roadmap. Further improvements to the AI model for greater accuracy and diversity in fashion recognition are also in the works. GitHub Link","apiintegration, deepfashion, python, swift, tensorflow",https://github.com/svenreyes/fit-checker
HackNC 2024,Spacecraft Override,https://devpost.com/software/spacecraft-override,False,"The primary inspiration for my game isUndertale. It is a low-graphics RPG which is the same theme I was going for with my videogame. Undertale is known for prioritizing world-building and interactive gameplay over the graphics, and that was the same philosophy I wanted to carry into my game.","The player is an astronaut in a spaceship that just got hijacked by a group of aliens. You have a weapon to stop them, but unfortunately pieces of it are scattered around the spaceship. You have to explore the ship and complete puzzles to build the sword, and then slay the aliens to free the ship. The fighting gameplay is not like a normal RPG. You are expected to focus exclusively on dodging the enemy's attacks, and you will automatically attack them at the end of your dodges. The less damage you take, the stronger you hit back.",,,,"As I just stated earlier, this is my first time ever using a game development software. I had to learn completely new syntax to work in GML, but I feel I most improved in handling player choices and building complex dialogue systems. I also improved my skills in game design and balancing, especially when creating engaging battles and interactions that drive the story forward.","Future updates will add new character interactions, alternative endings based on player choices, and more complex puzzles. I want to add many more levels to the game, more complex bosses, and a co-op mode with exclusive bosses for it. Last thing I want to add is a character progression system, so people are not able to easily beat high level bosses early on in the game.","c++, frameworks, gamemakerlanguage, gamemakerstudio",https://github.com/saswath-06/SPACECRAFT_OVERRIDE
HackNC 2024,Tar Travel,https://devpost.com/software/tar-travel,False,"Each year, over 2,100 Tar Heels study abroad and explore different countries, experiencing diverse cultures and educational opportunities worldwide. However, some students find the current HeelsAbroad platform to be less intuitive for researching programs. This difficulty inspired our team to create a user-friendly website designed to help UNC students discover their perfect study abroad destination. One of our team members experienced firsthand how overwhelming it was to research and decide on a program, and another is a current exchange student, which motivated us even further. Our solution is an interactive and dynamic world map that provides necessary information for each study abroad program, simplifying the process and helping students make the best traveling choices.","Tar Travel is an interactive study abroad platform designed to make exploring global opportunities easy and engaging for UNC students. Tar Travel features a world map filled with interactive pins. Each pin represents a different study abroad program, and when clicked, displays detailed information on the sidebar. This sidebar contains program details, including the name, location, description, an image of the host school, and a direct link for further information. To help students customize their search, Tar Travel includes a dropdown filter that allows users to tailor their views based on specific preferences. Students can filter programs by type--such as exchange programs, summer programs, direct enrollment options, internships, and budget-friendly choices. This ensures that every user can find a program that matches their academic needs, schedule, and budget. Our user-friendly and interactive design streamlines the often overwhelming process of researching and choosing an abroad program. With detailed, accessible information at their fingertips, students can make well-informed decisions and easily plan their academic journey abroad.","For the frontend, we utilized React.js to create a dynamic and responsive user interface. The interactive world map was implemented using Leaflet, providing students with a visually appealing and easy-to-navigate map experience. For designs and animations, we used a combination of CSS for styling and React Icons for enhanced visual components. The backend was powered by Python alongside Beautiful Soup for webscraping, allowing us to gather and process data efficiently. To manage and manipulate data, we integrated Pandas for data handling and Selenium for automating data collection from interactive websites.",Web scraping our data from heels abroad proved a challenge as the website was dynamically loaded and did not have a fixed format.,"As beginners, we were able to develop a complete web application from scratch. Creating a tool with the potential to benefit the entire student body is an achievement that exceeded our initial expectations.","The development of Tar Travel taught all of us invaluable lessons about teamwork and developing a full-stack application from start to finish. We learned how to effectively collaborate in a team setting, utilizing Git and GitHub to manage conflicts. The project deepened not only our technical knowledge","Tar Travel has shown to be an invaluable tool when researching different programs and has great potential to become widely used by students. Additionally, we plan on developing a mobile app to provide users with on-the-go access to Tar Travel. We want to make Tar Travel the go-to platform for UNC students to choose their ideal abroad experience confidently.","beautiful-soup, css3, leaflet.js, pandas, python, react, selenium","https://ahkim76.github.io/tar-travel/, https://github.com/ahkim76/tar-travel/tree/main"
HackNC 2024,The Dormganizer,https://devpost.com/software/the-dormganizer,False,"Dorm rooms are small, and sometimes weirdly shaped. Sometimes, it can feel impossible to to find the perfect shelf for your one free corner or a futon that can slide under your bed. Plus, college students don't have hours to spend searching multiple websites for the one item that fits.","The user inputs what kind of furniture they're looking for along with the maximum height, width and depth that they want, and The Dormganizer will return a list of furniture that fits their specifications, along with a picture and a link.","React.js for the frontend, Node.js with an express server for the backend.","Over half of our team had never done a hackathon before, and none of us were familiar with any of the tools that we were using, so we were learning completely from scratch while building the project. It was also incredibly difficult to parse the data, especially in regards to getting the dimensions of the furniture and putting it into a format that we could filter with.","Learning totally new tools for development, and completing a project!","How to use new development tools, and how to better deal with getting and manipulating a large amount of data from the internet.","Right now it's only pulling data from one website, so we need to expand the dataset so that it's truly a compiled list of furniture from multiple places. We also need to work on UI design, and implement sorting functionality for the list that's given to the user.","cheerio, express.js, node.js, react.js",https://github.com/DavidMajernik/Dormganizer
HackNC 2024,One,https://devpost.com/software/one-rgd5fj,False,"At One, we believe that every great achievement begins with a single goal. Life is a journey filled with dreams, but it’s the focus on one goal at a time that transforms those dreams into reality. Imagine what you could accomplish if you dedicated your energy to just one aspiration—how much progress you could make, how much growth you could experience. Our app is here to guide you through that journey, providing the tools and community support to keep you motivated and accountable.","One is designed to help you achieve your goals by providing a focused, step-by-step approach. It allows you to set specific, actionable goals and break them down into smaller, manageable tasks. You can create a personalized action plan with deadlines and reminders, making it easier to stay on track. The app features a progress tracking dashboard, giving you visual insights into your journey and celebrating your achievements along the way. Additionally, One fosters a supportive community where users can share experiences, seek advice, and motivate each other, turning individual aspirations into collective success.","We built it using HTML, CSS, and JavaScript.",The database was a mass challenge that we ran into.,We are proud of everything that worked out.,We should develop our skill better and plan things out better.,A working backend and add more feature that we didn't get to add this time.,"css, html, javascript",https://one-tau-three.vercel.app/
HackNC 2024,Asteroid Field,https://devpost.com/software/aerospace-evolution,False,"Humanity has always looked to the skies, daring to dream beyond our reach. From the earliest moments when we watched birds soar, we were captivated by the idea of breaking free from the ground. Airplanes gave us wings of steel, and rockets propelled us beyond the atmosphere, into the vast unknown.","In this game, players embark on a journey through the space. Join us on the adventure exploring space and look out for the lethal asteroids!","We built this game using Unity, which uses C#, utilized sprites from 2D space kit.","The asteroids that are supposed to be cloned from a spawn, shot down randomly, and finally delete themselves when they go out of boundary did not delete properly at first. It increased the RAM usage the computer on a large scale. We fixed this problem at the end by tweaking the asteroid spawn script.","None of us had prior experience with Unity, Microsoft Visual Studio, or C# before. We learned and built the project on the way and really like what we have in the end.","We learned the greatness of collaboration and teamwork, the skills and knowledge on the way, and how to make great projects.","We would like to add more details in the advancing of aerospace technology. We want to add levels such that players will collect essential parts that signify major leaps in aerospace technology, e.g., picking up an engine to transform a glider into a plane.","c#, unity, visualstudiocode",https://github.com/rclai1/asteroid-field
HackNC 2024,Orbital Simulation and the N-Body Problem,https://devpost.com/software/orbital-simulation-and-the-n-body-problem,False,Our team wanted to contribute our visualizations to the body of n-body research in the hopes that a general solution would be found.,,Our website is based on HTML5 and CSS3. We used p5.js to create the 2-D physics simulations in Javascript. AI was used to make this project in the following ways:,,,,,"css3, html5, javascript, p5","https://orbitalsimulations.github.io/hacknc24/, https://github.com/OrbitalSimulations/hacknc24"
HackNC 2024,Tar Heel Tutor,https://devpost.com/software/tar-heel-tutor,False,We have looked for tutors in the past and had some trouble finding one that had taken the class before.,Our program connects tutors that have taken a course in the past to students who are currently in that class looking for help.,"We built this react based web app utilizing js, tailwind, and firebase.","Automatically assigning grades to the classes they wanted to tutor proved difficult, but we were able to accomplish it by the end.",,,,"firebase, react, tailwind",https://github.com/kobeit2003/Hack.NC
HackNC 2024,FlatMate,https://devpost.com/software/flatmate-tlzcja,False,"As students, we're navigating the transition between living at home and living independently. We're both moving out of the dorms into real houses next year (!), so we wanted to create a solution that would minimize financial conflict and confusion between ourselves and our future roommates.","FlatMate is an app that lets you manage your shared bills through a simple interface. Add a bill to update your roommates on their portion of your shared expenses, or add a payment to update your total balance. You can even invite new roommates through iMessage!",FlatMate was built entirely using SwiftUI.,"We're relatively new to Swift, so we hit a few bumps while learning a new language! We also conqueredsomany merge conflicts.","We're so proud of what we were able to accomplish over 24 hours! We're both very new to hackathons, and we had so much fun watching our project grow.","We learned a ton! We had to adapt to working in a group and managing Git workflow. We also learned so much about bringing a project from start to finish, and got much more comfortable working with the intricacies of Swift.","We'd love to expand non-financial functions such as chore tracking. We're also working on integrating Venmo to allow direct payments between roommates! Finally, we'd like to add a deadline notification system to make sure every bill gets paid on time.",swift,https://github.com/charlottetsuii/FlatMate.git
HackNC 2024,Mood Music,https://devpost.com/software/mood-music-sbx562,False,"Our inspiration for Mood Music came from the way music shapes our daily lives—sometimes, though, it’s hard to decide what to listen to. So, we created Mood Music to take the guesswork out of choosing. Based on your emotions, Mood Music intuitively selects songs that match your mood, letting you relax and enjoy the moment without having to think about what to play.","Mood Music intuitively captures the user’s emotions and delivers real-time music recommendations, creating a seamless, hands-free listening experience tailored to their mood.","For our project, we leveraged the Hugging Face EMO Affect Net model to recognize user emotions in real-time using their desktop camera, all through a custom Chrome extension. The model, integrated into our extension, captures and analyzes facial expressions to detect emotional states seamlessly in the background. We also aimed to connect to the Spotify API to retrieve user information and playlists, enabling us to run a feature extraction model that could analyze both user data and playlists. The goal was to match the detected emotion to an ideal song recommendation, populating the user's queue in real-time for a personalized experience. Due to connectivity challenges with Spotify’s API, we couldn’t fully integrate the live data feed. Instead, we opted for a curated set of songs pre-assigned to specific emotions, allowing the extension to recommend music that complements the detected mood without a Spotify connection. This workaround maintained the essence of the feature, offering users a dynamic, emotion-driven music experience.","Our primary challenges revolved around image capture functionality and implementing Spotify's OAuth authorization. First, we encountered issues with capturing background images when the extension’s popup was closed. Since the popup typically triggers active processes, closing it halted the image-capturing functionality, requiring us to explore background script solutions to maintain continuous operation. The second challenge was managing Spotify's OAuth authorization. This involved securely handling token requests, redirection, and storage to maintain access without frequent re-authentication. Instead, we used YouTube videos for the demo as we couldn't fully implement Spotify's OAuth authorization and image capture within the extension.","Given our inexperience with image capturing and Chrome extension development, we're proud to have successfully brought these components together. Tackling both new technologies at once challenged us to learn quickly, and we’re thrilled with the results we achieved in integrating them seamlessly.","We gained a strong understanding of image capturing components, exploring techniques for integrating real-time emotion detection. Additionally, we learned how to work with API keys and manage OAuth authentication, even though we faced challenges with full implementation.","After successfully integrating the ability to access music from users' API playlists, we aim to enhance the experience by introducing a feature that recommends new and related songs tailored to their preferences. Additionally, we envision developing a hands-free control system that enables users to skip, pause, and like tracks using simple hand gestures, creating an interactive listening experience.","api, css, html, javascript, rest, spotify",https://github.com/V3D4N7V2/MoodMusic
HackNC 2024,ConnectMax,https://devpost.com/software/connectmax,False,"Professional networking today is fundamentally broken. Platforms like LinkedIn connect people, but they don't optimize for actual career growth or provide strategic pathways for professional development. We wanted to create something that doesn't just connect people, but actually computes the optimal path for each person's career growth using advanced AI. The inspiration came from asking: ""What if we could treat career development like a mathematical optimization problem, where each connection and interaction is strategically chosen to maximize someone's chances of success?"" ConnectMax is an AI-powered professional networking platform that uses active inference (the computational framework that models how the human brain makes decisions) to create personalized networking strategies. The system works in two key phases: Profile Building Phase: Strategy Generation Phase: How we built it
The system is built on three main technical components: Active Inference Engine: Multi-Agent System: Frontend: Challenges we ran into Active Inference Implementation: User Experience: Technical Integration: Accomplishments that we're proud of What we learned What's next for ConnectMax Technical Enhancements: Feature Expansion: Privacy and Security: Platform Growth: Our ultimate goal is to make ConnectMax the standard platform for strategic professional development, where career growth and community building is treated as a mathematical optimization problem that gets better at finding solutions with each new user interaction.",,,,,,,"langgraph, nextjs, pymdp, python",https://github.com/RPD123-byte/ConnectMax
HackNC 2024,Dungeon Escape Game,https://devpost.com/software/dungeon-escape-game,False,"Project Description: Dungeon Escape Game Player is trapped in a dungeon, and must survive through a randomly generated number of rooms to escape. Each room poses its own challenge, from monsters to riddles and more! If player doesn't survive a challenge, they die and must restart their journey.",Our program takes the user through a detailed journey in an underground dungeon full of dangerous situations the user must overcome to escape.,"We used VS Code and created the program using C++. We utilized multiple functions to randomly generate each of the different room environments, as well as arrays to keep track of which ones have been experienced by the player and to keep track of the player's inventory.","We found brainstorming to be challenging at times. We also ran into challenges when adding more intricate details to the program later on, such as the random riddle generation and the player's inventory.","We're proud of the details and creativity that we put into the program, adding personality to each situation.",We learned the strcat( ) function to concatenate strings. We had never used this function before in our previous endeavors.,"As we continue our education, we could add visual graphics to make the program more advanced and visually interactive.","c++, vs-code",https://github.com/bricehamlett/HackNC
HackNC 2024,Tasktronaut,https://devpost.com/software/tasktronaut,False,,,,,,,,"css3, html5, javascript, vscode","https://github.com/DragonCloudBurst/HackNC-2024, http://tasktronaut.tech/"
HackNC 2024,Time Value Vault,https://devpost.com/software/time-value-vault,False,We wanted to make loan estimates more accessible and easy in an effort to increase financial literacy.,"Time Value Vault provides loan payment estimates based on user input of interest, years, payments per year, and initial loan value. It returns the dollar amount of how much you'll be paying for each payment and how much total interest you'll pay to the bank over the life of the loan and it will send all the results to your email address. It also includes a finance chat where the user can input a finance question and Gemini AI provides an answer.","Using VsCode and GitHub, we used Javascript, Python, HTML, and the React library. We also used Gemini and API.","Had to learn javascript, react, and HTML from scratch. It was also our entire group's first-ever hackathon so it was difficult getting started and finding direction. Connecting all of our code was another challenge for us.",Our site is running and functional. We were also able to research financial materials and use them in our program. Another thing we are proud of is our use of an API,We learned about making web applications and the overall details of how a hackathon works.,"We would like to add financial APIs, we didn't have the time or experience to call many APIs but would like to in the future. This would add a lot more financial functionality to the website. We would also like it to integrate with an iOS application. we would like more features as well such as transaction tracking, stock market info, and budgeting.","api, css, gemini, github, html, javascript, react",https://github.com/kcrozb/hackNC2024.git
HackNC 2024,StonkQuest,https://devpost.com/software/stonkquest,False,"Nearly everyone knows about investing, but HOW to invest? That's a mystery to most. Targeting younger audiences, one thing is for sure: everyone loves Gaming. Games are accessible, and research shows that gamifying education allows for better learning. One of us is a huge gamer, and old Nintendo games inspired the UI. We wanted to make a fun platform for stock market simulation and practicing market scenarios. It would be hard to invest real money to gain experience selling, holding, and buying stocks to gain insight into what to do in market scenarios. This game aims to do just that! Hope you learn something. And most of all, we hope you have fun!!!","StonkQuest simulates four scenarios that can affect the stock market and make it playable. Practice trading in four real-world scenarios: market crashes, company mergers, new IPOs, and earnings reports. Each scenario teaches the user the basics with a simple goal: make a profit. Once the user gets the hang of it, they can test their skills in a surprise final challenge where they'll need to trust their gut on whether to buy, sell, or hold.","We began by brainstorming features and functionalities that would make the learning experience engaging and intuitive for users. Utilizing React for the front end allowed us to create a dynamic user interface that mimicked the nostalgic aesthetics of childhood Nintendo games, making financial education more accessible and enjoyable. We decided to go with TypeScript because it is easier to spot errors, and has strong typing and advanced tooling. To manage the state effectively, we implemented hooks and used TypeScript for its robust type-checking capabilities, which reduced bugs and improved code clarity. We keep the stock graphs simple but realistic. Each day, stock prices move up or down based on what's happening in the game - just like real markets. When good news hits (like strong earnings), prices typically rise. And for bad news, the user can expect some dips. Each stock has its own personality too - tech stocks might swing wildly while others stay steadier. This way, the user can practice spotting patterns and learn how different events affect stock prices, all without the real-world stress.","We had a couple of challenges. First, we struggled for a bit while calculating the net gain/profit. There were a lot of codependent values, which would cause a waterfall of errors in the profit. We had to be really meticulous and had to whip out a pen and paper to calculate the values by hand and debug it step by step. It didn’t help that it was 3am and we were really sleepy. We were able to pinpoint the issue and fix it, but it was quite challenging. Second, we had to come up with scenarios and generate appropriate data to simulate the stock market for those scenarios. We also had to consider a wide range of situations like market crashes, IPOs, for which we had to do some learning on the stock market ourselves. Third, we were challenged by the idea of bringing our artistic vision of a retro game to life through our UI design. We found a perfect framework for that, but it was a struggle getting components, elements and actions to align to our vision. They needed to be functional while still maintaining a Nintendo-esque feel, and I think we were able to achieve that at the end of the day.","We developed an interactive fun application for people with little to no knowledge about stock markets to learn about it. By developing a gamified version with different realistic market scenarios, the users can have an engaging platform to learn the actual trading principles. We were able to develop a user interface by taking inspiration from games like Nintendo which can be appealing to both beginner and experienced traders.","As backend engineers, we learned a lot about the front end. Considering that the entire app is in React.js and Typescript, we learned how to build an entire front-end application from scratch. We learned how to deploy and create build scripts for projects, something that some of us hadn’t tried before. We weren’t familiar with TypeScript before this project, but I think we’re all confident enough to code in now. We also learned about the stock market in the process of creating this game. As beginners ourselves, we were also able to learn how to design web pages for our target user, a beginner to stock market trading with some knowledge of what markets are.",,"github, react, typescript",https://stonkquest.netlify.app/
HackNC 2024,Bolt,https://devpost.com/software/bolt-yaveuc,False,"Learning material in college can be challenging, and sifting through Panopto VODs to find the information you need can be tedious and stressful. This pain point led us to create Bolt, a multimedia tool that skips all the needless video scrubbing.",Bolt is a web application that allows users to generate notes that serve as clickable timestamps for lightning-fast video scrubbing.,"We built the application with React and used Supabase to handle our data. After retrieving VODs from Supabase, we used OpenAI's Whisper and GPT4 to transcribe the videos and generate the notes structure.",A few challenges we ran into include:,"Developing the notes architecture was what we felt was the feat of this project. It was pretty cool to be able to hover on a note and just click it, enabling a seamless augmentation of text with video.",,Future features will possibly include:,"react, supabase, tailwind, whisper",https://github.com/tr4ce123/hacknc2024
HackNC 2024,OptiTime,https://devpost.com/software/optitime,False,"As a friend group, we often struggle with creating plans where everyone can join, so we thought we could attempt to hit two birds with one stone by building a website that allows users to create a heatmap for group availability.","A user can generate or join a code that can be shared with the intended members. All users using this code can contribute to the heatmap by selecting the times they are available. After submission, it displays a heatmap, with the darker boxes indicating times when more people are available.","It took a lot of trial and error, pain and suffering, drawing boxes and algorithms, a part of Parsa's soul, all while watching YouTube tutorials by Net Ninja. We spent a lot of time building up the API which we were able to make functional for this project.",There were a lot of things planned for this project that we didn't have the time (or the skill) to incorporate. We ran into trouble trying to incorporate a Weather API into our website that would have displayed the weather at a certain location. We also wanted to add an additional function to request a carpool ride and show if others were available to drive. Making a button centered.,We are proud to have figured out how to utilize Databases and develop a decent front-end interface with only some assistance from tutorials.,"We learned elements of back-end coding, front-end coding, and HTML organization and formatting to get a neat and presentable UI","When we have time and more experience as a group, we hope to be able to grow this further with more features that we initially had envisioned (that we ultimately had to cut due to time constraints)","css, firebase, html, javascript, node.js","https://opal.ils.unc.edu/~sjain4/HACKNC/dist/, https://github.com/VazirYaNazir/HackNC2024-OptiTime"
HackNC 2024,UNC Marketplace,https://devpost.com/software/unc-marketplace,False,"We recognized the need for a more organized and reliable platform for UNC students to buy and sell items among themselves. Currently, students often resort to using Snapchat Stories to communicate their buying and selling intentions, which lacks structure and can lead to missed opportunities. Our marketplace aims to streamline this process, providing a dedicated space where students can connect, trade, and find what they need efficiently.f","UNC Marketplace is your one-stop shop for Carolina students to buy and sell used school and dorm supplies. Our platform features a user-friendly format that allows students to easily upload listings for items they wish to sell while others can browse through the website in search of what they need. With categories that filter listings, users can quickly find exactly what they're looking for. Each listing includes the item, price, a brief description, and the seller's contact information. Students can conveniently submit their listings directly through their profile.","We built the front-end using HTML, CSS, and Javascript. We built the back-end using Python and Flask.","We had some difficulty managing our front-end and back-end components and somehow combining them into a functional website. Additionally, we had difficulty updating the listings page once a new one was added by the user.","We are proud of how the examples listings show up with their images as well as how we added the filtering component so users are able to swift through multiple listings to find exactly what they are looking for. This was also our first time trying to create a back-end, but were able to understand the process with the help of AI and the mentors.","We learned how to cohesively create a functional website with the help of various tools. Additionally, we gained insights into responsive design, ensuring that our website is accessible and user-friendly across different devices and screen sizes. We also focused on user experience, striving to make navigation intuitive and listings easy to find, which enhances overall satisfaction for our users.","We would like to see an addition of a sign-in page where UNC students must create a profile with their school email to access the website, incorporate AI usage where submitting an image listing will give an automated description, and find a way to conduct quality control on products being sold in the marketplace. Additionally, we would love to go mobile with UNC Marketplace, and create a user-friendly app!","css, flask, html, javascript, python",https://github.com/mayasivakumar/unc-marketplace
HackNC 2024,Satellite Image Guesser,https://devpost.com/software/satellite-image-project,False,,,,"Creating a formula to convert the latitude and longitude coordinates of the globe into the pixel coordinates of the world map. 
Scaling the world map into the game.","We were able to create a function that converts the actual coordinates of the world map into the pixel coordinates of the world map. 
We also successfully used API from Google Maps and imported other files such as mapGen, cordConvert, pygame, and math. 
We were able to organize everything together. ## What we learnedWe learned how to incorporate API from Google Maps.
We learned precision is really important in this Satellite Image Guesser, where we had to create a function that converts the actual coordinates of the world map into the pixel coordinates of the world map. ## What's next for Satellite Image GuesserThe next step is to incorporate web development components into the UL. 
We also want to make our world map to scroll in and out.",,,"chatgpt, css, google-maps, html, javascript, python, vscode",https://github.com/jeffc2024/SatelliteImageGuesser/tree/main
HackNC 2024,Centsible,https://devpost.com/software/centsible-0p65au,False,"The idea for Centsible was inspired by the small but frequent amounts of spare change that often go unutilized or lost in our day-to-day transactions. We realized that if we could capture these small amounts and offer meaningful ways to save, invest, or donate them, we could create an impactful tool that transforms minor cash remnants into significant financial resources.","The Centsible project helps users collect and make use of their spare change from cash transactions in stores. Instead of letting change go unutilized, users can choose to save, invest, or donate their collected amounts through the app. The app also provides insights into spending patterns, highlights monthly savings progress, and offers personalized financial tips based on transaction data. By capturing these small contributions, Centsible empowers users to turn leftover change into meaningful financial resources over time.","To bring this project to life, we started by hosting our database on TimescaleDB, which allows us to efficiently handle and query transaction data to power our analysis and insights. We used FastAPI to create our backend and expose APIs to connect with the app's frontend, ensuring efficient communication and data transfer. The frontend was built using React, providing a user-friendly interface for users to track, manage, and analyze their spare change. Throughout the development process, we focused on seamless integration across components to deliver a smooth user experience.",,,,,"decimal, fastapi, git, github, javascript, psycopg2, pydantic, python, react, timespacedb","https://github.com/iam-yashpradhan/centsible-backend, https://github.com/iam-yashpradhan/centsible"
HackNC 2024,Fabrik,https://devpost.com/software/fabrik,False,"I was planning on refreshing myself on FastAPI and learning how to create a web app with React + Vite when I came across the FASHN AI API. It caught my attention because I was interested in the use case of the FASHN AI platform, as I've personally struggled with decision paralysis when online shopping for clothes. From there, I wanted to utilize their API as a means to both learned about React + Vite and work on something that can help me and others. I also realized that this could be a game-changer for people wanted to start their own clothing brands without needing expensive photoshoots and models (as long as they get the right permissions first, of course).","Fabrik lets you virtually try on clothes using FASHN AI's pre-trained model.  Fabrik enables users to upload a photo, select desired clothing pieces, and instantly generate realistic try-on results. It's perfect for both shoppers who want to avoid the whole order-and-return hassle, and for aspiring designers who want to showcase their ideas without breaking the bank.","The initial repository was bootstrapped using a template that I had previously created for React + Vite projects (here). It was set up to help any interested developers to efficiently start working on React & FastAPI projects without the need for excessive configuration and debugging. After that, I incorporated the following Technologies:","There were some initial hiccups with the setup as I had to do some debugging anyways. But aside from that, the main challenges were primarily setting up the various models I needed for the backend and the database. First, I needed to create the various request and response models for the FASHN AI API, which required trial and error along with a lot of back-and-forth with referencing the API documentation. Second was the Image model, mostly because of Pydantic's confusing error messages (aka a giant wall of text) along with the differences between Pydantic V1 and V2. For this, I had to use ChatGPT to pinpoint where the issue was originating from. Surprisingly, the MongoDB Atlas setup was fairly hassle-free, and I really liked that they provided a previous example of a FastAPI implementation.","Overall, I'm proud that I was able to create and to be able to submit a comprehensive web application within a short timeframe despite working solo. I successfully integrated the FASHN AI API and implemented a responsive, user-friendly interface using React and shadcn/ui. Additionally, setting up a robust backend with FastAPI and MongoDB Atlas ensured that Fabrik is scalable and efficient.",The project improved my understanding of FastAPI and highlighted the benefits of React + Vite over Create React App. Working with MongoDB Atlas gave me practical experience with NoSQL databases and their implementation patterns.,"The immediate focus is to implement JWT authentication, ensuring that users can access only their saved images. Next, I plan on added a batch upload features, enabling users to upload multiple images simultaneously. Which will offer users the ability to mix and match different outfits and models seamlessly. On the more technical side, I plan on adding additional unit tests to cover a wider range of use cases and edge scenarios.","fashnai, fastapi, mongodb, python, react, shadcn, typescript, vite",https://github.com/IW714/fabrik_proto/tree/main
HackNC 2024,Rocket Builder,https://devpost.com/software/rocket-builder-q5y3px,False,Our game was inspired by the space theme of the whole hackathon and being able to build a rocket that can launch into outer space.,"In this game, the player can create a rocket ship that launches into space. Based on the parts that the user chooses, the rocket will go at different speeds based on the mass of the total components. The player can launch the rocket by pressing the space bar.",We built the game in Unity and coded it in Virtual Studio Code using C#. Sprites were made using Aseprite.,"We ran into the challenge of half of the team having to download and learn Unity, a program that none of us had experience with before today. It took a while to get a handle on the software. Another difficult part of the project was the calculations required – we were literally doing rocket science.","We are very proud to present a functional game, especially considering we had no prior experience with Unity. This hackathon provided an excellent opportunity to learn new skills as programmers which will be helpful to have for future endeavors.","We learned how to use Unity to create a video game. We also learned how to make an object, not just go up, but also come down. This was also our group's first hackathon, so we learned much about the general experience and proper practices to follow. Two of us learned C# and we all gained much more insight on coding in different languages.",The next step for Rocket Builder is even more parts that the player can choose from for more customizability. And who knows? Maybe the Little Rocket will even be able to land on planets! We would also like to be able to publish this game and have the public be able to enjoy Rocket Builder.,"c#, unity, visual-studio-code",
HackNC 2024,AstroFinance,https://devpost.com/software/astrofinance,False,Our team had interest in game design and personal finance. We wanted to gamify the financial educational experience with a space theme.,"At each planet, purchase assets, pay off loans, and collect paydays in order to save up for each planet's rocket to advance to the next planet.  The value of your assets and liabilities fluctuate over time randomly, so be wise about your financial decisions. Additionally, watch out for random catastrophes that can occur and wipe out your assets!",We used Tkinter to build the GUI and Python for the logic. We used Adobe Express to generate background images for each of the levels and ChatGPT to create seed data.,"Since we worked concurrently on each of Model, View, and Controller, it was difficult to connect all of the pieces together.  We also did not use a game engine, so we had to manually handle game states, animations, and user interactions, which added complexity to the project. Additionally, balancing the financial mechanics to keep gameplay both educational and engaging was challenging, as we wanted to ensure accuracy without overwhelming the player.","Some cool features include: buying and selling assets, getting and paying off a loan, getting a job, paying income, property, capital gains, business taxes, asset appreciation, credit score, and risk score. The game is also fluid and has a retro-space theme!",,,"chatgpt, python, tkinter",https://github.com/HatsuHodowa/space-VC
HackNC 2024,InnoVenture,https://devpost.com/software/innoventure,False,"We wanted to do this project since we figured that there might be a niche market here. We have not seen an AI agent framework catered specifically tostartup investment advice. Startups and traditional businesses are a little bit different in how they should be approached from an investment perspective.For example, in venture capitalist funding, there are many rounds of investing - starting from pre-seed. Pre-seed is quite often one of the first investments a company receives. In that scenario, it wouldn't make sense to analyze the potential of the startup based on its business metrics like you would a traditional company.",InnoVentureis an AI agent framework based project that provides investment advice to venture capitalists for startups.,"We came up with an agent framework to model a venture capitalist interacting with anInvestment Insights Manager. The manager coordinates a team comprising of 3 other members with the job roles ofBusiness Metrics Analyst,Social Media AnalystandNews Trend Analyst.We did this by utilizing thecrewaiagent framework andpoetryto manage the project.We utilizedOllama Llama 3.2model locally to create and use our agent framework.We utlizedSerperDevToolas our web scraping API.","Since we've never utilized AI at such a large scale for a project, we struggled initially to understand what was required of us. Originally we started the project thinking that it needs to be a traditional web development project with a classic Flask-React structure and api calls to a LLM model. However, we later realized at a workshop by Infosys, what AI agent framework really meant.",We're proud to have it running!,"The AI agent framework is a very efficient way to implement roles and simulate human-oriented tasks like discussions, consensus etc. And we want to do more of it!",,"agent, ai, crewai, python, serperdevtool",https://github.com/robo4575/InnoVenture
HackNC 2024,CampusCare,https://devpost.com/software/campuscare-6lbq9p,False,"CampusCare --Inspiration
College can be an exciting but stressful time, and many students struggle with mental health challenges. We wanted to create a safe, accessible platform for students to find support and tools for managing their well-being right from their devices. CampusCare was inspired by the need for easy access to mental health resources and AI-powered emotional support on campus. --What it does
CampusCare provides a comprehensive suite of mental health tools for students, including: An AI-powered chatbot for real-time, empathetic support
A personal journaling feature to help users reflect on their emotions and experiences
Wellness tips and resources with emergency contact information and support hotlines
How we built it
We built CampusCare using the MERN stack (MongoDB, Express, React, Node.js) for a full-stack solution. We used OpenAI’s API for the AI bot to provide personalized responses, and Tailwind CSS for a user-friendly, calming interface. --Challenges we ran into
API Integration: Ensuring secure and efficient communication with the OpenAI API for the chatbot was initially challenging.
User Experience: Balancing an aesthetic yet calming UI that enhances the user experience was a key focus.
Accomplishments that we're proud of
Successfully integrating the AI chatbot to provide empathetic responses.
Designing an intuitive, relaxing user interface that encourages students to engage with mental health resources.
Building a fully functioning journaling feature to promote self-reflection.
What we learned
We gained valuable experience in API integration, UI/UX design for mental health support, and the importance of accessibility and privacy in mental health apps. --What's next for CampusCare
We plan to add: Real-time mental health assessment features to personalize resources based on user needs.
Community support groups within the platform.
Enhanced security features to ensure user data is fully private and secure.",,,,,,,"express.js, mongodb, next.js, node.js, openai",https://github.com/Erickferpinedo/CareCampus
HackNC 2024,Exploring Emotions,https://devpost.com/software/exploring-emotions,False,We build things all the time but it's hard to capture honest data from the user so see if it's affective.,"At this stage, it essentially captures your feelings and tries to match you with pre-selected experiences (games).",Lots of last minute brainstorming and Google Form,I didn't have as much time (I voted then went to work then came here Saturday evening and then left to study to study for my exam),I came to HackNC and I decided to submit this lol,I don't really hack overnight anymore which helps my sleep schedule lol,It's definitely not done in any sense of the word. Looking forward to exploring all the emotions,love,https://docs.google.com/forms/d/e/1FAIpQLSePEhKWtPbPFNUxRv6WehMkCljBLZx0gltqPQSV8Qsg2LGKtA/viewform?usp=sharing
HackNC 2024,What's The Move?,https://devpost.com/software/what-the-move,False,"As UNC students, we often struggle to find events and feel truly connected to the UNC and local community. What The Move aims to solve this by providing a platform where users can easily discover UNC and local events, connect with other attendees, and even host their own events. This is our inspiration for creating a more connected campus experience.","Users can find personalized event recommendations based on their preferences. Once they match with an event, they can access detailed information and connect with other attendees. Unlike existing platforms like HeelLife or Eventbrite, What The Move also empowers users to host their own events, making it easier to bring people together.","Using a backend built with Python, Flask, and Node.js, we process user input and gather event information from local websites. To ensure the best event matches for each user, we leverage Hugging Face’s transformer NLP algorithm to analyze preferences. Finally, we bring everything together on the frontend using Figma, HTML, CSS, and the React.js framework.","Integrating the backend and frontend was a significant challenge, particularly when it came to retrieving data and saving user inputs. After countless hours of debugging and testing new solutions, we ultimately succeeded in bringing everything together seamlessly.","We are incredibly proud of our NLP recommendation algorithm, as well as the platform’s ability to enable users to connect with others and host their own community events.","We learned how to seamlessly integrate the front end and back end, enhancing our skills in building a cohesive application. Additionally, we refined our frontend skills to design a visually appealing and user-friendly interface.","We aim to grow our user base, as the app’s success relies on users coming together to interact. Additionally, we hope to expand our database to include events from more towns and schools, allowing our impact to reach beyond UNC and the Chapel Hill area.","css, figma, html, javascript, json, node.js, pandas, python, pytorch, react.js, transformers",https://github.com/danielwang23/Whats-the-move
HackNC 2024,Where The Heel?,https://devpost.com/software/where-the-heel,False,"Geoguessr is a viral game where you are placed on a street somewhere in the world and have to pinpoint your location exactly. It's definitely fun to play and entertaining to watch, but the average person will have a hard time getting any sort of decent score. In a chase of the same high that skilled Geoguessr players experience after a good guess, we decided to take inspiration from the game, but reduce the scope of the pictures to only UNC's campus. While some areas will be easy to recognize, UNC's campus has no shortage of weird nooks and crannies that hide their own unique sights.","After starting a game, you are put on a page with a pannable/zoomable 360 image. Once you are ready to make a guess, you can navigate a map widget and click on where you think the image was taken. After submitting your guess, a score is calculated based on how far away you are from the actual capture point. Each game has 5 rounds and a final score is calculated at the end by adding together all of the round scores.","We built this project by working on the backend and frontend simultaneously then combining it together at the end. Vikram worked on the game screen using pannellum and leaflet to create an interactive map and panorama. Colin worked on images for our project, taking images across campus and creating our logo. Owen worked on the website design using bootstrap. Jason used express and sqlite to create a backend so game sessions could be tracked.","Initially we were planning on using tailwind to design our frontend, but after multiple hours we still struggled with integrating it with vite, so we decided to cut our losses and write plain html using bootstrap as a supplement. On the backend side there were multiple issues with sqlite, we were having issues with multiple responses being sent back to the frontend because we made mistakes implementing the sqlite library's asynchronous features; if an error occured in a previous database call it wouldn't propagate until data had already been sent to the user.",The one thing we are really proud of is getting the 3d photo sphere to work. A the beginning of the project we were going to settle and only use panorama views that could only be scrolled left and right. We are proud of the fact that we were able to do a little more searching in order to provide a better experience.,"We learned multiple things in this project, including how to use panoramic images and maps. We also learned about how to make backend servers in js. Colin, the biggest beginner on our team learned a lot about web development including json and css.","While we were able to capture a fair amount of imagery Saturday night/Sunday morning, it is very possible to quickly get repeats across games. In the future, we will capture much more photography capturing not only north campus, but south campus as well as other recognizable UNC landmarks such as Gimghoul castle, the outdoor education center, and Franklin street. The site is in a very operable state, but there is definitely polish that we can add to improve the user experience.","axios, bootstrap, express.js, javascript, leaflet.js, openstreetmap, pannellum, sqlite","https://wheretheheel.tech, https://github.com/Awesomeness7/HackNC-24"
HackNC 2024,Delve The Dungeon,https://devpost.com/software/hacknc24-minigame,False,"I've always wanted to be a game developer, and HackNC was the perfect place to start working on my skills. I am currently a freshman computer science student at East Carolina, with the goal of becoming a video game engineer after graduating.","This is a small dungeon crawling adventure. You, the hero, must wield your bow and bombs against hordes of monsters. Explore and dash in order to survive.",,,,,"I plan to practice my game development skills so that I can expand the game. Because it is a top-down exploring based game, the possibilities of what I can make it into are endless. My idea is either an infinite, randomly generated dungeon crawler or a roguelike game with powerups that increase your power and other stats as you go along. Even a combo of both ideas would work well! That's what I love about the idea of game development. The possibilities are endless.",gml,https://github.com/zeldinz24/HackNC24
HackNC 2024,Constellearn,https://devpost.com/software/constellearn,False,We decided that the space theme for this year's HackNC must be the center of our project.,"Constellearn is an interactive web app that helps you explore and learn about constellations through engaging visuals and easy-to-follow lessons, bringing the wonders of the night sky to your screen.","React frontend, Flask backend, Firebase authentication and Firestore cloud database.","Integration with Flask and the database, mostly involved with posting progress and pulling data from endpoints",We were able to implement an infinite card feature to create a seamless experience and the theme of the website.,"Constellations are cool, and Flask is a versatile tool. We learned how to authenticate with Firebase, and a lot of useful React tricks.","Scalability, better database handling. A search feature for lessons, difficulty rating on the thumbnails before you commit too much time to a certain topic. Thumbnail images, support for more constellations. A profile page where you can display the constellations you've learned about.","firebase, flask, javascript, python, react",https://github.com/fategg555/hacknc2024
HackNC 2024,Roycie's Budget Tracker,https://devpost.com/software/roycie-s-budget-tracker,False,Watching the Hornets vs. Celtics game got me thinking about how many NBA and NFL players make huge earnings but still end up broke. That inspired me to create this project and add AI to help people make smarter financial decisions with the right guidance.,"It allows users to categorize their income and expenses, while an integrated AI bot acts as a financial advisor, providing personalized insights and advice based on individual financial data. All while having a friendly UI.","This project uses Next.js and TypeScript to build a smooth, responsive frontend, with Tailwind CSS making the UI look sharp and modern. For smart advice, GPT-4 steps in as the AI bot that gives personalized financial tips, while Google Text-to-Speech makes it more accessible with audio feedback. Prisma and SQLite handle data efficiently so users can easily track their budget info. Altogether, it’s designed to make budgeting feel easy and interactive.","This is my first hackathon, and due to some poor preparation, I'm working alone. My biggest challenge was feeling extremely tired, which got in the way of my main goal—using AI for the first time. After a break and some food, I felt refreshed and was finally able to integrate AI into my project.",USING AI FOR THE FIRST TIME.,I learned more about Next.js and basically using OpenAI.,"I plan to incorporate a feature that alerts users about upcoming expenses, helping them stay on top of their finances. Additionally, I will introduce an expense scanner that automatically categorizes receipts for effortless tracking. Lastly, I aim to make the budget tracker accessible on mobile devices, ensuring users can manage their finances on the go.","google-text-to-speech, gpt-4o, nextjs, prisma, sqlite, tailwindcss, typescript",https://github.com/Eugene1x/Royce-HackNC
HackNC 2024,Astraloq,https://devpost.com/software/astraloq,False,Beginner astronomers often find it difficult to get their foot in the door when it comes to astrophotography. We wanted to build a platform dedicated to helping astronomy enthusiasts find community.,"Astraloq is an event calendar, photo sharing platform, and discussion forum for astral events and community get-togethers. Users have the ability to attend events and connect with fellow attendees. Additionally, users can share and view photos of the events they're attending as another means to engage with the astronomy community.","To build Astraloq, we used React on the frontend, FastAPI on the backend, and NeonDB (serverless PostgreSQL) as our database. We used Chakra-UI for styled React components and SQLModel as an ORM to synchronize our Python data models with our database schema. On the development side, we used VSCode as our editor of choice and DBeaver as our database client. For some of our astronomy-related event data and photos, we pulled resources from an AstronomyAPI.","For this hackathon, we decided to learn some new technologies. Chakra-UI and FastAPI were completely new to us, so there was a bit of a learning curve for the first few hours of the hackathon. However, once the ball started rolling, we were able to build features quickly. Our team was split into two frontend devs and one full-stack dev. We were all working on the same repository, so we had to take steps to make sure we were stepping on each others' toes. To solve this, we used git branches and feature-based task delegation.","All in all, we're super proud of all that we were able to accomplish in just 24 hours with relatively new tech stack. We're also proud of how much we learned in a single weekend. Some key features we're particularly proud of include:","As mentioned above, we learned a lot of new tech during this hackathon. However, our biggest area of growth was learning how to work teammates we've never worked with before. We dedicated time to getting to know each other and setting up effective teamwork strategies to help us deliver our project on time.","We're very proud of what we've accomplished within the 24 hour time limit, so here are some features that we'd love to add in the future.","astronomyapi, chakra-ui, dbeaver, fastapi, javascript, neondb, postgresql, python, react, typescript, vscode",https://github.com/brucereier/hacknc2024
HackNC 2024,Floxis,https://devpost.com/software/floxis,False,"access control floxis controlled page mainpage uploading image on Floxis Inspiration
The inspiration behind Floxis stems from the increasing need for secure and efficient file sharing in a digital world where privacy and data integrity are paramount. Observing the limitations of traditional file-sharing methods, we aimed to create a platform that leverages decentralized technology to empower users with complete control over their shared content. What it does
Floxis is a decentralized file-sharing platform that allows users to upload images to the InterPlanetary File System (IPFS) and share them securely with specific wallet addresses on the AIA testnet blockchain. By using blockchain technology, Floxis ensures that files remain tamper-proof and accessible only to designated recipients, enhancing security and privacy for all users. How we built it
We built Floxis using a combination of technologies: React.js for the frontend, creating a responsive and intuitive user interface.
Node.js as the backend server, which manages requests and interactions with both IPFS and the blockchain.
IPFS for decentralized file storage, ensuring reliable and distributed access to shared images.
AIA Testnet to facilitate secure transactions and wallet management.
Challenges we ran into
During development, we faced several challenges, including ensuring smooth integration between the IPFS and blockchain components, managing user permissions for file access, and optimizing the user interface for an intuitive experience. Debugging these issues required collaboration and innovative problem-solving approaches. Accomplishments that we're proud of
We are proud of successfully creating a fully functional decentralized file-sharing platform that prioritizes user security and privacy. The seamless integration of IPFS and the AIA testnet showcases our technical expertise and commitment to delivering a high-quality product. Additionally, the user-friendly interface enhances accessibility for all users, regardless of their technical background. What we learned
Throughout the development of Floxis, we gained valuable insights into decentralized technologies, specifically how blockchain and IPFS can work together to create secure applications. We also learned the importance of user-centered design, which helped us create an interface that is both functional and easy to navigate. What's next for Floxis
Looking ahead, we plan to expand Floxis by introducing features such as advanced user permissions, support for multiple file types, and enhanced user analytics. We also aim to gather user feedback for continuous improvement and explore potential partnerships to increase platform visibility and user engagement.",,,,,,,"blockchain, css3, hardhat, javascript, metamask, react, solidity",https://github.com/YASH-YADAV-dynamo/Floxis?tab=readme-ov-file#overview
HackNC 2024,FlowGenie,https://devpost.com/software/flowgenie,False,"Flowcharts are an essential tool for organizing ideas, processes, and plans, but creating and editing them can often be time-consuming and complex. Inspired by the idea of making flowchart creation as simple as a conversation, we designed FlowGenie. By leveraging the power of Large Language Models (LLMs), FlowGenie enables users to generate, edit, and analyze flowcharts effortlessly, making it accessible to everyone from students to professionals.","FlowGenie simplifies the flowchart creation process. Using natural language input, users can describe their workflow, and FlowGenie will automatically generate a corresponding flowchart. The app also allows for easy editing and analysis, suggesting optimizations or adjustments as needed. With a user-friendly interface, FlowGenie is designed to enhance productivity by turning complex processes into clear, visual diagrams in just a few clicks.","Our tech stack for FlowGenie includes Next.js, Excalidraw API, and OpenAI’s API, each playing a crucial role in delivering a seamless user experience. Users can provide input in either text or real-time audio, which is then processed and fed into the OpenAI API. Leveraging the power of ChatGPT-4's mini model, the API interprets user prompts and generates flowchart components in Mermaid syntax, a text-based diagramming language. To ensure precise placement within an existing flowchart, we developed an intelligent placement algorithm that dynamically identifies and reserves suitable spaces within the grid, minimizing overlap and optimizing layout coherence. For every new prompt, we pass the current diagram’s structure to the LLM, enabling it to account for the present layout and make accurate adjustments to enhance flow and readability. Additionally, FlowGenie includes an analysis feature where users can request insights on existing flowcharts. The LLM then reviews the flowchart and, based on its extensive training, suggests missing or redundant elements to refine the design further. This setup, combining advanced language models with interactive graphics, allows FlowGenie to offer a unique, highly adaptive flowcharting experience that is both intuitive and insightful.","One major challenge was efficiently placing new shapes in an existing flowchart while maintaining clarity and structure. As users added elements, we needed to locate empty spaces without disrupting the flow or causing overlap with existing components. This required implementing an intelligent placement algorithm that could detect available areas, avoid collisions, and preserve logical connections between shapes. Additionally, we had to balance user-defined customizations, like specific shapes or colors, with the spatial constraints of complex diagrams, ensuring each addition integrated seamlessly without crowding or obscuring the chart’s readability. This process deepened our understanding of adaptive layouts and spatial arrangement, enabling us to create a more dynamic and user-friendly flowcharting experience.","We’re proud to have filled a significant market gap by creating a tool that offers unparalleled capabilities for creating, editing, and analyzing flowcharts with ease. Unlike other products, FlowGenie allows users to generate and modify flowcharts seamlessly, with an advanced level of control and AI-driven insights for optimizing their designs. Additionally, we’ve made FlowGenie accessible to a global audience by enabling multilingual support, allowing users to work in multiple languages and making the platform more inclusive. These accomplishments position FlowGenie as a unique, versatile solution that empowers users worldwide to visually organize and streamline their processes.","In building FlowGenie, we gained significant experience in managing public APIs and developing efficient placement algorithms. Creating an intelligent placement algorithm was a valuable learning experience, as it required us to carefully consider spatial dynamics, collision avoidance, and layout coherence. Balancing these factors to ensure smooth, visually appealing flowchart placement helped us develop a deeper understanding of adaptive layouts and user-centric design. Together, these insights have equipped us with a stronger foundation in building AI-powered, responsive applications.",We want add features like editing text inside a shape and deleting a shape just by simply using the chatbot. Additionally we want the ability to integrate multiple users and allow them to have access to the same flowchart space and chatbot at the same time.,"excalidraw, next, openai",https://github.com/tanmay143/FlowGenie
HackNC 2024,Nebula,https://devpost.com/software/budget_touhou,False,"Touhou, Galaga, and other similar games (bullet hell).",,"We followed a guide made by GameMaker on how to make Asteroids and modified it into something similar to Galaga. We did this by fixing the angle of the player's ship, creating enemy objects that fired bullets at the player, and menus to help with navigation.",,,,,gamemaker,https://github.com/brianx426/HackNC2024
HackNC 2024,Dress to Invade,https://devpost.com/software/dress-to-invade,False,"We were inspired by games such as Dress to Impress and online dress-up games from the 2000s and 2010s. Given that the theme for HackNC 2024 was space-related, we hoped to incorporate that into our game.","Dress to Invade is a dress-up game that encourages players to create stylish outfits. Depending on the player's outfits, it can lead to different outcomes. The player's goal is to put together the perfect outfit using items from a large collection to overcome challenges.","To build this project we used Godot. Procreate was used to create the art and LMMS was used to make the background music. We also utilized Github for collaboration. Figma was used to create mockups of several scenes of our game. ChatGPT was used for the early stages of brainstorming, debugging code, and writing. This helped save time when so we could spend more time on implementing additional features, creating art, and coding.","Since we wanted to have various clothing options and accessories for the player to use when dressing up as an alien, it was time-consuming to create the art and ensure that the options made by the player would be displayed correctly in the next scene. Near the end of the project, we also ran into some difficulty ensuring that our music would play on a loop while also allowing the player to toggle the music on and off.","We were proud to make this sort of game since we haven't personally seen other dress-up games at hackathons. In addition to creating a vast selection of items for the players to choose from, we were able to create a story that ties into the theme of dressing up and space.","Through this project we gained more experience using Godot, making music and art for video games, and working through challenges as they arise.","We would like to provide more customization with the items available. For example, a color pick or preset patterns that could be applied to the items. Additionally, due to the time limit, we only have time for 3 different levels of dressing up, but with more time we would be interested in exploring other themes.","figma, github, godot",https://doowop.itch.io/dress-to-invade
HackNC 2024,Space Donation,https://devpost.com/software/space-donation,False,"After seeing the tracks for HackNC, the space option is what stood out to our team. As a group, we all enjoy the game Planet Crafter, a retro open-world game that requires collecting resources in order to survive. Our team members brainstormed and chalked up the idea of a simple space-themed game that would integrate having fun and giving back to the community.","The user utilizes arrow keys to navigate a spaceship and collect 4 types of materials: iron, aluminum, lithium, and diamond, each of which contains different point values. In addition, there is a timer, which adds an element of excitement and urgency to the experience. The intuitive controls make the game accessible to players of all skill levels. Once the game ends, the player’s score is then converted into a suggested donation amount button along with a replay button. The donation link will then take you to the PayPal giving fund page of the charity. The game over pop-up screen makes it so that the connection between the gameplay and the charitable act is seamless.","We approached this game by first finding out what resources we had available and that were efficient to make a simple web browser game in the next 24 hours. We settled on using phraser.js which was perfect for the 2D spaceship aesthetic that we were going for. We went on an open-source online image distribution site called Kenny’s for our assets. After the foundation, we broke down the components of the game mechanics into 4-5 parts. We started with the spaceship movement, then we worked on collision detection with the different space materials. The scoring system and the basic timer were implemented later on after finishing the dynamic movement and responsive design. We wrote the code out into a javascript file and then began working on the HTML and CSS scripts for the UI and front-end development. We started our HTML script by adding all the necessary elements such as title, body, and game container. We began building off the HTML script adding different functions such as the charity information and the game-over screen until we had our finished project.","During the process of creating the code for our game, the first challenge we faced was with cloning our repository. Since we were working with different operating systems, such as MacOS and Microsoft, some of our team member’s GitHub accounts were not working well with the IDE we were using. To solve this issue, we needed to change to a different IDE in order for the repository to be cloned. The game itself was quite straightforward, however, we faced difficulties with the front-end aspect of our project. Once the game was operable, there were UI issues with the game containers within the body. The images of certain elements of the game such as the spaceship and collectibles were not sized to the margin. As a result, the game would disappear due to the dimensions of the elements being too small.","As a team, we’re proud of accomplishing overcoming each hurdle and successfully deploying the code as we intended to. Despite the hiccups, the outcome of the game and its functionality turned out to be what we had envisioned.","HackNC was ⅔ members of our team’s first hackathon. We learned a lot about the overall experience of team-building with coding and how having team members can show new perspectives on how to approach code. As for the technical side, utilizing phaser.js framework/library was useful for the gaming web development side of our project. Initially, we were going to use Ubisoft to create the game but decided on web development instead. It was an idea that we just researched on a whim and it worked out for the best.",,"intellij-idea, javascript, node.js, phraser.js",https://github.com/rrari84/SpaceDonation
HackNC 2024,I am secretly  not coding,https://devpost.com/software/i-am-secretly-not-coding,False,"The internet offers a wealth of productivity tools for programmers, such as Leetcode, Stack Exchange, and CLRS. However, there aren’t many options that allow programmers to take a break during work hours. We aimed to create a product that enables people to relax when they should be coding, whether in class or at work.","Users enter the name of a Subreddit they wish to browse, and it displays the first few posts disguised as VS Code. This setup allows users to appear focused on coding while actually browsing Reddit. It works with any Subreddit by simply refreshing the page.","We developed a website using HTML, CSS, and JavaScript, pulling information from Reddit's JSON files. We can access these files for any Subreddit using a URL format like ""https://www.reddit.com/r/"" + subreddit_name + "".json"". For example,https://www.reddit.com/r/ruby.jsonprovides data for r/ruby. With some CSS adjustments, we aimed to make the site resemble VS Code as closely as possible.","One of our main challenges was figuring out how to retrieve information from Reddit. While Reddit has an API, we found it unnecessary for our needs. After exploring various tutorials and documentation, we discovered the JSON conversion method, which proved effective.","Unlike many hackathon projects that struggle to function consistently, our product supports every Subreddit page as long as users refresh the page between searches. This is the most complete implementation we've achieved in a hackathon, and we’re pleased to present a fully functional product.",Creating a VS Code replica within VS Code itself was an interesting experience. The visual similarities between our development environment and our product led to some confusion along the way. It also prompted us to reconsider our desktop organization.,"We plan to refine the colors, fonts, and spacing on our website to enhance its resemblance to VS Code. Additionally, we're considering other projects, such as a Reddit browser disguised as Slack or a fake meeting scheduler for work calendars. With more companies enforcing return-to-office policies, we believe these tools could provide some lighthearted relief during the workday.","css, html, javascript, json",https://github.com/quackmyheart/HackNC2024
HackNC 2024,Aligned,https://devpost.com/software/aligned,False,"When we are far too busy in life it becomes easy for us to get off task and forget about our assignments. Thus we created Aligned, a web app to help people keep track of their everyday tasks and long term goals.",When a person starts up the website they are asked to login and once authenticated one can see their different tasks. When clicking on the task you are provided with an information screen where you can see your progress shown as a progress bar as well as a time to help you keep track of the time spent on a task. Having multiple timers and tasks makes it possible to keep track of many different items.,We first started with creating a wireframe of our project so we knew how to go about creating the final product:https://www.figma.com/design/LRvTEhek7ORVIvJLw8b93D/Aligned?node-id=0-1&t=bHDLITk4itoq5bDu-1. We created a react web app for the front end and used a sqlite database and connected the two with fastAPI. We created the authentication feature using firebase.,"We could not figure out how when one is authentication, connect them to their specific database such that they can only see theirs.",We are proud of creating a sqlite database and connecting it to the frontend using it in the database.,We learned how to use React and its components to create a web app.,Connect the authenticated user to their own database.,"css, firebase, html, javascript, python, react, sqlite",https://github.com/TJOKOLI17/HackNC-2024
HackNC 2024,Budget Bites,https://devpost.com/software/meals-for-less,False,"After discovering that our stomachs are often larger than our wallets, we set out to find a way to make shopping more efficient and meaningful. We wanted to prove that with proper planning you can eat well without breaking the bank.","Each morning our botmanuallygoes through Food Lion's website taking note of items on sale. We store each sale item in MongoDB, along with the discount amount and original price. We then configure Spoonacular's REST API to generate recipes that optimize the number of items on sale called for in the instructions. Each recipe is stored as a MongoDB document and presented on the website.","We built a Selenium agent to visit Food Lion's website and record information on sale items and discount percentages. Then, the data is sent by FastAPI to MongoDB where the ingredients and their metadata are stored as documents. We created API endpoints that are automatically triggered each morning to repeat scraping to ensure fresh data and generate recipes through the use of Spoonacular's API. Each call to Spoonacular requires a list of ingredients and returns JSON data describing possible recipes and the number of ingredients it was able to match from the input in its response. Using this ratio, we calculate the savings and send the information to our Next.js front end.","Food Lion, the grocery store we targeted our project at, uses a dynamically loaded website, making traditional static web scrappers useless on their own. To parse the HTML for data we had to pivot towards Selenium, building and leveraging an agent to manually interact with the page and scrape the data one page request at a time.","The scope of the project is large for the amount of time we've set aside to complete it. We're proud of the level of professionalism we've been able to put into an endeavor of this size and at such speed. We're also proud of the difficulty level of the project, particularly the web scraper, and that we've been able to push our technical ability.","It's very easy to be narrow-minded when coming up with your tech stack. Both of us came into this project with personal preferences and our own ways of doing things, but the challenges we faced during the time crunch helped us learn how to adapt to the situation on the fly. Albeit jumping ship from Flask to FastAPI early on in the process or discovering thateverygrocery website is dynamically loaded, we learned to pivot quickly while moving at full throttle towards the finish line.","We believe that big brands like Walmart and Target encode store locations in the URL of their website so people can log on and see the items on sale for their given area. We plan to take Meals For Less one step further by allowing you to share your location, choose the stores near you, and get specialized results for regional sales. We also have plans to optimize the suggestion of recipes further by giving higher promotions to meals that help you save more on our website.","beautiful-soup, fastapi, mongodb, next.js, selenium, spoonacular",https://github.com/sblizard/Budget-Bites
HackNC 2024,Set online game,https://devpost.com/software/set-online-game,False,"Eliza introduced me(Henry) to set, and it's been a game that we've both played a lot in and out of class.",Our project recreated this game.,We made it in Javascript/HTML/CSS,A lot of debugging and working with function closures in javascript.,"This is our first hackathon and we're glad we finished the game and proud of having done it all from scratch, without frameworks or using other peoples' images.","Understood closure of functions and arrow functions better and assignment of ""this"".",We would like to one day host this online with a server so our non comp sci friends can play it easily.,"css3, html5, javascript",https://github.com/henryliu213/SET
HackNC 2024,1Finance.ai,https://devpost.com/software/1finance-ai,False,,"1Finance.ai is a financial tracking and financial literacy app that connects to users' bank accounts and displays a clear, user-friendly dashboard of their financial data. Here’s what it offers: Account Aggregation: Connects to multiple bank accounts via Plaid to fetch account balances, transactions, and other relevant data. Loan Approval Prediction: Uses a machine learning model to predict loan approval probability based on user input like income, loan amount, and duration. Quiz-Based Learning: Includes an interactive quiz on financial literacy topics to help users understand and manage their finances better. Secure and Scalable Backend: Built using Flask, MongoDB Atlas, and Terraform to ensure seamless data handling, security, and scalability.","We used a range of technologies and tools to bring 1Finance.ai to life: Frontend: Built using SwiftUI for a seamless and intuitive user experience on iOS devices.
Backend: Created a Python Flask server that connects with MongoDB Atlas as our main database for storing user and financial data. Plaid API integration allows for secure access to users’ bank data. Backend: We used Python and Flask for API endpoints and deployed the server through Ngrok. The Swift will communicate with the backend through API endpoints Machine Learning Model: Trained a logistic regression model usingscikit-learnfor predicting loan approval probabilities, which we deployed usingJoblibfor efficient storage and retrieval.
EmployedTerraform CLIto configure and manage ourMongoDB Atlasinfrastructure in a fast, scalable way. Terraform allowed us to automate the setup, making our system more resilient and easier to replicate. Database: We usedMongoDB Atlasfor database, we have created one cluster and 4 collections: hackncCluster","Developing 1Finance.ai presented some unique challenges: Securing Financial Data: Integrating with Plaid required us to carefully handle sensitive financial data, following strict security protocols. Data Consistency and Syncing: Ensuring real-time data syncing across Plaid, MongoDB Atlas, and our app while maintaining data consistency. Terraform Setup for MongoDB Atlas: Configuring Terraform to work seamlessly with MongoDB Atlas had a steep learning curve, particularly for securing API keys and managing multiple environments. Machine Learning Integration: Integrating a machine learning model for loan approval predictions required efficient data handling and optimization for performance.
Accomplishments that we're proud of",,,"Financial Data Visualization: We were close to implementing Data visualization using SwiftChart, wish if there were more time left.","figma, flask, joblib, linkkit, mongodb, mongodb-atlas, ngrok, openai-api, pymongo, python, scikit-learn, swift, swiftui, terraform, terraform-cli, uikit, werkzeug","https://github.com/hlee18lee46/hackncserver, https://github.com/hlee18lee46/hacknc"
IDEA Hacks 2024,Echo Location,https://devpost.com/software/echo-location-bez8d2,True,"There are an estimated 40 million people with blindness worldwide and 285 million visually impaired. Thier current methods of navigating their enviroments relies on active methods such as canes, or using the help of a human or animal assistant.","The device detects nearby obstacles and informs the user of thier presence using gentle and proportional vibration in the direction of the obstacles. When the user moves closer to the obstacles, the vibration in that direction wil increase, warning the user they will soon bump into it. This can be used to detect people, walls, other obstacles, and also thelackof such obstacles in each direction.","The project uses HC-SR04 ultrasonic distance sesnors to detect nearby obstacles up to 3 meters. Then, this data is used to inform which vibration motors should vibrate and at what strength. It uses an Arduino Nano at its core, along with a 9V battery and a lm2940ct-5.0 LDO regulator.",,,,,"arduino, c, c++",https://docs.google.com/presentation/d/1hM8xdnEVdT65ouzAX3c50xzOPG38idlzJxzmG05kofQ/edit?usp=sharing
IDEA Hacks 2024,The Seeing Eye Vest,https://devpost.com/software/the-seeing-vest,True,"I can't go hiking with my grandparents anymore. In the last few years, it's become too difficult for them to climb steep stairs or uneven terrain without being prepared in advance. And without reallyknowingthe conditions on a trail, we can't go. It's even worse for people with limited sight or hearing. Limited sight makes it hard to stay on a trail and avoid obstacles without help. And the world has been getting increasingly dangerous for those with limited hearing due to the advent of large, fast-moving, and mostly silent autonomous vehicles. The Seeing Eye Vest is our suite of low-cost, open-source hardware and software devices made to help people with seeing, hearing, and motion disabilities navigate the outdoors more easily.",,"We first created the vest itself, a subtle undershirt with an integrated grid of forty-two vibration motors. Efficient electronics enable individual control of each motor. The 3D world around the vest is mapped onto the grid — the closer an obstacle gets, the stronger the vest vibrates in a given direction. This gives our user a type of extrasensory haptic ""sight"" that's independent of sight or hearing. We also created Flysight — a drone capable of semi-autonomous flight and mapping of an environment. With binocular cameras onboard, you can send Flysight ahead to check out a trail before you go. And its integrated mapping capabilities mean the Seeing Eye Vest itself can localize to a known environment and work even better. Soon, we might even be able to expand the framework with automatic pathfinding to avoid steep hills or hazardous terrain.","On the hardware side, controlling forty-two separate motors was extremely difficult. We designed two custom motor controller boards with over twenty transistor circuits hand-soldered on each. Then we spent multiple hours creating a wiring harness for the vest itself. We also had to develop a novel way of creating pockets with the vinyl heat press to mount each individual vibration motor. On the software side, we had a huge amount of trouble integrating cameras, binocular vision, and SLAM (simultaneous localization and mapping). We started out with the ORB-SLAM-3 pipeline, which is near state-of-the-art for low-cost embedded systems like ours. However, we ran into compilation issues on our Raspberry Pis and computational limits. We ended up running ORB-SLAM on our desktop and streaming imaging data back and forth. This meant we needed local depth cameras, like smartphone cameras, to reduce latency and make the vest work while simultaneously mapping the environment.","We're extremely happy with our clean electronics box and networked software and hardware. To get enough output pins to control all our vibration motors, we had to network 5 Pi Pico microcontrollers over a serial connection. And since SLAM and depth-mapping is such an intensive operation, we had to integrate multiple Raspberry Pis as well on the drone and vest. Finally, we're just happy we made such a useful system.","We learned a lot about localization and mapping and about transistor-level logic. Both should be useful — Julie and Ava are building a robot with a similar SLAM stack for their capstone this quarter, and we're all likely to work in the mechatronics field in the future :)","We're going to continue working to make the system more robust. It'll also be easy to miniaturize the electronics, which are currently the size of a small backpack. If this was a commercial product instead of a hackathon project, we could fit it all on a PCB the size of a credit card.","google-maps, javascript, mavsdk, opencv, orb-slam, pixhawk, python, ros, slam, sveltekit",
IDEA Hacks 2024,Bare Bones D&D,https://devpost.com/software/bare-bonez-dnd,True,"Dungeons and Dragons (D&D) is a fantasy roleplaying game where players take up larger-than-life characters to delve into dark dungeons and best legendary foes. It has fostered an extremely devoted and diverse community of players in its many years of existence. The dungeon master (DM) runs the show, piloting all the enemies and describing how the characters interact with the world around them. However, the DM can sometimes be a thankless role as they devote hours of prep for each session and do not get to play their own characters. This motivated the idea of a product that could supplement the role of DM for dedicated D&D players who want a break from DMing.","Bare Bones uses AI (Arcane Intelligence) to understand what players are saying and keep track of character, enemy health, and stats in addition to planning out the individual encounters that compose full-scale adventures. Players take turns saying what they want their character to do and the skull narrates what happens. If a skill check is required the skull will ask the player to roll a dice and then explain what happens as a result.
Behind the magic, the project converts user speech into text, which is then fed into an OpenAI API call along with other information to direct the response. The response is then received and processed before it is converted to audio and told back to the participants. ",,,,,,"openai, pico, python, raspberry-pi, servo, speaker",https://docs.google.com/presentation/d/1Kk9emfW2Sqimhvh-CDklk_KYS7gW755j73YrUF48Veg/edit#slide=id.g1093daabde8_0_536
IDEA Hacks 2024,Brailliance,https://devpost.com/software/brailliance,True,"Our inspiration for this product originated from one of our team members, who has poor eyesight. While modern technology offers solutions like glasses and contacts, we couldn't help but notice the gap for blind individuals who can't read or see letters, and the limited aids/resources available for them. Our mission? Create a product to empower blind individuals, helping them learn Braille and read with confidence.","Braille is a tactile writing system using raised dots arranged in two columns of three dots each. We created a six-button keyboard so blind individuals can press different combinations of buttons and automatically receive audio feedback on what letter they type. With enough practice, they will master Braille and master the various combinations of letters in a hands-on and engaging way.","We first put six buttons the breadboard and linked them to an Arduino, using the IDE to confirm that it was responding each time a button was pressed. Then we used a series of if-else statements to associate different combinations of buttons to alphabet letters. We imported the AdaFruit GFX and audio library to display the letters on the screen and play an audio pronunciation of the corresponding letter.",We ran into two primary challenges: the first being that we couldn't get the displays to activate even if though the code compiled so we used three different displays before we got one to work (this was the first time any of our team members worked with displays). The second problem was that we initially developed audio software on an Arduino Nano which was incompatible with the software needed for an Arduino Zero (the microcontroller we were working on) so we had to go through intensive debugging to integrate the two.,"software debugging (displays), addition of various libraries in Arduino","connecting displays to Arduino, playing audio and connecting speakers to Arduino, using Arduino Uno","To improve our product, we aim to incorporate audio instructions guiding users to press specific buttons. Additionally, we plan to offer compliments or constructive feedback based on whether they correctly input the Braille letter or not. This way, our product can be more like a virtual teacher.","arduino, c++",https://github.com/rachelchan0324/ideahacks2024
IDEA Hacks 2024,DeepCool,https://devpost.com/software/deepcool,True,"It is often difficult for people to keep track of food items in their fridge as they begin to expire, resulting in additional food waste. This motivated us to address this issue using a monitoring system.","Our fridge uses a camera to monitor ingredients present, and employs deep learning to identify food items and generate recipes to recommend using food that is close to expiring.","We built our project in two stages; the data-collection stage involving the Raspberry Pi and its associated camera, used to gather images at regular intervals and upload them to a database. Then, we used deep learning to classify food ingredients from the images and display both the image and generate recipes on a website.",Many of the datasets we encountered were too general or too noisy to use effectively. We also ran into issues updating certain variables such as the real-time fridge image.,"We managed to build an actual model for the fridge, and fully automate the process of data-collection and accurate classification of real foods, in addition to having a functional website that allows the user to generate recipes.","We learned how to effectively employ transfer learning in a practical project, and gained experience with front-end development and database usage.","We wish to improve the classification capabilities of our fridge in the future, such as being able to distinguish individual food items from groups.","css, javascript, mern, mongodb, mosfet, python, raspberry-pi, react",https://github.com/SimplyArsh/ideahacks2024
IDEA Hacks 2024,Run For Your Life!,https://devpost.com/software/run-for-your-life,True,"As CS and EE majors, our motivation to touch grass is rapidly diminishing as the quarter increases in difficulty. Since running is a fun way to go outside and experience fresh air, we wanted to provide a fun way to encourage all of us to go touch grass.",Run For Your Life! is a device that emerges you,,,,,,"amplifier, arduinos, c, imovie, mp3",
IDEA Hacks 2024,LIFE-line,https://devpost.com/software/lifeline-pcsmi2,True,"Cool 9DofIMU, 20lb Weighted Blanket","This device detects when you fall and notifies either a registered family member or the local emergency services depending on the severity of the fall. It also has an OLED display for text, intended for a speech-to-text implementation to help either overcome language barriers or communication disabilities.","Provided with materials by the sponsors and organizers of IDEAHACK, our device uses two N95 masks, an ESP32 microcontroller, and a 9 DOF IMU. The masks house the circuitry, the ESP32 comes with wifi features, allowing us to spend an email to emulate contacting emergency services, and the 9 DoF IMU detects the acceleration of one's head when one falls.",Much of the parts we wanted to interact and communicate between lacked detailed or correct documentation. This caused hours of debugging and researching that ended up in comprises for a lesser-designed product.,The project does properly detect dangerous accelerations and sends an email when the accelerations are reached.,Wireless connections are difficult to implement. 36 hours is not enough time for the product that we intended to make. Implementing a microphone is much more difficult than initially intended.,Improved Speech to Text Recognition. Stored Databases of fall records for hospitals to use to better prescribe treatments and care to patients.,"9dofimu, arduino, bamboo, c++, esp32, firebase, solidworks",https://drive.google.com/drive/folders/1mkiKFYJDhcDMMu8uCmlH2tiE-bL1_glK?usp=drive_link
IDEA Hacks 2024,Carbon E-Mission,https://devpost.com/software/carbon-e-mission,False,"Normally when we talk about ""saving the planet"" we rally up, muster some words, do a day's worth of ""saving"" and it ends there. We are pretty sure everyone acknowledges it as a serious situation and for those who don't, we need to make them aware that it is. People may give a speech or two, inspiring us to help save the planet, but it was never on a personal level. We want to make it personal. We want you to personally know how much you contribute to carbon emissions. Cars are the number one mediator for these emissions and we know how important they are. For most of us, daily life is very difficult without them. But we still need awareness. We need to know how much we emit when we travel in cars.","Our product theCarbon E-Missionis a GPS device that lets you know how much carbon emission you emit based on the make and model of your car. It uses a database and calculates the amount of emissions based on trips. Its is cumulative and resets every day, letting you know how much you emitted throughout the week.",We used a Neo-6M GPS with an ESP32 DEVKIT V1. We also used an SD card module to store data like longitude and latitude for the post-processing calculations. It took a lot of learning to make all the hardware work together but we did it in the end.,"A couple of challenges that we ran into were the limitation of a weak antenna that we needed to work with our GPS module. Due to multiple electromagnetic waves coming from different devices in our environment and originally stationed inside the ballroom, to get the GPS to work, we needed to go outside. We also ran into a bunch of issues regarding not knowing the documentation for the platforms we were using to code since this is a new experience for us.","We are really proud that we solved the antenna issue. It was by far the biggest issue and took the most time. It was such a difficult task due to the antenna's magnitude of strength and it being worked on outside. We are also very proud of how far we got with the two of us, as we had a relatively polished product in the end that performed its main functions.","We now better understand how to deal with microcontrollers and its components. The whole IDEAhacks environment. was exhilarating and helped us gain a newfound understanding of working under pressure. This made us pull 2 all-nighters, as we underestimated the amount of knowledge needed for what we wanted to accomplish. It was overall a good experience and helped us gain a lot of knowledge on in-the-moment engineering.","Carbon E-Mission has a bright future with a lot of potential. It can have a system like friendly competitions in weekly rankings against friends so it will keep it entertaining. Also, we would hope to integrate it with other application services and not just retain it to IOS.","api, arduino, c++, swift",
IDEA Hacks 2024,B-Box,https://devpost.com/software/to-do-utl0og,False,"As engineers, there is one stark problem that nearly all of us have heard about at one point in our lives.Basic hygiene.In reality, many of us engineers and fellow others with lower attention spans do not know how to adequately take care of ourselves or the environment. 
Therefore, we created the B-Box in order to incentivize people to build better bathroom habits through a mix of friendly competition and reminders.","Our device tracks several hygiene habits, as well as some information around the bathroom. These include: Tracking multiple users: Multiple sensors for tracking data: Showering",,,Surviving our first hackathon :D,"This is our first year participating in Idea Hacks for all of us, so it was our first exposure to a lot of the parts offered, as well as a large portion of the community. We had a lot of difficulties in trying to integrate all our components together, especially with trying to figure out new sensors and other parts. Even though we had to pick and choose what components to keep eventually, we still learned a lot. Despite us coming out with almost a completely different device than our original brainstormed idea, what matters the most is the friends we made along the way 😀👍",,"arduino, c++",
IDEA Hacks 2024,Sensor Sight Assist,https://devpost.com/software/sensor-sight-assist,False,"Image of Sensor Sight Assist Inspiration Our inspiration comes from a collective passion for making a meaningful impact on the lives of individuals facing vision challenges. One of our team members is directly working with vision impaired individuals, witnessing the daily struggles and obstacles inspired us to create Sensor Sight Assist, a revolutionary device aimed at enhancing accessibility and independence. Our product was inspired by the absence of spatial navigation for blind patients and civilians while entering someplace new. In a paper ""Navigation Systems for the Blind and Visually Impaired: Past Work, Challenges, and Open Problems"", in the peer reviewed journal PubMed states, “studies on global health estimate that 217 million people suffer from visual impairment, and 36 million from blindness. The affected have their autonomy jeopardized in terms of many everyday tasks, with the emphasis being placed on those that involve moving through an unknown environment.” We as a team of developers have come together to aid the crisis these people have been unfortunately faced with. What it does? Sensor Sight Assist is designed to be a versatile and adaptive tool, providing support for individuals with vision problems. The device serves as a temporary aid, helping users navigate and adapt to new environments, making it a dynamic and responsive solution. It is easily applicable in various settings, such as hospitals, private homes, and nursing facilities. How we built it? In order to incorporate our idea and transform it into a real life project we have used: As for coding language we have used C++. Furthermore, we have used voice recording of one of our teammates. Challenges we ran into: We had into several issues with the wires that were either damaged or unable to connect. Furthermore, DFPlayer Mini MP3 Player demonstrated issues while syncing with the Speaker. Accomplishments that we're proud of: We are proud to have created a responsive prototype and a website demonstrating our product, . What we learned? Throughout the hackathon, we gained valuable insights into teamwork, time management, and the intricacies of hardware and software integration. We learned to overcome challenges collectively, leverage each team member's strengths and become friends with each other. In addition to this, we have connected with other teams, volunteers and sponsors which would give us fruitful connections for the future. What's next for Sensor Sight Assist? The journey doesn't end here. We envision further enhancements and refinements for Sensor Sight Assist. Our goal is to continually improve and expand its capabilities, ensuring that it remains at the forefront of accessible solutions and affordable for individuals with vision impairments.",,,,,,,"arduino, c++, github, javascript, react, react-native",
IDEA Hacks 2024,Weight Assisted Linear Kinetic System (WALKS),https://devpost.com/software/walk-assisted-linear-kinetic-system-w-a-l-k-s,False,"Initially we wanted to focus on areas of untapped energy generation that were abundant and consistent. Our immediate thought turned towards children's playgrounds, specifically turning all of the mechanical equipment into mini generators. We quickly realized this was not feasible and frankly unsafe for the children. Then, while walking down Bruin Walk we noticed the quantity and consistency of people utilizing the path. We realized there was a potential to harness additional energy through this consistency by implementing a generator within our walkways.","WALKSis a linear generator which uses a weighted piston system and induction to generate energy when pressure is applied. When the piston is pressed, a row of magnets pass through a coil of conductive thread wrapped around the casing, generating an alternating current. Springs within the casing allow the piston to return to its original position after pressure is released. The alternating current  is then passed through a diode bridge rectifier to convert the current to DC and store it within a capacitor.","We began with a proof of concept using a conductive thread wrapped around a plastic container and manually moving magnets to test whether or not a current was generated. Once successful, our team split into 2, with one half working on the electronics and the other half working on the hardware. To design the casing we used SolidWorks and after multiple iterations, settled on a final design. The design was then 3D printed and assembled. We then connected the generator to the rectifier and ran finalizing tests to determine the effectiveness of the generator, as well as the potential for scalability.","The main challenge for the team as a whole was a lack of experience. This was our first Hackathon and many of our first times designing and building a piece of hardware. This meant we were teaching ourselves the skills necessary to work on the project while actively working on the project. In addition to this, we struggled with a lack of materials to make a larger scale version of our design. We were heavily limited by the size and strength of the magnets and the final design size was a direct result of that.","We are all very proud that the generator is able to generate some amount of measurable power. Since this was our first time, going from an idea, to a design, to a working prototype within such a short amount of time was very exciting.","We learned a significant amount about electronics, modeling, manufacturing, and the general design process. Most of us came into this with no engineering background so we had to learn at every step of the way.",We want to scale up our model and test the feasibility of the system on a more realistic level. If we find the energy generation to be sufficient as we scale up we may pursue it further. We genuinely believe this product can be a revolutionary in the way we think about generating power.,solidworks,
IDEA Hacks 2024,RizzBand,https://devpost.com/software/rizzband,False,"We wanted a way to meet people in the community without looking at our phones. Rizzband encourages meeting organically in person and getting outdoors. It is also helps those with social anxiety by eliminating barriers for initiating conversation, such as swiping on a dating app, or joining an organization, which can be overwhelming. Instead, you can meet people as you walk around daily!","First, the user fills out their profile and preferences for people they want to meet. Their preferences get downloaded to the watch so WiFi is not required as they are walking around. When the person passes another person with a RizzWatch, the watches will check if their preferences match. If they do, the watches will vibrate for both users, encouraging them to meet.","The front end of the web server is HTML, CSS, and Javascript, which is embedded in Arduino code. This Arduino code is uploaded and run on an ESP32 microcontroller. The ESP32 uses ESP-NOW to create a local network from which it can send and receive data from other ESPs. Each watch contains an ESP.","At first we tried using Bluetooth to communicate between the watches. However, because of the security measures implemented in Bluetooth, you cannot send information unless the other device accepts your request to pair. So, we used ESP-NOW instead.",We were very proud when first sent and received a message over the ESP-NOW protocol. Also when we first downloaded the response from the web server to the ESP.,"None of us had used ESP32 before this project, so learning its capabilities was exciting.",We hope to add more options for profile/preferences and improve the appearance of the prototype.,"c, css, esp32, html, javascript",https://github.com/aki0501/idea-hacks
IDEA Hacks 2024,Ducktor,https://devpost.com/software/ducktor,False,"Due to doctor burnout and biases, especially post COVID-19, patients may not always be receiving optimal care when they visit doctors.  Additionally, at-home telehealth services are currently limited. Current solutions aren't as accessible and affordable as they could be, often relying on expensive devices and protocols that not everyone can afford, or processes that are not easily adaptable o people across conditions (e.g. elderly patients, visual impairments). This is issue is compounded in rural areas, in which healthcare is imperative. Additionally, miscommunication between patient and physician due to language barriers further limits the connections between medical professionals and patients. 
In-person medical facilities may also be a source of infection, telemedicine reduces disease spread, making the flow of patients through healthcare more efficient.","To mitigate this, we propose an AI powered assistant that is both affordable and highly efficient in delivering consistent, recent literature-drive, instant, and understandable initial diagnosis. We implemented this in the form of a hardware assistant (shaped like a friendly duck!) that is also affordable enough to be implemented in any home. We envision Ducktor supporting taking vitals, engaging in conversation with patients and analyzing data to provide to both patients for initial diagnosis and doctors to aid and streamline their diagnosing.","We collected audio input from the user via a microphone connected to an ESP32. We sent this data to one of our Flask web servers, where the PCM signals are converted into .wav audio and transcribed into text. This text is then relayed to another web server where we utilize OpenAI's technology to determine potential diagnoses from the complains of the patient, and respond with an initial diagnosis, some home remedies that may help, and a further explanation of what this means and how they should proceed with their healthcare. The information is then relayed back to the microcontroller and displayed on a screen and spoken out loud.","We were unable to achieve a stable WiFi connection with IEEE_5G on our ESP32, which rendered us unable to send HTTP requests to our server. This delayed the implementation of communication between our device and the cloud processing where the magic occurs, forcing us to prioritize features. We also encountered hardware issues between faulty boards and incompatibility between devices","We were able to make data sent from the device to the server secure and encrypted to ensure patient privacy throughout these exchanges. We were also able to take most of the processing to the cloud, so the device can remain affordable, small, and accessible to patients across the world.",,"We would love to continue to refine our WiFi connectivity issues and ensure that the device can maintain a stable connection as it runs, as referred to in our slides. We will also continue refining our diagnosis process.  process.","amazon-web-services, c++, cad, dsp, esp32, fft, flask, heroku, lilypad, openai, python, spi","https://github.com/rathull/idea-hacks-2024-hardware-client, https://github.com/rathull/idea-hacks-2024-server"
IDEA Hacks 2024,The Plant Buddy,https://devpost.com/software/the-plant-buddy,False,,,,,,,,arduino,
IDEA Hacks 2024,PAHS (Portable Automatic Hydroponic System),https://devpost.com/software/pahs-portable-automatic-hydroponic-system,False,,,,,,,,"arduino, onshape, tinkercad",
IDEA Hacks 2024,SolarHome,https://devpost.com/software/solarhome,False,,Solar panel powers a light and a windmill. The solar panel can swivel to capture as much sunlight as possible even as the sun moves.,We used a servo and an axle to rotate the solar panel. The solar panel uses a relay to send power to the windmill and light.,"We did not have enough adhesive to stick the servo to the solar panel holder. We also could not get a solar powered USB charger to work, and the solar panel has very low wattage. Many of our teammates left early as well.",We made good progress in relatively little time.,Move more quickly and communicate better.,Fixing issues with binding servo to solar panel and connecting all components together.,arduino,
IDEA Hacks 2024,The Collision Cap,https://devpost.com/software/the-collision-cap,False,"We suspected that a reason some people might not partake in certain outdoors activities is because they are afraid of getting injured. Especially in the cases of bikers and mountain climbers, who often partake in their activities alone, it can be hard to get and find help should they need it. Consequently, we wanted to create a product that can help the user receive the help they need.","The Collision Cap detects whether or not the user has suffered a traumatic head injury. If some impact has been deemed severe by our software, buzzers on the Cap will go off to alert others around them. Additionally, the Cap, through Bluetooth, opens our app and saves information like time of the impact, which can then be used by medical professionals. Our cap also monitors your pulse and alerts you if your pulse is too low or too high. Should the user fall unconscious, the app will detect this and send an SOS signal.","We started with testing the force sensitive resistors. We spent time figuring out how we can extract data and how to connect them to Arduino. We then added the HC06 Bluetooth module. We successfully connected our phones to it and were able to communicate back and forth. Afterward, we added more features like pulse sensing and buzzing on impact. Then, we planned out how the electronics would be placed on the headgear. To make more permanent connections and reduce the size of what is carried, we used a PERF board. We then assembled the final version. We used React Native on VScode to create the app.",,,,,"arduino, bluetooth, react, vscode",https://github.com/amberwang2/ideahacks-2024
IDEA Hacks 2024,NeedleKnows,https://devpost.com/software/needleknows,False,"Drug addiction is a critical medical crisis that’s affecting Americans everywhere, worsened by the increase in fentanyl in recent years. The issue is especially noticeable in densely populated cities like San Francisco where there has been an increase in the number of used and discarded needles that litter the streets. Every month there are 20,000 needles picked up a month by the San Francisco health and public departments (SF Chronicle). Contaminated, sharp needles can be dangerous to collect and make everyday life more dangerous for the homeless population, and thus we must come up with a safe and effective method for collecting these needles while city governments come up with effective support systems for the drug addiction crisis. Our group was inspired to design an autonomous robot to safely collect needles because we wanted to improve conditions in Los Angeles and San Francisco, cities we grew up in or close to.","Our robot NeedleKnows drives around cities and collects used needles to be safely discarded at a proper waste site. It begins gathering data about its surroundings with an ultrasonic sensor, then driving when it does not perceive anything in front of it. There is an electromagnet hovering close to the ground inside of the bot to pick up needles. When enough needles are collected to trigger the load sensor, an arm inside the bot moves the electromagnet with the needles attached and places them in a sharps bin. Additionally, one can drop a needle in from the top of the bot through a panel if desired. The sharps are then driven to the appropriate waste management site.","For the drivetrain we designed a two-wheel drive with a castor wheel in the front for balance. It’s controlled with a 5V relay and an H bridge. The electromagnet is attached to an arm inside the robot with a joint containing two servos that allow for two degrees of freedom. The chassis was designed in SolidWorks, is made of acrylic, and manufactured using a laser cutter. A load cell is attached to the electromagnet to determine when enough needles have been picked up. Lights were added to the bottom of the chassis for extra visibility at night. There is an ultrasonic sensor located at the front of the bot. Most of our internal mountain was done with various types of tape and adhesive glue.","Our first problem was determining whether or not a needle could even be picked up by a magnet, which was solved by finding a hypodermic needle for experimentation as a sample. We originally attempted to make our own electromagnet but quickly discovered that without an iron core, we were not going to generate enough force to lift the needles. Fortunately, we were able to acquire a different electromagnet. Moreover, our original chassis design was too limited in size to allow for the arm to swing across its entire travel arc inside of the chassis. We circumvented this challenge by increasing the dimensions of our robot, allowing for greater clearance.",,"We learned that creating custom electromagnetics is quite difficult as it either heats the resistor or the battery used in the circuit quite quickly. We learned that to power a strong electromagnet, servos, and a drivetrain much more powerful batteries are required. Creating a very clear work timeline from the beginning and making sure everyone on the team has the same priorities is also pertinent. Most importantly, we learned that friendship is key to building a fun project and ultimately having a meaningful hackathon experience.",,"arduinonano, electromagnetism, lasers, servos, soldering, ultrasound, wires",https://github.com/matthewguemmer/D.A.R.E.
IDEA Hacks 2024,Compost-Mate,https://devpost.com/software/compost-mate,False,"We represent Los Angeles Pierce College, an institution that has a deep history with farming and agriculture. Our project draws from this history and promotes the sustainable practice of composting.","Compost-Mate allows users to turn any type of bin into a smart composting bin. Using multiple sensor probes to track the moisture and temperature of different levels in the compost, allows users to gain real-time insight into the natural processes that foster the creation of compost. Users can access this information through a built-in LCD or online through a website.","With the power of WiFi-Enabled Arduino Uno, through multiple Adafruit MCP9808 temperature sensors and LM393 Soil Moisture sensors, our product sends real-time data to Google Firebase. 3D-printed brackets for each module were designed in Fusion 360, sliced on cura, and printed on an SV01 Pro.","Getting the Wifi Arduino connected to a network was difficult, and finding a way to secure the sensors onto the product but also have these electrical components exposed to moist soil without sacrificing their functionality was difficult.",,We refined our Arduino Skills by learning about I2C and wireless communication. We learned about Google Firebase and Svelte. We learned that composting is a much more intricate process.,Making Compost-Mate more compatible with being outside such as improving sensor housing durability and using solar would be the next steps.,"3dprinting, arduino, autodesk-fusion-360, cura, firebase, github, node.js, sensor, soil, soldering, svelte, temperature, vite, water.css, wifi",https://github.com/hkatzdev/IdeaHacks
IDEA Hacks 2024,"Red Light, Green Light (Sunflower Edition)",https://devpost.com/software/red-light-green-light-sunflower-edition,False,our flower :3,,,,,,,"arduino, c++, github, google, youtube",
IDEA Hacks 2024,The Recycling Bin Hat,https://devpost.com/software/the-recycling-bin-hat,False,"As we walked around our school campus, we noticed that non-recyclable items were often thrown away with recyclable items and vice versa. So we aimed to create something that would encourage students to recycle. Our device initially asks our users to input their login information. From there, they will dispose of their recyclable items in the bin. The number of items the user drops in the bin will determine the number of points that will be added to their account.",,,,,,,"c++, python",
IDEA Hacks 2024,Grass Buddy,https://devpost.com/software/grass-buddy,False,"We wanted to make a retro looking game console inspired by pokewalkers and tamagotchis with cute pets, and a special spin with community interaction and encouraging healthy habits.","Grass Buddy transforms daily walks into exciting journeys of connection and healthy habits. Using a cutting-edge 9 DOF IMU, it precisely monitors steps, allowing your buddy to become healthier, happier, and level up as you trek across town and interact with other players!","We used an arduino nano to control an OLED display, RFID reader/writer, and piezo buzzer. We designed it with easy of use in mind which is why we 3D printed a hand sized case that completely encases all of the hardware nessacary.",The arduino nano has only 2Kb of memory and unfortunately the arduino libraries to operate the OLED required a 8Kb back buffer in memory to write to it. So we spent a several hours writing a custom graphics library to draw to the screen from disk.,"We are proud of overcoming our time crunch near the end. We had several set backs due to software bugs, broken hardware, and bottle necks, however we overcame those challenges and managed to get a product we're proud of.","Through the development of Grass Buddy, we gained valuable insights into blending technology, gaming, and community engagement. Overcoming challenges taught us the importance of creative problem-solving and collaboration.","The future for Grass Buddy involves continuous refinement based on user feedback. We aim to expand features, enhance community interactions, and explore opportunities for integration with health and fitness platforms. Grass Buddy's journey is just beginning!","arduino, oled",
IDEA Hacks 2024,ECCPD,https://devpost.com/software/eccpd,False,"In my chemical engineering internship, I learned a lot about how different chemicals from refineries are produced and how they are emitted into the atmosphere. VOC (volatile organic compounds) is one of the main emission categories and imposes severe health risks on humans. We come into contact with VOC contents through various activities, such as gasoline, paints, and spray cans. They all contain chemicals like benzene, xylene, and toluene, which are all under the family of VOC and are carcinogenic.",Our project will detect VOC and CO2 contents in ppm (parts per million) in the surrounding air and alert the user that the environment is not safe to stay in for a prolonged period.,"We built the pollutant detector with Arduino Nano, SGP30 VOC & eCO2 sensor, and temperature & humidity sensor. We coded the circuit through Arduino IDE. We also included an OLED screen to display the necessary information.",One challenge we ran into was trying to get the OLED display to turn on. Another one was to have the SGP30 sensor to display VOC contents.,We are proud of being able to solve the turning on OLED display issue.,We learned new pins such as SDA and SCL and terms like I2C.,The next thing for ECCPD is to use an Arduino MCU that can connect to the internet and upload areas where it detects a high concentration of VOC and people can have access to it in real time.,"arduino, c++",
IDEA Hacks 2024,PEMDAS: Personality-Enhanced Matching Dating ASsistant,https://devpost.com/software/pemdas-personality-enhanced-matching-dating-assistant,False,"Loneliness amongst adolescents has skyrocketed in recent years, to the point where it has been declared a public health crisis in the US. Ironically, we have more technology than ever which is supposed to help connect us. Studies suggest that the problem is the replacement of in-person interaction with social media. Therefore, we sought to build a service which encouraged users to go outside and meet each other the “old-fashioned way,” while giving them the best chance possible of finding a compatible match.","The first step to setting up PEMDAS is to fill out the questionnaire online, which offers insights into your personality and gives you the chance to specify what exactly you’re looking for. Then, the webpage sends the results to your device over a serial connection. After that, you can take it anywhere you go, and it will scan for other PEMDAS devices in your vicinity. If you two have compatible profiles, both your devices will alert you about the other person, giving you a quick description of what you have in common.","The web page with the questionnaire was made with React. The device scans for other PEMDAS users nearby using Bluetooth, and they exchange profiles. The “algorithm” for determining personal compatibility is very simple, relying on a few essential conditions (gender, type of relationship sought) and cosine similarity for the rest of your answers.","We had bad batteries and the devices did not always get enough voltage to operate for more than a minute at a time. Additionally, there were some software bugs that came up regarding bad memory accesses, but we managed to fix them. Additionally, we lost a group member the night before the hackathon.","Our devices actually worked in a field test, and we completed all the features we wanted for the MVP.","We learned the details of the Bluetooth Low Energy protocol, the Web Serial API, and one group member learned how to use React JS.",The current package is kind of bulky and it would be much more convenient to have it fit in a watch form factor. We could improve the efficiency and battery life by using a larger battery and by using a switching converter in place of the LDO on the microcontroller board. We could also use a better display which would allow more information to be displayed. A RTC (Real Time Clock) could also be integrated so it could double as a watch.,"bluetooth, c++, platformio, react, webserial",https://github.com/scoliono/ideahax24
IDEA Hacks 2024,Go Save Grass,https://devpost.com/software/placeholder-hmzxlg,False,My fondest childhood memories were playing games with my cousins. I was inspired to create a game to build community between people.,It provides entertainment.,"I used electronics, an Arduino, and laser cutting to construct the final product.",My hands were too shaky to solder the wires.,I thought I wouldn't have a functional product and I did!,Sleep is good.,Implementing the touch sensor for reloading water.,"arduino, solidworks",https://github.com/CrimsonFire1/IDEAHacks-2024
IDEA Hacks 2024,Hear Me Out: A Bruin Espionage,https://devpost.com/software/hear-me-out-a-bruin-espionage,False,,"A team-based game with the primary focus of engaging players in activities around UCLA and to learn UCLA lore bystepping outdoorsand into nature. Players gather together and separate into two teams. They must split up and find randomly selected “EMP” devices hidden at well-known UCLA spots. Each team is given clues via a main display that also shows a countdown timer and the number of EMP devices left. They must race to find these devices as soon as possible with the quirk that the clues they receive are not their own clues but theother team’s. Using afaulty radio(one that intentionally flips on and off to make communication more difficult), they have to communicate with the other team to lead them to their destination. After the first clue, each team can hold onto the button on their gadget to reveal a hint. Once both teams have made it to their respective device, they must both swipe their spy gadget over the device at the same time (within a certain threshold) in order to deactivate it and move onto the next EMP device. After all devices are deactivated, the players (spies) win the game. Iftime runs out, they lose to USC.","The backbone of our idea was to build personal handheld devices that would allow teams to play our game. We chose theESP32family of system on a chip microcontrollers because of its onboardWiFiandBluetoothcapabilities, giving us easy access to wireless device-to-device communication. The microcontroller communicates with the display andRFID readerthrough a Serial Peripheral Interface, and a relay was used to program the transmission of the walkie talkies. For inter-device communication, we decided to have one of the devices be the “controller” device which sends and receives information to the other device through web servers. This way, both devices can keep track of which locations have been visited, what the total elapsed time is, and when to start/stop the game. 
Each ESP32 is equipped with a 2.8"" LCD graphical display, an RFID reader, and a walkie talkie for communicating with the other team. We used relays to implement the one-way walkie talkie toggling feature to allow forlimited communicationbetween the teams, and unique rfid tags for the “EMP devices” that are verified upon scanning.","As a team, we fought through many challenges during this project. We all came into the hackathon with a general idea of what we had wanted to gain from the experience, but none of us had a specific idea in mind that we wanted to create. As such, we spent most of the first couple hours brainstorming ideas together. We struggled to settle on a concept that would not only incorporate some interesting pieces of hardware, but also bring together communities and friends in its implementation. The game that we present today is the culmination of many rounds of revisions, edits, and changes to our original design. From a technical standpoint, we encountered many more issues that resulted in hours of troubleshooting and engineering workarounds. One notable example being that we initially wanted to use direct device-to-device communication over a common LAN network. However as we soon realized, our ESPs were only allowing for one-way communication, which made synchronization impossible. As a workaround, we hosted a web server on each of the devices, used HTTP post requests to send game-critical data to the web, and read the website information with HTTP get requests. This way, both ESPs could both send and receive any information they needed. This ended up being more reliable than sending information between the 2 devices directly as it decreased interference significantly and removed the need for a central wifi connection that was shared between the 2 boards, allowing them to be used anywhere. During development of the various components of our devices, our team split up to work on the individual parts of the final handheld device. But when we attempted to combine the LCD display and RFID reader together, we ran into another issue. Both devices were configured to communicate through SPI, and we initially wanted to let them share a common SPI bus. However, we soon found out that the wrapper libraries we were using to configure our devices would intrinsically control the Chip Select pin, not allowing us to determine when each device would use the bus. Ultimately, we hit a stroke of luck and were saved by the capabilities of the ESP32: there was a second, lesser known SPI bus that we configured to drive the LCD display.","We are proud that we planned and implemented our primary idea to the best of our abilities. Within the hardware realm, we are proud to have brought a variety of interworking components together into one device. Those being the ESP32, LCD display, RFID, and radio. From there, we’re proud to have made a compact and usable design where we could play the game together–it was a lot of fun running around and attempting to communicate with each other over the faulty radio system.","Within our team, we learned a lot from each other. Each of us came into the event with different skills which we were able to share with the team. We learned to solder, breadboard, program wireless communications, write to graphical displays, build timers for voltage relays, hack radios, and much, much more. We also learned that the ESP32 is an amazing development board for a wide variety of applications.","For starters, we’d like to add more locations to our roster, each with their own set of clues. Because the walkie-talkies and ESP32’s had limited range, we would implement a new way of communication between each ESP32 so that each team could speak and deactivate their respective EMP’s at opposite ends of the campus–which was not possible with our current setup. Still in development is a leaderboard system in which teams with the fastest times are placed at the top of the ranking. An implementation of this, we hope, would lead to more involvement from the community across UCLA. Another feature we would love to add would be images of locations that teams would need to describe to each other. Rather than a written clue, these images would serve as marking locations that would slowly lead teams to the final destination, given that the other half of the team is talented at communication.","arduino, c++, esp32, iot, lcd, radio, relay, rfid, server, spi, web","https://github.com/sethferrell/idea-hacks, https://docs.google.com/document/d/1yy6_-Os2J6VGbX7sH1tKwUtuG7Rf6PeJl1XpabVnCL4/edit"
IDEA Hacks 2024,The Helping Cane,https://devpost.com/software/the-helping-cane,False,The cane is inspired by the lack of resources for making things accessible for people. An improved cane can help a visually impaired person navigate better; the world becomes a better place.,"It is a cane that is used by blind people, but improved it many ways. Our helping cane alerts visually-impaired people about the obstacles in front of them. It communicates with the users through sound alert. It can help them walk with less accidents. The cane can sense a puddle preventing blind users from stepping onto wet surfaces. It also alerts objects that are 2 feet or less away, so they do not run into them.","We used Arduino Nano, and Water Level Sensor to detect the water level, and utilized Ultrasonic Distance Sensor to detect the object 2 feet or less away. There is also a switch for on/off, and everything is powered by a Duracell Battery. We programmed a speaker to give two different frequencies for the water level and the distance calculator. We used 3d printing to make the handle and the tip of the cane. We used 0.2 inch thick wood for the body of the cane and used a laser cutter get the desired shape.","Since there were no tubes available we had to laser cut four rectangular walls with edges that are like puzzle pieces so they can fit together. We also had to cut the wood twice because the wood was not labeled correctly therefore the pieces did not fit well. Since the wood pieces were slightly loose we had to use bolts, tape, and hot glue. We had a faulty distance sensor which we had to switch out because it was very inaccurate.","The 3D printed parts came out exactly the way we wanted them to, and they also fit perfectly. We did not have major wiring issues; most of the circuit worked perfectly. We also are proud of the design of the ridges of the wood pieces, to make them fit together.",We learned about how to utilize different types of sensors and how to wire them correctly. We also learned the necessary steps to take in order to make your model into reality.,We want to include a spring in the future in order to prevent the cane from jabbing the user in the rib.,"arduino, speaker",
IDEA Hacks 2024,bean,https://devpost.com/software/temp-gf0er2,False,Inspired by Tamagotchi keychain-sized digital pet and its gamification of maintenance tasks.,"Welcome a new virtual pet to your family, and don't forget to feed it! It only eats real life grass (or at least, the virtual idea of grass), so make sure to touch grass often! The project gives you a pet and the task of maintaining it. Over time, the happiness of the pet will decrease, but can be reverted if pictures of grass are taken for it using its build-in camera.","The project uses a Raspberry Pi 4 to run game code, utilizing a connected display and integrated speaker to show the happiness of the pet. Detection of grass in pictures is powered by a custom-trained AI model, utilizing manually labelled images of a preexisting landscape dataset.","Packaging was one of our big challenges. Due to our design decision to run an AI model locally on the device, we needed to fit a full Raspberry Pi 4 inside. We decided to try and keep the model similar to the original keychain form factor, so we needed to compactly fit all the wiring inside. We solved this challenge by making use of two subframe assemblies within the main case body, and creating special wires that took up less space by disassembling USB cables and making our own.",,,,"onns, python",https://github.com/Illuminati-CRAZ/Bean
IDEA Hacks 2024,GPT M.D.,https://devpost.com/software/gpt-m-d,False,"Am I healthy? Is this pain I'm feeling in my side normal? Is my heart rate to fast or too slow? Is my blood pressure normal? These are questions that many people may find asking themselves everyday. Wondering about your health is natural and wanting answers on how to improve your health can help everyone live their best lives. But getting to a doctor's office or even a Telehealth appointment can be costly and wait times to meet with doctors may be very long. For people that want some reliable, verified health advice (no, not searching up symptoms on Web MD) a little sooner than their scheduled appointment or just want to check in but maybe don't have the money to pay health insurance fees, there needs to be a way for anyone to access reliable health advice, when it is convenient for them. Large language models (LLMs) have been on the rise lately and we believe it is important to explore ways that new technologies can improve education of and provide more health data of patients, thereby improving the health care that doctor's provide to their patients and reducing the probability of mistakes in routine medical visits.","GPT M.D. is a kiosk that gives understandable and comprehensible health advice based on a person's medical history, measured vitals and health data powered by AI. The LLM will give the person analysis on their vitals based on health data and medical history, recommend and provide information about a doctor's visit, if necessary,  and answer any other questions that the person may have. This kiosk would be available to use for free at pharmacies.","The flask backend queries a custom GPT given user medical records as a knowledge base. The front end is a react application powered by developed using Next.js served via the Vercel hosting service. The application is deployed via Github actions to Vercel. Another flask server is used for the Raspberry Pi to gather data to serves in the application. Data is send between layers via HTTP.
A fingerprint sensor is used to identify the person and pull specific patient data. A pulse sensor, temperature sensor and blood pressure monitor gather vitals data from the patient. These sensors communicate with the Raspberry Pi via I2C, UART and USB communication protocols. 
The enclosure was made using laser cutting.","Software side: Integrating front end and back end which are both complex. Finding the right architecture was difficult and we ending up doing a lot more networking than we initially expected.
Hardware side: Hacking into a BP monitor's communication protocol was very difficult. We had to test every single register on the EEPROM IC in order to find the data we were looking for because we couldn't find the IC's datasheet anywhere. The Rapsberry Pi's Tx and Rx GPIO pins gave us lots of trouble and we ended up having to use a UART to USB driver to receive the data differently.
Laser cutting was a long and tiring process.",,,,"gpt, i2c, machine-learning, next.js, raspberry-pi, react, uart",https://github.com/akhilmanthina/gptmd
IDEA Hacks 2024,iThirsty,https://devpost.com/software/ithirsty,False,"    iThirsty Developed by
Jerard Agravante
Nathan Chen
Simba Chen
Victoria Choi IOT Plant watering system. iThirsty is an automatic plant watering system that monitors the moisture levels in the soil and determines whether to water the plant or not depending on that plants ideal watering conditions. iThirsty is connected to the internet through wifi allowing you to monitor the plant and its health through our mobile app. Our app will allow you to keep track of the plants moisture, temperature, and water level through visual analytics. The iThirsty mobile app also serves as a comprehensive plant information hub, providing valuable insights into different plant species and their care requirements. iThirsty will also notify you if the water source is reaching a low level and whenever it decides to water the plant through a ringer. It waters the plant by directly feeding water into the soil. This is a more sustainable alternative to commonly used sprinklers, which waste a lot of water.",,,,,,,"arduino, expo.io",
IDEA Hacks 2024,Boximizer,https://devpost.com/software/custom-size-box-cutter,False,"Receiving a shoe-box-sized package all for a pack of lead! There are 22+ billion packages delivered a year, creating millions of tons of waste every year. We can reduce the amount of waste by only using as much packaging as the item requires.","Users can input the dimensions of the item (l x w x h). According to these measurements, we cut out the perfect box size with minimal waste. Our machine will also cut perforated lines for the fold lines of the box.","After defining our product, we got straight to CADing. We needed to have accurate measurements for parts as certain parts needed to mesh with others. Our machine would need four servos all controlled by an Arduino. The servos enable the blade to move in both the x, y, and z planes. All the motors were propped into position with 3D-printed beams. To create perforated lines, we would raise the blade slightly higher than when cutting. The Arduino was coded to collect the dimensions, calculate the minimal needed package dimensions, and cut out a box correspondingly.","Getting all the 3D-printed parts to fit our motors or other parts well! As we hand-measured materials to determine the sizes, we often came across a piece that was just a tad big or a tag too small to fit well with another. Calibrating our servos was another huge challenge. As we used continuous servos over positional servos, it was difficult to get the servos to turn at an exact angle. Hence it was hard to get it to move a precise number of inches.",Pushing through the whole hackathon!,To come up with a sound design before we start building so that we minimize the parts that have to be redone.,"Ideally, the users don't have to manually enter dimensions. Hence, we want to make it so that a link to the item can be used to extract dimensions and used for our calculations. Additionally, though we are already minimizing the amount of packaging used for an item, we want to take it a step further and fit more cut-outs in the areas surrounding a cut.","arduino, c++, cad",
IDEA Hacks 2024,VisionMate,https://devpost.com/software/visionmate-btxw05,False,A hope to make public spaces more accessible to visually-impaired users in an innovative way. Hope to improve patient safety for visually impaired individuals in hospitals by reducing fall risk.,Uses ultrasonic technology to detect distance from surfaces and large objects and indicate to users an approximate distance.,Implemented an Expo React Native app. Built a speaker and sensor ultrasound technology.,Nuances with the path variable. Expo go limitations. Lack of bluetooth compatibility with React Native. Hardware sensitivity.,Researched how to make an accessible interface for blind users and implementing text to speech and double click features that are not native to React Native.,"How to use React Native. Bluetooth versus wifi connectivity pros and cons. Development builds, including understanding when to use Expo Go.",Improving wireless connectivity and I/O with the sensor device.,"arduino, esp32, expo.io, html, javascript, react-native, speechapi",https://github.com/chrisgharghour/VisionMate.git
IDEA Hacks 2024,Collective Cadence,https://devpost.com/software/remote-music-transcriber-and-synchronizer,False,"We were inspired by our own experiences jamming out to music on our own, but being unable to easily transcribe it in the form of sheet music and share with other people.",,,,We're proud of how this project increases both accessibility and community. We aim to reduce the skill barrier in transcribing and sharing improvised music and jam sessions. This product is unique in the field of music transcription software because it features customizable rhythm correction and a livestream feature to better connect with other musicians.,,We hope that we can fully integrate both musical instruments into the wireless remote protocol; which was a barrier we faced from the parts availability at the hackathon.,"arduino, esp32, lilypond, machine-learning, matlab, midi.js, websockets",https://github.com/SeanTang001/harmonizer
IDEA Hacks 2024,Patient Focused Pill Dispenser,https://devpost.com/software/patient-focused-pill-dispenser,False,"Remembering and keeping track of pills is challenging. Using pill boxes can be time consuming and can lead to medication mistakes, especially when patients have complex medicine regiments.","The device automatically dispenses pills based on a programmable schedule. A light turns on and a buzzer makes noise to alert the user when the pills have dispensed. If the pills are not taken, an alarm will go off again after a set time period to remind the user to take them. The device can dispense two different medications for a morning regiment and night regiment.",The device was designed in SolidWorks. The base of the device and the pill ramp were 3D printed PLA. The housing and motor mounts were made of laser cut acrylic. The motor mount was put together with epoxy. The housing and motor mounts were secured to the base with screws. The device uses an Arduino Nano Microcontroller to run necessary software.,"The first challenge we faced was with the sensing pill removal. We intended to use a force sensor to detect the pills, but the force sensors required significant forces. We realized they would not be able to sense the pills. We pivoted to the ultrasonic sensor and instead recognize when a hand enters the pill tray to determine if the pills were taken. Another challenge was during our first test dispensing a pill. We found that when one pill was dispensed, all the other pills would fall out after it. We expected the pill friction to be high enough that only one would fall out at a time, but that was not the case. To address this issue we added small flaps to the pill exit to prevent more than one pill from dispensing at a time.",We are proud of the successful implementation of our electrical systems and the device programmability. We are also proud of the rapid design iteration done on the dispensing mechanism and flaps.,"Through this project we learned how to collect user needs and formulate design requirements to create a patient safety device. We also learned how to translate these design requirements into multiple coordinated systems, integrating sensors, motors, LEDs, and buzzers.","There are many improvements that we would make in a future iteration. The primary design improvement would be to make a motor-driven door instead of a door that relies on gravity. This would add to the reliability of the device because the door could be raised when the pill is pushed out and closed rapidly so that other pills do not fall out. Another addition to the device would be to add more pill chutes of varying sizes so that the device can accommodate more pill sizes and shapes.  We also would make the device food safe. Another improvement would be a phone app that connects to the device and reminds users to take their pills when the pills are released. Finally, a locking mechanism could be added to the box that the pills drop into if any of the pills are addictive drugs. This could be important if the person is in a shared household. It would make it so only the person prescribed the drug would have access to the medicine.","arduino, platform.io",
IDEA Hacks 2024,"Garvey - the half garden, half robo-pal rover.",https://devpost.com/software/garvey-the-half-garden-half-robo-pal-rover,False,"As engineers, we rarely get to enjoy basking in sunshine or feel grass softly brushing against our palms. Most of our time is spent in lab, makerspace, and the classroom. If not these places, we're holed up in our dorms catching up on lost sleep. However, the human need to connect with nature still pertains. That's what inspired this project.","Garvey is a community garden that roams the halls bringing the outdoors inside. It is powered by a Raspberry Pi, some DC motors, and a lot of electrical engineering prowess. Garvey cannot function without its community either though. Garvey is fit with a soil sensor that lets students and other caretakers know when Garvey needs some care (in the form of water or temperature). Garvey brings joy wherever it goes from its cute LCD smile.","The chassis made of acrylic was designed on Fusion 360, laser cut to shape, and glued together. Next, the circuit that consisted of 4 DC motors, soil temperature & humidity sensor, LCD color display, and ultra sonic sensor was wired to the GPIO pins of raspberry pi. Then, through VS Code SSH extension, the Raspberry Pi was wirelessly programmed to move forward & backward, display full color GIFs or images, collect soil humidity & temperature data in real time, and stop moving forward when detecting obstacles. Lastly, the whole robot was put together to conceal the circuits, prop up the display in front of the chassis, and secure the structure.","This project had a large scope to begin with. We weren't sure we were going to achieve everything we sought out to at first.  However, everything came together in the last moments, from the LCD that seemed impossible to program, to the intricate assembly process that tore our wires out more than a dozen times over. Our team had the ability to take on adversity with a smile, and keep on trying a thousand more times, especially with cute Garvey cheering us on with its little dance.","Our team was an amalgamation of many majors with varying backgrounds in engineering. For many of the parts we used, it was our first time ever working with them. However, we learned to lean on each other when we weren't sure of what we were doing. We are proud of the collaborative process we had even while splitting up much of the work.","We learned a lot about robotics, using Python virtual environments, and some of the best areas to collect dirt at UCLA :)","Garvey's ready to roam far and wide. Hopefully, we can have a Garvey test-trial in the dorms, to see how communities react to his presence. Beyond befriending students across campus, Garvey can be a great friend and bring smiles to students in primary/secondary school, and even for children in the hospital too. Garvey will grow as it meets new people as well, including being able to walk with anyone at their pace, give vocal affirmations to everyone, navigate by itself, and so much more.","adafruit, circuitpython, json, python, raspberry-pi, seesaw, ssh, vnc, vscode",
IDEA Hacks 2024,Health and Exercise Regulation Buddy - H.E.R.B.,https://devpost.com/software/health-and-exercise-regulation-buddy-h-e-r-b,False,"HERB was brought about by a love for fitness and our campus! Everyday we find ourselves walking miles on end to and from classes, clubs, meeting with friends, and wherever we call home, we conceived HERB as a companion for those long walks.","Simple, HERB counts your steps and reacts to your progress! It aims for you to reach 500 steps every three hours to keep you moving! Exceed that goal and it'll reward you but fall behind and HERB's health will begin to decline. The whole way it'll respond to your progress with a varied series of chimes to keep you motivated.","Since we wanted people to fit HERB in their pocket we wanted a small form factor with all of the circuitry fitting onto a single breadboard. Much of the development time was spent in the Arduino IDE making sure steps were counted and the proper responses were recieved. HERB gained features as time went on, getting a voice with the addition of a passive buzzer and a face with a simple small display.","As a team of first time hackers our problems were numerous. Our first major challenge was even getting our microcontroller to work as it wasn't recognized by our development environment but we remained flexible and changed hardware on the fly. Learning new libraries also posed quite a challenge as this was the first time any of us had worked with sensors and microcontrollers at this scale, everything was new to us and we made full use of all of our resources but we had fun the whole way!","We're proud of HERB's varying personality and the way it behaves in response to the user's actions, we didn't want just a number giving HERB's condition but his dynamic face gives him more of a personality.","We learned in hackathons you always have to stay on your feet and be flexible, pick and choose your battles with hardware and software, and always be willing to listen to your teammates, they have good ideas! Don't start with too big of an idea, start with something small and add features once the basics work!",Our next realistic step would be to give HERB a shell and integrated battery. If we want people to take it walking it should probably be pretty solid and as it stands he's pretty exposed! Additionally the need to plug it into your own portable battery source means it isn't the most seamless use.,"adafruit, arduino, c++",
IDEA Hacks 2024,Eye-as: Assisted Sight,https://devpost.com/software/project-name-tbnuld,False,"With its sonar sensor always facing forward, Eye-as' cap accessory is always on the lookout for walking hazards. Eye-as' bracelet accessory listens for a signal from the cap accessory and buzzes when a hazard is detected. The bracelet (left) and cap (right) accessories shown side-by-side. In action, the two modules interface via Bluetooth. Close-up of the front of the cap accessory, showing the sonar sensor mounted on a breadboard which can be rotated by the servo motor. Close-up of the back of the cap accessory, showing the connections of the Arduino, accelerometer, servo motor, and Bluetooth module. Thewhite caneis an effective and beloved tool for the visually impaired. It allows the blind to sense their surroundings through tactile feedback, giving them a greater degree of independence in commuting to work or navigating their neighborhood (Perkins Institute). We were shocked to learn that, despite the white cane's benefits, less than 10% of the visually impaired use one on a daily basis (Perkins Institute). For one, cane users can face a steep learning curve (Yin), which can be intimidating without mobility training. Secondly, canes can't detect objects that hang at head height (Yin), like raised decks or tree branches. A secondary system, like a sonar-based sensor, would provide an additional layer of protection. With that, we set out to create Eye-as to help novice and experienced cane users detect upper-body obstacles. We had two principles in mind: Keep things simple -- and keep the white cane.There have been many attempts at making electronic ""smart canes"" that augment the standard white cane with additional sensors, but none of them have gained traction (Yin). In fact, electronic feedback could interfere or distract with the natural tactile feedback of the walking stick (Yin). Aim for cost-effective and scalable.While white canes cost anywhere between $20 and $80, many electronic canes can set someone back by hundreds of dollars (Yin). Any extra bulk the visually impaired have to carry should also be minimized. We're confident that our Eye-as breadboard prototype can be made into a small, wearable PCB-based device under $80. When we first decided on addressing the theme of accessibility for the visually impaired, one of our first ideas was to modify a walking stick with ultrasonic (obstacles, stairs) and water (puddle) sensors. However, after some online research, we saw how non-electronic canes were already sufficient for the problems the electronics would aim to solve. Instead, we sought to instead provide a secondary aid that could act alongside the already established walking stick. Blind cane users generally sweep the ground more than the space in front of their eyes for danger, making them vulnerable to upper-body obstacles. The wearable accessories we propose use a sonar sensor mounted on a visually impaired person's head to protect against these hazards. Eye-as' cap and bracelet accessories are in constant Bluetooth communication. When the cap detects a potential obstacle within a certain range, it sends a signal to the bracelet. Upon receiving a consistent signal from the cap, the bracelet then emits a sound using an onboard buzzer, alerting a blind user to a hazard. In addition to this, Eye-as' cap makes use of an accelerometer and a servo motor to always position the sonar sensor forward -- perfect for maximizing the sensor's utility and handling the many different walking postures cane users can have. To better the accessory’s effective sensory field, we mounted our sonar sensor on a servo in tandem with an accelerometer. We had not used a servo with a microcontroller before, so this experience would be valuable for future robotics projects. For instance, we learned the difference between the non-continuous and the continuous servo. Also, we learned how to read values from the accelerometer to stabilize servo rotation. This pairing has application in robotics systems, particularly those that demand precise motion. This hackathon was also our first experience with setting up wireless connections. Using the HC-05 Bluetooth chip, we studied how to set up a master-slave relationship between two Bluetooth modules, allowing us to get multiple Arduinos to talk to each other. This involved executing AT commands to set up a Bluetooth pairing and working through the unexpected surprises of the wireless world. The HC-05 chip setup also included soldering - we soldered a logic level converter to reduce the Arduino’s 5V transmission signals to the Bluetooth module’s 3.3V demands. This endeavor into wireless technology offers exciting possibilities for future Arduino projects, as we now can bind multiple sensors wirelessly and at a distance. Less bulk equals better wearability: The breadboard prototype of Eye-as is pretty unwieldy -- not ideal for wearable technology. Luckily, most of its size comes from using boards for components such as the accelerometer and microcontroller instead of individual components. In fact, with a PCB-based design rather than a breadboard-based one, we believe Eye-as can be shrunk to a size capable of fitting on every hat and wrist. The many possibilities of Bluetooth: Because of its Bluetooth compatibility, Eye-as can be expanded to many different applications. Using tactile feedback (like a phone vibration) in addition to an audible buzzer could improve Eye-as' usability for a wider visually impaired audience. Aneyas(pronounced eye-us!) is another name for a young falcon. We thought the idea of Eye-as' cap accessory on a hat was similar to a falcon perched on a ledge -- keeping a steady and vigilant eye for any hazards. Perkins Institute for the Blind. ""10 fascinating facts about the white cane"" (2024). [https://www.perkins.org/10-fascinating-facts-about-the-white-cane/] Yin, Steph. ""Why is creating electronic canes for the blind so hard?"" (2019). [https://whyy.org/segments/why-is-creating-electronic-canes-for-the-blind-so-hard/]",,,,,,,"adxl345-accelerometer, arduino-nano, c-programming-language, giant-continuous-rotation-servo, hc-05-bluetooth-module, logic-level-converter, sonar-range-finder",
IDEA Hacks 2024,Doornamo,https://devpost.com/software/doornimo,False,"Doornamo Waveform produced by Doornimo's motor Doornamo + rectifier circuit Doornimo aims to utilize the every day actions of human beings (such as opening and closing a door) to produce energy. Its set up includes a motor anchored to a door frame as well as a wheel against the ground. The wheel spins the motor's spindle, producing a voltage potential difference between the motor's pins. These pins are connected to a rectifier circuit so that Doornimo generates a positive voltage both opening and closing the door. Once rectified, the voltage is then used to charge a capacitor and store the generated energy. We built this project through the use of 3D-printing as well as bread-boarding for the electronics. We were challenged by the mechanics of this design. It was difficult to integrate different aspects of our design into one functioning object. Despite the challenges, we're proud to have made a stable proof of concept, capturing waveforms of the energy Doornamo has generated. Future iterations of Doornamo would include a more simplistic design, reducing the wheel count from three to one. We would also want to look into ways to improve Doornamo's efficiency, to produce more substantial amounts of energy.",,,,,,,"dc-motor, rectifier",
IDEA Hacks 2024,Little Free Libgen,https://devpost.com/software/little-free-libgen,False,"Over winter break, one of our team members went to a neighborhood to look at the Christmas lights, and stumbled upon a mailbox looking object that contained books in front of a house. His aunt explained that it was a Little Free Library, whose premise is that people bring a book, then take a book from the box.","The library box itself is equipped with a locking mechanism. To open the library, you use the app we created and scan the book you brought. After taking a book, you close the door on the library and scan the book you took out, which would trigger the door to lock. You get points for every ""transaction"" you make at a library, and the app has a leaderboard to see the most active members of the community. Along with the scanning feature, the app also has a map that allows you to see nearby Little Free Libgen libraries. In addition, you will be able to see the books inside each of the libraries.","The box was designed in SolidWorks. The outside of the box was laser cut from acrylic, while the rest of the parts were 3D printed. We used an arduino nano with a wifi chip to control the lock of our library enclosure as well as drive the OLED display. For the backend, we used a MongoDB database. We built the mobile app using react-native, and used a barcode library for the scanner. We used an ISBN API to fetch the book title, author, and cover image.  Furthermore, we connected the frontend and the backend through express. The mobile app is able to connect to the arduino module through HTTP POST requests over wifi.","We wanted the box be easily taken apart, since we had a difficult time accessing the inside of our project last year. Designing connectors that made the roof of the box easily detachable but still effective  was difficult, while not damaging its looks.
We planned to make the outside of the library out of wood, but when we arrived at the Makerspace, there was only enough wood to make half of the outside. We decided to change the outside to acrylic, which took some time because of their different thicknesses. We also ran into some issues relating to the software development process. We had difficulty finding compatible libraries to use for React-Native in particular some React libraries were only compatible with the web browser React, and not React-Native. Things such as editing fonts as well as importing libraries/package dependencies often resulted in conflicts since the packages themselves were often deprecated.","We're proud to have a complete product that combines both software and hardware. When we first came up with the idea of my little libgen, we had a clear picture in mind, and we were able to deliver the project with very few compromises. While we definitely had some struggles while developing, but we overcame them together as a group.","We learned many technical skills such as using react-native and connecting to an arduino via wi-fi. But we are equally as proud of our personal growth: we learn effective time management, team organization, and how to manage the pressure of a hackathon environment.","Our next step would be to make a review feed. Users could write reviews on the books that they read and they could see the reviews of other readers in the area or people they follow. Users could also search up a book to read more specific reviews.
Another feature we could add is a point redemption program, where users can redeem the points that they get from exchanging books for prizes, such as cool bookmarks, gift cards to local book stores, or actual books.
Another feature we could add is Books of the Month, where we would choose 4 books to feature monthly. There can be a discussion forum where people can discuss what they like/dislike about the book and there will be a table to see of how many people have read each book and a poll to see which one is the most popular.
Yet another feature we can add is the bookle, a daily book trivia game. Users will get 6 guesses to guess the book, and after each wrong guess, users will be given hints on what the book could be, such as a quote from the book, a character's name, the author, etc.
On the physical side, the next step would be to replace the acrylic with wood, and the 3D printed parts with machined parts, to make the structure more sturdy. In addition, we would increase the display screen so that it is an appropriate size compared to the box.
We could add more information on the display screen. It could display how many transactions the specific library has accumulated, and it could display how full the library is.
We could also add a scanner on the physical library so that people don't necessarily have to have their phones or an account in order to participate in the exchanges.
We could also engrave instructions on how to use the Little Free Libgen on its side. Almost everyone who sees the box in the real world wouldn't know what it is or how to use it, so having physical instructions on it would make it more accessible.","arduino, c++, isbn-db, javascript, mongodb, react-native, typescript",https://github.com/ajtadeo/little-free-libgen
IDEA Hacks 2024,Dust Dolphin,https://devpost.com/software/dust-dolphin-fviawt,False,My family has weak lungs,Provide clean air to facilitate the development of healthy lungs through notifications of when to clean ur room through a website,Using an esp32,Not being familiar with syntax and losing lots of time because of debugging issues,Making something out of nothing,"Patience and the value of every result, also how to navigate the internet for resources",The world,"arduino, c, firebase, javascript",
IDEA Hacks 2024,ECHO: Enhanced Communication Hearing Oscillator,https://devpost.com/software/echo-enhanced-communication-hearing-oscillator,False,We want to empower the community by increasing access to the experiences that life has to offer through the use of connecting technologies.,"The ECHO bracelet has three features to help those who are hard of hearing including a metronome, morse code translator, and music interpreter.","The ECHO bracelet was built using an Arduino Nano, transistor, and vibration motor.",Getting the vibration motor to be controlled by the microcontroller proved to be difficult due to power constraints until we added a transistor.,We are proud of having a working product that is inexpensive and intuitive to use.,The combination of software and hardware for embedded systems can prove to difficult yet rewarding to develop.,We want to apply ML models to enable the use of sound and phrase alerts to help those hard of hearing know when an important audio event has occurred such as a bus stop announcement.,"arduino, c++, python","https://github.com/IdeaHacks2024/ECHO, https://docs.google.com/presentation/d/1q6A8gcRu9KjJ0YxXY-ysIrMz9bKHyEjt8Bx0nMohTIM/edit?usp=sharing"
IDEA Hacks 2024,Lithophane Lightup Fixture,https://devpost.com/software/lithophane-lightup-fixture,False,We wanted a personalizable decoration that you could put in your room. This eventually grew into the fixture that it is today.,"The fixture funnels light from an LED strip through a lithophane (a 3D-printed picture) to create a glowing effect. The user can cycle through different color settings and adjust the brightness of the image using the controls on the box. The light only turns on when the environment is dark in order to get the best glowing effect, otherwise it will turn off to save energy. The user can swap out the lithophane in order to personalize it for themselves.","The enclosure, frame, and lithophane were 3D printed. The rest of the circuitry uses basic sensors, jumper cables, and an Arduino nano.","Programming the LED strip in order to get it to work properly was probably the hardest part of the project. The photoresistor to detect darkness was also hard to hook up properly. Both of these were difficult because we had no prior experience with them, but we were able to follow it eventually using documentation online.","We were finally able to get it to work at the end of it and it looks super cool!
Anthony was able to CAD the box with minimal prior CAD experience!",,,arduino,
IDEA Hacks 2024,Go Towards the Light,https://devpost.com/software/tbd-5b4clx,False,"Wristwatch circuit Chair circuit Newer generations are increasingly spending more time indoors than health experts recommend, which could impact physical and mental health. Our invention will drive users to get off their chairs and experience the outdoors more. We implement two circuits, one that stays on the user's workstation and one that they wear like a wristwatch. When the user has been sitting for too long, a high-pitched buzzer sounds to incentivize the user to go outside. When the user has been outside for long enough, as determined by the outside light intensity, a light on the wristwatch will turn on. The user can hold the light to the circuit with the buzzer to turn the buzzer off.",,,,,,,"arduino, firebase, wifi",
IDEA Hacks 2024,Score Master,https://devpost.com/software/score-master,False,"The inspiration behind ""Score Master"" derives from a passion to amplify community engagement and enjoyment through an inclusive and accessible platform. Guided by the spirit of ""Go Touch Grass,"" the project was conceived to create a product that not only promotes outdoor activity but also cultivates a sense of unity. Inspired by the vibrant moments of a basketball game—where people from diverse backgrounds come together to share joy and create enduring memories—we envisioned a straightforward yet impactful solution. By integrating technology, we transformed ordinary basketball hoops into smart scoring systems. The goal was to provide a universal and affordable experience, allowing individuals of all backgrounds to participate without financial constraints. ""Score Master"" embodies our dedication to inclusivity, uniting communities through the shared love of sports while embracing the essence of outdoor enjoyment and social connection.","""Score Master"" revolutionizes the basketball experience by introducing a smart scoring system that seamlessly integrates with any hoop. Our sensors, easily attached to basketball rims, detect successful shots and transmit the data via Bluetooth to a user-friendly mobile application. Developed with React Native and Expo, the app provides real-time score updates, turning ordinary games into spirited competitions. The project's focus on the ""Go Touch Grass"" theme translates into fostering community engagement, bringing people outdoors, and promoting social interactions through the universal language of basketball. Every basket becomes a moment of shared joy, building connections among diverse individuals. The product not only enhances the thrill of the game but also serves as a catalyst for community bonding, exemplifying the power of technology to create accessible and enjoyable experiences that transcend backgrounds and bring people together in the spirit of friendly competition and fun.","Regarding the firmware, we needed a microcontroller which supported bluetooth. For this we settled on the ESP32 Devkit V1 which allowed us to program using the arduino esp32 bluetooth library.  Additionally we used PlatformIO as opposed to Arduino IDE for flexibility. To detect shots, we used a 3.3v distance sensor along with its associated library. Additionally we used a physical OLED display to show the current supplemental on board score. For this we used the Adafruit SSD1306 library. We also had to implement our own algorithms to increase the reliability of the score detector. For the mobile app, we decided to use the Expo framework. We realized we needed a way to communicate with the MCU, so we settled on using Bluetooth for our communication protocol, specifically BLE. In order to achieve this, we found an online bluetooth API for react-native known as react-native-ble-plx.","A big challenge that we ran into was connecting the mobile device with the scoreboard using Bluetooth. This required us to delve into the details of how the BLE protocol works. Through thorough debugging, we were able to get the communication to perform smoothly. We also ran into issues compiling our app do to java version bugs in the expo codebase. Another challenge was reliably reading the score data from the sensor as well as writing to the display. The display library we used was very buggy, and thus much iteration and library source code analysis was used to figure out a solution.","One accomplishment that we are proud of was achieving perfect synchronization between the app and the scoreboard. We were also able to responsive feedback for tracking when the ball is scored. A highlight of our project was also that we were able to get our device in a very portable form, as well as the appealing display driver we wrote.","Through this project, we were able to achieve a much greater understanding of how the BLE protocol works. We also gained a lot of experience in mobile app development and embedded systems programming.","Given the cost-effectiveness and versatility of our project, there is potential to enhance and integrate its components for mass production and widespread availability. A key aspect of this improvement could involve minimizing the hardware size for increased portability and adaptability to various basketball hoops. Additionally, exploring methods to differentiate between 2-point and 3-point shots is essential. While our initial plan involved calculating ball velocity, we realized that numerous edge cases could not be addressed with such a straightforward algorithm.","bluetooth, c++, esp32, expo.io, react-native",https://github.com/ScoreMaster1999/ScoreMaster1999
IDEA Hacks 2024,Nature Nurturer,https://devpost.com/software/nature-nurturer,False,"We wanted to create another incentive for people to go outside and interact with others. Many programs use punishment in order to encourage people to leave their house, creating a negative association with the outdoors. We decided to encourage outdoor activities with gamification. We've created a device that must be played outdoors. It promotes learning as well as collaboration and competition with friends.","Out project is able to identify plants based on camera data from ESP 32 MCU. It then displays the scientific name of the plant and the picture taken in the plant encyclopedia. Our application is also able to collect sensor data and live display the conditions around a player. Using the sensor data, we also include achievements for the player to complete.","One part was camera, which we connected to a database of plant pictures. The other device was sensors, connected to a multitude of devices. After wiring the sensors, we decided to link everything up to the web application. This allows for all the components to be connected with each other.","One challenge that we ran into was incorporating the camera and sensors to our web server. Additionally, we had difficulty soldering the screen. Creating the proper hardware was the greatest challenge in this project. However, after going through documentation multiple times and trouble-shooting code, we were able to finish our project.","We've created two functioning devices. Our first accomplishment was getting the camera to work. Shortly after, we managed to get outputs of plant information. Another accomplishment that we are proud of was getting the screen to work since setting up the software for the screen was complicated.",We learned about sensor and camera incorporation into web applications. We were also able to learn more about web development and how to live update the web page based on input from our sensors.,We plan on slimming down our product so that is is more easily wearable in the future. We also hope to develop a database so that players can explore nature together and compete with who can get the most plants.,"arduino, c++, flask, python",https://github.com/YonatanAhituv/IdeaHACKS2024
IDEA Hacks 2024,Health App,https://devpost.com/software/health-app-c098o7,False,,,,,,,,"arduino, breadboard, c, c#, http, sensors, unity, wifi, xcode",
IDEA Hacks 2024,Cheapest Flight Ticket Generation and Booking CLI-BOT,https://devpost.com/software/cheapest-flight-ticket-generation-and-booking-cli-bot,False,"Flight-Ticket Bot Functioning of Langchain Agents in scraping and processing data. Command Line interface UI Yatra.com real-time information. Scraped Data of all Real-Time Flights from Yatra.com Options for user to select which flight to book. User prompting his choice. Booking initiated. Booking itenary. Filling details of the passenger. Filling details and selecting Promo-Codes. Payment initiated. Payment notification on user's phone. A Command Line Interface (CLI) bot designed to streamline the process of finding and booking the cheapest flight tickets for users. This bot integrates the latest langchain Model to gather user preferences and employs Playwright to automate interactions with Yatra.com. The langchain model analyzes scraped JSON data, presenting the top 5 cheapest flights. The bot guides the user through selecting a preferred ticket and utilizes Playwright for the final booking process. The entire end-to-end ticket booking process is automated, with user confirmation needed only for payment via phone notification. Node.js: The backend scripting language.
JavaScript: Language used for programming logic.
Langchain Model: Utilized for natural language processing and understanding.
Open-ai 3.5 turbo: AI model for advanced text generation and analysis.
PlayWright: Automation tool for web interactions.
IDE_VS CODE: Integrated Development Environment for code development. Our inspiration stems from the desire to simplify and enhance the flight booking process. We aim to provide users with a seamless experience by leveraging advanced technologies. The Cheapest Flight Ticket Generation and Booking CLI-BOT streamlines the entire flight booking process. It integrates the Langchain Agents Model for user preferences and utilizes Playwright to automate interactions with Yatra.com. The bot analyzes data, presents the top 5 cheapest flights, guides users through selection, and automates the booking process. We built the CLI-BOT by combining the power of the latest Langchain Agents Model and Playwright. The integration involves scraping and analyzing JSON data to present the most cost-effective flight options. The user interface is simplified through the command line, providing a user-friendly experience. During development, we faced challenges in optimizing the integration between Langchain and Playwright. Ensuring a smooth end-to-end automation process posed technical hurdles that required creative problem-solving. We are proud to have successfully created a CLI-BOT that not only incorporates cutting-edge technologies but also delivers a fully automated and user-centric flight booking solution. The seamless integration of Langchain and Playwright represents a significant achievement. Throughout the development process, we gained valuable insights into the intricacies of integrating language models and automation tools. Overcoming challenges taught us innovative approaches to enhance user experiences in the travel domain. In the future, we plan to enhance the bot's capabilities by integrating additional features such as personalized recommendations, real-time updates on ticket prices, and further streamlining the payment confirmation process. Continuous improvement and user satisfaction remain our primary focus.",,,,,,,"3.5, agents, figma, javascript, langchain, midjourney, node.js, open-ai, playwright, turbo, vscode",https://github.com/Soumyojyotisaha/flight_ticket_bot.git
IDEA Hacks 2024,GAUSS - Pinpoint geodata,https://devpost.com/software/gauss-for-the-environment,False,"GAUSS interface GAUSS schematic Drought is a problem that is often found in several regions in Indonesia. Especially in areas with minimal soil absorption. With climate change and global warming being one of the factors of drought, many sectors are adversely affected. Qualitative methods will be used to compare the changes due to drought in each region. This paper will discuss one of the solutions to drought by implementing a device to detect water content in the soil. With this research, it is hoped that public awareness of drought can increase and pay more attention to the condition of the surrounding environment. Keywords: drought, climate change, global warming, early prevention. Drought is a condition in which the frequency of rainfall decreases drastically over a long period of time. The frequency of rainfall greatly affects the survival of many organisms in an ecosystem. Drought can be influenced by several factors such as the dry season, low rainfall and human activities. Changes in sea surface temperature, as well as changes in atmospheric pressure are also factors that cause drought in some areas. As in the case of the""El Niño Southern Oscillation (ENSO)"", rising atmospheric pressure in the Indian Ocean with decreasing atmospheric pressure in the Pacific Ocean. It can be summarized that drought is the condition of an area being dry, or lacking moisture for a long time and can have adverse effects on humans, animals, or plants. This drought event can be triggered by several things: Global climate change is expected to continue to worsen as air temperature and CO2 content rise, which also results in distorted rainfall patterns and distribution. (Seleiman et al., 2021) Drought There is no definitive definition of what a drought is, but there are several opinions that state their own definitions of drought. If we look at some of the opinions above, we can conclude that drought is a condition of dry land in an area caused by a lack of rainfall, a long drought, or less moist soil in an area that can have a negative impact on humans, animals, plants, and the environment in the area. By nature, drought can be divided into three types, namely: This type of drought occurs when drought lasts for a long time and rainfall is less than average. Hydrological drought can occur when water reserves in water sources such as lakes or reservoirs are significantly reduced. These droughts arise when changes in irrigation rainfall levels and erosion triggered by unplanned agricultural endeavors result in a lack of moisture in the soil for crops. Drought has far-reaching impacts on several sectors including agriculture, fisheries, energy, transportation, health and the economy. It can cause damage to crops, reduce yields, disrupt food production, increase the risk of water crisis and sanitation shortages. Drought impacts can also lead to decreased energy production from hydropower plants, reduce water availability for transportation systems and lead to worsening economic conditions. Problem Statement: Is it true that the soil is dry or not? Hypothesis 1: this soil is not dry because it has normal or more moisture content. Hypothesis 2: this soil is dry because it has a moisture content below the standard. Climate Change Climate change is the pattern of change and intensity of climate elements over a long period of time. The main driver of climate change is global warming. Climate change can also occur due to external factors such as solar radiation or factors from within the Earth itself in the form of increasing carbon dioxide levels in the atmosphere. Soil moisture Soil moisture is the water content of the soil in an area (e.g. Hillel, 1998). The moisture content of the soil can be expressed in terms of weight or volume. Soilmoisture also has its own index called theSoil Moisture Index(SMI). SMI is between 0 and 1. If the SMI value is close to 0, the less water content in the soil and the drier the soil condition. Vice versa, if the SMI value is close to 1, then the water content in the soil is more and the condition is more humid. Links between Drought, Soil Moisture and Climate Change Climate change is triggered by global warming. Global warming can occur because greenhouse gases such as carbon dioxide, methane, chlorofluorocarbons, and others continue to increase in the air. Deforestation also contributes to global warming because trees absorb less carbon. When global warming occurs, temperatures rise. Soil moisture also decreases as the water content in the soil decreases and drought occurs. In writing this paper, we used a qualitative research approach. We use qualitative methods because this method raises facts, circumstances, variables, and phenomena that occur when research takes place and presents them as they are. In accordance with the opinion of Lexy J. Moleong (2005: 6), a qualitative approach is research that intends to understand phenomena about what is experienced by research subjects, for example behavior, perceptions, motivations, actions, etc. holistically and by means of descriptions in the form of words and language in a special natural context and by utilizing various scientific methods. Climate change can affect soil moisture. To measure soil moisture, we can use the Soil Moisture Index (SMI) as a guide. The value of SMI ranges from 0 to 1, indicating 0 as the driest condition and 1 for the wettest (moist) condition. The following table shows observations of droughts on a global scale since 1950: Description: Unlikely: Few - some evidence available, few people agreeing. Moderate possibility: Some strong evidence is available, some people agree.Most likely: Some strong evidence available, many people agree. Australia & New Zealand Table 1: Regional drought observations. (Stanke et al., 2013)  Figure 1: GAUSS experiment results Table 2: GAUSS Experiment Results Table It can be seen that the Geodata Acquisition and Uti;izing Semi-System (GAUSS) can work well. It is characterized by the coordinate data and water content data that enter the website, To prevent drought, there are several things we can do: By reforesting, we can preserve the environment by planting trees. Oxygen also increases due to the photosynthesis process that comes from the trees we plant. The environment planted with trees is also more humid. If we limit our daily water use, then we can use water effectively. That way, when we are faced with a dry season, we can still use water. Soil conservation can help control erosion and returning organic matter to the soil can improve the soil's ability to absorb water and prevent soil drying. To implement and achieve one of the above solutions, we created a tool calledGeodata Acquisition and Utilizing Semi-System(GAUSS). This tool serves to detect the moisture content of the soil at a point. This tool works by generating two values, namely, location value and soil dryness level. The location value contains a coordinate where the tool is plugged in. When this tool is plugged in, it will conduct an electric current to the ground and the value of the soil dryness level is obtained based on the amount of current that returns to this tool. After obtaining the data, the data is sent to a website that contains a map and the map will show the data that has been sent earlier in the form of areas that have known soil drought levels.  Figure 2: GAUSS Schematic The tools and materials used for the manufacture of the Geodata Acquisition and Utilizing Semi-System (GAUSS) are as follows: First, we make a sketch drawing for electronics. After that, make a sketch for the website display. If so, the electronic components are assembled and arranged according to the sketch and programmed. Once you have programmed the electronics, the next thing to do is to program and connect the website and electronics. Don't forget to fix bugs and test. What we get from this is that by using GAUSS, we can avoid things we want such as the death of plants due to dry soil, and so on. With GAUSS, we can provide sufficient water content to the soil according to the data obtained by GAUSS. For the future, we will improve both software and hardware. We will also implement AI in the way it works.  In addition, we will try to develop this tool not only to detect drought at a point, but in an area so that it can be more useful in everyday life. We are also working on GAUSS in the future to not only detect things like water content, but other important things like pH can also be detected. Douville, H., K. Raghavan, J. Renwick, R.P. Allan, P.A. Arias, M. Barlow, R. Cerezo-Mota, A. Cherchi, T.Y. Gan, J. Gergis, D. Jiang, A. Khan, W. Pokam Mba, D. Rosenfeld, J. Tierney, and O. Zolina, 2021:Water Cycle ChangesArchived, accessed April 10, 2023, at theWayback Machine. Jia, G.; Shevliakova, E.; Artaxo, P. E.; De Noblet-Ducoudré, N.; et al. (2019).""Chapter 2: Land-Climate Interactions""(PDF).IPCC SRCCL 2019. pp. 133 Lexy, J.M., 2002. Qualitative research methodology.Bandung: Teenage Workshop. National Drought Mitigation Center.""Types of Drought"".  Accessed fromdrought.unl.edu. Accessed on April 10, 2023 NOAA | National Centers for Environmental Information (NCEI).""Definition of Drought"".Accessed fromwww.ncei.noaa.gov. Accessed April 13, 2023. NOAA National Integrated Drought Information System.""Drought Basics"". Accessed fromDrought.gov. Accessed on April 13, 2023 Stanke, C., Kerac, M., Prudhomme, C., Medlock, J., & Murray, V. (2013). Health effects of drought: a systematic review of the evidence.PLoS currents,5, ecurrents.dis.7a2cee9e980f91ad7697b570bcc4b004.https://doi.org/10.1371/currents.dis.7a2cee9e980f91ad7697b570bcc4b004 Surtani, S. (2015). Greenhouse Effect in Global Perspective (Global Warming Due to Greenhouse Effect).Journal of Geography,4(1), 49-55.",,,,,,,"arduino, express.js, javascript, mongodb, node.js, react","https://gauss-rawr.netlify.app/, https://github.com/icowibu/gauss-ui, https://github.com/icowibu/gauss-api, https://github.com/dimcoord/gauss-arduino"
MIT Reality Hack 2024,In Arm's Reach,https://devpost.com/software/test-h62z5t,True,"Sometimes, problems can seem far away. Yet, the impact of our consumption and production patterns on the environment is no longer a distant concern—it reaches each and every one of us. More than a third of the United Nations Sustainable Development Goals deal with the impact of human interaction with our ecosystems – 6 of 17 – and we believe this is a great reason to raise awareness and encourage people to take action through as many channels as possible. 
Today, media like TV, internet and VR allow us to witness the sight of a melting glacier, plunge into polluted water, or be trapped in the middle of a raging wildfire. However, many of us have desensitized ourselves to topics like global warming and pollution. By leveraging mixed reality and bringing scenes of crisis into living rooms, we want people to feel the consequences of climate change alongside the beauty of nature. We want to inspire people to build empathy towards our nature. Fostering change alone is hard; hence, we want to take this experience into a shared space where people can connect with nature and each other.","By simply touching the walls around you, step into three scenarios – pollution, rising sea levels, and raging wildfires – and invite friends and family to take action, together. Concrete actions, such as changes in everyday behavior and donation opportunities, are presented at key points. 
When you touch a wall in any room or urban setting, it creates a rift to another world. You can peer through that portal and see the first scenery: the serene beauty of the underwater world. Turtles and marine creatures swim by as though you could reach out and touch them. But then, something strange happens. The waste and pollution of the ocean start to invade your room, and you feel overwhelmed. You can collect the trash simply by touching it with your hand. Each time you eliminate a plastic bottle, information about our climate problem pops up.  The portal then shifts to the Arctic, where icebergs approach you and break apart while seagulls flee through the room. As the pieces of icebergs fall right in front of your feet, you find yourself stepping back in fear.  Finally, the portal takes you to a raging wildfire, and you see the flames engulfing your room. ","We found a great team to tackle a big challenge. It was crucial to have interaction design, development, 3D creation, and development go hand in hand with very tight feedback circles.
Once we found a core interaction that we liked, we iterated live on Quest 3 to find the right pacing and timing. 
The scenes were assembled in Unity, and exported to the web with Needle Engine. This allowed us to iterate super fast, right on device, which allowed us to find performance issues and optimization opportunities early, for example for adjusting models and textures.
Through spatial understanding and passthrough, Meta’s presence platform, combined with our shared social experience, achieves a high level of connection. But as new technologies can also be barriers to entry for many, we designed our hack from the ground up for the open web. Built for Web-XR on devices like Meta Quest 3, our key message is also delivered on smartphones and desktops. By leveraging modern web technologies, we can make In Arm’s Reach accessible to everyone with a browser. The best experience is provided on Quest 3 in AR with Room Setup completed and hand tracking. Mobile phones, desktop screens, and immersive VR headsets are also supported.   The water scene demonstrates a concept we coined “information bubbles”: reaching out to the pieces of trash makes them crumble and disappear, and in their spot we provide imagery of the amount of waste produced daily. The images were taken at the RealityHack event. The ice scene demonstrates a dynamic switch between AR and VR: once the pieces have settled, the user can step closer to fully immersive into the scenery. One step back and they’re in mixed reality again, seeing both the virtual iceberg and their living room.  Using Unity’s Timeline, we were able to sequence events for each of the scenes. This is especially apparent for the Ice scene: once the user has touched the wall for the first time, a mix of predefined motion and secondary animation brings the iceberg into the user’s living room and raises the sea level. The seagulls, fleeing from the event, soar above the user’s head, and then stay around in your space. Similarly, the water scene is timed to have one of the turtles float into the user’s space a while after starting the interaction, and the fire scene makes the flames engulf the user. Additionally, a few more trees grow from the floor to increase immersion. Instead of hand-animating each and every object, we used natural noise patterns to animate creatures, seaweed, trees and pieces of ice. In some cases, this secondary animation is combined with hand animation as outlined in the storytelling section above.
Particle systems add an additional sense of space in the fire and water scenes. All models were created as glTF files in Blender and imported into Unity. This retains all material and animation data, ready to be used interactively. The iceberg animation has been baked from a physics simulation in Blender and exported as glTF as well. Audio files were created as mp3s. All created files undergo the automatic optimizations built into Needle, which ensure that meshes are compressed with Draco (for smaller file size) and textures with KTX2-ETC1S (for super efficient memory usage). Since we added multi-user capabilities to our app early on, we were able to leverage that to record footage right from Quest in Mixed Reality. By aligning the spaces of two or more users, we can record exactly what the other person does. A main feature of In Arm's Reach is the ability to look behind your walls and bring a virtual environment into your room. This is enabled by WebXR features that are backed by Meta's Presence Platform. We're rendering in a very specific order: In a future update, we'd like to incorporate Quest Depth Sensing into rendering this pipeline, which provides real-time occlusion, for even greater immersion. To start a session with other people:","A conceptual challenge we ran into was deciding which scenarios to use for our hack, and what feeling to convey to our users. We wanted to avoid showing only negative scenarios, and show that taking action can have a clear impact. We also didn’t want to make it seem too easy to “solve climate change”, so we avoided overly gamified interactions, instead focusing on minimal interactions and calls to action. 
One design challenge was finding a good balance between abstraction and realism. We wanted to avoid falling into the “uncanny valley”, but still provide enough realism to understand that these challenges are real. An example of this challenge we’re not super happy with yet is the wildfire – it conveys the right impression, but we may want to spend more time on it in the future.","We’re very proud that everyone who tried the prototype so far loved the feeling of gently touching a wall to reveal a world so close to our hearts. This physical connection adds an emotional component that has often been lacking from other immersive experiences dealing with environmental challenges.
We think we managed our time pretty well – we had enough time to iterate on each of the scenes and get the feeling right. Audio got into the app very early instead of being an afterthought. 
Our approach to portal rendering feels pretty novel – typically, stencil buffer rendering is used to provide “portals” into various scenarios, but our depth buffer-based approach allows for greater creative flexibility when it comes to objects exiting from portals into your space.","It was great to learn more about the UN’s sustainable development goals and challenges during the first day of the hack. This broadened our understanding of what challenges we can meaningfully tackle.
This project has taught us that everything is connected—urban actions and nature have a symbiotic relationship. By using mixed reality, we've found a powerful way to bring nature closer to urban spaces and give people the experience of seeing and feeling this connection firsthand.
This project has solidified our intuition that an immersive experience, when done right, can create a stronger bond with the environment, leading to a greater awareness of the need for sustainable urban development.","This is merely the tip of the iceberg (no pun intended). Through partnerships with various organizations, we aim to demonstrate the tangible impact that proactive measures can have on our climate. Our vision involves enhancing engagement by seamlessly integrating additional information and data into enjoyable interactions. 
Furthermore, we plan to expand the project's reach by weaving in more narratives surrounding climate change. Additionally, exploring educational collaborations with schools and universities could broaden our audience and deepen the project's impact. We envision evolving into a comprehensive platform that not only raises awareness but also equips people with the knowledge and motivation to take meaningful actions against climate change.","abletonlive, blender, meta-presence-platform, needle, unity, webxr","https://hybridherbst.github.io/in-arms-reach/, https://codeberg.org/reality-hack-2024/TABLE_102/"
MIT Reality Hack 2024,Block Party,https://devpost.com/software/block-party-ov0jmc,True,"Use VR to connect the physical and digital realities.Build,ideate, &immersein a VR blockscape. Connect with friends & collaborators, enabling people in both VR and IRL to interact simultaneously. We aimed for a lego-like experience which could be useful to play with family or kids remotely, or could be used in a classroom environment as a teaching tool.","Place a cube IRL. See it in VR. Touch a cube in VR. See it light up IRL. Stack cubes to unlock other objects in VR. Shrink down in VR to see your physical build from a whole new perspective. VR Block Party allows someone to be in the real world stacking blocks and building something new. At the same time, someone in VR can be interacting with those blocks and seeing exactly what the person in the real world is building live. When the person in VR interacts with the cubes, the real world blocks light up to indicate this interaction.",,"In general the project took an incredibly large time to make because of the large number of steps that go into producing them. However the electrical contacts on the cubes were an especially big problem because we are just using copper tape. The copper tape is unreliable at making a connection between cubes and the board. If we had additional materials we could have instead used magnets as contacts which we think would make a much more reliable connection. For this project, each cube needed to be able to make electrical contacts on any square so consistency in the construction was very important. However, due to the tool set limitations, the copper tape was cut and placed by hand so there is a limit to how accurately we could position the electrical contacts on each cube and on each square on the board. We had to balance between making the contacts large enough to overcome the inconsistencies in the positions of the copper tape and making them small enough to prevent shorting out the board. The singularity API (which is used to communicate between the ESP32 and our VR headset) took a very long time to get working because of some unclear documentation that was provided.",The communication between the ESP32 and the headset has nearly no noticeable lag and it feels very responsive to interact with the cubes. We're especially proud of how satisfying it feels to touch the cubes in VR and see them light up in real life.,,We would like to rework the project so that the electrical contacts are more reliable and we can use PCBs instead of point to point wiring inside the cubes. Once those changes are made it would be incredibly easy to miniaturize the cubes and create massive boards which could contain hundreds of cubes and we could create complex structures. It would also be nice to add magnets and contacts on the sides of the cubes so you can create overhangs. We might make a kickstarter so you can hold your own Block Party.,"c#, c++, esp32, meta, onshape, singularity, unity",https://codeberg.org/reality-hack-2024/TABLE_6/
MIT Reality Hack 2024,Noir,https://devpost.com/software/noir-mibces,True,,,We built it using Niantic 8th wall.,,,,,"8thwall, blender, canva, capcut, css, gpt, html5, inshot, luma, meshy, midjourney, muglife, react, shapesxr, unity",https://codeberg.org/reality-hack-2024/TABLE_80_Noir
MIT Reality Hack 2024,Team 71 - Bounce Ball,https://devpost.com/software/team-71-bounce-ball,True,"Since the dawn of humanity, we have been tied together through competition. From the Ancient Olympic Games to modern day major sporting events, games have promoted global unity. And so, we, Team 71, have set out to make a project that can utilize the transformative power of spatial technologies to both help connect us and make us healthier. That project is Bounce Ball.","Bounce Ball is a game where two to four players compete in a co-located mixed reality space. The goal of the game is to transform as much of the arena as possible into your color. This is accomplished through strategically bouncing the zero G game ball around the cube shaped arena. When the ball hits a wall, it’ll turn the area of impact into that player’s color. And so round and round they go, trying to hit the ball more than the other player, and trying to aim the ball towards parts of the arena devoid of their color. And the beauty of using hand tracking to do this is that it’s so intuitive that no tutorial is needed. The music and sounds of Bounce Ball reinforce the intense, fast paced energy of the game. Not only does it make the interaction satisfying, it also encourages physical movement. Play brings us joy. It gets our bodies moving and our hearts pumping. 
That joy is what sparks the very connection that this year’s theme asked us to explore.","Designed and prototyped in ShapesXR, then built Unity, Photon, & Meta Presence Platform","Game Design
Alignment on a specific idea
Multiplayer networking","Working Multiplayer!
We executed an idea from start to finish and ended up with a polished result we're happy with",We all taught each other new skills :),Launch on the Quest 3 and mayb port to Vision Pro!,"photon, quest, shapesxr, unity",https://codeberg.org/reality-hack-2024/TABLE_71
MIT Reality Hack 2024,RECOVR,https://devpost.com/software/recovr-5ubkl0,True,"The idea for this project came when a friend of ours got into a motorcycle accident and lost all control of his left arm. What really shocked us was not only the physical pain he had to endure but also the emotional. The pain of losing control, dignity, and connection. And he isn't the only one who suffers from those pains. Today, two million Americans are living with a lost limb. So, we began to ask ourselves how we could create a tool and experience that could tackle this emotional burden.",,"Utilizing the strengths of our team members, we built a prototype to prove the concept using EMG sensors to track users' muscle movement, which controls a virtual arm to interact with different objects in an immersive environment created using Unity Game Engine.",,,,,"arduino, c#, dotnet, unity",
MIT Reality Hack 2024,SKYWARD,https://devpost.com/software/c-r-a-s-h-critical-runway-assistance-for-safe-handling,True,,,"As our focus was on having a hands/controller-free approach, we used a combination of tools to turn our idea into an(augmented)reality. We decided to develop our project on the Meta Quest 3, using its advanced mixed reality pass-through capabilities. Meta's all-in-one SDK on Unity allowed us to iterate together on different features such as hand tracking and voice recognition. Voice recognition is handled by Wit.AI. For our 3D assets, we designed our UI/UX in ShapesXR, allowing us to port our models directly into Unity. Other assets such as the airport and airplanes were found on Sketchfab/CGTrader. (7,8,9) All our progress in Unity was synchronized using Codeberg with Fork, allowing us to easily work in parallel on multiple features. Our 3D airplane animations were created in Blender, where multiple animation tracks can be executed on the same model.","One of our challenges came with the use of the Meta Voice SDK with Wit.AI. We implemented Meta Presence Platform's Voice SDK and trained custom voice detection models, but ran into some challenges with the accuracy of audio detection within Unity. We also ran into a few challenges setting up our Codeberg project and synchronizing large project files.",,"One of our biggest learnings was the importance of Air Traffic Controllers. The role they play at airports around the world cannot be understated, and we were not aware of how deeply the shortage crisis has been affecting the industry. We also greatly broadened our technical horizons by learning to integrate several Meta Presence Platform technologies, cinematic video footage, custom shaders, and audio, into an immersive MR experience. We used ShapesXR for the first time, and learned how to import 3D models into a Unity scene. We also improved in creating 3D models and animations in Blender.",Thank you for reading! :),"adobe-illustrator, blender, c#, codeberg, fork, meta-interaction-sdk, meta-presence-platform, meta-voice-sdk, photoshop, quest3, shapesxr, unity, wit.ai","https://codeberg.org/reality-hack-2024/TABLE_58, https://www.canva.com/design/DAF7Hgm3WaM/XDHZ9VmSHlTbzZ2hmez33g/edit?utm_content=DAF7Hgm3WaM&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
MIT Reality Hack 2024,"A ""Fire"" Training App",https://devpost.com/software/a-fire-training-app,True,,,,,,,We'd like to add more fire escape safety tips and features:,"handtracking, metadepthapi, metainteractionsdk, metaquest3, metaxraudiosdk, mixedreality, unity",https://codeberg.org/reality-hack-2024/TABLE_32
MIT Reality Hack 2024,Talk to Me,https://devpost.com/software/talk-to-me-a8ow0x,True,,"Revolutionize Collaboration between XR and non-XR users with Talk to Me: 
Boost productivity and deepen workplace connections. Embrace the future of communication today!","We used Presence Platform building blocks including Voice SDK interacting with the Wit.ai, Interaction SDK, and passthrough. The building blocks allowed us to build the immersive AR experience rapidly. The passthrough feature allows users to use their current productivity tools while enjoying the features in the Quest3 headset. We represented the user's colleague using an avatar and animated the avatar. When a text message is received on Slack, the message is uploaded to a Photon Server and sent to Quest 3. The avatar then runs into the user's field of view and reads the text to the user, using the Voice SDK powered by Wit.ai. If the user wants to respond to the message, the user can simply use the controller powered by Interactive SDK to start the recording of their voice to the avatar. Then, the Voice SDK will transcribe this back to a text message and send it to the user's colleague. This process is again facilitated by the Photon Server.","Boy oh Boy were there challenges 
We encountered several main challenges and countless small challenges. The biggest two are related to the integration of Photon Chat and the Wit.AI speech-to-text functionalities into our product flow.
Regarding technical problems, we had to read the super obscure and obsolete javascript documentation of Photon Chat API and integrate it organically into a Slack Bot backend. We also had to figure out how to set up the speech-to-text and text-to-speech SDKs within and beneath the Meta Presence Platforms integration layer.
On the other hand, in terms of product design decisions, we had to decide when and how should we determine the start of conversations, which finally resulted in this version of a flexible and fluent flow of communication.
﻿","One of our most significant accomplishments as a team was our dedication to extensive brainstorming and research, which led us to identify a unique and impactful problem space within the market. We could have easily settled on creating another XR platform for workplace efficiency as our hackathon theme. However, we delved deeper, exploring the nuances of human interactions in the real world and how they could be authentically augmented in XR environments. Leveraging the latest technology, we successfully incorporated several advanced features found in the Meta Presence platform.
But our proudest achievement lies in developing a compelling and insightful demo. This demo not only showcases the technological capabilities we've harnessed but also eloquently communicates the problem space we are addressing. Our dedication to going beyond the superficial and striving for a deeper understanding of human connectivity in virtual environments sets our project apart and defines our journey in this hackathon.","Throughout this hackathon, our journey into the realm of XR unveiled its tremendous potential, particularly in reshaping workplace cultures and human interactions. We were fascinated by the relative ease of developing XR products using Meta's Presence Platform, which opened new vistas for innovation and creativity. A pivotal lesson for us was the critical importance of human elements within XR environments. We delved into the challenge of augmenting these virtual experiences to reduce the sense of disconnection from the real world. This exploration underscored the necessity of designing XR platforms that are not only technologically advanced but also deeply empathetic to human needs and interactions.
Moreover, the hackathon was a testament to the power of collaboration and effective communication within our team. Engaging with a diverse array of mentors, each bringing a wealth of domain expertise, significantly enriched our understanding and approach. This experience has not only broadened our technical horizons but also deepened our appreciation for the multifaceted aspects of XR development.","Several ideas and feature thinking occured to us as we were developing ""Talk to me""
Some of them include : 
1) Building an SDK to connect all mainstream work communication apps on the Quest series of devices. 
2) Support group chat communication 
3)Trigger activation of chat using word cues
4) Use intent analysis to enrich feature","c#, javascript, presenceplatform, quest3, unity",https://codeberg.org/reality-hack-2024/TABLE_56
MIT Reality Hack 2024,Legacy,https://devpost.com/software/legacy-q9dmvf,True,"The inspiration behind the project stems from the recognition of a growing disconnect between generations and their cultural heritage. As societies evolve, there's often a challenge for younger generations to connect with the traditions, stories, and experiences of their elders. Additionally, factors such as adopted kids searching for roots, parents reconnecting with their heritage, and grandkids on a quest to know their origins further contribute to this cultural gap.
Everyone in the group lives in a different country than where their heritage is from. This dynamic brings an added layer of complexity, highlighting the impact of global mobility on cultural preservation. 
The project seeks to bridge these gaps by leveraging mixed reality experiences. It draws inspiration from the desire to create a platform where people, regardless of age or cultural background, can explore, learn, and reconnect with their heritage. The initial focus on the Chinese experience serves as a starting point, with a broader vision to extend the platform to encompass diverse cultures worldwide. Ultimately, the inspiration revolves around fostering understanding, preserving cultural legacies, and creating a shared space where individuals can embark on a journey of exploration and connection with their roots.","Legacy is an immersive Mixed Reality Experience designed for use with the Meta Quest. Users find themselves transported to the environment of their grandparents, delving into the rich tapestry of their cultures, habits, and lifestyles. Interaction with the experience requires a table. Initially, 3D assets materialize, featuring the engaging task of dumpling folding. As users progress, they not only master the art but also gain insights into their cultural heritage, ensuring they do not lose their connection to their roots.","Legacy is built primarily with the use of the Meta Presence Platform plugin in Unity and Meta Quest 3. The Interaction SDK, Meta MR Utility Kit, and Lean Tween were all used in Unity to develop the experience. We used Audacity to adjust the sound effects, voice over and music. 3D assets used in the scene were found online and created by team members through the mixed use of Blender, Rhino, and Keyshot. To prototype and see the interactions with each object, the designers used Bezi’s shared collaborative space and animations. For UI info cards and branding, Figma was used to keep the overall project cohesive. Last but not least, we placed high importance on the open communication we had with each other and the willingness to have fun.","For the project, there were many ideas we wanted to explore and features to create in this experience. Something we slowly realized was how much we had to narrow our scope in. Also, we received a lot of great feedback from mentors on how we could expand our project. However, we had to really focus on what was key to this experience. 
In terms of technical issues, Bezi was crashing from our use of their beta tools and generally trying their tools out for intense modeling. We later discovered how to best use Bezi as an interaction prototyping tool. For development, there was a learning curve to integrate the Meta Presence Platform and customize the templates to the interactions we want to have. The naming in the SDK was confusing and not consistent between the kit, so it was hard to understand the documentation and apply it to our use case. Also, setting up the Meta Quest 3 for development took some time, since it required factory reset to connect the device to the Meta Quest Developer Hub.","We assembled a balanced team with different skill sets and worked together in a unified manner, respectfully supporting each other’s needs and ideas. We built a Mixed Reality application that runs on top of a table with interactive learning experiences including how to make a dumpling, play Chinese cultural musical instruments, play with a new year lantern, play with a mahajong piece and learn about fortune cookies! Learn language, and some info on them with speech and music in the background to immerse yourself into the culture :)",,"With Legacy, our next goal is to create new worlds featuring diverse cultures, allowing grandparents to experience the world from the perspective of our generation. Our vision is to make every world open, enabling everyone to learn about different cultures and ultimately contributing to fostering understanding and harmony.","adobe-aftereffects, adobe-illustrator, adobe-premiere, audacity, bezi, blender, figma, keyshot, leantween, meta-interaction-sdk, meta-mr-utility-kit, meta-presence-platform, metaquest3, rhino, unity","https://www.bezi.com/file/f7752aa4-1251-4efe-b56c-6b13747a8485, https://docs.google.com/presentation/d/1-vII8EGAunBwjH6zJ2oTL28BDmPVSkc0Yt9emaNdGLU/edit?usp=sharing"
MIT Reality Hack 2024,Community Canvas,https://devpost.com/software/community-canvas,True,"Our inspiration comes from a profound realization: the fading prominence of community in the narrative of our lives. The once deeply rooted sense of identity tied to groups and localities is giving way to a more detached individual existence. In this context, we believe it's time to redirect our attention to the community, the neighborhood, and the collective spirit that binds us together. By embracing AR as a tool for collective creation, we can enhance the value of our shared spaces, foster a stronger sense of belonging, and reignite the community spirit that seems to be fading.",,,,,,,"ar, dualrenderfusionaddon, figma, meshy, motorola, qualcomm, snapdragonspaces, thinkreality, unity",https://codeberg.org/reality-hack-2024/TABLE_106
MIT Reality Hack 2024,snAIder,https://devpost.com/software/snaider,True,We wanted to create a collaborative tool that let multiple people be involved in the creative process of design. We wanted to make communication a visual experience and not just written,Our platform allows screen-writers to generate scenes in mixed-reality and convert them into a script. Changes to the script are reflected back in a replayable MR experience,We used Meshy.ai to generate custom images using voice commands and used LLMs to convert scenes into scripts and vice-versa. Our platform runs on Unity and Quest,Describing a scene as a set of animations or datapoints involved imagination,"We were able to get the end-2-end process of converting movement into scripts, modifying those scripts and loading those changes into the headset in real time","Using Rest APIs in Unity for multiple services allowed us to create experiences outside the Unity editor. For example, we were able to combine Hugging Face’s speech to text models to send a request to Meshy to create custom 3D models. We were then able to convert the history of moving objects to text descriptions using an a rest call to chatGPT","Multiplayer for collaboration
Implementing more complicated scenes involving many more objects. Expanding the range of objects that can be created and reducing latency. Training a custom LLM to generate more precise descriptions and movements","c#, cloud, mixedreality, oculus, python, unity",
MIT Reality Hack 2024,GEOQUEST AR,https://devpost.com/software/geoquest-ar,True,"The inspiration for GeoQuest AR emerged from a desire to merge the exploratory essence and exergame nature of Geocaching with the digital interactivity and health incentives of Pokémon GO, using AR to add the journey to the destination that is Geocaching.  We call it Geoquesting. (Research into pokemon go health benefits)[https://pubmed.ncbi.nlm.nih.gov/27923778/] Geo-questing gamify’s the journey to the cache and aims to increase physical and mental well-being by incentivizing outdoor exploration, learning, and connection with ones surroundings. The concept revolves around using people's innate curiosity about their surroundings to create an engaging, health-enhancing experience that transcends traditional gaming, making the journey through the city's history and landmarks as rewarding as the destination itself.",A geoquesting app prototype that takes a person on an interactive journey with quests and challenges that when successfully navigated lead to the destination and physical geocaching activities.,"Unity, NREAL XR SDK, Mapbox SDK, Meshy.AI, Mixamo animations, Trilib for glb support.",,,,,"blender, mapbox, meshy, mixamo, unity, xreal",https://codeberg.org/reality-hack-2024/TABLE_89
MIT Reality Hack 2024,Beesper,https://devpost.com/software/beesper,True,"   Welcome to Beesper! We live in a world rich with diverse ways of communicating, but for some, expressing themselves through American Sign Language (ASL) can be challenging. Traditional methods of learning ASL, such as books and videos, lack the immersive and interactive experience needed to master this beautiful language. ASL is not just about words; it's about connection. Beesper is here to change the game! Our Virtual Reality (VR) application transforms the way you learn ASL by providing an engaging and immersive experience packed with minigames designed to help learn/practice signing. We bring ASL to life in a virtual world where learning is as fun as playing your favorite video game. What is Beesper?Beesper is a VR application designed to teach American Sign Language (ASL) in an interactive and engaging manner. It provides real-time feedback, making the learning process enjoyable and effective. Why Beesper?Traditional methods of learning ASL can be dull and lack interaction. Beesper bridges the gap by creating a virtual space where learners can practice ASL through games and interactive experiences. Who is Beesper for?Beesper is for anyone who wants to learn ASL, whether you're a beginner or looking to improve your skills. It's designed to make learning accessible and fun for all. Immersive VR Learning:Dive into a virtual world where ASL comes to life. Learn by doing, not just watching. Real-time Feedback:Receive instant guidance and corrections to perfect your ASL gestures. Interactive Games:Engage in fun and educational games that reinforce your ASL skills. Community Learning:Connect with learners from around the world, making ASL learning a community experience. Personal ASL Coach:Benefit from a virtual ASL coach that guides you through the learning process. Breaking Barriers:Beesper aims to break down communication barriers, making ASL accessible to everyone. To get started with Beesper, you'll need: Launch the App:Put on your VR headset, launch Beesper, and sign in to your account. Choose Your Learning Path:Select your skill level and preferred ASL curriculum. Immerse Yourself:Dive into the virtual world of ASL learning. Follow the guided lessons and practice your gestures. Receive Feedback:Beesper provides real-time feedback on your gestures. Adjust and improve as you go along. Explore Games:Engage in interactive games and challenges to reinforce your learning. Connect with Others through Multi-Player:Join the Beesper community, connect with fellow learners, and practice ASL together. Beesper is released under theMIT License. You are free to use, modify, and distribute the software in accordance with the terms of the license. MVP award goes to Tahnee Gehm for being our local ASL expert and taking time to be interviewed and teach our team the basics of ASL. This project would have never succeeded without you and you made this journey incredibly rewarding. Your contributions and insights were invaluable! Special thanks to the following Mentors:
-Patrick Burton for providing guidance for polishing presentations, video documentation and expression of impact.
-Russel go guidance of structure for presentations.
-Barnabas Lee for the encouragement and direction of decisions for the project
-Paul Sorenson for assisting with multiple technical difficulties and for finding our out-of-norm table 105 location
-Julian Park for assisting in using the prototyping software Bezi 
-The entire Ultraleap engineering team at Reality Hacks to confirm that multi-player along with other features were running successfully. This project would not have been possible without your support! Finally, thank you to you the user for choosing Beesper as your ASL learning companion. Let's make the world a more connected place, one sign at a time!",,,,,,,"c#, netcode, ultraleap, unity",https://codeberg.org/reality-hack-2024/TABLE_105
MIT Reality Hack 2024,Dr. Trust,https://devpost.com/software/dr-trust,True,"Just one medical horror story or statistic is enough to deter many of us from seeking critical medical care. Even if there’s only a 1 in 10,000 chance of a complication, our complex human brains will find every possible way we’re going to be that 1—especially if we don’t know what will happen to us once we go under. According to the National Institute of Health (NIH), up to 92% of patients experience anxiety and doubt before going under an operation. There are many physiological symptoms of heightened anxiety, but the most impactful one is psychological: patient mistrust of doctors. When patients do not trust their medical professionals, they end up refusing life-saving care—which can have permanent consequences. Patient distrust stems from a lack of transparency. The flood of medical jargon that many receive before surgery only increases their fear, rather than builds understanding. We see XR as a connective technology that can help make some of these discussions more intuitive and less intimidating.","Dr. Trust is a Mixed Reality application built on Meta’s Presence Platform that helps facilitate difficult medical conversations. It will allow doctors to more effectively educate and reassure patients by clearly visualizing what the problem is and how it can be treated. For this hackathon, we specialized Dr.Trust in visualizing the pre-surgery consultation of a lung removal surgery. Doctors and patients can annotate a 3D model of the body, calling attention to specific areas and lifting models of organs out to highlight their relevance. It supports both an avatar appearance and a pass-through real-life appearance for the doctor and the patient to accommodate both in-person consultations and remote virtual consultations.","We used Meta’s Presence Platform and OpenXR on Unity to support Mixed Reality features, and the XR interaction toolkit to support a variety of features on Oculus Touch Controllers (preferred for their high degree of precision). We also wanted to create a real-time collaborative experience for patients, doctors, and family members, whether they’re in the same room or across the world. We used Normcore to host the app on a remote server, and enable live voice and hand interaction between multiple people.","First, trying out new technologies! We challenged ourselves to try spatial design software that was new to us, such as Bezi. This turned out to be the best kind of challenge - we clicked with Bezi amazingly well, and it was a valuable asset for our Unity development. On the flip-side, we tried the new building blocks in Unity and ran into substantial challenges. After spending nearly 5 hours on Friday night, we decided to pivot after learning that other teams were facing the same issues. This adaptability allowed us to adjust and build a well-functioning project within the time constraint.","We’re most proud of our team dynamic and the way we were able to seamlessly work together.
Our project’s name is Dr. Trust, and trust in each other is exactly what the doctor ordered for this hackathon! Some of us took a backseat in areas where we usually lead, others trailblazed in unfamiliar areas to them. The result was a group of people that wholeheartedly trusted each other - even as exhaustion from an all-nighter set in. This allowed us to move faster and not be scrambling at the last minute the way most of our prior hackathon experiences have been. And the vibes within the team were impeccable throughout.","First, it can be fun to take a backseat from areas one is typically experienced in. People who go to multiple hackathons sometimes tend to fall into niches - “I’m a technical person”, “I’m a designer”, “I’m an idea guy”. By relinquishing that self-framing and trying new things, we can grow so much more. Second, things don’t always go as planned in the demo as they do in a live test. Even after setting up our project’s real-time collaboration to work seamlessly when we tried it out, when we ran it for judges while everyone was using the WiFi, there was sometimes lag or disconnection.","Our vision for Dr. Trust is not just limited to pre-surgery consultations. Visibly seeing the condition of their body is an incredibly powerful experience, and that can be expanded to all medical diagnoses.","bezi, blender, c#, gltfast, openxr, unity",https://codeberg.org/reality-hack-2024/TABLE_103.git
MIT Reality Hack 2024,EduVision,https://devpost.com/software/eduvision-raosb8,True,"Think back to when you were a student and had the chance to experience a scientific lab for the first time. That may be either looking through a microscope or exploring matter that makes us who we are. Hands-on experience ignites a kid's curiosity, and engages them to want to learn more. Unfortunately, more than 50% of schools in the US alone lack the facilities to provide hands-on scientific experience to students. We wanted to leverage extended reality in order to make scientific education more accessible.","EduVision makes scientific experiments immersive, engaging, and accessible. Our first prototype is a 3D printed microscope coupled with a 3D display to replicate the real experience. The current prototype allows the exploration of blood cells either through a mobile phone which is very accessible or a more expensive Looking Glass for a better understanding of the spatial aspect.","To fabricate and programmatically control our EduVision microscope, we first selected an ideal scene to visualize: red and white blood cells. First, we set up our Unity scene with red and white blood cell GameObjects from Sketchfab – optimizing their geometry and adjusting materials in Blender. Once prepared for Unity, we exported them as .fbx files and positioned duplicates in the scene while implementing sinusoidal movements with C#. After setting up our scene, we diverged into two scenes: one with the Unity camera configured for a mobile phone display, and one configured for the Looking Glass’ Hologram Camera. For each scene, we implemented camera controls based on input from the potentiometer circuit to enable users to adjust the focus while zooming in or out. Our physical microscope serves an aesthetic purpose that emulates an expensive microscope, and we 3D modeled this withTinkerCadandOnshape. Aside from our ESP32 and wires, our components were entirely 3D printed and can connect to a mobile phone or Looking Glass display. Our circuit was developed in C++ with the Arduino IDE, while our interactive visualization was developed and integrated with the circuit in Unity with C# and Visual Studio. Collaboration was easy and engaging as weset up version controlwith a guide that Ashley – one of our team members – created :)",,We are proud that we were able to create a functional device that students can use at a low cost. We are also proud of our team and being able to work together efficiently without knowing each other previously.,We learned a lot about scaling businesses as we are all students with no prior experience in entrepreneurship. We made our first startup pitch and were able to discuss the potential of our product with investors.,EduVision wants students to use our microscope and learn from it. We want to inspire students to be curious and explore scientific careers in the future. We would like to simulate the microscopic view of other structures such as microorganisms. We are also looking into recreating other scientific experiments in XR.,"android-sdk-and-ndk, arduino, c#, c++, looking-glass, onshape, open-jdk, tinkercad, unity, visual-studio",https://codeberg.org/reality-hack-2024/TABLE_2
MIT Reality Hack 2024,PlantAR,https://devpost.com/software/plantar-db3rq0,True,"In the time of increased urbanization and less exposure to nature, we hope people can build a caring and compassionate relationship with nature and themselves.Inspired by their innate endless curiosity, we created a mixed reality experience for children to forge a connection with their plants. Research shows us how expressing compassion for nature’s living creatures directly impacts self-care. “Interaction with indoor plants may reduce psychological and physiological stress by suppressing autonomic nervous system activity in young adults” (J Physiol Anthropol. 2015; 34(1): 21.). This is put to the test in a classroom environment where kids have the chance to share their experiences with their classmates while learning to be empathetic. We are excited to explore the concept of connecting wellness in life through Mixed Reality Experience and our creation of PlantAR.","PlantAR plants self-growth concepts through interactions with a classroom flower, MR Plant. 
Kids learn to be caring and compassionate by connecting to nature through PlantAR. Bringing nature into the classroom with innovative and unique plant care methods. Taking care of MR Plant creates interactions in the Mixed Reality space that shows the growth of the flower. By forging a connection with nature, we created a fun and interactive experience for children to grow their curiosities.","We wanted to showcase the positive connections between Mixed Reality and Life.  In Unity, we made animations and speech recognition technology. We also utilized the PULSAR 3D printer to create portions of the experiences to enhance our concept. The hardware was developed using ESP32 microcontroller with the code written in  Arduino IDE. 3 sensors were utilized-- web speech API from hugging faces, in-built touch sensor, and raindrop sensor to measure communication, holding, and watering actions respectively. The servo motors carry out the primary action of pulling the petals downward so the flower can bloom.","These only inspired us to continue growing to find solutions for our project vision. ""I can hack that."" A few memorable challenges we experienced: establishing two way blue-tooth communication between the headset and hardware took many tries. We also experienced challenges with the speech and sound accessibility. In the first test phases of this, we found that accents created a barrier for our wanted actions in both MR and with MR Plant.",,"We learned that this Mixed Reality experience is intriguing for kids of any age. Although our main demographic remains kids. We also learned that we want our creation to be accessible across socio economic stratifications, encouraging curious natures. Through our cooperation and creative inputs we can accomplish our visions. Growing together through our creative processes we learned a lot about hacking, hardware and connections.",We envision Mr Plant becoming a part of learning experiences for children around the world. Possible developments for going to market where individuals of all ages can have their own PlantAR.,"esp32, huggingface, singularity, unity",https://www.canva.com/design/DAF7LDiLhBU/RIxvPaA58MAOMbXrrP49tA/view?utm_content=DAF7LDiLhBU&utm_campaign=designshare&utm_medium=link&utm_source=editor
MIT Reality Hack 2024,HeaVR,https://devpost.com/software/heavr,True,"In traditional VR training, while invaluable for learning new tools in a simulated environments, users often face a significant limitation: holding a standard hand controller fails to provide the weight and tactile sensation, which is crucial for simulating real-world tool usage accurately.
HeaVR addresses this critical gap with an innovative extended hand controller. Unlike conventional controllers, HeVR can automatically adjusts its length and weight to mimic the characteristics of virtual tools being used. This dynamic adaptation ensures that users experience a lifelike sensation, enabling them to better gauge the weight and feel of each virtual tool they interact with.
By seamlessly integrating virtual environments with tangible physical feedback, HeaVR revolutionizes the way professionals engage with VR training simulations.",,,,,,,unity,
MIT Reality Hack 2024,FlexVR Wellness,https://devpost.com/software/flexvr-wellness,True,There is proven research that VR environments can calm a patient down which helps reduce the stress of electro-stimulation therapy. This inspired to see how we can facilitate and make the process easier for both the patients and the therapists.,"We developed an entire ecosystem where the therapist can use an AR headset to enhance their workflow, communicate with patients get live data from them remotely, and control the patient’s electro-stimulation therapy while the patient is in a calming XR environment.","We used Unity, the Normcore multiplayer framework, and Snapdragon Spaces to interface all the devices.",Our greatest challenge was having to interface a Snapdragon Spaces AR headset to control a custom hardware device connected to a Quest 3. The therapist and patient are connected over the internet!,"We are the first team to successfully interface a Snapdragon Spaces AR headset with a Meta Quest 3 creating a system where two users in the different headsets can interact with each other remotely. In addition, we have successfully created an electro-stimulation device that can be used in a Quest 3 and controlled remotely by a user in a Snapdragon Spaces AR headset which relies on our custom firmware to interface everything.",We learned a lot with writing custom firmware for hardware devices!,We can make the device a lot smaller!,"ar, electric-stimuation, meta-presence-platform, meta-quest-3, normcore, snapdragon-spaces, unity, vr",https://codeberg.org/reality-hack-2024/TABLE_1
MIT Reality Hack 2024,MemoryMingle,https://devpost.com/software/memorymingle,False,"Turn Snapchat Maps and Memories functions into AR real-world interactions, allowing you to reflect on and interact with your memories.","Introducing MemoryMingle: Your Instant Personal Time Machine! Blend past and present with our Snapchat AR filter. Capture moments with your younger self and others using the AR camera for unforgettable memories. Embark on a Journey Through Time with MemoryPath: Let the AR MAP guide you to cherished places and relive great experiences. Immerse in nostalgic AR interactions with your past self! Transform Moments into Memories with TimeTraveler, a unique experience reflecting personal milestones, historical events, and self-reflection. Rediscover the essence of your past, present, and future on a timeless journey. Remember, we don't recall days; we cherish moments.","The software employs generative AI to create virtual avatars and utilizes Snapchat's API for geolocation recognition in photographs. It integrates AR technology to guide users in capturing images. Finally, it processes these images with generative AI to seamlessly blend them, enabling users to virtually coexist and take photos with their past selves or friends, transcending time in one place.","Facing challenges in time management and multitasking, especially with unfamiliar features, we encountered unexpected issues like the sudden closure of the Google Maps API. Swiftly, we had to find an alternative for obtaining the 3D map.","In a constrained timeframe, we accomplished the creation of a short film and demo, showcasing our expertise in storytelling, workflow design, user flow, and interactive prototypes.","We gained valuable insights into teamwork and time management. Additionally, we seized the opportunity to explore and learn about cutting-edge technologies such as Meshy, Bezi, Unity interaction, and 8th Wall.","Explore the possibility of evolving into a startup or a robust community section within Snapchat, envisioning Snapchat as a vault for every user's memories fostering high interactivity.","8thwall, c#, elementplus, genai, javascript, unity, vue",https://www.8thwall.com/maxliu/test3
MIT Reality Hack 2024,PocketFest,https://devpost.com/software/pocketfest,False,"PocketFest Logo PocketFestis an all in one hardware and software platform for hosting next generation silent disco events that include an audio and visual mixed reality experience and is inclusive to those not typically found at music festivals and raves, but desire to have a similar experience.
Rave and music festival culture has gained increasing popularity in recent years. Although many enjoy this festival experience, it’s found that those who frequent them are predominantly extroverted person personality types in nature. The nature of music festivals is challenging for introverted and neurodivergent people who also yearn to be a part of a shared audio and visual social experience. Specifically those who are tech enthusiasts, but also introverted and still seek forms of connection that might not be found in a typical music festival experience. One innovative solution to this has been silent disco in the past which may be more accessible to this demographic. We have built a platform and product that pushes this one step further to compete with the over the top audiovisual experience of traditional music, festivals, appealing to neurodivergent and introverted tech enthusiasts, or simply those that want a fun social experience that brings high fidelity music and visuals to any location. People may feel a sense of FOMO or loneliness when they see others on social media posting about their experiences at music festivals, isolating them further. PocketFest consist of two elements, one a mobile robotic docking bay for mixed reality headsets, and the audiovisual generative platform, which can be controlled by either a DJ or generative system in our platform. Because it is a multi-sensory, silent disco that is mobile, it can be conducted in different areas that may be more accessible to people who cannot, or choose not to attend traditional music festivals. reasons for this could be long lines or crowds, unwanted social contact from other festival attendees, or venues that are difficult to travel to for some based on their location. PocketFest is a comprehensive hardware and software platform designed for hosting next-generation silent disco events. It offers a unique audio and visual mixed reality experience, specifically catering to individuals who might not typically attend music festivals and raves but are still seeking a similar experience. Our project fits within the Hardware Hack with the theme of Connection. Target Audience:PocketFest is particularly aimed at introverted and neurodivergent individuals, as well as tech enthusiasts who desire social connections through innovative and less overwhelming experiences compared to traditional music festivals. Concept and Appeal:The platform builds on the concept of a silent disco, enhancing it to rival the over-the-top audiovisual experiences of conventional music festivals. This approach makes it appealing to those who might feel isolated or experience FOMO (Fear of Missing Out) due to their absence from such events. Hardware - Mobile Robotic Docking Bay:This is a key element of PocketFest. It serves as a docking system for mixed reality headsets, making the experience portable and accessible in various locations, accommodating those who cannot or choose not to attend traditional festivals due to issues like crowds, social contact, or travel constraints. Software - Audiovisual Generative Platform:This platform can be controlled by a DJ who could be remote or an automated generative system. It is responsible for creating the immersive audiovisual experience in the mixed reality headsets. Connectivity and Community:Users at different PocketFest events can connect globally. Virtual avatars of attendees from different locations can be instantiated within each local venue's system, creating a shared and expansive festival experience. User Experience:Attendees can adjust their level of immersion in the event using the mixed reality headset. This allows for a customizable experience, catering to individual comfort levels in social settings. The ease of entry and exit from the experience is facilitated by simply wearing or removing the headset. Solving Social Issues:PocketFest addresses the feelings of loneliness, isolation, and FOMO by providing a collective music experience that is more accessible and less overwhelming than traditional, highly stimulating music festivals. PocketFest aims to solve several key problems related to social inclusion and accessibility in the context of music festivals and similar social events. Here's a breakdown of the specific issues it addresses: Accessibility for Introverted and Neurodivergent Individuals:Traditional music festivals can be overwhelming due to their extroverted nature, loud music, and large crowds. PocketFest offers a more accessible option for those who might feel uncomfortable or overstimulated in such environments. Loneliness and Social Isolation:Some individuals may feel left out or experience FOMO (Fear of Missing Out) when they see others attending music festivals and sharing their experiences on social media. PocketFest provides a way for these individuals to participate in a similar, yet more comfortable, social experience. Global Connectivity and Shared Experiences:The platform allows users from different locations to connect and share the festival experience virtually. This feature breaks down geographical barriers, allowing a wider range of people to participate in a collective experience. Flexibility and Customizable Experience:Users can control their level of immersion in the festival experience, making it adaptable to their personal comfort levels. This flexibility is especially beneficial for those who might find certain aspects of social interactions challenging. Overcoming Physical and Geographical Limitations:The mobile aspect of PocketFest, through its robotic docking bay for mixed reality headsets, makes it possible to set up the experience in various locations. This is particularly useful for people who cannot travel to traditional festival venues due to physical, financial, or geographical constraints.
PocketFest addresses the need for a more inclusive, flexible, and accessible approach to music festival experiences, catering especially to introverted, neurodivergent, and tech-savvy individuals who seek social connections in a less overwhelming environment. The root causes of the problems addressed by PocketFest, such as social isolation, inaccessibility of traditional music festivals, and the discomfort experienced by introverted and neurodivergent individuals in such environments, stem from a variety of factors: Nature of Traditional Music Festivals:Conventional music festivals are typically designed for extroverted experiences. They often feature large crowds, loud music, and a high level of sensory stimulation. This environment can be overwhelming for introverts and neurodivergent individuals, who might find such settings overstimulating or anxiety-inducing. Limited Accessibility:Traditional festivals are usually held in specific locations, which might be difficult for some people to access due to geographical, physical, or financial constraints. Travel to these locations can be a barrier, excluding those who cannot afford the trip or those with physical disabilities that make travel challenging. Lack of Personalized Experience:Most music festivals do not offer customizable experiences. Attendees are generally expected to adapt to the environment rather than having the environment adapt to their individual needs and comfort levels. Social Expectations and Pressure:There is often a social expectation to enjoy and participate in large, communal events like music festivals. Those who feel uncomfortable in such settings can experience feelings of inadequacy or isolation, especially when they see others sharing their experiences on social media. Global Connectivity and Cultural Sharing:Traditional festivals are localized events, limiting the opportunity for cultural exchange and global connectivity. This restricts the experience to those physically present and excludes a wider audience that could benefit from such cultural and social interactions. Technological Advancements and Changing Preferences:With the advancement of technology, especially in the realm of mixed and virtual reality, people's preferences for entertainment and social interaction are evolving. Traditional music festivals might not fully leverage these technologies, missing out on engaging a tech-savvy audience. Several solutions have emerged in recent years to address the root causes of issues like social isolation, inaccessibility of traditional music festivals, and discomfort experienced by introverted and neurodivergent individuals in large social settings. Silent Discos:Silent discos, where participants wear headphones to listen to music, offer a more controlled and personal experience. They reduce the overwhelming impact of loud music and large crowds, making them more accessible to those who are sensitive to sensory overload.
2024 Hackathon Concept 5 Virtual Reality (VR) Concerts and Events:VR technology allows users to attend concerts and social events in a virtual space. This is highly accessible as it removes physical and geographical barriers, allowing people from all over the world to participate from the comfort of their homes. Online Streaming of Live Events:Many festivals and concerts are now streamed online, allowing people to participate remotely. This approach has become increasingly popular, especially in the wake of global events like the COVID-19 pandemic. Small-scale and Localized Music Events:These events provide a more intimate setting and are often more accessible to people who feel uncomfortable in larger crowds. They can offer a more relaxed and less overwhelming environment. Specialized Events for Neurodivergent Individuals:Some event organizers have started creating music and social events specifically tailored to neurodivergent and introverted audiences. These events are designed with sensory sensitivities in mind, featuring lower sound levels, relaxed lighting, and more personal space. Social Media and Online Communities:Online platforms allow people to connect and share experiences without the need for physical presence. This can alleviate feelings of FOMO and provide a sense of community for those who are unable or unwilling to attend physical events. Wearable Technology for Sensory Control:Products like noise-canceling headphones or sensory control wearables help individuals manage their sensory experiences in overwhelming environments.
how are the current solutions flawed? Limited Scope:They primarily change the auditory experience but don't address visual overstimulation or other sensory sensitivities.Social Interaction:While quieter, they might still not facilitate easier social interaction for introverted or neurodivergent individuals. Equipment and Accessibility:Requires specific VR equipment, which might not be accessible or affordable for everyone.Physical Isolation:While connecting people virtually, they can lead to physical isolation, lacking the tangible aspects of a shared physical experience. Lack of Interactivity:Streaming is typically a passive experience and lacks the interactive and communal aspects of live attendance.Time Zone Limitations:Live streaming can be inconvenient for international audiences due to time zone differences. Geographical Limitations:Still requires physical attendance, which can be a barrier for those living far away or with mobility issues.Limited Reach:These events can't reach a wider audience and lack the global community aspect. Availability and Awareness:Such events are not widely available and may not be well-publicized, limiting their accessibility.Variability in Needs:It can be challenging to cater to the wide range of needs and preferences within the neurodivergent community. Digital Overload:Excessive use can lead to digital fatigue and can sometimes
exacerbate feelings of isolation or FOMO.Lack of Physical Interaction:They don't provide the physical and multi- sensory engagement found in live events. Individual Solution:These are personal solutions that don't address the broader social or environmental aspects of event participation.Cost:Some of these technologies can be expensive, limiting their accessibility. In contrast, PocketFest attempts to address these limitations by combining aspects of both physical and virtual events, offering global connectivity, and tailoring experiences to individual comfort levels, aiming to create a more inclusive and adaptable social music experience. Combining Physical and Virtual Elements:Unlike traditional events or purely virtual experiences, PocketFest integrates mixed reality, offering both the tangible feel of a physical event and the accessibility of a virtual experience. This hybrid model allows for a more immersive and interactive experience than standard online streaming or VR concerts. Global and Local Connectivity:It enables users to connect with global events while participating locally, bridging the gap between the global reach of online events and the intimate experience of local events. This feature allows for a wider cultural exchange and community building beyond geographical boundaries. Customizable Experience:PocketFest allows users to control their level of immersion, addressing the varying comfort levels of individuals, particularly introverted and neurodivergent participants. This flexibility is not often found in traditional music festivals or even in many virtual event platforms. Accessibility and Mobility:The mobile robotic docking bay for mixed reality headsets makes the technology more accessible and allows for the creation of pop- up events in diverse locations. This aspect addresses the physical and geographical limitations associated with traditional festival venues. Inclusivity for Neurodivergent and Introverted Individuals:By providing a multi- sensory experience that can be personally tailored, PocketFest caters specifically to those who may find the sensory overload of conventional festivals overwhelming. This inclusivity is a significant step beyond what is typically offered at most music events. Reducing Social Isolation:PocketFest tackles the issue of social isolation by creating a shared social experience that is more accessible and less intimidating than large-scale, extroverted events. This approach can help reduce feelings of loneliness and FOMO. Technology Integration:By leveraging mixed reality technology, PocketFest provides a modern and tech-forward experience that can attract a wider, tech-savvy audience, filling a gap that traditional festivals might miss. PocketFest addresses the limitations of current solutions by offering a more inclusive, accessible, and customizable social experience. It combines the benefits of physical and virtual events, caters to a diverse range of needs and preferences, and uses technology to create a new kind of music festival experience. Technological Advancements:As mixed reality and related technologies continue to evolve, PocketFest can integrate these advancements to enhance the user experience. Improvements in VR/AR headsets, audio technology, and mobile connectivity will allow for more immersive and accessible experiences. Global Expansion:Given its ability to connect users globally, PocketFest has the potential to scale internationally. As awareness grows, more people worldwide can participate, creating a truly global community of festival-goers connected through technology. Diversification of Events:While initially focused on music festivals, the platform can expand to other types of events, such as art exhibitions, educational workshops, or cultural festivals. This diversification can attract a broader audience and increase the platform's utility. Partnerships and Collaborations:Collaborating with music artists, event organizers, and technology companies can provide opportunities for growth. These partnerships can enhance the content offered and expand the reach of PocketFest to new audiences and markets. Customizable and Adaptive Content:The platform can evolve to offer more personalized experiences, adapting to user feedback and preferences. This could include various genres of music, different types of visual experiences, or settings that cater to specific sensory sensitivities. Accessibility Improvements:Continuously improving the accessibility of the platform will be crucial. This means making the technology more affordable, user- friendly, and adaptable to various needs, ensuring that it remains inclusive for all potential users. Community Building and Social Features:Enhancing social features to foster a stronger sense of community among users can lead to greater user engagement. Features like social media integration, community forums, and live interaction options can enhance the social aspect of the platform. Sustainable and Ethical Growth:As the platform scales, it will be important to consider sustainability in terms of environmental impact and ethical considerations in content and community management. Adaptability to Market Changes:Being responsive to changes in entertainment preferences, technological trends, and global socio-economic factors will be key to staying relevant and successful.
In summary, the future of PocketFest lies in leveraging technological advancements, expanding globally, diversifying content, forming strategic partnerships, enhancing accessibility, fostering community engagement, and adapting to market changes. Scalability will depend on how effectively these factors are managed and integrated into the platform's growth strategy.",,,,,,,"c#, python, unity",https://codeberg.org/reality-hack-2024/TABLE_5.git
MIT Reality Hack 2024,Museum of Future Perfect,https://devpost.com/software/museum-of-future-perfect,False,"  Memory simulation for alzheimers Oxytocin  Jaren Lanier VR inspiration bot  Dream MMO Future Perfect is the museum of what happens next. Made from a large language model trained on outstanding feats of imagination, each piece is an art piece of the future, sculpted from the past. Todays exhibition includes a data sonification of the love hormone oxytocin to induce connection, an idea generator made from Jared Lanier’s 52 definitions of VR, a storytelling matrix to help us understand the stories that make us, a multimodal memory simulator for people suffering from Alzheimers. And a massive multiplayer online game made from the dreams of the players. Visitors to the gallery will be able to further develop the ideas completely open source, helping organically grow with the community. Our mission is to dive into the latent space of potential and make the gallery a spectacular machine of innovation, shortening the gap between thought and action, past and future, virtual and reality. The project was inspired by the hubbub of creation in the hackathon. Initially the idea was to write brief reviews of each teams endeavours and tune a LLM on the entries, making a collective consciousness of creation. We were inspired by the idea of growing out from this energy, using generative technology to find new paths to innovation. However people were a little busy and we decided to keep it simple. Instead of expanding on the works from the room we looked into cultural artefacts instead. . The pieces were made in concert with a tuned LLM trained on museum catalogues. As we are a team of two relatively untechnical thinkers it was a challenge finding a simple, succinct way of displaying our vision.",,,,,,,"ai, llm, meshy, natural-language-processing, runwayml, shapes",https://space.shapes.app/space/share/zqkjgppxzb6rhyvewyyvimjxysvi2ffwbrnvgaz0sfor/0/0
MIT Reality Hack 2024,Synergy & Survival,https://devpost.com/software/synergy-survival,False,"In an era where technology intersects with sustainability, our project for the 'Living Harmony' track at MIT Reality Hack reimagines urban living through Extended Reality (XR). We're not just innovating; we're connecting humanity with nature, embodying the philosophy of unity between humans and the environment.","Our educational game ""Synergy & Survival"" offers a unique perspective on ecosystems, addressing the interconnectedness of animals and humans. It emphasizes the importance of renewable energy, aiming to protect the environment from further disruption. This game not only connects the city with its ecosystem but also focuses on promoting the wider use of renewable energy.","VR Headset: Meta Quest 3
Development Tools: Unity, C#",,"Our project goes beyond technology. It's a testament to human-nature harmony, a unique blend of philosophical depth and interactive technology showcasing the symbiotic relationship between humans and the environment. It's not just a game; it's a vision for a sustainable, harmonious future.","Team collaboration, Game design pipeline and meta building blocks.","The game was designed to be fit in various context based on the need of our target audience, museum and renewable energy provider. Next step would be to customize the game to fit their need!","c#, quest, unity, vr",
MIT Reality Hack 2024,XR Wilderness Academy,https://devpost.com/software/xr-wilderness-academy,False,"Imagine a world where the thrill of ice climbing and the essential skills of survival are within everyone's reach. Our XR experience offers just that - an immersive, accessible journey into the world of extreme sports and survival, previously limited to a select few due to access to expert training and financial constraints.","XR Wilderness Academy is a skills-building game where the user learns outdoor activities and engages in hands-on learning. Players are gently onboarded into the mixed reality experience that uses passthrough. This provides visual comfort and stability, allowing the player to focus on learning new skills. Then as they progress in the training their surroundings become increasingly virtual and they are rewarded with a scenic virtual vista at the end of their experience. We aim to make adventures accessible (financially, psychologically) so everyone can enjoy the great outdoors… indoors. We see this as a platform that can take individuals from awareness, to skill building, to making a desired  outdoor activity a reality.","Development Platform: Unity version 20222.2.8 using:
Target Platform: Quest 3 Headset
Meta presence SDK leveraging:",,,,TBD,"bezi, c#, cognitive3d, meta, unity, xr",https://codeberg.org/reality-hack-2024/TABLE_38
MIT Reality Hack 2024,GAL-XR,https://devpost.com/software/gal-xr,False,Interactive and fun learning experiences to increase student stimulation,To provide an immersive learning experience learning chemistry without the fear of getting injured,"Using ShapesXR, Unity, Leap Motion Controller 2, and Meta Quest 3",Simulating fluid movements,Successfully simulated a chemical reaction,To use hand motion and gestures to interact with a virtual environment without a controller,Expand on details behind chemical reactions that occur in the environment and make the virtual equipment wearable for the avatar,"bezi, leap-motion, oculus, shapesxr, ultraleap, unity",https://codeberg.org/reality-hack-2024/TABLE_45
MIT Reality Hack 2024,Record Reality,https://devpost.com/software/record-reality,False,This application is inspirated in HoloNotes from ShapesXR,"Our application allows students record first-person educational experiences using mixed reality technology. This means students can relive their experiences as if they were there again, making learning more engaging and meaningful.""",With Unity and Meta Presence Platform,Learn about Meta Avatar SDK and record animations.,"To participate in this hackathon in person, we had to believe in our dreams and present an innovative project.","Record animations from Meta Avatar SDK
Mixed Reality with Meta Presence Platform","Make this applications Stand alone, in the future multiplayer .","c#, metaavatar, metaquest, unity",
MIT Reality Hack 2024,Code Hero,https://devpost.com/software/code-hero-a0x7bu,False,The Leetcode grind for most Software Developer jobs,Gamifies learning Datastructures and Algorithms with an introduction to competitive programming.,Built using the Unity platform and company specific SDKs,Some build issues due to libraries and dependencies issues with Unity. But they were overcome with help from some Mentors.,,,Deploying Free beta to Steam for user feedback,"meta-quest3, snapdragon-sdk, unity",https://codeberg.org/reality-hack-2024/TABLE_54
MIT Reality Hack 2024,Gesture VR,https://devpost.com/software/gesture-vr,False,"Everyone is temporarily abled, and someday, we might lose our ability to speak. One of our team members learned Tzuchi Sign Language, which is similar to Taiwanese Sign Language, when she was in high school. She is not deaf; she learned it to communicate with her grandmother who resides in a senior housing facility. We realized that sign language is a universal language that connects people. Based on the statistics, sign language is the fourth most spoken language in the United States, following English, Chinese, and Spanish. There are 500,000 sign language speakers, yet only 230 American Sign Language (ASL) teachers are in the US. This significant gap in the number of instructors underscores the importance of creating a solution like our app.
Of course, I'll provide a mix of bullet points and paragraphs for better readability:","The app primarily aims to facilitate sign language learning in a virtual reality (VR) environment. It offers a range of immersive features and functionalities. Users can access interactive game to learn sign language, engage in gesture recognition exercises. The app is designed to cater to learners of all proficiency levels, from beginners to advanced sign language users. It also adapts to individual learning preferences, providing a personalized and engaging learning experience.","Building Gesture VR requires a multidisciplinary approach, combining various technologies and expertise: AI, ML, VR, and API Deployment. We leveraged VR platforms like Oculus Rift, HTC Vive, and Oculus Quest to create an immersive learning environment.. Gesture recognition technology, powered by hand tracking modules from Ultraleap LMC and machine learning models, was integrated to provide real-time feedback on users' sign language gestures. A cloud-based backend infrastructure was implemented to store user progress, deliver recommendations, and facilitate live practice sessions. In addition, user-friendly interface design was a focal point to ensure accessibility for users of diverse backgrounds and ages.","Throughout the development process, we encountered several substantial challenges:","Throughout the development journey, Gesture VR achieved several noteworthy milestones:","Through the development of Gesture VR, we gained valuable insights:","In the future, we have ambitious plans to further enhance and expand Gesture VR: Our ultimate aim remains to make sign language learning accessible, enjoyable, and effective for everyone, thereby contributing to a more inclusive society.","blender, leap-motion, love, python, tensorflow, unreal-engine",https://quantumrealities-ui.vercel.app/
MIT Reality Hack 2024,SPHERE,https://devpost.com/software/sphere-ywnr15,False,Does it make sense that you want to get out of your comfort zone? Do you feel like you want to improve and you don't know how? We've also been there even when you want to be able to talk but you just don't know how to do it.,We are an immersive platform that can help you improve your conversations from starting a talk to giving a conference in the best style of a professional speaker. Thanks to our artificial intelligence assistant you will be able to get feedback on how to improve. You can also invite your friends to live a truly immersive experience.,"We use chatGPT, Unity, and Meta Quest 3",implement the project within the unit and then implement the functionalities we wanted,Fixed the Unity integration our team is so smart,Implement the Unity platform and work with international colleagues,,"openai, unity",
MIT Reality Hack 2024,StageFlow,https://devpost.com/software/stageflow,False,"As a team of musicians and creators, we wanted to develop an application to help performers connect better with their audiences, using rhythm games like osu! and Beat Saber as inspiration.","Our app provides a platform for practicing in front of a simulated live audience. The audience responds to the music you play on stage, and a meter indicates your ""stage flow,"" the level of engagement the audience has with you.","After ideating and wireframing with Figma, we developed our app using Meta's Building Blocks in Unity and the Meta XR SDK.","We developed several different parts of the application in parallel, with simultaneous work on the audio components, the volumetric effects and shaders for the simulated crowd, and the core game logic. When trying to combine these components together, we ran into a multitude of problems related to positioning of different elements in our scene.","Our team was able to leverage all of our unique skillsets to conquer a variety of issues. Additionally, several of our team members had little to no XR development experience prior to the hackathon, and were able to gain exposure during.",,This platform could be extended to,"meta, quest, unity",https://codeberg.org/reality-hack-2024/TABLE_40
MIT Reality Hack 2024,Moji,https://devpost.com/software/moji-9di4wl,False,We were inspired by the unique culture we share among our groups. And the unique ways we communicate among our groups' shared culture - cultivated through expression and interpersonal bonds. People share a fabric of social connection and a shared way of expression.,We used ShapesXR for interaction design and Bezi for 3D XR interface design. ShapesXR Bezi,Development Design Collaboration,,,,,"bezi, meshy, meta, meta-presence-platform, quest-3, shapesxr, unity","https://codeberg.org/reality-hack-2024/TABLE_66, https://www.bezi.com/play/95aa7169-be48-4aca-9bce-ebe5e3686bd1"
MIT Reality Hack 2024,Ever-present,https://devpost.com/software/ever-present,False,"Research indicates that long-distance relationships can significantly increase individual and relationship stress levels (Du Bois et al., 2016). Conventional communication methods like text messages and phone calls do not work well in creating new memories of time spent together and could also be routine-like. It 
 lacks in sentiment and gesture of affection. Could there be a new way to express our care and affection for our loved ones at a distance? Our particular inspiration for Ever-present comes from our own experience as international students living abroad. We want our parents to know that even if we are miles apart and perhaps even sleeping because of the time difference, when they see Ellowyn, they would know that our love for them isever-present.","Enter our enchanting digital companion, Ellowyn. This magical creature will gracefully travel between the homes of ours and our loved ones, and deliver tokens of affection across space and time. A Gift of Experience: With Ellowyn, you can deliver a variety of magical experiences called  “Greetings”. Like dazzling fireworks, serene shooting stars, or an enchanting undersea these experiences are designed to surprise and delight your loved ones. Your loved ones can collect these experiences as memories. Symbol of Mutual Care: To send and receive greetings, both parties need to nurture Ellowyn. As it grows on you, it becomes more willing to deliverGreetings. Every successful delivery of _Greetingrepresents mutual care and affection. Ambient: Ellowyn happily resides in your everyday MR spaces, and is not an individual app or game to be launched. It won't seek your continuous attention, but a calm, companion that connects you and your loved ones.",,,"Alignment of Ambition and Expectation Throughout the team formation process, we prioritized ensuring that every member shared a common vision and passion for the project. This alignment was crucial in setting a solid foundation for our collaboration. Adopting a Problem-First Approach We dedicated significant time to thoroughly understanding and defining the problem before jumping into solutions. This approach helped us stay grounded in the real needs of our users and ensured that our solutions were genuinely tailored to address these needs. Fostering Transparency and Efficiency in Communication Our team culture emphasized clarity and detail in all forms of communication, especially during debates or when providing feedback. We made it a point to articulate our reasoning comprehensively, ensuring that every team member understood the perspectives and rationale behind different viewpoints.","We realized that our product has a life of its own and it was telling us what it wanted to do, just like writers find their characters coming to life and the story telling itself. Ellowyn began to have a life and personality of its own!","Here are some of our plans! Increasing scalability We want to improve accessibility/HCI usability so that we can help more user demographics adopt the MR technology. This is especially important because we envision our parents using it. During our process, we put accessibility first and designed it to be low stimulus and complexity because we understand that older adults can become stressed and anxious with emerging technology. We hope refine the spatial instructions that we currently have in place. Increasing user retention Ellowyn will be equipped to deliver a range ofGreetings (multi-sensory experiences) and personalized messages. Imagine receiving a recording of your loved one's voice, tailored specifically for you by Ellowyn. Over time and through your interactions, Ellowyn will develop a unique personality and mature. This growth is not just in appearance but also in character, reflecting the depth of your relationship with it. We also want to introduce diverse companions like Orion, Lunara, Serapelle, and more! Each messenger has its distinct flair, allowing you to select one that resonates best with your style and preference. Future updates will also include synchronous mini-games and challenges, such as competitive enticement of Ellowyn to visit your room. These playful elements are designed to enhance engagement and shrink the distance between you and your loved ones.","blender, figma, meta, quest, unity",
MIT Reality Hack 2024,Traces,https://devpost.com/software/traces-b15sou,False,"There are 51 Nationalities represented at Reality Hack. 45 million people in the United States are immigrants. They left their homes, communities, their countries. Many struggle to feel a sense of belonging– the sense that your identity is accepted and celebrated in your community. Without belonging, we feel isolated, conflicted, and unable to participate in society. 1/4 of immigrants face anxiety, depression, and chronic stress. In the next decade, 54 million more people are expected to be displaced and will have to integrate into new cultures and societies. How might we support these people? 80% of our team is immigrants. We combined our stories to create a metaphorical memory box where you can immerse yourself in our .","Traces is an immersive tool to frame your mindset: to help immigrants feel less alone while giving locals empathy and teaching compassion. It answers three UN Development Goals: mental health, reduced inequality, and peace. You become a 12-year-old who has just moved to the US, and you move through the emotional moments of your new life here. Some folks bully you, others help you – but as you experience each memory, items stick to your body and become part of you. The envelope that denied your father’s visa. The ginger you helped your mother find in the huge American supermarket. After a group of kids steals your backpack, your teacher teaches you that the secret to belonging is helping others – and you help someone else feel at home in their new home.","Our tech stack integrates custom 3D models into an A-Frame powered Web VR app that can be streamed onto any headset, anywhere, making the experience accessible to everyone. We built out and lit all the scenes in Blender, exported them to AFrame, and assembled the videos. The character and 3D assets were also built in Blender. The dialogues were all written and recorded at the hackathon, casting people from the team and hackers around us. All the dialogues were then paired to an object. We added music to spark emotion and used lighting to guide the user.","Role setting was a challenge for us as many of our team members had overlap in their skills. We all tried to pitch into the story but the story kept growing, so we ended up assigning story writing to one member, and UI/UX, art direction, development, and asset collection to all other members. Setting Up AFrame for VR, Setting Up the Player in VR, Getting all the assets organized, and Making sure we have music and ambient audio. All the assets were compiled by day 2, but we were in a crunch so we had to implement all of them on day 3. We realized that a smaller scope would have been easier.","We're most proud of the story we wrote, the scenes we created, and the emotional storytelling we envisioned. Our VR experience has a unique character and smooth UX for teleportation and grabbing. The experience was built completely from scratch using Blender and Meshy to model objects and environments. We also used 3D prototyping tools like Bezi and ShapesXR.","We learned a lot about each other's stories of belonging, of coming to the US and needing a place to stay. We also researched the social and emotional issues that immigrants face, whether it be due to forced immigration or moving by choice- We all felt connected to each other and other immigrants. We felt our experience was more universal than we had realized, and that the dialogue we started can be very impactful. We learned how to create an impactful experience in VR. 
On the technical side, our team members shared a lot of skills in AFrame, Blender, video editing, script writing, and modeling. We helped each other out with tasks and moved very quickly on day 2. On day 3,","This piece will be integrated into a curriculum taught in three contexts: D&I corporate training, College international/transfer student orientations, and High School Freshmen Day. VR’s power is evoking emotion; the curriculum will help players start to practice the skills demonstrated in the piece by being curious about others’ experiences, offering help, and reframing negative experiences to not personalize them – they are universal. 
For the experience itself, we would like to add special effects, onboarding, and more interactivity, along with visual refinements in the environments.","aframe, bezi, blender, meshy, premiere, shapesxr","https://10.29.186.253:8080, http://codeberg.org/reality-hack-2024/Table_74"
MIT Reality Hack 2024,PastVista,https://devpost.com/software/pastvista,False,,,,,,"Choosing the Roman Republic as a starting point for this project at first seemed a bit of an esoteric way to explore how we can connect with history but as we did research, we noticed that we could identify cultural experiences that transcended time. Power struggles between social classes, concerns of money, social mobility, and legacy were just a few of the common themes we saw that we could identify in modern times.","For the purposes of the hackathon, we felt using the Roman Empire was an appropriate test to see how our ideas of historical empathy transferred through first person interaction and observing cause and effect through game choice. Moving forward, we would like to explore current history including the modern day effects of the Korean War, the social complexities surrounding the LA Riots of 1992, and the exploring the lives of people living in East and West Berlin in the 1980s.","bezi, c#, canva, figma, google-drive, meshy, mixamo, playht, procreate, unity, whisper",
MIT Reality Hack 2024,EmpoweredAR,https://devpost.com/software/empoweredar,False,"We wanted to tackle something that revolved around accessibility and how AR technology could be more than just entertainment or educational. One of the mentors at the hack is visually impaired, and motivated us to pursue a solution for him and others like him.","Using the Xreal lenses that generate a digital mesh, we established that we could provide the user with an auditory ping. This gives them information about distance to objects and the space around them. Not only are wearers able to build a larger spatial map, but they can safely navigate around obstacles in their path.","We utilized Unity audio for the audio playback, TensorFlow and YOLOV3 for object detection, and the XREAL glasses depth data for a mesh and raycasting to determine object distance.","The XREAL lenses do not have an RGB camera, and therefore, we cannot differentiate between obstacles or a wall that they use to walk through spaces. Working on the same project in git with Unity posed issues when merging and required careful pruning of files.",Getting object detection working when user moves head.,"XREAL development
TensorFlow/YOLOv3 in Unity
Spatial Audio in Unity","Implement object detection with AI support and voice audio so that users can complete tasks such as locate a handrail and safely navigate down a set of stairs. Our current build is optimal only for indoor spaces, and would have difficulty navigating open areas or dynamic objects in space.","adobeaftereffects, adobeillustrator, garageband, steamaudio, tensorflow, unity, yolov3",https://codeberg.org/reality-hack-2024/TABLE_72
MIT Reality Hack 2024,Mool,https://devpost.com/software/mool,False,"The inspiration for the MOOL project arises from the challenges in urban planning, architecture, and community development projects, where diverse stakeholders often struggle to find a common ground on which to negotiate and collaborate. There is a critical need to enhance collaboration and inclusivity in the field of spatial design. PlayPump, initially celebrated for its innovative approach to water access through children’s playfulness, was eventually abandoned due to a critical oversight: the lack of genuine engagement with its end-users the children, and the communities. This disconnect highlights the consequences of design decisions made without thorough and empathetic user feedback, leading to wasted resources, time, money, and trust, contributing to the 21st-century transition toward an unsustainable long-term future. This case exemplifies the pitfalls of designers’ biases overshadowing the needs and perspectives of actual users, especially those with accessibility issues or who are typically marginalized in professional design processes. MOOL emerges from this context, motivated to rectify such oversights. We aim to build a tool that not only involves but also empowers the voices of all stakeholders, particularly those who lack professional design or development skills and languages. MOOL seeks to bridge these gaps, ensuring a more inclusive, empathetic, and collaborative process, therefore mitigating the risks of costly design failures and fostering social innovation designs that resonate with and serve the needs of communities.",,,"The development of MOOL presented several significant technical challenges. Firstly, ensuring real-time synchronization of users’ actions in the multiplayer VR environment was a complex task. We had to devise a system where data transfer via Photon was efficient enough to support smooth interactions between users in different locations, using different VR hardware. Another major challenge was balancing the technical complexities of VR and networking with the need for a user-friendly experience. Designing an environment that was both technically robust and easy to use by people with varying levels of technical proficiency required a thoughtful approach to UI/UX design. Integrating voice control for 3D modeling presented its own set of hurdles. We had to fine-tune the interaction between the Meta Voice SDK and Meshy for Unity to ensure that voice commands were accurately translated into 3D model manipulations. Addressing latency issues and achieving precise voice command interpretation were critical in creating an effective hands-free design tool.","One of the key accomplishments of the MOOL project is the successful integration of AI technology into the spatial design process. MOOL has demonstrated its ability to streamline collaboration, improve design outcomes, and enhance the inclusivity of the design process. The positive feedback and adoption by designers and stakeholders are sources of pride for the team. Additionally, the project's commitment to ethical AI practices and data privacy has been a significant achievement.","Through the development of MOOL, the team gained valuable insights into the potential of AI in spatial design. The importance of involving stakeholders at all levels and the need for effective communication were reinforced. The project also highlighted the significance of ethical considerations in AI-enabled tools. Continuous learning and adaptation were essential in addressing the evolving needs and challenges of spatial design.","The future for MOOL holds exciting possibilities. The team envisions expanding the platform's capabilities to support an even wider range of spatial design projects and industries. This includes further enhancements in AI algorithms, user interface design, and data analytics. MOOL aims to become a go-to tool for designers and stakeholders seeking sustainable, inclusive, and efficient design solutions. Additionally, partnerships with industry leaders and organizations are on the horizon to promote the widespread adoption of ethical AI-enabled spatial tools like MOOL in addressing the complex challenges of the 21st century.","amazon-web-services, c#, generative3d, meta, natural-language-processing, voicesdk, wit.ai",
MIT Reality Hack 2024,Collage MR,https://devpost.com/software/collage-mr,False,"Our team shared a vision of collaborative collage creation in the future, and we want to develop an idea that all these parts of the experience like audio, photos, videos, and 3D scenes will be connected together. so the users can see the world they have visited before in a more wholesome way than simply photos.","It lets you click on one object and generate a related experience based on that object. Our example use case we prepared is to click on the RealityHack name card, and then multiple memories including photos, videos, and a 3D reconstruction scene related to this hack event will pop into the world. You can drag to see these memories, and you can also enter the 3D recon scene with a simple touch.","We use Meta Presence Platform and Quest 3 to leverage the mixed reality potential. We lost one of our teammates unfortunately at the beginning, but we, two developers and two designers on board, completed the development in time. We expedited asset design using Meshy, a 3D AI Generator. Scaniverse is used to scan in a physical space for a virtual scene.","We run into technical challenges with the new SDK a lot. Many implementations and API calls work some of the time and fail some of the time, and with little documentation, it is hard to make quick changes in time. We tried to hack our way through these interactions with SDK and building blocks in Unity with some good and old collider methods.","We made it! We made an app where we could achieve what we hoped for initially, and we are especially proud of our transition from MR to VR and then back to MR with a few simple hand gestures.","We learned a lot about Meta's presence platform and its SDK, especially on the interaction part, which is one essence of our application. Besides development, we also learned how to hack a project in a very short period of time and to manage the project in a timely fashion.","We accomplished 75% of what we were hoping for at the beginning, and the next thing for our project is to further push the interaction part to support more gestures and make UX better. We also like to implement the input part where users can put the things they want into the app, instead of experiencing a example we have for this hackathon.","c#, presence, quest3, unity",https://codeberg.org/reality-hack-2024/TABLE_69
MIT Reality Hack 2024,"WEDGE: Wireless, Ergonomic, Dynamic Gesture Enhancer",https://devpost.com/software/wedge-wireless-ergonomic-dynamic-gesture-enhancer,False,"At MIT Reality Hack 2024, Team 13 converged around the concept of asymmetrical XR interactions and a fascination with feet interaction. Identifying a gap for peripherals that connect to users outside the VR headset, we envisioned WEDGE – a tool to facilitate shared virtual experiences. Our backgrounds in crafting XR exhibits and educational software have often highlighted the challenges of integrating standalone VR with interactive devices. WEDGE is our answer to creating the versatility of PC peripherals like Xbox accessibility controllers and MIDI devices for the standalone VR realm. We thought about theFigma Creator Micro, which allows keybinds to be mapped to single buttons, and theXbox Adaptive Controller, an alternative means of input for differently-abled gamers.","WEDGE is a multi-faceted XR peripheral, bridging the digital divide between virtual interactions and tangible IoT applications. Designed for remapping and multipurpose use, it transcends gaming to augment utility tasks, creative displays, classroom learning, and accessibility. With a simple interface of two buttons and LEDs, WEDGE introduces a new dimension of control within XR environments.
An advantage of the WEDGE is that people outside of VR can see feedback from the LEDs","Our design process was rooted in collaborative prototyping, focusing on the ergonomics of foot-based interaction. Through iterations of 3D models, we arrived at an optimized triangular wedge shape, providing stability at angles of 30, 60, and 90 degrees. The team's expansion with the addition of Ye (a.k.a. Maggie) was a turning point, enriching our product vision and development. Unity was our workhorse for digital prototyping and an ESP-32 microcontroller enabled WEDGE to come to life for the physical build. The Singularity library allowed bi-directional communication between the ESP-32 and the Quest 2 headset.
Bezi was used early in the development process to mock up UI screens, and also later on to rapidly create 3D assets for the Unity demo.
Meshy was used to quickly generate some of the 3D assets.","The ergonomic design for versatile foot interaction posed a significant challenge. We researched pedal designs and engaged in rapid prototyping to refine the wedge's shape for functional and intuitive use. Each team member who drew their preferences on paper after trying out the cardboard prototype came up with a different layout for the two buttons and two LEDs. Most of us preferred a symmetrical layout, so we chose to go this route. We also discussed the versatility of the device with regards to physical orientations, such as being used while sitting; standing; or held on the lap.
The team had varying levels of experience working with command line Git, and we overcame some issues through sending Unity packages, coordinating who would work on what, and having members with more Git experience handle most of the Git operations. Early in development, our idea focused on productivity with the foot controls allowing the user to move through ""virtual desktops"" that could have multiple windows, allowing the user's hands or hand controllers to do fine movements. However, we realized that the foot control was much more versatile than that, and this led to us reimagining our ""Footwork"" device as a general-purpose input/output device with a variety of uses. The communication between the ESP-32 and Quest 2 headset was a major challenge for the project. There were several milestones: getting the ESP-32 and Quest 2 connected over Bluetooth with The Singularity, getting button presses to work, and overcoming a button delay issue (which turned out to be because the ESP-32 was attempting to simultaneously send and receive serial data). With the help of mentors and Dominic's skills and persistence, the problems were solved.","Our Unity demo scene offers a sample of our vision, demonstrating the impactful simplicity of WEDGE’s design. With just two buttons and LEDs, we’ve created a device that can significantly enhance XR interactions and IoT integration.
We were happy that the challenges of connecting the ESP-32 to the Quest 2 were overcome, and that the Unity demos were linked to the button interactions in a cute and engaging way.
We liked that the WEDGE device was physically customizable, because users can detach the face panels from the end triangles and attach longer or shorter panels with different button, light, or sensor configurations.","Our journey reinforced the critical role of ergonomic design in creating intuitive user experiences. We learned that rapid prototyping can be very helpful in understanding how users may interact with a device, and that insights may be gained from individuals' reasoning behind their preferences. We’ve deepened our understanding of Unity’s version control and prefab workflows, which may be invaluable in our future work.
We also learned that leveraging AI tools such as Meshy or ChatGPT may be helpful when building quick proof-of-concepts or getting code to work in a time crunch.","WEDGE is poised for evolution, with plans to expand connectivity through Bluetooth and Wi-Fi and adapt to a broad spectrum of consumer and industrial needs. A modular faceplate design is also in development, catering to the diverse preferences of our users, including those with disabilities or limited mobility.
As a contribution to the open source hardware community, WEDGE has potential to provide value to multiple audiences and be built or modded by interested hackers. The 3D-printed side pieces allow the face boards to be interchanged, so users can move or add buttons, lights, or sensors as they please to extend or remix the functionality of WEDGE.","3d-printing-technologies, arduino, bezi-+-unity-integration-demo?), bezi-unity-bridge, bezi-unity-integration, buttons, chatgpt/gpt-4, esps, figma, github-gists-and-repositories, gltfast-unity-package, google-slides-presentations-(e.g., led-lights), oculus/meta-quest-2-headset, prusa-slicer-&-mk3-printer, runwayml, sidequest, suno-ai, the-singularity-(open-source-library), the-singularity?, the-wedge-device-(featuring-esp-32-microcontroller, unity-game-engine-(version-2022.3.12f1-lts), unity-xr-interaction-toolkit, xrtk-for-oculus-quest-2?), youtube-demonstrations-(e.g.",https://codeberg.org/reality-hack-2024/TABLE_13/
MIT Reality Hack 2024,Memories of Kumi-Daiko,https://devpost.com/software/memories-of-kumi-daiko,False,"One of our  teammates, Dana, joined a taiko group in college. Having made incredible friends and learning a unique instrument, after graduation she was left without a drum and—more importantly—without her friends to play with.","""Memories of Kumi-Daiko"" (Memories of a Group of Taiko Drummers) is an innovative Virtual Reality (VR) narrative experience that combines storytelling and hands-on learning to create a compelling journey through the world of taiko drumming. Most drumming games are single-player, scored experiences. We wanted to provide users with a unique opportunity to experience formative memories, fostering connections and learning the art of taiko in a judgment-free environment that lets you advance at your own pace, focusing on the fun and connection of a taiko group. GameplayThe experience begins with a monologue of Dana reminiscing on her taiko drumming experience as artifacts of her past float in a dark environment with glowing orbs. The user is the transported to a space with a single drum spotlit under a smoke effect. As the user approaches and picks up the drumsticks, the rest of the environment appears, including a red tori gate, sakura trees, and additional taiko drums. A short tutorial ensues, first familiarizing the user with hitting the drum, then moving onto playing quarter notes. From there, hologram taiko drummers appear to play a song, which the user can jam to on their drum (either practicing the quarter notes or free playing). The experience ends with a short monologue about how Dana never thought she'd be able to play the taiko again, but that now all she needs are her two drumsticks (in this VR experience). Users are then given the option to explore the space and freeplay!","With the help of an interdisciplinary team, we built Kumi-Daiko within the Unity game engine utilizing the Meta Presence Platform, with the final iteration of the project living on the Meta Quest 3 headset in VR. More detail on the tools we iterated through (in case you might want to try them, too!):https://docs.google.com/document/d/1B-T1cWpCreUsrI9EFZP9D7UjLL3qOGzVqG604vQUaps/edit?usp=sharing","Naturally, as a 2.5-day hackathon, we had to make tough decisions in a condensed time frame. We managed to pivot from mixed reality (SDK issue) and an advanced tutorial (experience length) to a short, narrative focused around community and connection. We also iterated through many tools for 3D generation (Meshy, Genie) and motion capture (Ultra Leap, Rokoko camera, keyed out camera).","We really came together on the first night of the hackathon to focus on our goal and work together to support everyone's role in creating a final product. Everyone in the team contributed something special and important to the final result, and by focusing the narrative on uplifting Dana's personal story, it really brought us together in a way that working on a more technical-focused project may not have.",Breaking down the idea to a core MVP was pivotal to our ability to finish an experience. We would also consider using PC VR for our next hackathon experience to overcome some intermittent standalone framerate challenges.,"This was our first module to get people started just interacting with the drum in a group jam experience. For future modules, we'd like to progress to more advanced taiko techniques through the lens of the history of taiko drumming as a fundamentally group musical experience. We are actively working on bringing this experience to Side Quest so you can try it yourself!","blender, c#, meta, premiere, presenceplatform, unity",
MIT Reality Hack 2024,SubLynk,https://devpost.com/software/sublynk,False,"Our team has friends who are part of the deaf and hard-of-hearing community and through their anecdotes and experience, we wanted to tackle the disabling world in this context by helping people, especially hearing people (the responsibility being put on the disabling world rather than the deaf and hard-of-hearing community) learn sign language in a effective manner. In addition, our entire team is multi-lingual and know how important immersion and comprehensible input is, so we wanted to develop an app that helps people learn unconsciously by repetition and association. We also watched Marvel's Echo TV series which had a scene with a futuristic pair of eye contacts that does some of what our app does, we did extensive research to see if real-time captioning and sign language with 3D gestures exists, it didn't, so we took it upon ourselves to make it a reality.","Sublynk is an AR Quest3 App that displays subtitles (English & Signed Exact English) over the head of who is speaking in real time. We are SubLynk and we help deaf people, their family and friends learn and communicate simplified ‘SEE’ sign language using real time 3D interpretation and captioning.","This project is created using Unity 2022.3.18f1, Microsoft's Speech API (for desktop) / Meta Voice SDK dictation (for mobile), MediaPipe's BlazeFace model, and the Leap2 Motion Controller (for mocap capture of the signs). We also designed SubLynk with accessibility in mind, accounting for accessible colors and size and places of captioning.","Real time voice dictation is a challenge in a space without stable internet for cloud processing so we were limited on what libraries we could use. We couldn't access the camera render texture feed of Meta's passthrough so we attached a cheap regular USB webcam from Amazon to the top of the headset for our face detection. And finally, converting 3D animation to 2D space (video plane) and coding the maths for perspective correction was a very big challenge but we've gotten it in a workable state. We've had to continuously ask ourselves who the target audience for and how this could help others and how we can make ourselves stand out to what has already been invented and we're pleased to say we're the only SEE real-time dictation app, let alone the fact it's in AR and can be eventually be built for multiple platforms.","This team should have never on paper worked, but we found our groove and really love and appreciate one another.
One programmer smushing libraries together and implementing the project, two designers learning mocap for the first time, one researcher, and one storyteller. We managed to make a working open-source project that conveys our vision of a future where everyone is passively learning and knows some sign language.",With consultation with the community we learned of SEE (signed exact english) and its effectiveness in transferring to ASL (American Sign Language) and easier learning curve.,"To improve the accuracy of the audio speech to text we're going to buy an aux powered boom mic to reduce any ambient sound input. Additionally, if we were able to get access to the leap we'd build out the library of recorded SEE sign language animations to include more of the dictionary based on frequency and refactor our codebase to run even faster and on more platforms and devices. Imagine an AR phone filter that does this for snap or sparkAR in an even lighter form factor. Our solution can scale to two-way interpretation, multiplayer, and network. Based on existing research, SEE can also help children who are severely, moderately or profoundly intellectually disabled, aphasic, cerebral palsied or autistic. It also has the potential to be used by teachers who want to incorporate sign language as an enrichment tool in their regular curriculum with songs and storytelling.
Facial expression and conveying emotions is important in sign language. So implementing a combination of facial and mouth movement recognition in the future will add valuable features to our technology.
We are hoping that in the long run we can make it possible for everyone in the deaf and hard of hearing community to be able to communicate with each other regardless of the language. Utilizing captioning and interpretation using the database of all languages, they will be able to interact by using SubLynk. Furthermore, using the Leap Motion 2, we were able to create an initial database of 3D hand gestures for sign language. Moving forward, we would like to create an open source library of SEE so that other developers can also use this open source library for projects in XR, web, mobile and more to make the world just a little less dis-abling. We will stay in touch and hope to get the resources to expand upon this further.","leap-motion, mediapipe, metaquest, unity, windows-speech","https://stevenharmongames.itch.io/sublynk-prototype, https://codeberg.org/reality-hack-2024/TABLE_88"
MIT Reality Hack 2024,Fire Drill XR,https://devpost.com/software/fire-drill-xr,False,"If a fire started in your home,would you be prepared to escape?Most of us are not. In fact, more than 80% of us have never practiced a home fire drill, despite the USFA recommending people to practice their fire escape plan at least twice a year. With 59.1% of the 3,790 fire-related deaths and 54.3% of the fire-related injuries 13,250 are from households, people simply are not prepared to protect themselves during such a catastrophic event. Current methods for fire escape preparation are not reliable. The USFA only provides simple guidelines and blank grid paper for people to draw their home escape plans. While other fire training apps have been made in XR, they have all been in virtual reality, prohibiting the user from properly planning an escape plan in their home. Through Fire Drill XR, users will be able to practice escaping a fire in their household, ensuring that they will be prepared if a fire ever ignites.","Fire Drill XR is a mixed reality app on the Meta Quest 3 that teaches users the basics of fire safety and escape. The app consists of four modes: Room Scanning, Safety Tips, Learning, and Fire Simulation. In Room Scanning, the user will scan their environment, while placing markers to indicate where any smoke detectors and exits are. In Safety Tips, the user will receive basic guidelines and recommendations for fire protection. In Learning, the user can practice certain fire safety tips in small modules. In Fire Simulation, the app spawns a fire randomly in the user's scanned environment. The fire will slowly spread throughout the environment, creating smoke and alerting the smoke detector. The user must do their best to reach an exit while avoiding the growing fire and smoke.","We used the Meta Quest 3 and the Meta Presence Platform for its mixed reality capabilities to build our app. The app was made using Unity, Bezi, and Figma.","For the UI and smoke effect, we wanted to add post-processing effects to create a cleaner and more immersive experience, but struggled to integrate it onto the Meta Quest 3. We also struggled to make the fire spreading as realistic as possible. One factor was due to the particle system, with too much fire causing the app to lag or crash. Initially, we also wanted the fire to spread based on the surrounding objects and materials (i.e. the fire would spread faster on wood than metal objects), but, after learning more about Meta Presence, had to narrow our scope.","We are proud to have made a functional demo for the fire simulation, with fire progressively spreading throughout the user’s real environment and the user being able to interact with the fire, smoke, and exits.","We learned how to use the Meta Presence platform by incorporating the several building blocks the platform has to offer. Additionally, we learned about practices used in other applications for realistic fire spreading.","Fire Drill XR’s trajectory extends beyond homes into commercial and educational sectors. With plans for expansion, the app is set to become an integral tool in workplaces and schools, providing tailored safety solutions. Featuring advanced learning modules, realistic simulations, smart home integrations, and community collaboration, Fire Drill XR is poised to revolutionize safety training. As it broadens its scope to cover severe weather drills and lockdown scenarios, the app reinforces its commitment to ensuring preparedness in every aspect of safety, making it an indispensable asset for a wide range of users.","bezi, c#, figma, quest, unity",https://codeberg.org/reality-hack-2024/TABLE_46
MIT Reality Hack 2024,Ball Lightning,https://devpost.com/software/ball-lightning,False,,Entangles a 3D model with 3D print using Unity and SteamVR tracking and Tundra Tracker,"IMU sensor for the tracked cube 
magnets
SteamVR tracking and Tundra Tracker","We tried to use IMU sensor but decided to switch to Vuforia to more easily track the tracked cube. There were some technical challenges with tracking the tracked cube using Vuforia (for instance hollowed-out caves within the cube, more time hacking Vuforia on Friday would have been ideal ) Richard saved the day with SteamVR tracking with Tundra Trackers, clinching us entanglement — the most crucial part of the design.",We entangled a 3D model and a 3D print,"How to present the story, so we can help people understand quantum physics, and help people understand the value of the potential future application.",who knows,"c#, steamvr, unity, vuforia",https://codeberg.org/reality-hack-2024/TABLE_17
MIT Reality Hack 2024,A Day's Blur VR,https://devpost.com/software/a-day-s-blur-vr,False,"Early diagnosis and intervention is key to supporting the quality of life for those with Alzheimer’s, but sadly, 7 in 10 people diagnosed with dementia say they wish they had known it earlier to start treatment promptly. So, it’s important to know the symptoms. We already have many videos and articles to help us learn about Alzheimer through traditional media forms. But an immersive and interactive experience has more potential to be visceral and memorable.","The app we developed provides a novel take on the use of VR and the experiential learning simulation. One user goes through a VR simulation of an Alzheimer’s patient going through tasks in a world where memory is not reliable. Normal objects become unrecognizable as your grasp on reality fades and help is needed for the user to remember their daily tasks. The other user acts outside of the headset using an asymmetric display of the world to guide the VR user through the tasks. This caregiver user sees a “normal” view of the world via the PC monitor of the same system the VR headset is being run on, but as they try to guide the Alzheimer’s patient to do simple tasks like “eat an apple”, the Alzheimer’s patient struggles to see the same objects that the caregiver does. To the patient, the apple is an unrecognizable blob. Their memory of even basic concepts like food is failing them, and that can be frustrating. We will use our app to acknowledge that frustration and support a healthier connection to the reality of the disease so that caregivers know better what the Alzheimer’s experience feels like.","We build our project using the Meta presence platform, and we used Bezi for asset modeling.","This has been an educational experience for all of us. Many of us on the team were new to XR but we were passionate and willing to learn. The Meta presence platform required a bit of learning curve, which was our main obstacle on day 1. On day2, we faced a lot of difficulties transporting Bezi models to unity.",We are able to build a complete game which calls for awareness for people with Alzheimer.,,,"bezi, c#, presence, unity",https://codeberg.org/reality-hack-2024/TABLE_100
MIT Reality Hack 2024,Destiny,https://devpost.com/software/destiny-w0y4b7,False,"Intrigued by the original concept of the Destiny System, our team decided to do the “Hack the Hack” track in order to re-conceptualize the Destiny System while embracing this year's theme of connection.","Destiny is your personal reality hack guide. As soon as a participant is accepted, they are led to sign into a web-based app that hosts their own Destiny Guide (inspired by MIT’s Tim the Beaver) already familiarized with initial information provided by the participant during the application process through InWorld.ai. Through integrating the Meshy.ai “text to texture” API, individuals are able to generate and customize varying attributes to create a guide they personally resonate with. Before the hackathon, Destiny will: During the hackathon: After the hackathon: This is Destiny!","Meshy.ai, Inworld Studio, Blender, Flask API back-end",,,We learned that it's important to find an idea quickly and carefully add to the project as the main purpose of the project can get lost storm of new ideas.,We hope to continue working on the project and for the hackathon staff to see the potential of Destiny so it can get to a state where it can eventually be used at future hackathons.,"8thwall, figma, inworld, javascript, meshy.ai",https://www.canva.com/design/DAF7AjJWYa8/aiPVRWyt7HTMoRa2EfYrcQ/view?utm_content=DAF7AjJWYa8&utm_campaign=designshare&utm_medium=link&utm_source=editor
MIT Reality Hack 2024,Wave Goodbye,https://devpost.com/software/wave-goodbye,False,"We drew inspiration from the stress we experienced during the project. Additionally, the concept of sending a ""Message in a Bottle"" by the beach stemmed from childhood memories of team members. Our aim was to share our stress with the vastness of the ocean.","Our app ""WaveGoodbye"" provides a safe space for individuals to release stress and unwanted emotions. With a user-focused design, the app allows people to symbolically seal those feelings into a bottle and throw it away, whether into the ocean or sky. By letting go in this way, WaveGoodbye offers a method to rid oneself of negative thoughts and emotions.",We utilized Unity and Blender in collaboration with both a designer and a developer.,"Facing a shortage of developers for Unity posed a technological obstacle for this project. Despite this challenge, we came together, engaged in mutual learning, and sought mentorship to navigate through the hurdles.",We take pride in transforming our initial concept into an interactive space where individuals can openly discuss and release their deep-seated stress by casting a symbolic bottle into the ocean.,"Our learning journey encompassed collaboration, teamwork, and gaining insights into connecting VR headsets to computers. Additionally, we delved into understanding the significance of virtual environments.","Future Improvements 
Implementing user feedback.
Exploring the option to change the location.
Considering adjustments to the timing of the experience.","blender, figma, unity",
MIT Reality Hack 2024,Voyagery,https://devpost.com/software/voyagery,False,"We were inspired by games like Papers, Please that explore the challenges and hardships of the U.S. immigration process. We wanted to place players directly in the shoes of someone going through that process, and we felt that VR is the ideal medium for that kind of experience.","The experience places players in the shoes of an immigrant seeking United States citizenship. They go through the naturalization exam, answering U.S. civics questions, and are also asked a number of personal questions about their motivation for seeking U.S. citizenship.",Voyagery was built in Unity. We used the Meta Presence Platform to implement gesture-based input to interact with the game.,"We ran into a number of early developmental hurdles, particularly when it came to developing a VR experience on Mac. Our entire team uses Mac computers, so testing proved difficult, as tethered VR testing generally doesn't work properly on Macs. We were also new to the Codeberg version control platform, so there was also some learning to do when it came to understanding how it stores code and integrates with Unity and Github. Many members of the team were new to VR and Unity development, so there were a number of growing pains when it came to learning VR logic in Unity. Core interactions such as gesture-based input proved difficult to implement, though we managed to get them functioning for the final product. And as always, scope and ambition proved to be large hurdles. We weren't able to build everything that we had envisioned, so we had to make some tough early decisions (and some later ones as well!) about what to cut from our submission and what to keep.","We managed to develop core gameplay that we believe is fun and engaging. We also learned a lot about developing VR games for Meta headsets using Unity. We are proud that we managed to create a playable demo of our original concept, given the number of developmental hurdles that we faced.","Through researching for this project, we learned quite a bit about the U.S. citizenship application process. We learned how involved it is and how arduous it can be. We also learned a great deal about what motivates many people to immigrate to the U.S. We also learned a lot about developing for VR. We learned that developing and testing on Mac is difficult and not ideal at the current point in time. We learned that VR development, given the relative infancy of the medium, is very difficult. It's best practice to expose oneself to VR development environments to gain exposure before sailing the treacherous seas of VR hackathons and game jams. Most importantly, we learned the value of teamwork and collaboration. Even in challenging times, communication and camaraderie are key in getting a team across the finish line.","Despite the challenges we faced, we are proud of what we've built, and we believe that the foundational concept is strong. We would love to continue building the concept out in the future, as we believe that it fulfills an important humanitarian need.","presence, unity",https://codeberg.org/reality-hack-2024/TABLE_85.git
MIT Reality Hack 2024,Team #42 - Ancestral Apron,https://devpost.com/software/ancestral-apron,False,"Each of our member had different sources of inspiration! To keep it short however, Jessica’s read on Crying in H-Mart by Michelle Zauner was what interested her on the connection that immigrant children have to their parents’ cooking. And from another one of our memebers, Vencent, he felt inspired to built this project because of his previous experience at NASA, where he meant several Astronauts who expressed that one of their greatest desire during their missions was home-made food!","Features that are included within this are a functional recipe task list, a dynamic avatar of the loved one the recipe is based on that narrates the scene, and the video recipes with the simulated hands. To start, the user is to select a pre-loaded recipe done through a recording by their loved one. Afterwards, a task list is generated from the video itself as well as the avatar that is to resemble this loved one. Hands from the POV video will be placed in front of your own for you to then follow the cooking tutorial. Occasionally, the avatar will speak up as it recounts a story that was either stored by the loved one themselves or taken from the video.","We used a variety of open-source and offered recourses. Mainly, our project was built in Unity with a basis on the XR Interaction Tool Kit for our MR experience! In addition, we got lots of help from Sponsors such as Ultra Leap, Bezi, and Meta who not only offered great developmental tips but also amazing support.","Our greatest challenge during this hackathon was quite simply compile issues. Throughout the last 4 days and 3 nights, at least one of our members each had a problem where: case A - our Quest 3 Headset just decided to stop working, case B - someone couldn't commit or pull to our Codeberg Repository for whatever reason, and/or case C - when","Each of us have something we hoped for during this hackathon! For Jessica, this has been her first XR/VR/MR hackathon where she feels very grateful to be meeting and working with such a talented cohort of XR enthusiasts. For Vencent, it was learning about the ever-changing industry shown by the sponsor.","C# development, the Unity engine, navigating a Codeberg repo, and more!","Moving forward we plan to add more diverse recipes to cater to the different various tastes and cultural backgrounds around the world. We also hope to integrate more personal and meaningful story telling into the story, and as well incorporate a deeper level of ShapesXR to delve into a social scene.","bezi, c#, github, meta, python, ultraleap, unity",
MIT Reality Hack 2024,Mushin,https://devpost.com/software/mushin,False,"Inherited from a Chinese/Japanese Zen and Daoist expression meaning “no-mindedness”, “Mushin” describes a state of mental clarity and focus attained when one's mind is liberated from anger, fear, and ego. Research on Mushin showed an intuitive understanding of ourselves as opposed to logic. Mushin was born from the observation that mixed reality lacked dedicated mental health experiences. Recognizing the widespread challenges people face in mental health, we aimed to fill this gap by creating an enjoyable yet minimalistic experience within MR. Our vision extends beyond individual wellness. Mushin is not just a game; it's a fun and collaborative solution designed to help navigate the complexities of mental health in the immersive world of mixed reality.","Mushin is a mixed reality experience dedicated to mental health. Users can choose from a range of exercises, including meditation, stretching, journaling, and focused breathing, all within the immersive world of MR. It offers a personalized and holistic approach to well-being, empowering users to proactively engage in activities that contribute to their mental health and relaxation. With breathing exercises as the primary feature implemented for the initial prototype, “Mushin” serves as a tangible manifestation, guiding users to access their breath more mindfully—more than just a physical activity, but a key to unlocking composure and self-awareness. This project has a more active version with collision and interaction in VR with Unity and a more passive guided version in BEzi in AR.
""Mushin"" integrates the experience of breathing with meticulously crafted atmospheric environments and characters.","We built the UX/UI, scripts, and references in Figma. We made the 3D models in Blender. Characters in Procreate, Animations, and the rest in BEzi",The challenge we ran into was using Unity at the start as we experimented we found that BEzi was a better option for the scope of the project.,Being able to work with a great team and on an awesome project that was not compromised in such a short amount of time.,Making experiences in MR is not extremely difficult. BEzi is a great tool to use for making this.,,"bezi, blender, figma, metaquest, procreate",https://bezi.com/2r96ne
MIT Reality Hack 2024,GenZen,https://devpost.com/software/theta-8ot2z7,False,Millions experiencing social anxiety disorder and withdrawal from society lack adequate support and are often encouraged by pharmaceutical companies to rely on medication. Our goal is to assist these individuals in gradually reintegrating into society.,"Our distinctive approach invites users to embark on their journey of connection at their own pace. Begin by interacting with your own reflection and gradually ease into non-confrontational activities with others, such as standing on a circle, touching, and clapping hands. Progress naturally to more advanced levels of interaction, breaking free from isolation with each step.","Our shared experience involved employing a multiplayer server (Nakama) for co-presence among unlimited online participants. Utilizing Unity as the game engine and Meta all-in-one SDKs, we integrated Meta Quest Pro, Quest 2, and Quest 3 for an immersive experience.","While it was challenging to deploy multiplayer, the ability to enable our users to feel as if they are inhabiting the same space was crucial to the intimate connected feeling we were attempting to engender.","We take pride in our collaborative efforts, sharing a common goal and intension to create a support app for an underserved community. Despite the limited time frame of the hackathon, we are pleased with the progress we've made. Our aspiration is to persist in the development of this app beyond the hackathon, striving to bring it to its completion.","We gained valuable experience in collaborating within a diverse team, working alongside teammates with distinct skillsets and styles. Our journey included tackling technical challenges and finding solutions through effective teamwork. From the initial stages of ideation and brainstorming to the development of a minimum viable product, we have covered a comprehensive spectrum of the project development process.","We aim to persist in the development of this app, steering it towards completion, and eagerly anticipate users giving it a try once it reaches the finish line.","c#, love, nakama, unity",
MIT Reality Hack 2024,LivelyLens,https://devpost.com/software/livelylens,False,"Dementia has wide-ranging, direct, and indirect effects on the well-being of older adults, their families, and the costs imposed on public programs according to Dementia statistics, Alzheimer's Disease International (ADI). There are over 55 million people worldwide living with dementia in 2020. This number will almost double every 20 years, reaching 78 million in 2030 (Prohaska et al., 2009). The impending surge in dementia cases, particularly among the baby-boomer generation, poses a substantial burden on societal and economic resources, with an estimated $148 billion burden on direct healthcare services, respectively, according to World Health Organization. This demographic shift necessitates responsive health and social systems to cater to evolving senior needs (Godman, 2022).
There is no medication yet invented that can cut your risk of dementia by 50%, but walking about 10,000 steps per day might, suggests a study published online Sept. 6, 2022, by JAMA Neurology. 
At larger assisted living facilities, individuals who develop any level of cognitive difficulty are told that they must move into memory care units rather than be able to stay in mainstream assisted living. They’re lumped together in a one-size-fits-all model that often has individuals with mild or moderate confusion mixed in with others who have severe behavioral disturbances. These units can be quite distressing to individuals with less severe symptoms who don’t understand the disruptive behavior of others according to PMC.","LivelyLens is designed to enhance the well-being of older adults in senior assisted living environments – a Mixed Reality (MR) experience that seamlessly integrates nature into their living spaces. By creating captivating MR scenes, we integrated virtual gardens adorned with plants and butterflies into their rooms, fostering a sense of safety and tranquility. Our goal is to promote the activeness of older adults through immersive journeys that mimic the beauty and serenity of the outdoors.
But that's not all – our technology goes beyond mere escapism. We enable the 3D scanning of familiar objects, allowing seniors to surround themselves with cherished items, triggering memories, and providing a comforting environment. We transform the daily walk through a nursing home into a nostalgic journey down memory lane, creating a unique blend of technology and personalized experiences.
As we continue to evolve our product, upcoming features such as hand tracking promise to elevate interactivity, making the MR experience even more engaging for older adults. Our commitment aligns seamlessly with the United Nations Sustainable Development Goals, particularly targeting Goal #3 (good health and wellbeing), Goal #10 (reduced inequalities), Goal #11 (sustainable cities and communities), and Goal #17 (partnerships for the goals). Through cutting-edge technology and thoughtful design, we strive to enhance the lives of seniors, fostering a sense of joy, connection, and well-being.","We employed rapid prototyping techniques within Bezi, where we crafted the initial scene layout, devised comprehensive scene plans, and designed user interactions. The initial static assets were implemented during this phase. To enhance the overall scene aesthetics and interaction dynamics, we integrated the scene into Unity. The animated components were crafted using Blender. These animated assets were then imported into Unity. We also strategically incorporated Meta Presence Platform features such as passthrough, depth API, OVR Scene Manager - for the planes and volumes, and Audio SDK for the development of our MR application.","In terms of accessibility design, the majority of our users are older adults who have limited or no experience with extended reality (XR) technologies, leading us to make deliberate decisions such as opting not to use controllers or hand gestures. Asset optimization presented challenges as well, with the headset imposing overall limits and rendering difficulties for main scene components like natural elements and lighting. To address these, we strategically selected and modified static and animated elements, although we encountered some issues with importing animations from Blender to Unity. Our workflow from Bezi to Unity brought about considerations in prototyping, asset arrangement, collision boxes, functionality capabilities, and animation integration. Additionally, the implementation of the OVR Camera Rig posed challenges, especially when attempting collision triggers without teleportation using controllers. Despite these hurdles, our team has been resilient in finding solutions, ensuring a thoughtful and accessible mixed-reality experience for our users.","We conducted swift and thorough tests of our vision and workflow across diverse platforms, ensuring adaptability and seamless integration. This process allowed our team to rapidly acquire new skills and foster effective collaboration, contributing to a more dynamic and innovative development cycle. Through this experience, we deepened our understanding of the positive social impact extended reality (XR) can have on marginalized communities, becoming advocates for its potential to reduce inequalities. By aligning our efforts with the United Nations goals, we actively contribute to the promotion of a more inclusive and equitable future through the transformative capabilities of XR technology.","Learning cutting-edge technologies such as Unity, Bezi, and Blender, our team fostered effective collaborations among members with diverse skill sets. This collaborative environment not only facilitated the exchange of knowledge but also exposed us to emerging startups and innovative tech stacks. We delved into the intricacies of effective prototyping, honing our skills in efficiently bringing ideas to XR design. This experience not only broadened our technological toolkit but also reinforced the importance of interdisciplinary collaboration for achieving successful outcomes.","In envisioning the future of our project, we anticipate significant advancements in 3D scanning technology, making the process simpler and more interactive within the next 5-10 years. Imagine a scenario where older adults or caregivers can effortlessly capture 3D objects with a single photo from their smartphones or record scenes, creating a seamless way to share memories with friends and family. Moreover, we anticipate the integration of hand tracking and object interaction, empowering older adults to physically engage with the virtual environment. Future developments may include time awareness and seasonal changes within the scene, encouraging repeated visits by older adults. The potential for a multiplayer plant-sharing experience is also on the horizon, allowing seniors to witness the growth of friends' plants and offer assistance in nurturing them. Interactive audio features will enable communication with friends and family within the virtual space. Lastly, the incorporation of Machine Learning holds the promise of tracking older adults' medical records and demographic information, enabling the creation of personalized scenes and delivering enhanced wellness support. This vision represents our commitment to continuously evolving our project for the well-being and enjoyment of older adults (Cunha et al., 2023).","bezi, blender, meta, quest3, unity",
MIT Reality Hack 2024,StellarScope: Connecting with the Cosmos,https://devpost.com/software/stellarscope-connecting-with-the-cosmos,False,"Many constellations are only visible at certain times of the year for certain latitudes, for a few hours each evening. In addition, light pollution in urban cities and limited access to a telescope can make astronomy inaccessible to many people. VR makes a difference. A VR astronomy lesson is available to anyone regardless of where they live. Whether in a rural, urban, northern, equatorial, or southern hemisphere home, StellarScope can widen everyone’s horizons.","StellarScope is an immersive, interactive VR astronomy lesson for beginners. Players learn about convex, flat, and concave lenses, and how telescopes use lenses to bend light and magnify images. Players can connect the individual components to assemble a telescope. By looking through this telescope, players can view and learn about the constellations.","January 25: Team formation, brainstorming, scoping, and scheduling
January 26: Learning Unity and Blender, setting up the VR environment, writing the script for the project
January 27: Coding the lens optics, coding the telescope interior, coding the telescope assembly, finding and editing audio, learning Audacity.
January 28: Coding the constellation scene, fixing bugs, learning Premier, making the video.","Learning Unity from scratch was the biggest challenge by far–we often ran into bugs, coding issues, sync issues, and Github issues. By having one person work on the main Unity project, two people work on developing assets on different computers, and two people working on scripting, audio editing, graphic design, and logistics, we created an efficient workflow.","We were able to complete 99% of the things we said we wanted to accomplish. We created our project concept, timeline, and tasklist on the first day. We followed our timeline accurately. Our team values accessibility, and StellarScope makes astronomy accessible to everyone–narration is provided through both text and audio.","Most of the team learned every tool from scratch, including Unity, Adobe Premiere, Blender, Figma, ElevenLabs, and Audacity. Many people in the team also learned about optics, telescopes, and constellations from scratch!","We’re planning to reach out to astronomy classes and astronomy clubs around the country. One of our team members is a former astronomy club president, and can spread the VR game throughout a network of students. If the game continues to be successful, we’ll send it to libraries with VR headsets so it’s accessible to members of the public.","blender, c#, elevenlabs, sketchfab, turbosquid, unity",https://codeberg.org/reality-hack-2024/TABLE_52.git
MIT Reality Hack 2024,SupportXR,https://devpost.com/software/myit,False,"The inspiration behind our MR Virtual Technology Support system stems from a keen understanding of the daily challenges faced by individuals worldwide. We recognize that while some problems serve as healthy challenges, others are unnecessary obstacles that waste time and hinder efficiency. These are the issues that, with expert guidance, could be resolved swiftly and effectively. Additionally, the COVID-19 pandemic has prompted a shift in lifestyles, leading to a surge in remote work opportunities. This change, however, has resulted in reduced accessibility to technical support. Our MR technology is designed to bridge this gap, facilitating a smoother problem-solving process for both tech support personnel and users. Our initiative is driven by an ambition to refine the way expert assistance is delivered, especially in situations requiring hands-on intervention. From resolving issues like a flat tire, a malfunctioning refrigerator, or factory line troubles, our goal is to deliver instantaneous solutions right to the user's location, bypassing the traditional delays encountered in conventional support methods. Moreover, our aspiration extends beyond providing immediate solutions. We aim to create an environment where users not only receive solutions but also acquire practical knowledge through real-time, expert-led guidance. This method ensures immediate problem resolution while simultaneously empowering users with valuable skills and knowledge.","We aim to establish a virtual link between tech support professionals and individuals seeking assistance. This means that when a user requires help, both the user and the tech support simply don their headsets, instantly connecting with each other without the need to travel. Within moments, the tech support can visually guide and annotate the user's problem, whether it's a flat tire or another technical issue, providing step-by-step instructions for resolution. This innovative approach not only enhances the efficiency of problem-solving but also imparts practical skills to users, equipping them to independently handle similar challenges in the future.","Our project was created using Unity 2022.3.18f, a cutting-edge development platform renowned for its robust capabilities in creating immersive experiences. To further enhance this project, we utilized the Meta Quest 3, one of the latest and most advanced virtual reality headsets on the market. This choice was pivotal in achieving a high degree of realism and user immersion.
Central to our project's functionality is the Meta All in One SDK. This comprehensive software development kit has been instrumental in integrating groundbreaking features like passthrough and hand tracking, which are the cornerstones of our Extended Reality (XR) experience. These features not only elevate the user interaction but also ensure a seamless and intuitive interface.
For the multiplayer aspect of our project, we chose Normcore. This decision was driven by Normcore's exceptional performance in hosting multiplayer environments. Its robust framework allows for a smooth, real-time shared experience, essential for collaborative and interactive sessions in our XR environment. By leveraging Normcore's capabilities, we've been able to create a multiplayer platform that is both reliable and engaging, offering users a unique and collective virtual experience.","During the development of our application, we encountered numerous ideation and technical obstacles. Our team experimented with various methods to establish a client-agent connection. Ultimately, we created a dedicated server to facilitate stable and efficient communication.","We take pride in developing a fully operational UI/UX interactive prototype, incorporating Bezi and a Unity demo app. Notably, this project marked our first venture into creating a multi-player environment. The scenarios and functionalities align seamlessly with our objectives.","This project was our inaugural experience with Benzie, Unity, normcore, and participating in a reality hackathon.","Looking ahead, we plan to enhance SupportXR by integrating additional features and a user-friendly table list. This will assist users in resolving specific issues. Should the project garner positive feedback and successful pitches, our next step will be to advance the technology and strategize for market entry, aiming to aid a broader audience.","bezi, c#, meta, normcore, presence, unity",
MIT Reality Hack 2024,Super Smash Trash,https://devpost.com/software/super-smash-trash,False,,"Grab your trusty physical controller: a trash bag - and try to catch the trash falling from the heavens, before it hits the ground! With each round, the trash increases by the amount of trash an additional person produces per day. How low can you survive in this infinite game mode? You want to reduce the amount falling from the sky? Try to catch the power-ups - these are reusable products which can replace the disposable products and stop them from being added to the trash. Some amount of waste is inevitable, but by using reusable items whenever possible, we can greatly reduce the amount of trash we generate.",We built this in Unity for the Meta Quest 3 using the Presence Platform as well as ShapesXR. We generated many 3D assets using Meshy.,We had some trouble getting the 3D model materials from Blender into Unity at first - but got some help from mentors and were able to work through it!,"We created a seamless and intuitive UI and compelling visualizations for an overall immersive experience.
We were able to add a physical element which made the game much more immersive and fun. We also were able to add compelling sound effects!","Fun leads to fun! We focused on working well together as a team and enjoying the hackathon, and are very happy with what we were able to create. We learned to use Miro to organize our workflow and plan what we could get done. We also learned how to create and use 3D models in Unity and Blender, and how to navigate some of the challenges of using AI or photogrammetry to create 3D models. We learned how to rapid prototype with ShapesXR and port it to Unity, how important sound design is for guiding the user, new tools for sound design, iterating quickly on developing a UI, how engaging physical elements can be for mixed reality, how important it is to have a healthy team dynamic and build team spirit before starting work. We learned how to do 2D -> 3D iteration for working inside VR.","We hope that Super Smash Trash will inspire people to make more sustainable consumption choices. We had a wonderful time working together, and hope to work together again soon! Thanks to the 2024 MIT Reality Hack organizers and volunteers!","c#, meshy, metaxr, shapesxr, unity",https://codeberg.org/reality-hack-2024/TABLE_107
MIT Reality Hack 2024,Cycle,https://devpost.com/software/cycle-j4mrpa,False,"What if we could create a connection between manufacturers and consumers by providing information regarding product recycling to the consumer and connect them with available recycle deals? The vision we have is that the user will be able to easily capture an image of any object in their possession using AR glasses and receive information on how to recycle, where to recycle, as well as potential trade in programs, reducing the friction between intent and action.",,,"Technical Development: Because Google Bard does not have an official API or a Unity plugin, we had to create a web server using python code from an open source python package that queries the google bard API. This combination of having to rely on writing Unity code to communicate with an HTTP web server as well as having that web server use an unofficial API created a multitude of issues that dropped the success rate of image analysis. While writing Unity and python code that could get a Google Bard response proved to be time consuming, once we were able to more or less consistently get responses, the challenges weren't over. We had to engineer prompts that could provide more consistently formatted answers, and write parsing code that could handle the variation of response and display info in an organized way in AR. User Experience: Since team's designers had no previous experience with XR, visualizing and writing out the logic for the flow of the application and thinking how to make the navigation as intuitive as possible was challenging.","First, we are proud of the journey we took as a team. From being total strangers to each other, we became great friends that were able to create and execute the project we envisioned. Second, we are proud that we were able to make our demo application work and figure out how to connect the AR glasses to Google Bard without an official API. We are also proud of the flow and UI our designers came up with.",,"There are several important features we want to implement in the future: Visualize the outcome: By scanning an object, users can see the real-world environmental impact of recycling it. For instance, scanning a plastic bag might show how much lumber could be made from recycled bags, providing a tangible sense of the positive change. Repair and revive: The app's AR deconstruction and play features could inspire users to give new life to old items. Imagine virtually disassembling a chair and customizing it with new parts, or using AR to turn a broken vase into a unique planter. This interactive experience could foster creativity and appreciation for repairable items. Overall, Cylcle's potential lies in using AR to connect users with the consequences of their choices, motivating them to make more sustainable and resourceful decisions. By seeing the immediate and tangible impact of their actions, users are more likely to embrace recycling, reuse, and repair as ways to contribute to a greener future.","adobe-illustrator, figma, qualcomm, unity","https://codeberg.org/reality-hack-2024/TABLE_86, https://www.figma.com/file/qDkawVxH9je5CpBtiCHmvf/Natalia-Carrizo's-team-library?type=design&node-id=0%3A1&mode=design&t=CIPdYg8aAGx3RgLn-1"
MIT Reality Hack 2024,Mentor Reality,https://devpost.com/software/mentar,False,"Connection is crucial for us to feel support from each other, especially in mentoring scenarios such as workshops, design springs, boot camps, as well as hackathons. However, in lots of cases, the mentoring process encounters challenges due to the large amount of attendees, complex event structures, and multi-channel communication. These difficulties might cause inefficiency in mentoring because extra cognitive/workload is required for mentors in sourcing and digesting information and navigating to the teams and their problems. This slower reviewing and mentoring process will finally result in fewer mentorship opportunities and capability for teams who need help forming and building innovative ideas and solutions. We take MIT Reality Hack as a unique opportunity for us to build Mentor Reality, a Mixed Reality + Web app mentorship solution for collaborative events, innovative institutions, and other mentorship scenarios to help boost innovation by creating a connective and supportive mentorship process.","Mentor Reality is a mixed reality tool developed for the Meta Quest 3 to facilitate the connection between mentors and teams at hackathons in need of assistance. Through the team web portal, teams can submit their requests via tickets, and mentors will receive notifications directly on their headsets to locate and address these requests. Our solution introduces a controller-free hand tracking system with an interactive sphere that visualizes team requests, guiding mentors seamlessly to the teams in need. Before the hackathon begins, organizers visit each table and spawn a spatial anchor in the form of a glowing ball. This becomes the primary means of communication between mentors and teams, emitting different colors depending on the team's status. Throughout the hackathon, teams can use the web portal by submitting their team number to sign in, granting access to their personalized checklist, which event organizers can customize according to their preferences. To the right of the personal checklist is the help request form, where teams can initiate one by clicking ""create a help ticket."" The form prompts users to specify their request type, the hardware they're using, and a description of their problem. After submission, the sphere located at the corresponding table will change from purple (idle state) to red (problem state), indicating the active status of the help request. Because hacking rooms can be vast and crowded, an additional feature we added is the map feature, allowing mentors to quickly locate the table if it is in a different room. Additionally, the red status ball contrasts significantly with the purple idle ball, making it easy to find in a room. Once the mentor reaches the table, they can tap the red floating ball to get a summary of the team's help request. From there, the ball will turn yellow, signaling active assistance by mentors. If they can resolve the problem, the mentor can click the ""solved"" button, and the ball will turn green for a few seconds before returning to the purple idle state. However, if the mentor needs others to take a look, they can press ""place back,"" and the ball will return to the table, turning red. An additional interaction we added to foster mentor and team connections is the ""like"" system. While wearing the headset, mentors can give a real-life thumbs up, adding 1 like to the team's portal. With our system and web portal, we can easily assist mentors and teams in ensuring their project is on the right track!","We built this in Unity, Figma, and with the Meta presence platform, node.js, and HTML/CSS.",Our two biggest challenges were lack of experience and time. None of us had any prior experience in web servers and using the Meta 3! Time was another challenge as the development time flew by everyday!,We're really proud that we were able to get a good prototype down before the deadline! We really like the visual design of our app.,We learned how to do web socket networking and manage the product development cycle in a short time frame( eg. prioritizing features).,We would love to add a pathfinding feature where mentors can get led to the table rather than them locating it on the map.,"figma, meta, unity",
MIT Reality Hack 2024,netwARk,https://devpost.com/software/netwark,False,"For Reality Hack, we found ourselves in the typical team formation paradigm: too many people, too little time to talk, not sure who to talk to. Initially inspired to make an alternative team forming contingency plan, we are buildingnetwARk. We love AR, but AR tech needs three things to succeed: AR shared among a few may create friendships, but shared among hundreds creates a community. We envision a future wherelarge conferences, hackathons, and networking eventscan provide their attendees withcheap, easy-to-useAR glasses to supplement their experience - to extend their first impressions beyond reality. Over the last three days, we have built the tech and application to do just that.","netwARk allows attendees to fill out a fun survey to learn about their interests, background, hobbies, and contact info before the event. You also write in natural language what kind of people you want to meet at the event. Users upload an image of themselves and they're now immediately part of their event's personalized AR social ecosystem. As you walk around to meet new attendees, facial recognition is used to identify them and display their own personalized bio of themselves to the rest of their peers right on their headset. Everyone you meet is automatically logged and emailed to you, so you can save their contacts and connect with them afterwards on LinkedIn. As you walk around we highlight individuals you're looking for, whether because they work at your company, or have complementary skillsets that would make you a superstar hackathon team. Our AI automatically matches you with complementary people and highlights them with stars in AR so you know who to meet.","HardwarenetwARk is built upon the open-source AR headset,https://docs.projectnorthstar.org/project-north-star/, where we (did our best to) rebuild the graphics driver from OpenGL to Python and added an Ultimo 1080p $10 webcam for facial recognition. Front-endWe have a website where users register themselves that is designed on Figma and built with HTML, CSS, and vanilla JavaScript. It is connected to our Google Firebase database. Back-endWe use Google Firebase's NoSQL Firestore and Cloud Storage databases for storing new user's bio information and their headshot. We have a Python script running deepface with a SSD face detector backend along with OpenCV for rendering text on the headset and for animations. We use OpenAI to analyze the characteristics of the people and to help match people to the type of people they state they are looking for.","Using OpenGL to distort text for the headset was hard and we didn't get it working in time. Luckily, even without distortion the text is very readable in AR. We also built the headset and face a lot of hardware issues with the 3D printed parts. Bryant helped us a lot with getting the headset to work.","Our ground-up methodology. From the graphics, headset, facial recognition, website, database, and the entire software infrastructure connecting all of it, we were thrown in the deep end to get it all working together without the use of common AR/VR development environments.",We learned how to use the open-source North Star headset and about OpenGL. Graphics is hard. Face recognition is one-shot. AR can be cheap. Performance is hard to optimize when not using native VR/AR tools (lots of room for improvement).,"Improve performance. Add the capability for people to make custom graphics, banners, animations, and photos/videos around themselves. Make the platform more expressive and creative. It's like your personal website, but in AR! Hopefully have this potentially incorporated in next year's Reality Hack!","css3, firebase, html5, javascript, jpg, json, nosql, numpy, opencv, python, smtp, tensorflow",
MIT Reality Hack 2024,Starscape,https://devpost.com/software/starscape,False,"For secondary students, studying music can be daunting. 50% of students quit within 1-2 years of starting to learn an instrument. The complexity of mastering musical skills often demands substantial time and perseverance, potentially leading to feelings of intimidation and frustration. Additionally, the learning process traditionally lacks immediate gratification and those pivotal ""aha"" moments that spark sustained interest and motivation. We wanted to change that and provide an alternative music creation experience that would be rewarding, playful, simple and delightful.","Starscape is an immersive MR experience that inspires aspiring musicians to engage with and understand musical concepts. It does this by allowing users to create visual, shareable galaxies, thereby fostering connection, confidence, and creativity.
Players begin in free play mode, learning to use their bodies to create stars in their virtual environment.
When ready, they can effortlessly transition into learner mode, which guides them through the process of creating a galaxy. This mode bridges abstract music theory concepts with enchanting visuals. ML algorithms tailor the experience, enabling users to build knowledge at their own pace, explore their interests, and share these experiences with friends and family, taking pride in their creations.
This serves as a foundational stepping stone, nurturing an interest and appreciation for music while mastering abstract concepts and empowering users to take the next step when ready.",,,"We built a mixed reality experience, and using of natural gestures through full body tracking. We explored the concept through storyboards in Figma and assembled the experience in Unity using Meta Presence Platform, C# and particle effects. We used a series of colliders to simulate custom gesture and trigger events.","Creating an experience that straddles intuitive play and education taught us about learning mechanics. To ensure users learned but still had a delightful experience, we explored combining learning theory around assessment with gamification. As a team with diverse backgrounds and areas of expertise, we also learned to communicate and collaborate across disciplines.",We hope to continue developing our technology to enhance the curriculum and visual effects. We plan to partner with museums and entertainment companies to build trust and awareness before then launching offering subscriptions to schools.,"c#, meta-presence, meta-quest, particle, unity",https://codeberg.org/reality-hack-2024/TABLE_31
MIT Reality Hack 2024,RealAccess.VR,https://devpost.com/software/realaccess-vr,False,"VR and MR today lack accessibility for people with disabilities due to their heavy reliance on visual content, limited customization for visual impairments, challenging environment navigation lack of sensory cues of spatial awareness, and insufficient alternative inputs like audio or haptic feedback. Additionally, there's a lack of established accessibility standards and guidelines.","As the visitor enters the VR environment they find themselves at the studio of Joe Davis’s House. They see an AI avatar of Joe Davis in front of them and the environment gives visual descriptions of the avatar and the studio. The visitor can interact with Joe Davis using speech and text by asking questions about bioart. The avatar replies in speech with subtitles. The Joe Davis Avatar then invites the visitor to tour his house. As users get close to the boundaries they get a synesthetic sonic experience near collision with boundaries objects. As the visitor explores the artist’s residence they can change the contrast by pressing the button on the left-hand controller, and use the magnifying glass for magnifying within the environment.","Having access to Joe’s apartment we produced several volumetric scans and experimented with the real sense volumetric scanner. We used accuRIG to create, rig and animate Joe’s avatar and import it to Unreal Engine 5. After talking to Joe, reaching out to his closest friends, we gathered data about his personality that we later enriched with examples of work, his research papers, interviews and Wikipedia to create his virtual AI personality.","The mindset and methods to approach art and accessibility are very different from each other. In a team that has designers and artists, we managed to build a project that incorporates two perspectives.",We designed our way to approach the artist's work and the accessibility issue together in this project within limits.,,,"accurig, quest3, unreal-engine",
MIT Reality Hack 2024,Unity Enzo,https://devpost.com/software/sceneeditxr,False,"The main inspiration for 'Unity Enzo' stemmed from the growing demand for augmented productivity in diverse enterprise scenarios. As spatial computing becomes increasingly prevalent, the need for a unified, cross-platform workflow is more apparent than ever. The project was inspired by the vision of a seamlessly connected world, where the boundaries between physical and digital realms blur, enabling unprecedented collaboration and creativity.","'Unity Enzo' reimagines the workflow across various devices, from computers to headsets and mobile applications. It integrates the immediate rendering capabilities of Unity with the dynamic procedural generation of Houdini, creating a world where real-time interaction and procedural creation intersect. This integration allows for the construction of dynamic environments and landscapes that respond to user interactions, creating a rich, immersive experience.","The project began with a focus on VFX creation in Houdini, establishing a close loop with Magic Leap 2 and Meta Quest 3 through Photon for asset sharing and interactive modification. The MIT Building site served as the real-world canvas for testing and refining our collaborative and interactive features. The project leveraged a robust technology stack, including Unity 2022.3.f1, Chat GPT Vision API, and Meshy.ai, along with the extensive use of Codeberg/GitHub for version control and collaboration.",,"Despite the challenges, we are particularly proud of:","Throughout the project, we learned about the complexities of cross-platform integration and the importance of robust testing in real-world scenarios. We also gained insights into the potential of spatial computing and AI in transforming user interactions and productivity in collaborative environments.","By continuing to innovate and refine our approach, Unity Enzo is poised to set new standards in spatial computing and cross-platform productivity.","c#, gpt, photon, python, unity",https://codeberg.org/reality-hack-2024/TABLE_67
MIT Reality Hack 2024,AIRBLOCKS,https://devpost.com/software/airblocks-9rg4hk,False,"AIRBLOCKS AIRBLOCKS Mixed Reality AIRBLOCKS Vision AIRBLOCKS aims to take the visual, block-based coding concept into the spatial domain, allowing users to directly engage with logic and ""be the hero"" in their own learning process. It leverages mixed reality (MR) to provide an active, embodied learning experience. The primary goal of AIRBLOCKS is to provide an interactive, collaborative way to learn about computational thinking and introduce this concept through spatial learning. Users can connect 3D blocks to create interactive games, with blocks representing the smallest pieces of functionality, such as control, motion, manipulation, and looks. The project drew inspiration from constructionist learning principles, emphasizing hands-on, experiential learning. While the concept itself is not new, we identified its potential when combined with XR technology.
The Scratch programming language served as a source of reference, with its pioneering visual programming approach. With XR technology, we were able to expand the concept to the spatial domain, allowing for interactive, collaborative learning experiences. Research shows that utilising XR can lead to increased attention, satisfaction and confidence, further validating our approach (Khan et al., 2019). 
The decision to use mixed reality instead of virtual reality was driven by a lower entry barrier and the ability to naturally see the real-world environment and interact with it. It also allows for longer learning sessions, as there is no or less motion sickness involved. Additionally when collaborating with other users in the same room, users are able to see each other, allowing for natural interactions. AIRBLOCKS allows users to create their own logic programs by connecting blocks in a spatial environment. The blocks encompass various functionalities, including control (e.g., pressing start, wait for certain seconds), motion (e.g., walk for certain steps, turn around), manipulation (e.g., slow, speed up), and looks (e.g., change color). When connected, the character in the space immediately reacts to the changes, visualising the control flow built by the user. After team formation, we started to collect ideas and problems to work on. After settling on a problem to solve, additional research was conducted and collaborative ideation helped us define a solution and scope. We prototyped the basic concept and interactions in Bezi. The prototype was used to gather feedback, validate the approach and serve as a reference for the development. The scope was continuously adjusted throughout the hackathon to account for unforeseen technical challenges. The team worked simultaneously on 3D modeling, user flow mapping, interface design and programming. At the end we set up a working prototype running on Unity in a Meta Quest 3 headset. The development of AIRBLOCKS posed several challenges, including: Some key accomplishments of AIRBLOCKS include: We plan to include the following features in the future: The goal is to continue enabling interactive, collaborative learning and computational thinking through mixed reality experiences. https://vimeo.com/907232143?share=copy You can find the AIRBLOCKS project on CodeBerg by following thislink. You can find the demo video, logo and other assets here:link",,,,,,,"bezi, blender, figma, meta, mr, quest, unity, xr",https://codeberg.org/reality-hack-2024/TABLE_41/src/branch/W
MIT Reality Hack 2024,MindfulStride,https://devpost.com/software/mindfulstride,False,"In the U.S., nearly 90,000 people are diagnosed with Parkinson's disease annually, with that number increasing by 50% each year. Of those diagnosed, 60% experience anxiety. The changes in brain neurochemical factors, fear of social abandonment due to physical weakness, and deterioration of consciousness cause Parkinson's patients to experience chronic anxiety and isolation while suffering from the disease. Studies have shown that walking meditation can decrease anxiety levels by at least 37%. However, the loud environment sounds easily disrupt the meditation process, and the obstructed environment, with limited activity areas, diminishes the benefits of walking meditation. Furthermore, individuals with Parkinson's are easily startled by such noise, increasing the risk of collisions with obstacles and heightening anxiety and panic.","Unlike the current walking meditation method, MindfulStride is designed based on the needs of people with Parkinson's. MindfulStride transforms items in the user’s surroundings into natural objects during the walking session, providing a fluid boundary that ensures a more extensive walking space and safety. In this experience, the users will feel connected to society and the natural world. By incorporating spatial audio, the user is presented with an immersive natural scene that dynamically responds to their efforts and behavior. Even for those without Parkinson's, MindfulStride provides an easy way to convert any dreary scene into a more tranquil and immersive experience. In the modern day, where urbanization is rampant, it never hurts to reconnect with nature.","TechnologyWe utilized Mixed Reality features provided by the Meta Presence Platform to build this project. Specifically, the spatial anchors allowed us to put virtual objects where the physical objects are and the scene manager to understand the environment. For a more lightweight experience, we utilize the LookingGlass Portrait, which will display a fully animated and holographic scene of a woman talking to the user. If you don't feel like fully immersing yourself into the mixed reality world, this is one way to practice meditation. This woman is Sensei Hopkins, the voice you hear earlier while in the Meta Quest 3!",,,,,"lookingglass, meta, quest, unity",https://codeberg.org/reality-hack-2024/TABLE_19
MIT Reality Hack 2024,EmpathyEase,https://devpost.com/software/echo-101,False,Our inspiration came from the connection between two people. How would it be if you could use mixed reality to read the nuance of their emotions and understand them completely?,"EmpathyEase is an AI moderator that uses the Non-Violent Communication Framework of 4 steps, observe, feel, need, and request. NVC guides users to listen to their coworker's feelings and empathize with them. Then ask their needs and find requests that they can work towards meeting in the future.",EmpathyEase was built with unity and shapexr. The networking done with normcore and meta presence. We managed the voice elements with meta's voiceSDK.,"Spatial anchors, multiplayer shared spaces, meta presence. Learning curve of new ai.",Everything works in syncing positions and using ai.,How to use multiplayer meta presence platform and new skills in unity. Colocation is hard. How to use the voice sdk. Collaboration is important.,"Continue working through features, user testing.","figma, normcore, shapexr, unity","https://codeberg.org/reality-hack-2024/TABLE_101, https://youtu.be/ls9MqvkA5xI"
MIT Reality Hack 2024,AgriCommandXR,https://devpost.com/software/agricommandxr,False,"In the rapidly evolving landscape of agriculture, small farm businesses face significant challenges in making profitable and environmentally conscious decisions due to the complex interplay of factors such as climate variability, environmental risks, and market dynamics. The current lack of integrated technologies tailored to the unique needs of small-scale farmers impedes their ability to navigate these challenges effectively. Consequently, there is an urgent need to innovate and implement an integrated mixed reality and AI technology in agriculture to empower farmers with the tools necessary for informed decision-making and sustainable business practices.","The primary objective is to develop and implement an integrated mixed reality and AI technology that addresses the challenges faced by small farm businesses. This technology aims to enhance farmers' decision-making capabilities, promote sustainable practices, and contribute to the overall resilience and success of small-scale agriculture enterprises. AgriCommand XR revolutionizes farming by integrating Mixed Reality (MR) and AI, transforming agricultural management into an interactive, data-driven experience. Imagine putting on a headset and being presented with a 3D chessboard-like map of your farm. Every crop, machine, and worker is at your fingertips. With a simple gesture or voice command, you deploy AI-powered machinery, direct workers, monitor weather patterns, and manage harvests. Real-time AR popups provide insights into crop health, soil conditions, pest activity, and emission data, enabling precision agriculture and reducing waste. Streamlined, AI-optimized supply chains ensure your produce reaches markets in the most efficient way, addressing global food waste and inefficiency challenges. AgriCommand XR is not just a tool; it's your digital farming companion, making agriculture more sustainable, efficient, and profitable. AgriCommandXR aims to empower family farmers with modern technologies and mitigate the most challenging environmental problems. The smart algriculture design not only equips farmers to make a more informed decision, but also help connect them with all kinds of downstream stakeholders. In the United States alone, where the Department of Agriculture invests approximately $10 billion annually in crop insurance, the implementation of AgriCommandXR promises a substantial reduction in these costs. By leveraging the predictive analytics and resource optimization capabilities of our tool, farmers can better manage risks, potentially reducing the need for extensive insurance payouts. Meanwhile, our MR product responds to the United Nations Sustainable Development Goals (SDGs) in 3 ways:","Our talented design team is passionately exploring the frontiers of innovation by building our latest project entirely on Bezi. This platform isn't just a new technology for us; it's a canvas for our creativity, allowing us to transform imaginative concepts into real, impactful experiences. As we harness Bezi's unique capabilities, we're creating a synergy between cutting-edge technology and visionary design. This journey is not just about development; it's an exploration of potential, pushing boundaries, and a testament to our team's commitment to innovation. Stay tuned as we blend creativity with technology to bring something truly exciting to fruition. Note: To mitigate the WebAR latency problem, we are also providing alternative experience on Unity with Meta Presence Platform and have tested it with Meta Quest 3. Check the repository!","Our journey with Bezi has brought us face-to-face with a industry-wide challenge: WebAR latency. With suboptimal internet speeds, this obstacle has significantly impacted the efficacy of our WebAR experience. Instead of solely focusing on innovating within the WebAR space, our team has been compelled to pivot towards performing extensive optimizations and devising alternative solutions on Unity with Meta Presence Platform.",,"It's always the best to have a fast-learning, collaborative team! We also want to give special thanks to all the mentors who offered us advice. :)",,"ai, bezi, figma, unity, vr",https://codeberg.org/reality-hack-2024/TABLE_30
MIT Reality Hack 2024,Steeped in sereniTEA: From Virtuality to Reality,https://devpost.com/software/steeped-in-serenitea-from-virtuality-to-reality,False,,,,,,"During this project, we figured out that teamwork is a big deal for making an exciting project. Even though we had limited time, everyone brought their skills to the table, and we ended up with a cool concept and engaging presentations. Our team vibe was on point, making everything click and creating a real bond among us. It totally resonates with this year's theme:connection.",,"androidvr, blockadelabsskybox, metapresence, metaquest, sketchfab, unity",
MIT Reality Hack 2024,Visclude,https://devpost.com/software/visinclude,False,"Our inspiration stems from the belief that every individual, regardless of their visual capabilities, deserves to experience and interact with the world fully and independently. Recognizing the vast spectrum of visual experiences, we are driven by the mission to bridge the perceptual gap between the visually impaired and the sighted, fostering empathy and inclusivity in every space we inhabit.","Our product, VisInclude, enables designers, architects, developers, and even friends and family, to step into the world of the visually impaired. It offers an immersive experience that simulates various degrees of visual impairment, providing an authentic understanding of the challenges and needs. This profound insight empowers creators and influencers to design environments, products, and services that are not just accessible but truly inclusive, ensuring every space is a place where everyone feels understood, valued, and connected.",,,,,,"c#, meta, miro, mr, quest, unity, vr, xr",https://codeberg.org/reality-hack-2024/TABLE_81
MIT Reality Hack 2024,Braille XR,https://devpost.com/software/braille-xr,False,everything's going digital and the cost of braille devices is exorbitant,let's a user input and read braille in xr with hand tracking,"esp32 + 6 servos setup to communicate with our stereokit demo over udp
getting hand tracking from lmc2 (Ultraleap) through monado (headset optional)","PIN ID's on the board were mislabeled
3 different CAD programs failed to perform for our team
monado needed to modified and rebuilt
3d printer bowden tube fell out during critical print",overcoming challenges and maintaining progress,"hardware is hard, and probably requires twice the length of a software jam","test and improve with proficient braille readers, and refactor the repo too make it more accesssible","c#, stereokit",https://codeberg.org/reality-hack-2024/braille_xr
MIT Reality Hack 2024,DelivAR Angel,https://devpost.com/software/delivar-angel-qc2fjy,False,"Testing the DilevAR Angel system. DelivAR Angel Logo Group-22 Members: Barry, Dimaz, Dwi, Eric, Austin The birth of a newborn provides joy not only to the parents but also to the entire family and nation as a whole. However, the potential mortality of the mother and/or infant throughout the process of childbirth could bring feelings contrary to joy. Furthermore, it is worth noting that tension and dissatisfaction can be associated with the experience of loss. The child's mental well-being will be affected as they mature if their mother passes away during childbirth. The death of a woman in pregnancy and childbirth is globally considered an individual tragedy and a human rights violation"" (Miller & Belizan, 2015, p.1). Our team came from different nations, which share the same problems when discussing the number of mothers and baby mortality during labor. Our team recognized that time is an essential factor that must be considered when managing the labor process for the mother and baby. This initiative primarily examined the labor process throughout regions of Africa. According to UNICEF (2023), the maternal death rate in Africa experienced a 34% decrease from 2000 to 2020. According to the World Health Organization (2017), the daily mortality rate for women during pregnancy and childbirth is 430. Although the maternal mortality rate in Africa has decreased, it remains an urgent issue with severe consequences for both infants and their mothers. Given the rapid development of technology, we believe that technology in medical fields continues to progress to support physicians and nurses in their responsibilities. It persistently engages in innovation to address life's challenges, substantially decreasing maternal mortality rates. The primary factor contributing to a higher mortality rate is the lack of advanced technologies to assist both the mother and baby during the labor and delivery process. Specifically when it takes place in remote areas in developing countries. The predominant device utilized in Africa is a portable ultrasound device, which is used to tackle the issue of child mortality (Rao & Joseph, 2017). Nevertheless, we support the existence of additional devices that can assist in facilitating Africans' labor and delivery process. Our team believes that using augmented reality/virtual reality technology can effectively address this problem by serving as a tool for nurses and physicians to disseminate information throughout every phase of the delivery process. This program facilitates the provision of assistance by midwives, nurses, and health assistants throughout the childbirth procedure in rural regions of Africa. We are introducing ""DelivAR Angel"" an innovative solution designed to enhance the efficiency of midwives, nurses, and delivery helpers in managing labor at various stages, enabling them to make informed decisions that prioritize the safety of both mother and baby. DelivAR Angel is a user-friendly decision support system designed to assist frontline medical professionals, including midwives, nurses, and delivery assistants, efficiently manage labor at various stages. Its primary objective is to assist in making accurate decisions that prioritize the safety and well-being of both the mother and the baby. We have created a prototype consisting of four stages: pre-labor, active labor, delivery, and post-partum. The first stage is referred to as pre-labor, including the initial 12 hours of the labor process. During this period, the midwives, nurses, or delivery assistants must fulfill three essential responsibilities. Initially, do cervical examinations to ascertain the baby's position, as well as the degree of cervical dilatation and effacement. Next, assess the physical state of the mother. The procedure involves assessing respiration, blood pressure, body temperature, and pulse rate. Additionally, this can promote walking and alterations in posture. Thirdly, administer analgesia that may involve the administration of intravenous opioids or the inhalation of nitrous oxide. The second stage is active labor, which typically takes between 13 and 16 hours. In addition to regularly monitoring the mother's heart rate and offering psychological assistance, this period also emphasizes three primary factors. Initially, observe the perineum by employing perineal massage, applying warm or cold compresses, and utilizing perineal management strategies. This is a proactive measure taken to mitigate the occurrence of trauma. Next, monitor contractions occurring at intervals of around five to ten minutes. Next, assess cervix dilation by inserting fingers deeply to access the cervix. Furthermore, they must begin documenting the condition of both the mother and the infant. The third phase involves the process of delivering the baby, which typically consists of a labor duration of 16-18 hours. At this level, there are three primary factors. Initially, assessing the condition of both the mother and the infant. Monitoring the woman's advancement through the phases of childbirth and detecting possible difficulties for both the mother and the baby. Next, making arrangements for the delivery. Thirdly, making arrangements for amniotomy if deemed essential to initiate labor. The labor assistants must perform amniotomy by intentionally breaking the artificial rupture of the amniotic fluid bag around the fetus. The fourth stage is the postpartum period, which is the time immediately following childbirth. The postpartum period encompasses three tasks that must be accomplished. Initially, the baby's condition is assessed promptly after birth. Additionally, it encompasses auditory perception, bilirubin levels, and serious congenital cardiac conditions. To perform the task, use a sterile, sharp instrument like a scalpel or sturdy scissors to cut and secure the umbilical cord. Furthermore, in this stage, it is important to observe and track any vaginal discharge, which may consist of mucus, blood, or amniotic fluid. The user group consists of midwives, nurses, and health assistants who assist women in delivering babies in remote areas with limited support. We posit that this virtual reality/augmented reality application has the potential to alleviate maternal anxiety and stress during the labor and delivery procedure. We developed this application by creating prototypes utilizing the Bezi platform. This platform offers a comprehensive 3D workflow design capability, including prototyping, greatly facilitating collaborative design with our staff. We are utilizing Bezi to create the prototype compatible with the Oculus Quest 3. Before implementing the design on the Bezi platform, we initially formulated the system requirements using the Balsamiq wireframe. This prototype features a dashboard that displays pertinent medical information for expectant mothers. Furthermore, the prototype also offers a real-time data feed of blood pressure, heart rate, and oxygen levels. This prototyping application provides a sequential list of tasks that delivery assistants must adhere to at each stage of the birth process, ensuring proper support for delivering a baby. We also included an AI-powered medical virtual assistant to help address the health worker's questions during the delivery.  The virtual assistant is a placeholder for an actual implementation in the future. We encountered challenges throughout developing a prototype utilizing Bezi because some of our team members still needed to familiarize themselves with the Bezi platform. We also encountered challenges when executing Bezi on Snapdragon AR. Incorporating voice commands in the future to facilitate the tasks of delivery assistants throughout the birthing process is necessary. Ultimately, we have successfully finished the process of creating the prototype. In the future, we anticipate enhancing these prototypes by incorporating other functionalities, such as postpartum monitoring for the mother and baby once they have completed labor and returned to their homes. These supplementary features can be highly beneficial in assisting mothers and mitigating postpartum stress. Additionally, we believe this prototype can be applied to various scenarios, such as assisting health supports in managing cardiac concussions, correcting fractures, and addressing dislocated shoulders. Miller, S., & Belizán, J. M. (2015). The true cost of maternal death: individual tragedy impacts family, community and nations. Reproductive Health, 12(1).https://doi.org/10.1186/s12978-015-0046-3 Rao, P., & Joseph, D. (2017). Africa Wired: Portable ultrasound device to tackle child mortality. Africa Renewal.https://www.un.org/africarenewal/magazine/december-2016-march-2017/africa-wired-portable-ultrasound-device-tackle-child-mortality Ritchie, H., & Mathieu, E. (2023). How many people die, and how many are born each year? Our World in Data.https://ourworldindata.org/births-and-deaths#:~:text=Population%20projections%20show%20that%20the UNICEF. (2023). Maternal mortality rates and statistics. UNICEF DATA.https://data.unicef.org/topic/maternal-health/maternal-mortality/#:~:text=In%202020%2C%20sub%2DSaharan%20Africa",,,,,,,"ai, ar, bezi, quest",https://www.bezi.com/file/5251f123-b7ac-40a5-a47e-f5b9f6c844a3
MIT Reality Hack 2024,SensEasy,https://devpost.com/software/sensoryease-ar,False," Our inspiration comes from the daily challenges faced by those diagnosed with the Autism Spectrum Disorder (ASD), anxiety disorders, and hypersensitivity. Current accommodations like sunglasses or light covers are helpful but lack the ability for the user to tailor their experience and manage their focus. Recognizing the need for a seamless solution, we aim to empower individuals to customize their sensory environment in real-time. A one-size-fits-all approach doesn't cater to the diverse preferences of people with ASD. As per our research, the number of individuals diagnosed with ASD has risen over the decades so we need to devise a  user-friendly solution that promotes independence and inclusivity by respecting the unique sensory needs of individuals on the spectrum. SensEasyis an Augmented Reality application that allows users who face challenges due to sensory overstimulation to customize their audio and visual sensory input to alleviate overstimulation. It includes tailoring to the individual’s needs by managing:
• Background  noise management - we have a slider button that enables the users to adjust their background  noise in any real time situation to alleviate any audio overload.
• Visual background alteration- A toggle switch that allows the users to focus on the person they are conversing with by diluting everything else in the environment.
This enables users to interact with their environment in a way that minimizes stress and anxiety and allows them to feel more connected with society. 1) We started with the Literature review, to find out existing work on improving user engagement and how we can build something more immersive and useful.
2) Conducted Informal Interviews with end users to gain more insight and to verify if we understood if we were meeting their requirements.
3) Collecting data(audio and visual) that actually support our research and how that data actually is inconvenient to our end users.
4) Use Meta's presence platform to build the AR application. 1) Figuring out the most effective way that we could help the autistic community and researching on how audio and visual overstimulation can cause distress and anxiety.
2) To create a customizable experience with our research that can cater to the wide variance in sensory preferences among individuals with ASD. 
3) Additionally, ensuring that the tool is a supplement, not a replacement, for professional therapy posed a conceptual challenge. We are proud of being able to contribute toUN's 10th Sustainability Goal- i.e reduce inequality. We aim to provide a connective environment without any discrimination ,developing the Emergency De-Stimulate Feature that provides instant relief in overwhelming scenarios. 
This tool stands as a testament to our commitment to enhancing the quality of life for individuals who experience difficulties due to sensory overstimulation. We gained insights into the complexity of sensory processing issues and the importance of flexibility and customization in developing assistive technologies. The feedback loop from potential users was invaluable in shaping the tool's features. Looking ahead, we plan to refine SensEasy by incorporating machine learning to predict and adapt to user needs more effectively in the form of sentiment analysis in order for our user to understand the emotion better.
 Further research into cognitive-behavioral strategies will inform the development of features that can assist users in managing anxiety and sensory overload proactively.",,,,,,,"blender, meta, mixamo, presence, quest-3, segmentation-api, unity",https://codeberg.org/reality-hack-2024/TABLE_90/commits/branch/main
MIT Reality Hack 2024,Rat in the Hat,https://devpost.com/software/rat-in-the-hat,False,"Ratatouille (2007), a movie by Walt Disney","✅ 1. Augmented Info
Get informed of what’s going on in the kitchen by AR ✅ 2. Real-time Tutor
Complete a dish with guidance from Remy ✅ 3. Connect Online
Talk with your friends or family who are cooking experts","Through Unity, Oculus Quest 3, and tons of passion towards this topic 🔥","We faced some technical issue in Unity. Since we're first time doing hackathon, there are challenges into integrating things on the platform.  However it worked out eventually - thanks to our wonderful developers and the supports from mentors. 🥰","Our Ideas, teamwork & high-level of commitment. 🧀","Everything about developing a XR project from sketch - from team building, idea generating, integrating systems and developing with Unity, making real-time updates from sensors to our system, and designing in a 3D interface.","We have a bold vision. By empowering users with access to information and integrating data into daily life, we plan to expand our service and build Smart Homes. 1️⃣Expand user scopeTo extend the service to seniors, cooking lovers, and educators 2️⃣Fulfill scenarios in kitchenTo dive in more scenes, (i.e. grocery management) in kitchens 3️⃣Cover more spaces at homeTo further extend this service into spaces like gardens/living rooms","bezi, c#, c++, esp32, figma, iot, loadcells, meta, mosquittomqtt, mqtt, oculus, unity",https://vimeo.com/907250273?share=copy
MIT Reality Hack 2024,Jump On It,https://devpost.com/software/jump-on-it,False,"We wanted to build a jam sesh in XR, but after talking to musicians we heard the same complaint: learning and playing instruments in XR isn't very fun. There's no haptic feedback, the devices can get laggy, and the acoustics just aren't the same. Given all this, we wanted to figure out to make music more enjoyable in XR.","The experience is multiplayer, with any number of people able to join in. Players can see on the floor a giant floor piano in AR. As they jump around and play on it, they can discover secrets that build out an audiovisual world around them -- depending on what chords are played, new music and new visuals will appear.","We used the Quest 3's passthrough ability to render the piano in XR. The game runs on Unity, and implements multiplayer with Photon.","Multiplayer is really hard, especially at a hackathon where everyone else is also trying to use the Internet.",The multiplayer works and people have a lot a lot of fun jumping around in the piano in 3D space.,Multiplayer is really hard. But jumping on the big piano is really fun.,"We want to implement new environments, like a forest or an ocean, and new interactions: being able to put the piano on the wall and throw things at it, or on the ceiling and jumping up to hit it, or even shadowboxing it!","meta, photon, presence, quest, unity",
MIT Reality Hack 2024,Shroom Hunt,https://devpost.com/software/shroom-hunt,False,SnapLens Snapcode,,,,,,,"8thwall, blender, javascript",https://simonswartout.8thwall.app/shroomhunt/
MIT Reality Hack 2024,Operator,https://devpost.com/software/operator-o4qdk8,False,"Gameplay is designed to work in layers; each layer bringing you closer to the truth behind the event. The goal of this iteration of Operator is to gather enough details to identify the mole who has ties to the mysterious events that have occurred. Target Audience: Students, Educators, VR Enthusiasts, History buffs, and Lifelong learners Things we wanted to do if not for time constraints
Immersive Listener: Requires the Player to remain quiet when listening in on calls, else the callers will get suspicious 
Switchboard lighting and unguided exploration plugging patch cable in different combinations to solve puzzles and unlock event details
Directory with all town resident switchboard numbers, addresses, and their photos including call history and log
Interactables: can be viewed in Caller’s quarters or interacted with in person on the Operator’s workspace, all to be logged in the notebook
Clues Notebook that inventories clues, guiding the player on the path to solving the mystery using computational thinking
Unassigned ports on the switchboard to be explored and reveal other clues
Project animations into Caller’s scenes for a more immersive feel COMPUTATIONAL THINKING Pattern Recognition 
Harriet must identify patterns in the calls to discern anomalies or suspicious behavior.
Analyzing sequences of events during and after the flash of light can reveal hidden connections. Abstraction 
The player needs to distill information from calls and scenes, extracting essential details to form a coherent narrative. Throughout the game, Harriet will have heard many calls and viewed a variety of objects in the callers’ spaces. Abstracting information helps Harriet focus on crucial clues to solve the mystery. Algorithmic Thinking
Harriet's decision-making process in connecting calls or eavesdropping requires algorithmic thinking. Players strategize on which calls to prioritize, balancing the need for information with the risk of discovery. Decomposition
Breaking down the larger mystery into smaller, manageable tasks allows players to tackle each aspect systematically. Harriet needs to decompose the investigation, considering both auditory and visual cues separately to piece together the puzzle. Debugging
Players must navigate through misinformation and red herrings, honing their ability to debug false leads and focus on relevant clues. Harriet's journey involves refining her understanding of events through continuous debugging of her assumptions. GAMEPLAY MECHANICS Connecting Or Not Connecting Calls. 
Harriet can choose to connect calls and listen in, analyzing conversations for verbal clues.
Players must recognize patterns, anomalies, and keywords to identify potential suspects or areas of interest. Caller-vision
After the mysterious event, Harriet gains the ability to see through the eyes of the caller.
In VR, players navigate through the caller's room, able to view objects and environments for visual clues. Problem-Solving Challenges
Computational puzzles and challenges immerse players in the essence of Computational Thinking. Solving these challenges unveils more layers of the mystery.",,,,,,,"blender, c#, logicpro, metaquest3, unity",
MIT Reality Hack 2024,p.e.c.k.,https://devpost.com/software/p-e-c-k,False,"Child A: Bird Image Target (Scan Me for AR Experience!) Child B: Bird Image Target (Scan Me for AR Experience!)                     Link to 8th Wall Projecthttps://www.8thwall.com/pozlong/realitysloths *note: Please Use Bird Image Targets in the slides above to trigger the demo * Divya Srinivasan 
I'm a designer with a background in Materials Science, currently a student of MDes at UC Berkeley. Esen KaCreative technologist and researcher in VR and Clinical Psychology based in Barcelona. Poz Long 
Creative Technologist with background in film & TV production. LGBTQ parent. Coffee snob. Fabian JiménézI'm a XR designer & PM seeking a way to merge my VFX background, VR/MR enthusiast, parent, gamer at heart Indira Ardolic
I'm a New Media Artist that creates audio visual art using Unreal Engine. I focus on surrealism and dreamscapes. In a globalized world, misunderstanding and lack of knowledge about different cultures can lead to stereotypes, prejudices, and social divisions. We wanted to come up with a project where students from 5th to 7th grade can connect with their peers from different countries, creating deep and meaningful friendships. AR adds a new dimensionality to sharing their favorite foods. Research demonstrates students with pen pals develop a greater sense of empathy and understanding of the world around them. Future Constructors p.e.c.k. is a web-based AR app that connects students across classrooms globally. It is intended for use in educational settings for 5th to 7th graders. p.e.c.k. encourages social emotional skills and empathy for individuals from different backgrounds. Students will share audio messages and a 3D model generated based on a cultural theme provided by a teacher. Their pen pal, or peck pal, will be able to access their messages after it has been approved by the teacher. This will act as a seed of knowledge and understanding. 
p.e.c.k. brings a new dimensionality that allows students to interface with 3D models representing cultural experiences, providing detailed spatial and visual context otherwise lost in a regular letter. p.e.c.k. also provides connection through voice messages in a safe and moderated environment. Imagine being able to hear the emotion in another’s voice as they speak on their favorite memory, or the pain you might share as they speak on a time they felt wronged. Voice is an incredibly unique identifier that is often specific to the individual. It carries immense weight, context, and personal connection to hear another person speak as opposed to reading their message. We used quite a variety of tools to develop the application and to generate the assets. We’ve used 8thWall’s image target framework to and set the birds as targets for students to interact with. We’ve used Meshy.ai to generate the 3D models from photos, Bezi for prototyping the interaction which will take place, ensuring that children can have a smooth experience without prior knowledge and complicated setup. As we wanted to go with the WebXR due to its easy use and accessibility, we soon realized that the direct integration of Meshy.ai won’t be possible. We’ve investigated various different options to host the 3D models online, but we did not have enough time to properly implement it. We are proud to make an accessible and fun application that reimagines social connection and intercultural communication among the younger generation. As social media use grows among children it is important to have deep individual connections, to ensure mental health and social skills among children. Making the app accessible requires serious considerations for the age group, the socio-economic state and various other factors. Like any intervention, the application has to go through various pedagogical design thinking steps. Once p.e.c.k. finishes its first round of peck pals, the application can be disseminated through a wider student group. It can be incorporated to the curriculum in various countries and the content type can be adapted to different age groups.",,,,,,,"8thwall, aframe, bezi, blender, genie, html, javascript, lumaai, runway, shapesxr, sketchfab","https://codeberg.org/reality-hack-2024/TABLE_14, https://www.canva.com/design/DAF7G9UX2bE/2D9FmPkH1bBirtpajmxGuA/edit?utm_content=DAF7G9UX2bE&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
MIT Reality Hack 2024,HealMotion,https://devpost.com/software/healmotion,False,"Everyday tasks can be hard for people with fine motor impairments who may have suffered strokes or other health problems. Many of them feel depressed and helpless because rebuilding hand skills is difficult without proper guidance from doctors, or too boring to stick with even with guidance. HealMotion is here to help - by gamifying hand exercises, users can rehab their hand skills independently or with others who have the same issues around the globe. We believe that treatment can be fun and accessible to everyone.","HealMotion is a mixed reality recovery community that immerses users with fine motor impairments in collaborative play to not only strengthen hand dexterity, but to foster social connection, self-confidence, and ultimately, destigmatize what it means to have an impairment. No one is left behind in this exciting new reality.","We utilized hand tracking from the Meta Presence Platform SDK for Unity to capture the user's hand movement, and the Photon Ecosystem to build connection between players. User interactions are prototyped using Figma and Bezi, with guidance animation done by Blender to better familiarize users with their mixed reality treatment.",Multiplayer function and Wi-Fi.,"Contribution to networking with physical and digital world settings, real time hand tracking and user status syncing, public health and social impact, collaboration and connection between people.","Connection is important, not only for users in our app's community, but also for teammates - that's us.","Enhance multiplayer stability, enrich exercises with more gameplay.","bezi, blender, c#, meta, photon, unity",https://codeberg.org/reality-hack-2024/TABLE_4
MIT Reality Hack 2024,PinPals,https://devpost.com/software/pinpals,False,Moving to new communities and feeling disconnected from local happenings and other community members.,Shows hyper local events and allows you to create events easily.,"Unity, Niantic Lightship, API webscrapers for google, google API's",API's you think would exist and think would cover what you need potential don't and won't.,Creating a compelling design and video asset.,Check the main functionality early for all major components of the product.,"Develop the idea further, use it in our local communities to see if any traction can be had.","google, lightship, niantic, serpapi, unity","https://codeberg.org/reality-hack-2024/TABLE_70, https://www.figma.com/file/HDqyMDLqtRiJkT7xh1y6RT/Project-Draft?type=design&node-id=1-182&mode=design, https://www.figma.com/proto/HDqyMDLqtRiJkT7xh1y6RT/Project-Draft?page-id=0%3A1&type=design&node-id=36-37282&viewport=132%2C2907%2C0.78&t=DZlQtQHmbPsl8yIw-1&scaling=min-zoom&starting-point-node-id=36%3A37282"
MIT Reality Hack 2024,ReadyXR,https://devpost.com/software/readyxr,False,"In a world full of unexpected disasters and situations, we often forget the vital in-the-moment decisions that can save our lives. How can we simulate these sometimes costly and boring training programs? Introducing ""Ready XR"": Your Emergency Training Protocol experience. Designed for individuals from diverse backgrounds, we offer those with limited exposure to natural disasters in new areas, our app offers immersive XR simulations to learn and practice emergency preparedness in a fun yet fast-paced way. With interactive guides tailored to various scenarios, users gain invaluable experience and confidence in handling emergencies effectively and quickly. One of our team members, Sea Won, who is from Korea shared her story about her time there and in the States. She mentioned the lack of preparation for earthquakes in Korea due to the rarity of the natural disaster. This resulted in people in Korea being unable to respond properly when an actual earthquake recently occurred, reflecting the lack of education on emergency protocols in schools. Learning from this experience, Sea Won realized the importance of comprehensive emergency training, especially in regions with infrequent disasters. This inspired her and our team to develop accessible and memorable protocols to ensure readiness for any unforeseen event, regardless of experience or geographical location. Arriving in Boston for the MIT Reality Hackathon, we pondered how many attendees from different backgrounds possessed knowledge of emergency protocols relevant to their regions. This inspired us to develop a universal protocol easily learnable anywhere, leveraging XR technology for immersive, game-based training that ensures quick comprehension and retention.","ReadyXR is a mixed reality experience to learn and practice emergency preparedness guides and tips with a gamified approach. With the power of immersive technology, it enables users to be exposed to dangerous situations. By detecting the action of the player, real-time feedback would correct you to lead to the correct action in certain situations.","With mixed reality in mind, we built the game around the Meta Quest 3. We used the game engine Unreal Engine 5.3.2 to build the application. With blueprints and C++, we developed the obstacle interactions. Our Design process consisted of research, ideation, low-fidelity sketching, High-fidelity prototyping, testing, and iteration. For finalizing the UI/UX for the game using Figma, and building 3D objects through Blender.","We had to discuss and think thoroughly about the gameplay direction and design as a team. How could we teach important themes and concepts, while still keeping fun and mimicking real-life intense and fast scenarios? We had to devise clever ideas and designs around this obstacle of our own. Another point of contention was whether we wanted to develop this project for mixed reality, or virtual reality. Seeing other similar applications like Beat Saber virtual reality seemed like the obvious direction, but we decided on mixed reality for better learning outcomes.",We managed to create a working game that mimics our original idea of learning these quick survival decisions in a quick and gamified approach. The obstacle designs and interactions we came up with were well thought out and refined. We are very proud of the final product we were able to produce with the limited time we have.,,"ReadyXR hopes to continue preparing people from around the world for a variety of disaster situations in a fun and immersive way. We hope to incorporate more mixed reality features that take aspects and objects from users' real-life rooms like tables and valuables instead of pre-designed objects. In the future, we aim to expand to include more emergencies like fires, lockdowns, storms, extreme heat, and human encounters.","blender, c++, figma, meta, quest, unreal-engine","https://drive.google.com/file/d/1Xte3KeCK--gMWnhwHson7CkhVpvINKMh/view, https://codeberg.org/reality-hack-2024/TABLE_34"
MIT Reality Hack 2024,MindToBoard,https://devpost.com/software/mindtoboard,False,"We were inspired by the inefficiencies in workplace meetings—rambling discussions, lack of structure, and unclear outcomes. MindToBoard aims to address these issues.","MindToBoard combines XR Kanban and AI summarization. In 15 mins, it transforms spoken words into actionable bullet points, facilitating real-time 3D collaboration on physical products and improving decision-making.","Built using Unity, Meta Quest Presence platform, Bezi, and Snapdragon Spaces. Faced challenges but learned valuable skills in Meta Quest platform, OpenAI integration, and more.",,,"Mastered skills in Meta Quest platform, OpenAI integration, Bezi, and Snapdragon Spaces.",Seeking user feedback and exploring potential applications for further development.,"bezi, c#, maya, meta-presence-platform, openai, snapdragon-spaces, unity",https://codeberg.org/reality-hack-2024/TABLE_59
MIT Reality Hack 2024,VerseVerse - Spatial Language Learning,https://devpost.com/software/tbd-a9d3mk,False,Language is a complex interplay between the physical and the social aspects. It changes constantly and learners can find it hard especially if you are not a native speaker. We wanted to know if we can create an environment for anyone who wants to learn a new language through a virtual sensorial experience without the need to go outside the country.  Can a non-Japanese speaking person for example learn Japanese using poetry(haiku) in the comfort of one's home?,"VerseVerse enables learners to learn a new language by building semantic constellations and spatial memories using VR, Haptics, and Spatial Audio. In our example, we use Japanese poetry (Haiku) to show that even a simple line can have words that have similar meanings when translated to English. The learner once immersed in our space, will see a Haiku written in Japanese. Next, in the semantic galaxy of English words, they will build a constellation to express the meaning of Haiku in this semantic galaxy. The closer the English word is to the meaning of the Japanese Haiku, the learner will experience an increasing volume of the sound and increased vibration (haptics). These multimodal sensory inputs enhance the users' immersion in the language, building spatial memories.  ",,,,"We learned the value of teamwork and technical skills such as building using Unity, C# and wwise. In addition beecause of the diversity of the teams skill set, we were also able to share our experience to come up with a great application.",Next for VerseVerse is to test more users and eventually deploy in the Quest Marketplace. ,"blender, c#, collab, python, unity, wwise","https://codeberg.org/reality-hack-2024/TABLE_60#connectionvialanguage, https://www.youtube.com/watch?v=mSSdxio3Rvk, https://www.youtube.com/watch?v=mSSdxio3Rvk"
MIT Reality Hack 2024,Pando,https://devpost.com/software/pando,False,Our team discovered a shared and deeply personal understanding and connection with the topic of cultural identity and the challenges surrounding it for many Chinese-Americans (and many immigrant families in the US).,Our mixed reality application aims to bridge generational gaps by immersing both young (ages 13+) and senior users through shared cultural heritage experiences with the help of XR.,,All the challenges. One of our devs had to drop out due to family emergency. Wifi was failing us. The usual technical issues with hardware. Unity versions across members. Recording content from Snapdragon Hardware.,Being a solid team with a solid concept and having the complimentary skills to share our project with pride.,"To just chill out and do what we can with what we have, in the moment.",We would love to see this concept come to life. We would love to execute it if funding found it's way to us and enabled us to hire the right team to get this out into the world.,"bezi, qualcomm, quest3, unity","https://codeberg.org/reality-hack-2024/TABLE_36/, https://docs.google.com/presentation/d/e/2PACX-1vRmsevs9aeK2lswF2OiP3jzfZXoCZvzjOpgdNkaSOAEIJ_IRJdq9eXzOfWb9e7gcr17kILGRISeRx3O/pub?start=false&loop=false&delayms=3000&slide=id.g1efc31d5916_1_19, https://www.bezi.com/play/3a9638d8-fbfc-467b-8b96-d7982805fc1f"
MIT Reality Hack 2024,Virtual Fantasy Productivity Weapon,https://devpost.com/software/virtual-fantasy-productivity-weapon,False,"Have you ever come across a moment that you feel so daunted about the muti-stops traveling planning that you miss the catchup encounters of your family, friends or the connections that could have happened?","Our MR experience helps you compare and organize time, cost, locations and destinations on a multi-leg journey in a playful way with our space shield and time sword.",,,,,,"bezi, blender, c#, shadergraph, unity",https://codeberg.org/reality-hack-2024/TABLE_68
MIT Reality Hack 2024,Swing Coach,https://devpost.com/software/swing-coach,False,"In recent years, golf has experienced a remarkable surge in popularity, attracting a diverse range of individuals eager to embrace the sport. The allure of spending time outdoors in beautiful green landscapes and the social aspects of the game have also contributed to its widespread appeal.. However, for beginners looking to venture into the world of golf, the initial challenges can be both exciting and intimidating. Mastering the fundamentals of proper posture, grip, and swing mechanics requires patience and dedicated practice. Coordination and timing, essential elements of a successful golf swing, often pose challenges for those new to the sport. On the other hand, more advanced players seeking to elevate their game face a unique set of challenges. Fine-tuning an established swing to achieve greater precision and power demands a nuanced approach. Identifying and addressing specific weaknesses or inconsistencies in technique becomes crucial for improvement. In both cases, a supportive and tailored learning environment is essential. Beginners benefit from structured guidance and accessible learning resources, while advanced players thrive with personalized feedback and opportunities for targeted skill enhancement. Recognizing the challenges unique to each stage of proficiency in golf is crucial for fostering a positive and rewarding learning journey, whether one is just starting or seeking to reach new heights in their game.","Our innovative golf learning app is designed to cater to both beginners and advanced players, fostering a comprehensive and immersive golfing experience. For beginners, the app offers step-by-step tutorials and interactive lessons that guide them through the fundamentals of proper posture and swing mechanics. After providing posture tips, the app provides a visual representation of the ideal swing path traced around the wrist, personalized to the height of the player. For advanced players, the app introduces a comparative analysis feature, allowing them to assess their own swing path against that of professional players like Tiger Woods. This not only serves as a source of inspiration but also as a practical tool for identifying areas of improvement. The app acts as a personalized coach, offering targeted feedback and insights to help players refine their techniques and take their game to the next level. This year’s theme of CONNECTION is weaved into our app through three ways:","Our Swing Coach application helps golfers to visualize a proper swing plane and execute a golf swing with this swing plane guide in their field of view. As a golfer, it is often quite difficult to self-diagnose issues and understand whether your technique is correct. The primary way golfers practice their swing is by going to the driving range and performing swing drills that help them understand the difference between ""feel"" and ""real"". Sometimes, it may feel like your swing is on plane from your point of view, but in reality a 3rd party observer could easily tell that it is not. This is one of the reasons it can be so useful to have a golf coach or to take videos of your swing. Each of these methods for training involves a few pain points. Videos are 2D in nature so multiple angles are required to understand how the swing looks an all axes. This can be quite cumbersome to setup or expensive to get equipment that makes it easy. Also, golf coaches can be quite expensive and only accessible for an hour or two at a time. Swingcoach allows golfers to easily view the ""real"" spatial data of how their swing was executed by showcasing a drawn line of their swing alongslide an ideal swing plane. Lastly, a score is calculated! This is score helps the golfer understand how close their swing was to the ideal swing plane. A higher score indicates a better swing, and a score of 100 indicates a near perfect swing! In summary, with swingcoach, there is now an inexpensive and accessible options for golfers to practie their swings at all hours of the day with the important spatial data that will help them self-correct their swing without the need for expensive coaches or traveling to a driving range.",,Helping one of our teammates learn how to play golf,,"In the future we would like to add a few important features: We plan to use machine learning to help players compare their swing to professional golf players. The vision model we plan to use is based on a paper and algorithm calledGolfDB. It analyzes the eight different golf swing sequences. Top golfers' motion data is used to train this model, showing the gap between an XR player and elites statistically. The model then predicts the scores, thus enabling the skill improvement of the player. We want the app to be a platform for golf enthusiasts from all walks of life and experiences who can come together to share insights, help improve their game, and  encourage one another on their respective journeys. To enable that, we want to build features such as community forums and collaborative challenges within our app to transform golf into a communal pursuit, fostering connections among players irrespective of their skill levels.","bezi, c#, figma, meta-presence-platform, numpy, opencv, pandas, python, pytorch, unity",
MIT Reality Hack 2024,63 Galleria,https://devpost.com/software/63-galleria,False,"In the past, we cherished our important moments with loved ones by flipping through photo albums. However, with the emergence of the digital world and technology, despite the convenience of storing and sharing photos, we lost the chance to truly immerse ourselves in those special moments. It's as if we lost something precious along the way. Envision a world where XR technologies have become universal. We can break through limitations and connect our relationships with loved ones, experiencing time and space like never before.","63 Galleriaprovides a spatial way to view your media, relive your memories, and share them with others. You can explore your photo gallery through a timeline, location, or a single photo, using intuitive gestures that mimic the way we naturally interact with photos.","We decided to build a XR gallery since it best fits our concept. We wireframed the scenarios in ShapesXR, assembled the gallery and bubbles in Unity, and created the UI elements in Figma.",,"We extracted the ""bubble cluster"" concept from file management to categorize our photos and images.
We achieved smooth interaction with gestures, making the experience enjoyable and engaging.","The importance of iteration to find the best solution and interactive approach.
Learning from the team's expertise and perspectives.",,"meta-presence-platform, shapesxr, unity",https://codeberg.org/reality-hack-2024/TABLE_63.git
MIT Reality Hack 2024,DELTA REAL/ation - Empowering Migrant Futures,https://devpost.com/software/delta-real-ation,False," Delta Real/ation introduces a groundbreaking approach where migrants take the lead in designing their new settlements. Rooted in a mixed reality XR game, this innovative project follows a more-than-human design ethos, placing players in the shoes of Koda and Nisa recent migrants tasked with planning a sustainable and inclusive community. Aligned with the UN Sustainability Goal #11, the game aims to foster inclusive, safe, resilient, and sustainable cities and human settlements. In the face of climate disasters, authoritarian regimes, racism, and crime, migration has become a shared experience, prompting us to rethink settlement development. Delta Real/ation brings together a team of diasporic XR creatives—developers, designers, and dreamers—dedicated to offering a transformative experience. Participants immerse themselves in Koda's journey, choosing to build from art centers and schools to hospitals and homes. The game emphasizes the importance of building with care, as butterflies symbolize biodiversity, which may dwindle in population if areas become too densely populated. The Looking Glass display serves as a window into Koda's design process, making visible to Nisa what is usually invisible - the decision-making process. After a round players swap roles and are then encouraged to contemplate gestures of welcome for newcomers in their communities. Operating at multiple scales, Delta Real/ation addresses individual, community, city, country, and global levels. We hope this invites collaborations with urban planners bringing forward inclusive policies underscoring UN Sustainable Development Goal #11 with a commitment to creating sustainable, inclusive, and resilient urban living. The project draws inspiration from speculative design principles, embracing the Protopia Futures framework to explore possible, plausible, probable, and preferable futures. Through ideation processes, world-building, and collaborative activities, Delta Real/ation envisions a future where diverse communities can thrive. The accompanying short video script introduces how players become Koda and Nisa, new migrants who embark on the journey of planning their sustainable community. The game soundtrack sets the tone, incorporating contemplative music to enhance the immersive experience. Building types and capacity, along with a diverse set of credits, contribute to the rich, collaborative tapestry of Delta Real/ation. In conclusion, Delta Real/ation transcends the commercial city-building simulation genre, challenging participants to consider environmental impact and sustainability. By placing migrants at the forefront of community planning, the project fosters empathy, connection, and a sense of belonging. Through its unique approach, Delta Real/ation invites players to envision and actively contribute to a future where inclusivity and sustainability define our cities and communities.",,,,,,,"audacity, blender, c#, photoshop, unity",https://codeberg.org/reality-hack-2024/TABLE_82
MIT Reality Hack 2024,WeSources,https://devpost.com/software/wesources-aniqu5,False,"We live in a complex and dynamic world, where cities face many challenges and opportunities. Food insecurity, homelessness, inequality is on the rise. Public services are facing increased demand. Community members need info about assistance in their area. We need to increase civic engagement in this at-risk population.However, there is often a gap between the people who make decisions for the city and the people who live in the city. This gap can lead to inefficiencies, inequalities, and dissatisfaction. Community members are not actively engaged in the policy-making process. According to research, some community members are not aware of the resources in their area. Major reasons include: Engagement by email or phone is tedious for the average resident. Difficulties for community members to provide feedback to their local governments and receive replies from their representatives.","WeSources XR integrates Augmented Reality (AR) and Mixed Reality (MR) technologies to bridge the communication gap between community members and decision-makers, aligning with the goals of SDG 17. The AR tool, designed for residents, displays a 3D map that highlights essential community resources like food pantries and shelters. Additionally, it allows residents to report specific local needs or issues directly, such as shortages in food pantries. 
On the other hand, it is a powerful spatial computing solution for city administrators and policymakers. The MR tool for decision-makers presents a data-driven 3D city model, enabling them to monitor these community inputs in real-time. This feature facilitates informed and responsive decision-making based on the actual needs and voices of citizens. 
By establishing a feedback loop, where decision-makers' responses are visible on the residents' AR app, your product not only promotes transparency but also encourages ongoing engagement and trust within the community. This dual-technology approach ensures that policies and actions are more closely aligned with the real-time and specific needs of the community, fostering a more connected and responsive urban environment.","We started with open-source 3D model assets and generated more assets with Blender. We designed user interfaces as the 2D assets. The mobile AR is built with Lens Studio, and the mixed-reality experience for Meta Quest 3 is based on the Meta Presence Platform and Ultraleap.",Unexpected system issues at the early stage of development brought challenges to our development process.,"Multiple-device mixed-reality experiences are utilized in a meaningful way for different groups of audiences.
Team members all contributed extensively to the whole project, leveraging everybody’s strengths.",Research helps understand the problem context and shape the design decisions.,We believe that “WeSources” has the potential to expand and evolve in several impactful ways:,"blender, meta-presence-platform, ultraleap, unity",https://www.snapchat.com/lens/8bbd0f95c85045de978a6dd29a83c513
MIT Reality Hack 2024,MR Fossil,https://devpost.com/software/mr-fossil,False,,"MR Fossil is a game designed to help users draw connections between the present and the past by discovering fossils. The Adventurer embarks on a journey through the history of life on Earth with Meta Quest 3. Being in the present time surrounded by rocks, the user explores how to use the tool on the scene. With the help of the tool, the adventurer discovers MR Fossil and his friends. Every finding unveils information about the fossils in the Cambrian Era, and as a result the game creates an opportunity to learn the history of life on Earth through the MR technology experience.","MR Fossil was built in Unity specifically for the Meta Quest 3, using the Meta Presence Platform. We used elements of the Interaction SDK, including Hand Tracking; the Audio SDK (specifically, WitAI for text to speech); and the Mixed Reality SDK for Passthrough and Scene Understanding, using MRUK (Meta's Mixed Reality Utility Kit) to spawn rocks in the passthrough environment. 3D assets were hand-built and animated in Cinema4D and textured by hand in Substance Painter by our 3D designer. Ambient audio and sound effects were sourced from FreeSound and mixed in Reaper by one of our developers.","The initial challenge was to generate and decide the idea. We started to direct our focus towards a different domain in learning but we ultimately decided on historical education due to connection with the past and the Earth.
Hardware problems such as the Meta Quest 3 having issues pairing with the devices on which we wanted to build the game.
Meta Presence Platform helped us develop complex interaction quickly but posed its own challenges. Because of the newness of the software, there are limited resources for help and documentation.","We are proud to be early users of Meta’s Presence Platform, and that our game can contribute to education by connecting kids to the history of the world around them.
We’re proud of how efficient and focused our team was and how well we got along and brainstormed and implemented our idea while staying calm and friendly.
We’re proud of our assets being almost completely built from scratch by our designers.",We learned the power of working in a team where each member is encouraged to learn and grow through the process of building. We also honed our problem solving skills through the challenge of working with new technology including the Quest 3 and Meta Presence Platform SDKs.,"-Make the game location based and give the adventurer more places to visit to explore new species that lived in certain time periods and areas. For example implementing a globe map that allows users to change location and learn about a variety of natural history.
-Utilize Meta's Voice SDK and WitAI further by scraping fresh facts and information for players as the game expands.
-Enhance accessibility by adding options for subtitles and for seated gameplay.
-Adding a multiplayer feature enables users to play together, this will foster socialization and collaborative learning by connecting them with other adventurers.
-Adding an option to choose different sub topics from natural history (i.e: Mesozoic Era, Cenozoic era, human archeological history etc.)
-Implement a journal of discovered fossils to store facts about them and record progress.
General optimization and cleanup of the codebase to be more production-ready.","cinema4d, meta-presence-platform, meta-quest-3, meta-xr-all-in-one-sdk, reaper, substance-painter, unity",https://codeberg.org/reality-hack-2024/TABLE_35
MIT Reality Hack 2024,Treasure Trove,https://devpost.com/software/treasure-trove-72vhft,False,As children of East Asian parents and cultures abroad we have a hard time connecting with our parents.,,,,,,,"bezi, c#, nomad, norncore, whisper",https://codeberg.org/reality-hack-2024/TABLE_55/src/branch/mergeinprogress
MIT Reality Hack 2024,Hungry Gaussians,https://devpost.com/software/hungry-gaussians,False,"Our memories make us who we are. Memory shapes our identity, allowing us to learn from the past, recognize loved ones, and share important life events. Unfortunately, memory loss threatens this for many people. Diseases that impair our brain are on the rise, robbing away our memories. More than 100 million people are diagnosed, and even younger adults are experiencing ""covid brain fog"" that impairs memory and cognition. Memory loss isolates people from their lived experiences and connecting with loved ones. It's heartbreaking to witness someone forget their family or their own life story. We believe memories are too valuable to lose, and we might even want to share some of them. Studies show that sharing positive memories can reduce depressive feelings. Reminiscing keeps us linked to meaningful times and helps us pass on our life story. This is especially important for those who've had to relocate, or migrate from their homeland. Preserving cultural and family memories allows traditions to continue and connections to be maintained across distance. Digital archiving enables this sharing of intergenerational knowledge","(Re)live is an innovative VR memory archive utilizing the newest 3d reconstruction technologies, allowing users to scan spaces from their lives right from their phone, and add photos, videos, audio, text and more as interactive exhibits. This time capsule lets you curate meaningful memories and share them with others. Visitors can view your memories, and you can allow them to add their own memories to your space to enrich the collective story. It's a place to reconnect with nostalgic moments and pass them down, and share a live-like VR that makes you feel like you are right back in that moment. Our memories make us who we are. Let's make sure they are never forgotten. The (Re)live virtual archive preserves your memories so you can revisit the experiences that shaped you and share your evolving story with future generations.",We build the app using Unreal Engine 5 and captured Gaussian Splatts using Luma AI and OpenXR. We tested the app using quest 3 with Quest Link.,,,,https://github.com/GP2P/HungryGaussians,"luma-ai, ue5, unreal-engine",https://codeberg.org/reality-hack-2024/TABLE_94
MIT Reality Hack 2024,OurBloc,https://devpost.com/software/ourbloc,False,"Faye's oppressive studio environment at Parsons served as a powerful motivator. He and his classmates craved a change, longing for a more engaging and open space. Realization:Faye realized that the discomfort he felt was a sentiment shared by many, yet it remained unspoken. Recognizing the power of a collective voice, he envisioned a platform for change.",,,,,,,"8thwall, a-frame, css, firebase, html, javascript, lightship, vps",https://emeadevroadshow.8thwall.app/cityhubvps/
MIT Reality Hack 2024,NetConnect,https://devpost.com/software/ai-learning-assistant,False,"Learning as a neuro-divergent individual can be challenging, and I know I'm not alone. Teenagers with special needs often face difficulties in their educational and social lives due to the lack of suitable learning resources. This not only affects their personal growth but also has significant societal consequences, leading to a staggering macro-economic cost of over $130 billion. Unemployment rates among adults with neuro-divergent conditions such as ADHD, Dyslexia, and Autism are alarmingly high, resulting in limited employment opportunities and reduced financial independence. Studies have shown that high school graduates with learning disabilities earn an average of 17% less income compared to their neurotypical peers.",,The core of our solution is built with Unity SDKs:,,,,,"blender, inworld, natural-language-processing, oculus, unity",
MIT Reality Hack 2024,Hacker Quest,https://devpost.com/software/hacker-quest,False,"The project is inspired by Hollywood Hacking trope, where hackers would use neon monitors and salad of sophisticated words to break into security systems in minutes. 
The second source of inspiration for Hacker Quest was the crash of the highly advertised Destiny sorting system on the very first day of its work. 
The initial idea of the project was the hacking-themed scavenger hunt that would play on these two tropes while also helping hackers socialize and get used to navigating around MIT.","Hacker Quest is an AR scavenger hunt game focused around the MIT buildings. The players can access the game from a browser, and will need to input passwords that are based on MIT- and hackathon-themed riddles, as well as complete combat encounters with bugs in order to complete the game. The game consists of two separate quest lines. The first one is focused on navigating around MIT and completing challenges bound to specific terminals around the university. The second one involves achieving passwords from hackers of all six tracks within the hackathon. Upon completion of both quest lines, the player manages to successfully “save the crashed system” and win the game.","We used Midjourney to create the 2D art for the NPCs. We walked around the MIT campus to scout out suitable locations for terminal hacking puzzles. Then we assembled the questline using EarthMMO Studio. We created a custom 3D bug enemy model to use in a kill quest using Blender, Substance 3D and Procreate.","The weather was cold and rainy the entire week, which discouraged people from exploring the campus.
The ice-breaker quest we designed requires talking to each hacker to tell them their “purpose password”, which is very inefficient - it would have been great to coordinate with the organizers to relay this information through the realityhack.world website.
Technical issues involved minor production mistakes: the encounters would appear to be moving and attacking players sideways despite playing with rotation coordinates during export. Aside from this, it took a moment to figure out narrative writing to ensure it is concise.","We designed and built the questline in a day and a half, and had plenty of time to market it to hackers and get some sleep!","People generally avoid reading, and would skip dialogues to get to the gameplay.
In addition to this, we have learned the way to store multiple animations in Blender at the same time, and practiced Midjourney prompt engineering to make it drastically more precise.
Aside from this, we have managed to achieve a pretty polished vertical slice of the project in a brief period of time, which gave us a good grasp of what is possible within the two-day hackathon.","We will continue to create more immersive storylines for live events on the EarthMMO platform!
Follow our progress atour Discord","8thwall, croquet, express.js, mapbox, mongodb, node.js, react, react-three-fiber","https://www.earthmmo.com/, https://github.com/EarthMMO"
MIT Reality Hack 2024,Penpals,https://devpost.com/software/penpals-zui5o2,False,"Our team is driven by the desire to make language learning an enjoyable and accessible experience for children. We draw inspiration from the idea that technology can play a pivotal role in creating interactive and immersive educational platforms. Not only do we see the potential of connecting children from different parts of the world through our learning platform, but we also wish to instill a sense of curiosity and learning amongst children.","Penpals makes language learning fun and interactive for pen pals. In this time, we made a mixed reality word learning game that prompts users to find the 3D model representation of the word that matches the flashcard. If correct, the user gets the opportunity to throw the object at an alien to win the game. If incorrect, the object disappears and the correct definition of the word is given. Given more time, we would make a web app to allow users to send words for their penpals to learn. Once a word is submitted, our app would turn it into a flashcard and create a 3D model representation of the word in real-time using AI. Users can then learn words from their pen pals through the game and be able to better connect to their penpals.","PenPals is a Unity application built for the Meta Quest 3. We used the new Presence Platform toolset, which allowed us to add utility like handtracking and passthrough mixed reality with relative ease. To generate our 3D assets, we used Luma AI's Genie tool to quickly create models for the words. For the prototype, we pre-generate the models based on a set word list, but future versions will generate 3D models in real-time. The UI elements like the flashcard are a combination of custom 3D-modeled Blender assets made by us and elements from the Presence Platform and Unity's UI.","We ran into many issues with building to the headset from Macs, and this required a day of troubleshooting. We also had a difficult time figuring out how to use MRUK to randomly spawn objects into the room. Some other issues we ran into involved game logic with figuring out how to identify which object the user was picking up.","We are proud of being able to maintain our vision throughout the whole process. Because of this, we didn't have to pivot later to simplify the MVP on or remove features that weren't possible. We also faced build issues for almost a full day but persisted and were eventually able to build to the headset from all devices (both Windows and Mac), allowing us to test faster given that in-editor testing wasn't possible on the Macs.",We learned a lot about troubleshooting and prototyping effectively.,"Although the web app was a part of our vision for the product, this was out of our scope given the time and nature of this hackathon. In the future, it would be great to make the web app and integrate it with the MR game. We would also like to add more sharing features to make it more like a sharing platform between penpals. Pen pals would be able share stats and pictures of them learning through the game.","ai, meta, metaquest, mruk, presence, quest3, unity",https://codeberg.org/reality-hack-2024/TABLE_53
MIT Reality Hack 2024,Happy Balloon Fun Time,https://devpost.com/software/happy-balloon-fun-time,False,We are inspired by current research into proprioception and body illusion. We are looking into developing a testbed for running experiments involving hand and body dissociative effects.,It is a simple game that hides a method for adapting to and from a hand displacement (dissociative effect),"Slowly with lots of experiments to determine a powerful and achievable effect within the hackathon time limit. We built it with Unity, XRITK, Blender, and lots of testing.",Sometimes it was hard to work on it for long periods of time due to experiencing the effects ourselves. The meta virtual hand implementation really does not want us to do this so it was a fight but fun.,"The effect works, and we are able to demonstrate it fairly consistently","From the prototyping and experimentation we learned that most people when put into this game experience the effect. There seems to be limits as to where how far we can displace the hands, in what direction and how long the participant needs to play the game.","Adding support for exporting data from the session:
-Time
-Game Score
-Position and Rotation of all tracked limbs
-Hand offset from real hands Designing an experiment as an example for future use.","blender, c#, jetbrains-rider, unity, visual-studio-code, xritk",https://codeberg.org/reality-hack-2024/TABLE_75
MIT Reality Hack 2024,ThriveHive,https://devpost.com/software/thrivehive,False,"We were driven by a desire to add some ease, fun and efficiency to a mentally and emotionally heavy process.","ThriveHive is a web based app that gives users the power to crowdsource and easily access information regarding affordable healthcare, foodbanks, shelters and other essential needs and services",We developed ThriveHive using 8thwall's XR development platform in conjunction with Bezi for additional prototyping,Getting the real-world location data into the platform while testing,,We learned a lot about just how many Americans make use of what the govt dubs the Social Safety Net (approx 99.1 million or 30% of the population). We also learned that,"We want to expand the features of the app, start exploring institutions and company partnerships and explore possible integrations. Additional features:","8thwall, a-frame, javascript, three.js","https://codeberg.org/reality-hack-2024/TABLE_83, https://www.figma.com/proto/SQ5hzZBFNHHAlKUpYvv8Ds/Pitch-Presentation-Draft?page-id=2532%3A114&type=design&node-id=2536-464&viewport=3463%2C208%2C0.2&t=oPQGuHZxcaKA8iZE-1&scaling=scale-down&starting-point-node-id=2536%3A464"
MIT Reality Hack 2024,FarMR,https://devpost.com/software/farmr-am6hbe,False,"Fueled by a passion for connection, our project delves into the dual realms of Mixed Reality technology and the profound role food plays in bonding and culture. As urbanization widens the gap between society and nature, our mini-game strives to make children more aware of plant upkeep, fostering responsibility and a genuine connection to the origins of their meals.","FarMR offers a real farming experience, simulating crop growth while emphasizing nutrient management. It provides a hands-on learning experience, allowing users to understand the intricacies of farming by actively managing nitrogen, phosphorous, and potassium levels in the soil. Each crop's unique growth process is visually represented, enhancing comprehension, with practical activities like watering and testing nutrient values, providing a truly immersive and educational farming simulation.","The game was developed on Unity for Quest 3 using Meta's Presence Platform. We utilized Meshy for all the 3D assets generation and ensured accurate textures for each crop stage through manual adjustments in Blender and ClipStudio. Leveraging Meta XR All-in-one SDK, we achieved precise hand tracking, enabling users to pick up seeds and wield tools realistically. The game mechanics were programmed in C#, seamlessly integrated using Unity, bringing together a dynamic and engaging FarMR experience.","In terms of the theme we were going for, a challenge was creating an educational yet simple and comprehensible game for younger users. The XR technology market for children is currently fairly limited, so a goal was to invent an innovative yet entertaining way to present our idea with as little confusion as possible in terms of mechanics. However, we still wanted to present a small game challenge in the upkeep system of maintaining the individual plants the users are expected to care for.","With how new Meta's Presence Platform is, we're proud of how quickly we were able to adapt and develop a workable prototype with a team of junior developers and designers.","Although the game data presented may come easily to an adult, we had to learn how to simplify it enough for our intended audience to understand. This included various aspects of UI and visual cues needed to present a specific need for each plant. Because of our target audience's age range, the idea of hand tracking seemed more feasible rather than having them memorize each button's usage on the Quest's controllers. This way they can use their natural hand movements to interact with objects.","Refining the system and crop management would be a great next step. Due to time limitations, much of the assets were AI generated and mechanics were limited. However, we would like to create our own original models and add inventory management, plus a shop option in the future.","blender, meshy, meta-xr-all-in-one-sdk, openai-api, unity",https://codeberg.org/reality-hack-2024/TABLE_50.git
MIT Reality Hack 2024,the library of beautiful things,https://devpost.com/software/the-library-of-beautiful-things,False,"""...the goal isn’t to carry you off to dreamland; it’s to keep you acutely conscious of where you are and why you’ve come” ― Jackson Arn for The New Yorker in a piece titled ""The Sphere and our ""Immersion"" Complex"" We're seeking solutions to bring awe and gratitude for the beauty in everyday things, with the hope of improving public mental and physical wellbeing as well as community bonds and safety.","Spreads the message that feeling gratitude and wonder when observing our everyday environments improves our mental and physical health, sense of connectedness, and ability to thrive.","We created a VR ordinary walk home, where users marvel at the mundane, observing ""beautiful things"" that we have noticed and collected from urban environments. Each of us contributed one ""beautiful thing"" into our ""Library of Beautiful Things"", which we hope to eventually expand into a shared XR community.","We originally hoped to build this in XR through geo-located objects. We lacked proper documentation of mixed reality in MetaQuest 3 as it's a new product, and movability throughout the city during the hackathon so we adjusted to a VR prototype with the hope to develop into XR in the future. Lack of high performance computing for rendering.","Successful implementation of mixed reality transition out of the VR experience, a city scale virtual world within tight time constraints, an emotional core to our project. Our team was able to integrate spatial audio for the scenes in the story board.",Additional Unity development skills and 3-D design experience within Mixed Reality.,Creating a means for the public to add their stories of relished observations through their individual contributions to a shared XR space.,"c#, unity",https://codeberg.org/reality-hack-2024/TABLE_87
MIT Reality Hack 2024,Adulting 101,https://devpost.com/software/adulting-101-p5of32,False,"As we learn how to become real adults in our twenties, there’s a lot of skills that we wished that school had taught us. How do you wash your clothes? how do you tie a tie? How do you fix a broken toilet? Imagine the time, effort, and confusion spent on learning these basic life skills. 
Considering those concerns, we created Adulting 101 - your best companion who will assist you in your daily learning! With our product, our users can reclaim their sense of self-reliance and will feel empowered to navigate this thing we call adulthood. Adulting 101 was inspired by our own experience of being international students, figuring out adulthood in America for the first time. It was also inspired by the people we know. Recent college grads living alone, our friends who grew up without fathers, and veterans transitioning to civilian responsibilities.","Our first module teaches the basics of washing white clothes. Handling white fabrics can be tricky sometimes. While it makes a perfect wardrobe addition, washing and keeping clothes white requires special care, such as choosing hot temperature and a specific cycle mode. Don't worry, we are here to teach you how to keep your favorite garments in perfect condition!","First, we conducted user research and asked seven hackers between the ages of 20 and 25 about their most common adulting challenges. Some of our questions are: do you consider yourself good when it comes to basic practical skills? How often do you find yourself stuck in everyday household problems? When a problem like this occurs, what are your reactions? what is your first action? From our user research, we found that young people encountered an ‘adulting challenge’ at least three times a week and felt very low confidence in their abilities to complete these tasks. Moreover, young people tend to figure things out themselves before seeking help from adults, and from our survey, it was revealed that some solutions they find on the internet are not always helpful. They would prefer an assistant who will assist them in real time. We then selected operating the washing machine as our first module. Next, we prototyped with Figma and Bezi and implemented our vision using Meta's Quest 3 and Presence Platform. Lastly, we enhanced immersion through Meshy 3D objects and Pixabay audio.","All the tools we used were new to us. For example, one engineer could not start developing until the final day of the hackathon because we dealt with difficulties while setting up the headset devices. We also tried out Bezi - the new collaborative prototyping tool for a spatial environment - for the first time and learned everything from scratch in under 1 day. Due to time constraints, we could not fully transform our mockup prototype into a final implementation, which meant that we had to focus on only essential parts of the entire demo.","Despite the challenges, we created a fun, useful product, with the debugging help of our mentors. Although we met each other for the first time, we combined our skills and talents and worked as a strong team. Even though we came up with different ideas at first while brainstorming, we still found common ground and worked effectively together towards the end of the hack.","We learned how to debug unity and prototype in 3D scenes. We also learned how to ask for help effectively. Also, working in a diverse team means we learned the importance of collaboration and effective communication.","We want to expand our modules. Some learning tasks examples are ""how to jump a car"", ""how to fix a running toilet"", and ""how to tie a tie"". We also want to improve user interfaces, narrative, detailed steps to replicate in real life. Another approach we want to go with is build an AI companion who will help the user in real time by demonstrating the task side by side with the user, thus enhancing the values of learning - which is also the theme of our track.","bezi, c#, figma, meshy, meta-presence-platform, pixabay, unity",https://codeberg.org/reality-hack-2024/TABLE_18
MIT Reality Hack 2024,Memory Lane,https://devpost.com/software/life-of-a-story,False,"1. empty capsule 2. create memory 3. receive capule 4. experience memory 5. connection-building interaction Would you like to take a trip down memory lane? Immerse yourself in a world where memories are finite and precious. Our XR app transforms everyday objects into capsules, each containing a unique, finite story. Watch memories unfold, listen to narrations, and experience the magic as these stories gradually dissipate with each viewing. With RFID-enabled capsules, any small object can hold not just a story, but emotions and significance. Elevate the value of your memories in an unforgettable way. Living in an era of heightened disconnection, catalyzed by technology, our screens paradoxically isolate us from ourselves and others. Our busy lives often neglect embodied connections, which jeopardizes our health, mental and physical well-being, increasing risk for numerous diseases such as diabetes, depression and anxiety, addiction, heart diseases, dementia. While existing solutions rooted in digital photographic exchange strive to connect us worldwide, they distance us from embodied and multisensory connections.  Using Mixed Reality approach to sharing we enable proven methods of human connection and introduce new modalities to expand through XR. Our busy lives are continuously choosing convenience and abstraction over human connection. Everything continues to converge into a tiny 6.1” screen. But with this all powerful device we still choose to use photobooths, cherish notes, and save souvenirs, despite knowing these items won’t last forever. While digitizing the world around us, we have lost something in what it means to own an object and share them with others. Memory Lane is a way to reimagine connection in the future that brings the joy, and serendipity of sharing with others, taking advantage of new digital mediums, and enabling connection through creation and shared exchange. Memory Lane allows us to imbue physical objects with spatial multimedia that can be played back by the receiver. However, there is a catch. Much like physical objects, your digital layer will slowly dissolve with each playback through degenerative A.I. until eventually it is unrecognizable. Once depleted, objects can be rewritten with new digital layers, ready to be shared once again. Participants begin by selecting an item of their choice and adding a unique ID tag to it. The item is then placed onto our memory station to imbue the object with a digital memory. Participants use their mixed reality headsets to create a spatial multimedia experience around the object. This can include photos, videos, audio, 3D painting, 3D objects and more. Once complete, the memory is tied to the physical object and ready to be shared. The receiver of the physical object can view the memory anytime on their own base station and choose to destructively review or overwrite with a new memory. As digital memories are played back, they slowly degrade over time. Memories are tied to physical objects using standard, rewriteable RFID tags. These tags communicate with our proprietary base station that uses an RFID reader and Neopixel LEDs to create,read, and overwrite content. Users can craft their memories with a variety of MR headsets including the Meta Quest 3 and Snapdragon supported “ThinkReality” headsets. Content can take the form of any standard multimedia file formats including photos, video, audio, 3d models but it really comes to live when you arrange it in a spatial environment and narrate your story to it. The idea went back-and-forth for a long time before we could finalize it, which took a large proportion of time on the first day. Several members of the team are having inconveniences in physical conditions, and one of the team was absent for a whole day for emergency care. Thus lacking of time becomes the largest challenges we faced. We are proud of creating an experience enabling both introspective and shared connections, cherishing both physical and digital worlds and showing significance for physical and mental health and wellbeing creating connections and appreciating life in the moment. We are also proud of fully functional and beautiful prototype, communication between hardware and mixed reality headsets. We chose this project because we as a team understand what it means to connect with friends, family, and especially after this weekend, strangers. We know these moments are special but fleeting. We wanted to capture the magic of a temporal experience through technology and share that with the world. General Technical",,,,,,,"c#, esp, openxr, quest3, snapdragonspaces, socialxr, stereokit","https://www.figma.com/proto/enxhZuOxuPKmOR4AvmXUBY/MIT-REALITY-HACK?type=design&node-id=1-867&t=ofEUU7DnnBbZhLHz-1&scaling=min-zoom&page-id=0%3A1&starting-point-node-id=1%3A2&mode=design, https://codeberg.org/reality-hack-2024/TABLE_15, https://youtu.be/rkWVgYX2xoc"
MIT Reality Hack 2024,Echoes of Silence,https://devpost.com/software/echoes-of-silence,False,,,"We used Foundry and Photon Fusion for our simple multiplayer experience, the Meta Presence Platform for the Haptics SDK for the Oculus Quest, and Unity as the foundation for our VR experience.",,,,,"c#, foundry, fusion, photon, unity",https://drive.google.com/file/d/15eIfzvMTbxRIB458sdq8AvuUCTQFMnDM/view?usp=drive_link
MIT Reality Hack 2024,"3Datalyze [Team B306, Table 61]",https://devpost.com/software/table-61,False,"What will make your indie game take off faster - adding 3 marketers or 3 more devs? What will keep the company alive longer, 10 more devs or 10 sales people? We were inspired by our first-hand difficulty forecasting the effects of decisions in small businesses - without access to enterprise-level BI tools and consultants.",Unlimited node-based relationships between resources and activities and a 3D visualization of resources flowing around over time based on business decisions made. Ability to run unlimited scenarios over and over.,"Unity, Meta Presence Platform",,,,,"meta, quest, unity",
RevolutionUC 2025,Needlehelp,https://devpost.com/software/needlehelp,True,"While volunteering in the operating room, I noticed that placing subdermal electrodes was a slow, manual process prone to needle sticks. This inspired me to create a device that could automate the procedure, improving safety and efficiency.","Our device automates the placement and retraction of subdermal electrodes using a robotic arm controlled by an Arduino Uno. It accurately inserts and removes the needle, reducing human error and speeding up the process.","We used an Arduino Uno to control the robotic arm and integrate real-time sensors for precise needle placement. The device was designed to be portable, sterilizable, and easy to incorporate into existing surgical workflows. We also compiled a dataset and trained an AI model to optimize needle movement and placement accuracy.",,,,,".net, arduino, c#","https://github.com/nechaeai/MLPredictor, https://docs.google.com/presentation/d/1gPxqjEVPVaSvv48ip4rvbd8i7Uc08aV4P4qJ7q-H_ws/edit?usp=sharing"
RevolutionUC 2025,"Memora: Clear Your Mind, Conquer Your Day.",https://devpost.com/software/memora-clear-your-mind-conquer-your-day,True,"Have you ever had this feeling where you walk into a room and just forget why you entered, lose an idea you really wanted to remember, or just forget an assignment until it’s too late to submit? This is a problem that every single person goes through in normal day-to-day activities, whether it’s something serious like taking your meds or just a simple reminder of lowering your caffeine intake. Having something that will give you a physical reminder when you need it is a necessity for almost everyone. That’s why we created Memora. Being inspired by the problems we face currently and what our parents have been facing for a long time is exactly why our team of four passionate engineers who are eager to grow and learn came up with an incredible solution. Digging deeper, we found that this solution is revolutionary for people facing challenges like memory impairment. Imagine the fear, the isolation, of losing your memories, your connection to the world around you. This is the reality for millions living with dementia. We've seen heartbreak in families, the strain on caregivers, and the silent struggle of those affected. It's this shared human experience, coupled with our team's deep-seated passion, that fuels our drive to make Memora a reality, a tangible solution for a universal challenge.","Memora operates on a foundation of robust, localized machine learning. To begin, we utilize a standard ESP32 overclocked, enabling us to capture and process high-resolution images and 2D scans of faces and objects. This initial data collection phase is crucial, forming the basis of our proprietary machine learning dataset. For instance, in a real-world application, this allows a user to rapidly train the device to recognize frequently misplaced items like keys or wallets or to identify the faces of loved ones, ensuring they are never forgotten. Subsequently, we leverage the power of Edge Impulse and TensorFlow to develop a custom machine learning algorithm. This algorithm is then distilled into a dedicated, efficient library optimized for the ESP32's architecture. This enables Memora to accurately identify faces and objects with a predetermined level of precision, even in diverse lighting and environmental conditions. Imagine a user, in a busy market, needing to quickly identify a family member; Memora's algorithm can do this swiftly and reliably. Upon successful identification, the ESP32 initiates a wireless data transfer to the user’s bracelet. This is achieved through a secure, locally hosted private Wi-Fi connection, transmitting encoded numeric values. This ensures data privacy and speed. In a practical scenario, if a user is reminded to control their caffeine intake when it sees a Red Bull can, the identification of that can triggers the reminder. On the bracelet, the received numeric values trigger a script that sends the programmed reminder text to Google Cloud’s Text-to-Speech (TTS) AI. This conversion to audio happens rapidly, allowing for seamless communication. For example, a user might be reminded to ""take your medicine now"" via a clear, natural-sounding voice. The Google Cloud TTS output is then converted into an encoded audio format compatible with the ESP32’s I2S amplifier, ensuring high-quality audio playback. Simultaneously, the onboard LCD display on the bracelet displays the text of the reminder, providing a visual cue. This dual-sensory approach ensures that users with varying needs can effectively receive and understand the reminders. All of these processes, from image capture and ML processing to audio and text output, are executed within a remarkable three seconds in our current prototyping phase. This rapid response time is critical for real-life applications, ensuring that Memora provides timely and effective support for users, whether it's reminding them of daily tasks or helping them navigate their surroundings with confidence.","The journey to create Memora was a testament to our team's ingenuity and relentless problem-solving. Faced with the challenge of building a sophisticated assistive device with limited resources, we began by meticulously evaluating every available component. Through rigorous brainstorming and testing, we arrived at an optimal configuration, carefully selecting our key components and processors for their balance of performance, affordability, and power efficiency. This initial phase was crucial, setting the stage for the innovative approach we would take. Next, we delved into the intricacies of programming these processors. Recognizing the need for offline machine learning capabilities, we focused on integrating ML processing into basic, readily available Chinese microcontroller boards. This required extensive research and experimentation, pushing the boundaries of what these boards could achieve. After countless hours of coding and testing, we successfully developed our first working, basic single object detecting algorithm. This breakthrough was a pivotal moment, validating our approach and providing a foundation for further development. Building upon this success, we expanded our dataset and refined our ML algorithm using the powerful tools provided by Edge Impulse. This platform enabled us to iterate quickly, improving accuracy and reliability. With a robust ML algorithm in place, we shifted our focus to the bracelet component. We established a seamless wireless communication system, enabling the necklace to transmit signals to the bracelet when a designated face or object was recognized. This involved setting up secure local Wi-Fi networks for both ESP32s, ensuring reliable and private data transfer. With wireless communication established, we turned our attention to the user interface. We implemented a system for setting reminders associated with identified objects and faces. Leveraging Google Cloud's Text-to-Speech (TTS) AI, we converted these text reminders into encoded audio files, ensuring high-quality playback through our I2S amplifier and speakers. Simultaneously, we displayed the reminder text on an LCD 16x2 display with an I2C shield, providing a clear visual cue. This dual sensory approach ensures that users can receive and understand reminders effectively, regardless of potential sensory impairments. Throughout this process, we prioritized efficiency, ensuring that all operations, from object detection to audio and text output, are executed rapidly. This commitment to speed and reliability is essential for creating a truly useful and impactful assistive device.","We encountered several key challenges during Memora's development. Primarily, we faced significant hardware limitations. Optimizing machine learning algorithms for an entry-level microcontroller demanded extensive resource management and pushed the device to its processing limits. Achieving reliable facial recognition with a 2MP camera required meticulous data collection and algorithm refinement, ensuring accuracy in diverse lighting conditions. A critical challenge arose just before our presentation: a sudden LCD display failure. This forced us to quickly diagnose the issue and implement a DIY solution using a variable resistance, showcasing our ability to adapt under pressure. These practical obstacles tested our resourcefulness and problem-solving skills, demanding rapid, creative engineering. Each challenge reinforced our commitment to Memora, proving that ingenuity and perseverance are essential for overcoming technical hurdles and bringing innovative ideas to fruition.","We're immensely proud of several key accomplishments that define Memora's journey. First, we achieved robust face and object recognition on a platform never intended for such complex tasks. By building our own ML library, we pushed a cheap microcontroller to its maximum limits, demonstrating that sophisticated assistive technology doesn't require expensive hardware. We created something incredible from scratch, proving that innovation can thrive with limited resources. Our commitment to making extremely affordable assistive technology is a significant achievement. We've developed a solution that's accessible to a wider demographic, addressing a critical need without financial barriers. Finally, we're proud to have developed something that was never thought of before, a unique integrated system that combines advanced ML with practical, user-friendly design. We've proven that groundbreaking technology can be both powerful and accessible, and we're excited to bring Memora to those who need it most.","We pushed the limits of what we believed to be feasible with assistive technology today as we started a steep and quick learning curve. By learning firsthand about the enormous potential of optimizing algorithms for resource-constrained situations, we expanded our knowledge of embedded machine learning. We gained an understanding of the complex interplay between software creativity and hardware constraints, learning how to get the most performance possible from widely accessible microcontrollers. In order to ensure effective and confidential data flow between devices, we acquired vital experience in developing a seamless, localized wireless communication system. We learned the value of modular architecture and strong error handling from the challenges of integrating many technologies, from audio output and display to camera input and machine learning. We also improved our user-centered design abilities, realizing how important accessibility and ease of use are in assistive technology. We understood the subtleties of turning intricate algorithms into user-friendly interfaces and realized that the real worth of technology is found in its capacity to empower and assist. Additionally, we understood the value of iterative development and rapid prototyping, which let us quickly adjust to obstacles and improve our strategy in response to immediate feedback. Lastly, we reiterated our conviction that affordable innovation can be powerful. We discovered that innovative technology can be created with little funding, demonstrating that drive and creativity can get beyond even the most difficult challenges. We departed today with a fresh perspective on how technology can improve people's lives and a greater comprehension of the commitment needed to turn an idea into a workable, game-changing solution.","Our immediate focus is on solidifying Memora's foundation, ensuring it's ready for widespread impact. We will meticulously refine the user experience, prioritizing clarity and ease of use. This involves a comprehensive front-end development cycle, concentrating on the LCD display and audio feedback to guarantee seamless interaction for all users. We are also committed to enhancing the core functionality through rigorous machine learning model training. By significantly expanding our dataset with diverse faces and objects, and by exposing the model to numerous real-world prompts, we will achieve a marked increase in recognition accuracy and reliability. Crucially, we will also begin integrating more advanced and convenient hardware components. This includes exploring next-generation microcontrollers with faster processing prowess, enabling us to execute our machine learning algorithms even more efficiently and reduce latency. We will also look into more compact and power-efficient camera modules, and audio components, making Memora more comfortable and discreet for everyday wear. These hardware upgrades will allow us to further enhance the device's performance, ensuring seamless and reliable operation in diverse environments. To protect our unique innovation and pave the way for sustainable growth, we will actively pursue patent protection for our technology and design. This will solidify our market position and ensure our groundbreaking approach remains exclusive. We are also dedicated to minimizing online dependencies, striving for a truly self-contained, offline solution that respects user privacy and guarantees uninterrupted functionality. Our initial market entry will concentrate on individuals living with dementia and their caregivers. We will forge strategic alliances with Alzheimer's associations, senior living facilities, and healthcare providers to facilitate targeted adoption and gather crucial user feedback. We will also prioritize building a supportive online community. To expand Memora's reach to the general public, we will develop a versatile user interface that caters to a wide range of needs, from daily organization to enhanced personal safety. We will explore partnerships with retail channels and online marketplaces to broaden our distribution. From a business standpoint, we will prioritize building a robust and scalable model. This includes refining our pricing strategy, optimizing manufacturing processes, and investing in continuous research and development. We will also explore subscription-based services for premium features and ongoing support, ensuring long-term value for our customers. Through strategic partnerships and a steadfast commitment to innovation, we are confident that Memora will become a leading force in the assistive technology market, empowering individuals and fostering greater independence.","aruduinopromicro, c++, css, edge-impulse, esp32, google-tts-ai, html, i2c, i2s, javascript, json, machine-learning, ov2640, seeed-studio-xiao, tensorflow, wifi",https://github.com/DhruvBhilare/RevUC25
RevolutionUC 2025,FUNDR,https://devpost.com/software/founderr,True,,,,,,,,"amazon-web-services, android-studio, figma, flutter, node.js","https://dev.drs7acho082ae.amplifyapp.com/, https://github.com/BingChilllling/flutter_application_1"
RevolutionUC 2025,RealityCheck,https://devpost.com/software/realitycheck-t35pxd,True,"In an era of misinformation, distinguishing fact from fiction has become increasingly challenging. RealityCheck was created to address this problem by providing a fast, reliable, and AI-powered fact-verification system that enables users to verify information effortlessly. The idea for RealityCheck was inspired by an experience one of our team members, Ani, had at a Forbes 30 Under 30 event, where he spoke with a BBC anchor about accessing reliable information. When Ani asked how an average person could confidently determine the truthfulness of the news they consume, the anchor responded that it was the reader’s responsibility to research multiple sources before forming a conclusion. This conversation sparked a realization: with today's advancements in Agentic AI technology, we have the potential to automate this process, making fact-checking more accessible, efficient, and reliable for everyone.
RealityCheck aims to process claims, retrieve relevant data, and provide evidence-backed verdicts in real-time, helping users navigate the complexities of modern information consumption with confidence.","RealityCheck is an AI-powered fact-verification system designed to help users quickly assess the accuracy of claims. It retrieves relevant information from trusted sources, analyzes the data using advanced AI models, and delivers a clear, evidence-backed verdict. By providing reliable and explainable fact-checking, RealityCheck aims to combat misinformation and empower users with the truth.","RealityCheck is built using a combination of AWS services and AI models to deliver fast and reliable fact verification. AI Agents: We leverage four AI agents for key tasks: chunking/decomposition, search, summarization, and sentiment analysis.
Data Storage: Pre-embedded factual data is securely stored in Amazon S3.
Vector Search: Amazon OpenSearch with vector search capabilities efficiently indexes and retrieves relevant information.
AI Processing: Amazon Bedrock powers the LLM responsible for analyzing retrieved data and generating verification results, we also used the Gemini API and Ollama models for Agentic AI workflows and text summarization.
Backend Services:  Using Python’s FastAPI, we handle requests and manage data flow.
Frontend Interface: A Next.js-based web app provides an intuitive interface for users to submit claims and view fact-checking results.",,,,,"amazon, amazon-web-services, api, bedrock, javascript, lambda, opensearch, python, s3, serverless",https://github.com/AmaanBilwar/RealityCheck
RevolutionUC 2025,genarrex,https://devpost.com/software/genarrex,True,"Originally I had been showing off some of my technical projects to a pharmacist buddy of mine, and randomly the topic of my AncestryDNA came up. He pitched me the idea, and I loved it, but I never had the opportunity to build a demo. Personalized medicine is the future of healthcare. With treatments becoming more advanced with the advent of unheard-of compute, and even quantum compute, we wanted to pioneer this emerging field to empower individuals by providing clear, actionable insights into how their unique genetic makeup influences their health, medication responses, and disease risks. By combining cutting-edge genetic analysis with intuitive 3D visualizations, I aimed to make complex genetic information accessible and understandable for everyone.",Genarrex is an interactive genetic analysis platform that integrates personalized genetic data with AI-driven insights. It allows users to:,,,,,"Looking forward, I really want to work more on this and have better integrations:","amazon-web-services, claude, cloudflare, javascript, jetbrains, nextjs, openai, react, s3, tailwind, twilio, typescript, vercel",https://www.genarrex.com/
RevolutionUC 2025,EcoLLM,https://devpost.com/software/ecollm,True,"Training a single AI model can emit more than 626,000 pounds of carbon dioxide equivalent to nearly 5 times the lifetime emissions of a car. Vast amounts of resources are typically utilized such as powering tons of processing units, spending energy on training LLMs, and intensive manufacturing and mining of resources. Such factors, overwhelm the environment and can emit a significant load of carbon dioxide into the atmosphere, making it harmful. Nowadays, pollution is a big problem in today's world and that's primarily due to high carbon intensities and carbon levels. We took initiative to find a solution, where we create and train a LLM using minimal resources, maximizing efficiency, while optimizing it.","EcoLLM aims to present an intuitive, interactive and real-time visual dashboard for local LLM enthusiasts, allowing them to train their models without harming the environment as much. We provide a variety of AWS data centers across the globe and use the Co2Signal API to provide you with the servers with the lowest carbon emissions. We make the training process extremely quick and intuitive, taking no more than 3 clicks to upload a model and get straight to training and saving the environment.","We built the dashboard using Next.js and Tailwind to visualize a map of carbon levels spanning from different regions. Also we used AWS to make an LLM model in order to analyze carbon data and learn from them to make accurate evaluations. Additionally we use OpenAI API to make a scheduling feature to determine the least amount of carbon levels in a certain timeframe for a particular region. As for caching we utilized Redis and to obtain recent datum of carbon intensities, we used supabase to store it and be able to render it onto the dashboard.","The greatest challenge was integrating the two major parts of the project together. The first part was creating the dashboard, making API routes, and integrating the APIs into the UI through the Next framework. The second part was creating the LLM model through AWS services and training the model with the necessary data needed. These two parts are critical and if one of them fails, the project won't be flawless. Besides, there were external challenges that occured. One of them was getting the LLM model through AWS to run because of its slow runtimes and large data processing. Another challenge was working with the Co2Signal API because there were only a limited number of calls per hour that we had to work with so we decided to cache it using Redis.","The integration was super rough! Especially figuring out how to debug and get console logs with AWS Sagemaker. However, We powered through hard and got a super clean UI that has real-time updates, which is always a sweat visual.",,"We plan on branching out to lots of more data centers, giving local LLM developers endless choices to contribute to the environment while doing what they love. We also aim to add additional training customizations per model, such as quantization, weight pruning, knowledge distillation, etc etc.","amazon-web-services, co2signal-api, next-js, openai-api, redis, supabase, tailwind",https://github.com/kllarena07/revolutionuc
RevolutionUC 2025,BassGPT,https://devpost.com/software/bassgpt,True,"We came across Billy in a time of need. Decided to help himreelin his full potential and become Billiam Bass the BassGPT. Now he's ready to ""dive deep"" and make asplashin the tech world, ""hooking"" everyone with his fishy wisdom!","With the turn of a key and a press of a button, you're able to ask BassGPT anything that your heart desires. ""Billiam"" will enthusiastically respond accurately with a quirky personality, providing companionship from the deep blue sea.",BassGPT contains 3 systems:,Our biggest challenge was figuring out how to work the motors without a driver or shield. With a lot of trial and error and with the big help of MLH desk we were able to leverage transistors and other spare parts to get everything up andswimming.,We were the proudest the first time we were able to make this fish's motors to move,"A lot of hardware, Arduino's, circuits, communicating between hardware and software, API's, audio processing, and having other programs be able to interpret serial signals from the Arduino IDE code.","If we had more time, utilize on board audio processing to limit the use of API's, we would implement a implement a microphone so that you can directly talk with the fish, we would put a new speaker inside of the mount rather than relying on external Bluetooth speaker, and we would add motor shields and drivers so we could have better control and power distribution so that we could ideally also use the tail.","api, arduino, c++, esp8266, gemini, hardware, java, webserver",https://github.com/collin-b1/rev-uc-2025
RevolutionUC 2025,FinDaddy,https://devpost.com/software/findaddy,True,"Growing up, our parents played a huge role in managing finances—they set budgets, tracked expenses, and ensured financial stability. But as we step into independence, that structured guidance fades, leaving many struggling to navigate personal finance alone. Findaddy was built to bridge this gap, offering personalized financial insights, automated tracking, and real-time advice, just like the support system we had at home.","FinDaddy is your friendly, all-in-one budgeting parent designed to make managing money feel effortless and fun! With FinDaddy, you can quickly log your monthly income, set spending goals, and categorize every expense—from groceries to online shopping—all in one streamlined dashboard. Engaging visuals like bar charts and pie charts help you see exactly where your money goes, while FinDaddy’s playful yet practical advice keeps you on track to hit your financial targets. Whether you’re new to budgeting or a pro at crunching numbers, FinDaddy makes it simple to save more, spend smarter, and invest in the things you truly care about. This is your one-stop solution for financial clarity.","Python & CSS for core functionality and styling.
API Integrations to enhance automation and financial guidance.
Dash Library for interactive dashboards.
Plotly library for graphical representation.
Caffeinated Coding Sessions to bring it all together.","Front-End Complexity – Crafting a user-friendly and responsive interface.
Database Integration – Ensuring seamless interaction between front-end and MongoDB.
Bank Integration (Plaid API) – Overcoming cost barriers for real-time financial tracking.
Deployement- Domain.tech was not working","Built a fully functional financial assistant that provides real-time insights and advice.
Successfully integrated Gemini-API to enhance AI-driven financial recommendations.
Designed an interactive dashboard using Dash library for clear financial visualization.
Developed a smooth front-end by converting Figma designs into CSS.
Overcame accessibility challenges by running the project locally despite domain issues.","Throughout development, we explored:
API Integration & Automation – Leveraging Gemini-API 
Graphical Data Representation – Using Dash library for intuitive financial visuals.
Front-End Development – Converting Figma designs to CSS for a smooth user experience.
Database Management – Implementing MongoDB for secure financial data storage.
Python Proficiency","Expand banking integration for real-time tracking.
Integrate a receipt and invoice scanner","css, dash, figma, gemini, mongodb, plotly, python",https://github.com/SohumKanojia/RevUC_25.git
RevolutionUC 2025,RxRecruits,https://devpost.com/software/rxrecruits,True,"Clinical trials are crucial for medical advancements, yet patient recruitment remains a major challenge. RxRecruit was built to bridge the gap between doctors and patients, ensuring faster recruitment and real-time engagement in clinical research.","RxRecruit connects doctors with eligible patients for clinical trials, streamlining recruitment and medical logistics. It provides real-time chat for emergency assistance, appointment tracking, and medication management, making healthcare more efficient and accessible.","RxRecruit was built usingNext.jsfor the frontend,Node.jsandSocket.iofor real-time communication, andMySQLfor managing patient and doctor data. WebSockets enable instant messaging, while a dynamic dashboard enhances user experience.",One of the biggest challenges was integrating real-time chat while maintaining a smooth user experience. Ensuring accurate patient-to-doctor matching and handling dynamic updates efficiently also required optimization.,"We successfully implemented aseamless patient recruitment systemandlive chat for immediate doctor-patient interaction, which enhances engagement and trust. Our intuitive UI and smart filtering system make trial recruitment significantly easier.","We learned the importance ofefficient database design, handlingWebSockets for real-time communication, and creating aresponsive, user-friendly UIthat simplifies complex healthcare workflows.","We plan to integrateAI-driven patient matching, expand tomore clinical institutions, and implementautomated alertsfor trial updates. Addingvideo consultationandinsurance integrationare also on our roadmap to make RxRecruit an all-in-one healthcare platform.","express.js, javascript, nextjs, node.js",https://github.com/nishit2003/Hackathon_RevUC_2025
RevolutionUC 2025,MedVision,https://devpost.com/software/medvision-o397zh,True,"We were inspired by the complexity and inaccessibility of clinical trial participation. We wanted to create a solution that makes it easier for users to find, register, and engage in clinical trials with the help of technology.","MedVision is an AR-based app that allows users to find relevant clinical trials, register, receive real-time doctor instructions, and securely send health data to healthcare professionals, all in one seamless experience.","We built MedVision using Vision Pro AR for an immersive user interface, integrating Cloudflare AI for intelligent data processing, and Apple HealthKit to securely collect and share health data from users' devices.",We faced challenges integrating Cloudflare AI for real-time data processing and ensuring compatibility with Apple HealthKit while maintaining data security and smooth user interaction. Overcoming these technical obstacles took extensive testing and iteration.,"We're proud of building a functional AR experience that combines Cloudflare AI and Apple HealthKit, making clinical trials more accessible and simplifying communication between patients and doctors. We've also improved the accuracy of health data submission.","We learned how to integrate Cloudflare AI for enhanced data management, and how to work with Apple HealthKit to securely collect and share health data. We also gained experience with AR technology and its potential in healthcare.","Next, we plan to expand MedVision's features by incorporating more trial types, improving data analysis with AI, and exploring additional ways to enhance user experience and accessibility. We also aim to ensure compliance with healthcare regulations as we scale.","flask, python, swift",https://github.com/dezdoesit/MedVision
RevolutionUC 2025,ARise AR Flashcards,https://devpost.com/software/arise-ar-flashcards-kih1x6,True,"Our team comes from diverse backgrounds, but we share a common belief in the power of education. Aymaan has experience as a volunteer tutor, while Anuj has worked with non-profits supporting children. Through our experiences, we saw firsthand how many underprivileged children are eager to learn but lack the necessary resources. This realization inspired us to take action. Education is the greatest equalizer—it has the power to bridge the gap of inequality and uplift entire communities. We were driven by the humanitarian cause of making quality learning accessible to those who need it most. Our goal was to create a tool that could provide an engaging and interactive way for children to learn, regardless of their financial or social background.","ur project aims to provide an interactive learning tool for underprivileged children, helping them access quality education through augmented reality (AR). By using ARUCO markers, we allow children to view and interact with 3D models, making learning more engaging and dynamic. When a child scans an ARUCO marker with their device, the app detects the marker and overlays a 3D model on the screen, providing an immersive experience that helps visualize complex concepts in subjects like math, science, and more. This tool is designed to be simple and accessible, requiring only a basic smartphone or tablet, making it ideal for children who lack traditional learning resources.","During this project, we gained valuable insights, including working with OpenCV in Python, integrating contrib libraries, and developing a mobile application. We used JavaScript and Android Studio for the app, while for generating ARUCO markers, we combined NumPy with OpenCV to define an ARUCO marker dictionary. Aymaan created the 3D models using Blender, Anuj handled ARUCO marker generation and worked on the presentation, while Ethan and Neha developed the mobile application using JavaScript. Eventually, we all collaborated on refining the Python script to detect ARUCO markers and display our 3D models.","We faced several challenges throughout the project, especially with installing the correct libraries and ensuring the proper syntax. Both our Python and JavaScript code were met with countless errors, which slowed us down. However, we persevered, troubleshooting and refining our work until everything came together. Despite the technical hurdles, our dedication to the project and its cause kept us motivated, and we were determined to make it a success.",,,,"blender, java, python",
RevolutionUC 2025,SightSense,https://devpost.com/software/sightsense-r53gl1,True,"According to the International Agency for the Prevention of Blindness, as of 2020, approximately 1.1 billion people worldwide were living with vision loss. This includes 43 million individuals who are blind and 295 million with moderate to severe visual impairment.",Press the button on the side and the center camera takes a picture of whatever it's pointed at; the web app then reads Gemini's detailed description to you either out loud or through your headphones.,"Soldering, Hot glue, Gemini API, Python, React,","Soldering to a while, Hardware errors (there is now a constant red filter), Nicla Vision not having support for troubleshooting or for OpenMV Python.","Even with the red filter error, it can still take a picture and describe relatively quickly and accurately.",We learned a lot about how to build a hardware project which we had little prior experience in,"We hope to expand on the glasses and improve upon the camera quality, image detection and text detection.","gemini, python, react",https://github.com/muhlenlogan8/RevolutionUC2025
RevolutionUC 2025,GemiStudi,https://devpost.com/software/gemistudi,True,"As Computer Science students, we are constantly working with different languages. As we learn more through our classes and forget syntaxes, it can be really easy to turn to AI for help. However, leaning on AI for the sake of completing homework or relearning syntax can be more harmful than it is helpful. In order to aid the learning process, an AI bot needs to be properly prompted instead of giving you the answer from the start.","Our new extension does just this. By selecting code in your file and right-clicking, it prompts Gemini to explain the fundamental concepts to you. It also provides documentation for the code itself, that way users can go to the original source. There is also a compiler provided so the user can test what they are learning.","Using Visual Studio Code, we learned the basics of creating an extension for the app. We then reviewed documentation of typescript for front end and back end, and the latest version of the Gemini API.","As a team, we were all learning new things during this project. Whether it was Typescript, Gemini, or the different APIs that make GemiStudi, we all ran into unfamiliar errors that needed troubleshooting on the fly. We also encountered many issues when learning how to use GitHub, create branches, and push and pull from the right origins.","This is our first time creating an extension, working with typescript, and using AI in a project. We are proud of the rate that we learned the new tools and properly implemented our findings. We also used Figma to present our project-which was taught at a seminar during this Hackathon-and GitHub to have a smooth team workflow.","During this project, we learned how to utilize the Gemini API, organize our project on a GitHub repository, properly format a UI, and integrate the UI and backend with CSS and Typescript. We also used how to use Figma to create the final presentation.","As of now, GemiStudi only works for the latest version of Python. In later models, we would allow the extension to work across most widely-known languages. We would also like to create a better User Experience that allows a more personalized prompt to the Gemini bot, without allowing the user to cheat by asking for the answer directly.","full-stack, gemini, typescript, vscode",https://github.com/MaxwellABlevins/GemiStudi
RevolutionUC 2025,ClinSync,https://devpost.com/software/clinsync,True,"We all experienced COVID, anxiously awaiting a cure or vaccine. Despite being a global priority, development still took years to go from research to clinical trials and mass production. But what if AI could speed up this process? How many more lives could be saved? Recognizing this inefficiency, we created ClinSync to enhance trial participation and accelerate medical advancements.","ClinSync is an AI-powered technology that automates clinical trial recruiting by connecting participants to appropriate studies. Organizations such as pharmaceutical companies, hospitals, and research labs can post trial information, while participants can check their eligibility by entering health data in a variety of formats, including PDFs, pictures, and manual inputs. This data is processed by AI agents through extraction, eligibility matching, and report production, which provides volunteers with an accurate eligibility summary. If they are matched, users are directed to register and can contact an AI chatbot with trial-related questions. Organizations handle candidates using a dashboard, allowing for rapid approvals and follow-ups. ClinSync increases trial recruitment by automating screening and interaction, eliminating delays and delivering life-saving therapies to patients faster.","Frontend: React.js, Tailwind CSS, DaisyUI Backend: FastAPI, MongoDB AI Intergration: Gemini API, LangChain/LangGraph, ChromaDB (RAG pipeline, Multi AI Agents)","Confidentiality is a major concern in healthcare, and we aim to limit third-party intervention when managing sensitive information. Instead of directly managing personal data, ClinSync aims to optimize the application process by sending users’ summarized reports to organizations. Our purpose is to broaden the pool of potential applicants, offering companies a more diverse selection to evaluate while keeping volunteers’ personal information safe. While this may appear to be some minor tweaks, removing excessive wait periods greatly accelerates recruitment, and avoiding third party access to personal data allows clinical trials to know about their participants without getting their sensitive information.","Our integration of an AI agent into ClinSync to speed up the clinical trial matching process was one of our greatest successes. In order to create a system that speeds up and improves volunteer screening, we integrated data extraction, eligibility matching, and report generating. Despite working with new technology, we were able to integrate everything, improving accuracy and usability.",,"Our next step is to deploy ClinSync on a cloud platform, making it publicly accessible to organizations and volunteers. We will ensure a scalable backend that can handle multiple users efficiently while implementing strong security measures to protect data privacy and maintain compliance. To support a growing user base, we plan to optimize database performance for handling large volumes of clinical trial data, improve AI processing speed for real-time eligibility matching, and enhance the frontend for a smoother user experience. A key focus will be fine-tuning our AI models to improve accuracy in health data extraction and eligibility matching. By refining the RAG pipeline, we aim to enhance chatbot responses and provide clearer eligibility explanations. Additionally, we will train our AI agents with more diverse datasets and implement feedback loops to continuously improve their performance. Finally, we will conduct extensive bug fixes and user testing to refine ClinSync. By gathering feedback from organizations and volunteers, we can improve workflows, resolve any technical issues, and enhance the overall UI/UX for better navigation and interaction. Through these steps, we aim to create a fully functional, scalable, and user-friendly platform that accelerates clinical trial recruitment and drives real-world impact.","daisyui, fastapi, gemini, langchain, langgraph, mongodb, react, tailwind","https://github.com/hiepnguyenduc2005/RevUC, https://docs.google.com/presentation/d/19xR1A0T51wTvhrToDVAecIAuWnURyvhLoNz-larPRlY/edit"
RevolutionUC 2025,Rapid.Tech,https://devpost.com/software/rapid-tech,True,We were originally inspired by the research using slime molds to create optimal paths through cities. After some more ideating we expanded our idea to simulating what it would be like to add high speed trains between any city in the US.,"Rapid.Tech is a simulation tool with the. goal of allowing anyone to learn more about high speed rail in an interactive way.  Rapid.Tech Is built to allow users to input two different locations and get back the cost of the train being implemented, an optimal path for it to take between the two locations, and the time that it would take for trip between the two locations.","We used Googles API and WorldPop's Census data to figure out pathing, population data, and location information. As well as these we used apps like Replit to help get us started, allow for simultaneous programming, and help with coding problems.","We had many difficulties that we had to learn from throughout our 24 hours. At the start we barely understood what an API was, and had to get some help in understand how to use them. We also had to completely change our programming location after trying to upload a cache full of lots of census data caused replit to stop working. This eventually lead to most of our hopes for the project becoming much harder to reach than initially expected.","We are proud of our quick learning of topics such as APIs, and our ability to implement and use the different API tools to allow for Rapid.Tech to do much more than we originally expected. We are also proud to have learned more how to use AI not as a tool to answer questions for us, but allow us to improve and create things that would have taken much more time in the past.",,"We are hoping to fix our project and get it into a more working state. As well as this we want to host it on an actual website that will allow for other users to access Rapid.Tech. In the future we would like to expand Rapid.Tech to allow us to teach more people about high speed rail. This project was made by Isabelle Hageman, Evan O'Leary, and Reece Whitaker for the March 2025 Revolution UC. The project was initially worked on over a span of 24 consecutive hours with mentorship from upperclassmen and mentors. For this project, Google Dynamic Mapping API was implemented. The API was utilized using free and limited usage. The project also utilized machine learning to quicken the coding process (largely due to the 24 hour timer) and fill in gaps of our current knowledge. We'd like to thank KineticVision and Major League Hacking for their informational sessions on APIs and Machine Learning Usage during the Hackathon, as well as Revolution UC and all their other sponsors for hosting the event.","css, flask, google-directions, google-maps, html5, javascript, python",https://github.com/EvanOleary26/rapidtech.github.io
RevolutionUC 2025,PomoFlash,https://devpost.com/software/pomoflash,False,My college grades,Helps maintain focus and productivity,"Front end technologies (Next.js, react, TypeScript",Implementing all of our ideas,Our backend and the learning that we got done,I ended up expanding a lot more on front end and have expanded my understanding on a hackathons time duration,further development,"css, html, next.js, react, sql, typescript",https://pomoflash-prod.vercel.app/
RevolutionUC 2025,Pick playlist,https://devpost.com/software/pick-playlist,False," I watch a lot of MOOCs. Selecting a good MOOC, in a new subject area, is a hard problem. So I built this tool. Originally I made it to summarize playlists using video descriptions (video transcripts are long and I don't like to wait too much for the LLM response). Then I extended the idea to pull playlists from channels. It is not a highly polished project; but hopefully, it'll give you some inspiration to build something on your own. Prompt away! What's next for Pick playlist?
No big plans. But if you insist: I ran into several API quota limitation problems. Would like to explore options to make it all smooth.",,,,,,,"gemini, python",
RevolutionUC 2025,MedSage,https://devpost.com/software/medsage,False,I came up with this project idea when I brought my girlfriend to have a health check. I had driven 1 hour to take her there (a good place). I found it tired and wonder what if you miss an appointment. It leads to an idea of connecting patients and doctors.,"Basically, this app is planned to have 2 main functions. The first one is managing the data (information of patients, which can be enter by doctors after health check. The second one is chatting, which help the doctors connect to patients and patients and doctors. Doctors can give advices, pills information, book appointments.","Currently, we just finished one of those functions using HTML and CSS to build front-end and JavaScript (Node.js, ReactJS) to form the back-end.","The one that is most challenging is the time and our knowledge of programming. In addition, because our team initially just have 2 people, therefore, the brainstorming step became a little bit challenge for us.","The most outstanding one is how we manage the file/codes. From a beginner, we helped each other in team has the basis understanding of programming and how to actually work on a project.","Of course, advanced coding is one of them. In addition, we enhance our overall team-working, time management and project management skills.","As we told above, we gonna develop other features later.","css, html5, javascript",https://github.com/trnahnh/HealthCheck-Therapy
RevolutionUC 2025,Invent-ory,https://devpost.com/software/invent-ory,False,"This was inspired by a need to support the community and small businesses, specifically craft fair and food truck business. We wanted to build a cost-effective and portable tool for creators, artists, bakers, and cooks to help manage materials, costs, and sales for them.",Our system lets owners store: It uses this information to provide a dashboard with low stock alerts and pricing recommendations based on labor costs and selected profit margin.,"We built it on AWS with a RDS mysql database and EC2 server and a front end with javascript. 
The database has 3 main tables: customers, stocks, and materials, and 2 junction table to link all of the data.",,This was a challenging project because it was full stack. We are really proud that we were able to build out the database and we are really proud of our user interface.,We learned a lot about how much work goes into infructure setup. We also learned about how to build APIs and the different types of API types to use.,The second generation of our product would act as a personal assistant to business owners. We want to integration AI or smart analytics to make recommendations to owners about what stock to build based on material inventories or events scheduled.,"css, html, javascript, mysql, python",https://github.com/Trfitzy/RevUC
RevolutionUC 2025,Bankify,https://devpost.com/software/bankify,False,,,,,,,"Problem Statement
• What problem does this idea solve?
Many areas suffer from ATM unavailability, cash shortages, and high maintenance costs. Traditional banking infrastructure is slow and costly to expand, leaving users struggling with cash access.
• Who faces this issue, and why is it important?
o   People in rural/semi-urban areas with limited ATM access.
o   Urban users facing cash shortages due to ATM downtime.
o   Banks struggling with high ATM maintenance costs.
o   Small businesses seeking additional revenue streams.","kafka, mysql, react, redis, springboot","https://github.com/kumbamkarthik/bankify/tree/master/bankify, https://github.com/VamsiMakke87/Bankify"
RevolutionUC 2025,Sarcastic Bearcat,https://devpost.com/software/sarcastic-bearcat,False,"I was inspired by my love for quirky animal facts and a desire to create an AI chatbot with personality—unlike the usual bland bots. The binturong (bearcat), with its popcorn-like smell and sassy vibe, became the perfect muse for a snarky, entertaining assistant.",,,,,,,"fastapi, giphyapi, qwen, react, render, vercel",https://sarcastic-bearcat.vercel.app/
RevolutionUC 2025,Legit Company,https://devpost.com/software/legit-company,False,"   We've all been there before.  Getting off at work ready to be home, only for your daily commute to be met with long waits and angry drivers dragging you down.  The worst of it is always having it be unexpected.  Because of this, we decided to design a revolutionary web app that integrates AI with navigation.  Now, users are able to predict what are some possible traffic conditions on their commute home!",,,,,,,"auth0, css3, endpoint-exchange, flask, gemini-api, google-maps, html5, javascript, python, weather-api",https://github.com/RevUC-2025/TrafficMonitor
RevolutionUC 2025,Commit,https://devpost.com/software/commit-hdag7s,False,Passion in tackling problems that we encountered.,Motivate students to go classes more often,"Using HTML, CSS for front end, Figma for design, along with python, Javascript to call API for backend","Everyone have different specialities, so we have a difficult time linking our part together.",Idea was yeeted.,"We learn how to use certain API call to communicate with server, how to go through this as a team, despite our differences.",Commit (v).,"canvas-api, css, html, python",https://github.com/Thend911/RevUC-2025
RevolutionUC 2025,Feng's Way,https://devpost.com/software/feng-s-way,False,"One of our group members had a childhood friend whose family was from China. Every time our group member went to his friend's house, he felt welcomed, invited, and cozy, in a way that no place other than his own home made him feel. He eventually told his friend's mother this, and she explained that it was likely due to her implementation of the Chinese design philosophy of Feng Shui. Ever since then, he's wanted to learn more about the philosophy, but has had a hard time replicating that feeling at his friend's house all those years ago. To help with this, we decided to create Feng's Way, an app which allows for one to scan in the furniture in a room, and uses machine learning to suggest better arrangements of furniture according to Feng Shui.","Feng's Way is an app which allows for one to scan in the furniture in a room, using machine learning to suggest better arrangements of furniture according to the design principles of Feng Shui.","We built this by using object scanning, machine learning, and a lot of math!",We had a lot of trouble with implementing both scanning and understanding the math behind things such as reinforcement learning and the calculation of chi flow.,"Everything. Everyone on the team did a great job, and we're proud of everyone's work.","We learned a lot about machine learning, applying differential equations, and scanning in objects from the real world.","We plan to refactor the code, add room sharing, and add more support for different types of rooms. Keep an eye out for future updates to Feng's Way","python, swift",https://github.com/c-degni/fengsway
RevolutionUC 2025,MolCure,https://devpost.com/software/healthgenome,False,"AI isn’t just for software development—it can transform biology and chemistry too. Antibiotic resistance is a growing threat; once-treatable infections are becoming life-threatening due to resistant bacteria. We saw an opportunity to use AI to accelerate the discovery of new antibiotics and address this urgent need. Our goal was also to bridge the gap between AI and bioinformatics, showing how these tools can be accessible and impactful in drug discovery. AI-driven drug discovery can help identify new antibiotics to combat resistant bacteria.",,,,,,,"amazon-web-services, amplify, bedrock, biopython, genai, git, machine-learning, python, react, tailwind, typescript",https://github.com/joshuajerin/revolutionuc2025
RevolutionUC 2025,SwiftDrop,https://devpost.com/software/swiftdrop,False,"SwiftDrop was inspired by the need for a seamless and efficient delivery platform that provides real-time tracking and communication, similar to Uber. The idea stemmed from the challenges faced in traditional delivery systems, where tracking is often delayed, and communication between customers and delivery personnel is inefficient.","SwiftDrop enables users to place delivery requests, track their deliveries in real-time, and communicate directly with the assigned delivery personnel through an integrated chat system. It ensures a smooth, transparent, and efficient delivery experience.","We developed SwiftDrop usingReact.jsandTailwind CSSfor the frontend, ensuring a sleek and responsive UI. The backend is powered byFirebase, handling authentication, database management, and cloud functions.Google Maps APIwas integrated for real-time tracking, while WebSockets andFirestorewere used for real-time chat functionality. The platform is deployed onVercel, ensuring quick and efficient deployment.","One of the biggest challenges was ensuring real-time tracking updates and message synchronization between different users. Implementing smooth communication without latency required optimizing Firestore queries and handling WebSockets efficiently. Additionally, ensuring authentication security and making the UI responsive across different devices required careful optimization.","We successfully built a fully functional, real-time delivery tracking system and a chat feature that allows seamless communication between customers and delivery personnel. The smooth integration ofGoogle Maps APIfor real-time updates and the optimized UI for better user experience were significant milestones.","Throughout the development of SwiftDrop, we gained experience in integratingGoogle Maps API, handlingreal-time data synchronizationusing Firebase, optimizing database queries for performance, and improving UI responsiveness. We also learned how to troubleshoot real-time update issues and ensure a seamless user experience.","Moving forward, we plan to enhance SwiftDrop by introducingAI-based route optimization,multi-user group chat, andpayment gateway integrationfor seamless transactions. Additionally, we aim to improve security measures and introduce a rating system for better accountability.","cloud-functions, express.js, firebase-(authentication, firestore, google-maps, hosting), node.js, react-router, react.js, realtime-database, tailwind-css, vercel, websockets",https://famous-chebakia-be2433.netlify.app
RevolutionUC 2025,Outbreak_Nexus,https://devpost.com/software/outbreak_nexus,False,"Our epidemiology professor highlighted the devastating toll and alarming frequency of epidemic outbreaks. Researching further, we found that events like the Black Death, the 1918 flu pandemic, and COVID-19 claimed millions of lives, often driven by mutated or novel pathogens that take weeks to years for vaccine development. That’s why we created OUTBREAK_NEXUS—an app that tracks global outbreaks with real-time WHO, UN, and CDC data, detailing symptoms, treatments, and prevention. For government officials, it provides outbreak control drafts and cost estimates, empowering rapid, effective responses to save lives.","OUTBREAK_NEXUS not only provides you a map with the disease outbreaks across the world in 2024,2025 but also shows you the symptoms, treatments, and prevention as scraped from reliable sources like WHO,UN and CDC. You can view all of this as part of individual plan ( designed for public use). Currently, it shows the cases and death rates for 5 diseases. We plan to expand it.
For officials working to control these outbreaks, this app also provides a general plan to control/ mitigate the outbreak. There is an extensive formula consisting of 20+ parameters to provide the overall report. Most of the references are from:
1) Madhav, Nita, et al. ""Pandemics: risks, impacts, and mitigation."" Disease control priorities: improving health and reducing poverty. 3rd edition (2017).
2) Dasaklis, Thomas K., Costas P. Pappis, and Nikolaos P. Rachaniotis. ""Epidemics control and logistics operations: A review."" International Journal of Production Economics 139.2 (2012): 393-410.
3) Rachaniotis, Nikolaos P., Tom K. Dasaklis, and Costas P. Pappis. ""A deterministic resource scheduling model in epidemic control: A case study."" European Journal of Operational Research 216.1 (2012): 225-231. At the end, there is also a gemini-powered RAG chatbot that talks to you regarding recent disease outbreaks.","1) Collecting Symptoms/Treatment data: Collected 200 disease data from WHO. If user enter a disease that is not in the saved disease list, Gemini will provide the details of the disease.
Saab, Khaled, et al. ""Capabilities of gemini models in medicine."" arXiv preprint arXiv:2404.18416 (2024).
2) Collecting Cases/Death data: Collected data regarding cases and deaths of the population by 5 recent outbreaks in the USA. Collected data for all over the world. 
3) Calculating Ro value: This value shows the transmission of the disease. Needed for formula calculation. This is collected from several academic papers.
4) Generating Maps: Map is created with MapBox for the 5 diseases.
5) Report Generation: Using the 20+ parameters that we collected ourselves, this app analyses and prepares a repot for diseases.
6) RAG: Talk to the Gemini bot to learn more about the disease outbreaks happening in the world right now.","The frontend is developed using react and backend through Fastapi. Since it was first time working with these two configs, it is hard for use to deploy the website into the Web.
We only found population data for 5 diseases due to lack of time and extensive search period. We will get more data in the future to improve this.","Unique report generation technique for officers trying to control/ mitigate outbreaks.
Proper Data Collection/ Data Preprocessing for both maps and report.
We were able to reproduce formulas written in almost 5+ scientific papers for disease control.","Data Collection, cleaning and Merging from almost 15 sources.
Report generation( One of a kind, never seen anwhere)
Better understanding of disease outbreaks and the essential to control it.
Teamwork, Task Delegation and Leadership.","Collection of more diseases for even better output.
Automation in creation of report.
Consider even more factors for disease control prevention.","fastapi, mapbox, numpy, pandas, python, react",
RevolutionUC 2025,DoseWise,https://devpost.com/software/dosewise-woi94h,False,"The inspiration behindDoseWisestems from the challenges faced by individuals with Type 1 diabetes in managing their insulin dosages based on food intake. Observing how difficult it can be for people with diabetes to track their meals and calculate insulin requirements, I wanted to create a tool that simplifies this process. The project was also driven by a desire to have a positive impact on people’s health, particularly in managing fast food consumption, which is often unpredictable and hard to track.",,,,,,,"css, django, html5, javascript, onshape, python, sql",https://github.com/Viplav17/REVUC25.git
RevolutionUC 2025,HireFlow,https://devpost.com/software/hireflow-qog2u3,False,"HireFlow is inspired by the challenges of modern job hunting. We saw job seekers struggling to craft standout resumes and nail interviews, so we harnessed AI to simplify both. Our goal is to empower candidates with tailored resumes and realistic interview practice, ensuring they enter the job market with confidence and clarity.",,"Inspired by our experience as career guides at NKU, we understood firsthand what job seekers need. This insight drove us to build an AI career platform that truly supports and guides users on their professional journeys. We used our background as tech major and definitely some tech stacks like Next.js, mongodb, Gemini-2.0-pro-exe new model. shacdn ui , clerk authentication and many more.","API routing across multiple features, Gemini SDK initialization.",Made a strong base and collected multiple ideas/ feature to advance this project.,"New tools, speech-to- speech synthesis,  Modern UI libraries.","Fix backend API issues
Implement agentic job search
Add other interesting features","amazon-web-services, clerk, gemini, next.js, perplexity, shadcn",https://github.com/ak-pydev/interview-iq
RevolutionUC 2025,Team MedScribe,https://devpost.com/software/jgps,False,"We set out to make doctor-patient interactions in telehealth smoother and more efficient. One of the biggest challenges we saw was the difficulty of capturing and documenting important details from online consultations. To solve this, we designed an application that not only enables seamless video communication but also automates report generation. Today, we've turned our idea into reality with a Minimum Viable Product (MVP). Our MVP allows doctors to have live video consultations, generates real-time transcripts, and creates automated reports. This is a big step toward our goal of simplifying medical documentation, making doctors’ work easier, and improving the overall patient experience.",,,,,,,"amazon-web-services, aws-sdk-(boto3)-aws-services:-ec2-for-deployment-rds-for-database-and-iam-aws-transcribe-s3-for-storing-health-records-(blob-storage)-api:-gemini-api-for-llm-analysis-of-transcripts-and-reports-(generative-ai), boto3, css, css3, ec2, gemini, google-generativeai, iam, mongodb, ngrok, postman, python, rds, react, rest, rest-api-endpoints, s3, services, tailwindcss-back-end:-flask, transcribe, webrtc","https://github.com/Gowtham-369/MedScribe, https://drive.google.com/file/d/1scR29pIRW6jiTiMr9W-RWjM1GMPgu3FQ/view"
RevolutionUC 2025,Facebook Marketplace Cleanser,https://devpost.com/software/facebook-marketplace-cleanser,False,The inspiration was for this project was that the members in our group liked going on Facebook marketplace to find good deals but we didn't like dealing with all of the garbage and scam posts that litter the website.,The chrome add on scans the Facebook marketplace listing for terms that we have deemed to be affiliated with scams or low effort listings.,We built it with html and CSS for the visual add on part of the add on and then JavaScript for the functional part of the project.,"One of the challenges that we ran into was that some content would load in when the page was loaded, and some content was loaded when the user scrolled down to see more. We had to figure out a way to scan all the listings the user was going to see.","We are proud that we were able to get a working prototype done in 24 hours, in the middle of the hackathon we didn't think that we were going to be able to get done.",We learned a lot about web development,We still need to fix some of the bugs in the code right now and we also have some new ideas that we want to implement in the future.,"css, html, javascript",
RevolutionUC 2025,Know-How Hub,https://devpost.com/software/know-how-hub,False,"Name: Know-How Hub What our project is: Know-How Hub is a web app where people can explore new interests and learn different skills from each other while also teaching them skills or hobbies in return. Technology Stack Used: For the front end, we used HTML, and CSS in the form of Bootstrap, and Javascript. For the backend, we used Python. Setbacks Faced: We are not very used to programming in Python nor are we very familiar with how to use it. We figured that we would figure it out as we went and ended up spending an embarrassing amount of time trying to fix something that we didn't know how to fix, which was a bad move. What we are proud of: We are proud to have tried our best to make our project work, even though we continued to face setbacks to the very end. What we learned: We learned where we stand at the moment for our programming skills and also that we need to practice more and be better prepared for our next hackathon or project. What's next: We plan to get better at our programming skills and pursue this project and make it better than the vision that we had in mind when we started programming it.",,,,,,,"css, html, javascript, python",https://github.com/nashitakausar/KnowHowHub
RevolutionUC 2025,BullBear,https://devpost.com/software/bullbear,False,We all got interested in stocks and thought it would be an interesting topic,Skims articles using Gemini to estimate if the certain stock being talked about will go up or down,HTML and CSS for frontend JS and Python for backend Fire base for Single Sign on,Lots of syntax errors and other errors,Using Firebase for SSO,Overcome challenges using teamwork,Expanding on so we can collaborate with different articles to create a better AI that predicts stock prices.,"firebase, html2pdf, python",
RevolutionUC 2025,Forky – Your Open-Source AI Copilot!,https://devpost.com/software/forky-your-open-source-ai-copilot,False,"Forky was born during theRevolutionUC Hackathon, supported byGoogle Cloud's #VertexAISprintprogram. Our inspiration came from a common challenge in open source: the overwhelming complexity of joining new projects. Many developers want to contribute but get lost in: We wanted tomake open-source contribution more approachable and funby leveraging AI to:✅Help developers understand codebases quickly✅Identify beginner-friendly issues✅Provide intelligent suggestions for contributions✅Make the entire experience more interactive and less intimidating Forky is powered by acombination of cutting-edge AI techniquesdesigned to streamline open-source collaboration: 🔹RAG (Retrieval-Augmented Generation)– Enhances Forky’s ability to analyze repositories and extract the most relevant information for contributors.🔹AI Agents– Providereal-time PR insights, smart issue recommendations, and repository analysis.🔹GitHub API Integration– Fetches repository data, including issues, pull requests, and contributor activity.🔹Gemini 2.0– Enablesintelligent code explanationsand natural-language interaction with repositories.🔹Visualization Tools– Generatesinteractive diagramsto help contributors understand code structure.🔹Flask & Jinja2– Backend framework for handling AI requests and chat interactions. 🔹 UsesRAG (Retrieval-Augmented Generation)to understandrepository structure.🔹 Generatescomprehensive project summariesfor new contributors.🔹 Createsvisual diagramsof code architecture to simplify repo navigation. 🔹Automatically categorizes issues by difficulty(Beginner, Intermediate, Advanced).🔹Suggests beginner-friendly starting pointsfor new contributors.🔹Generates creative contribution ideasto help developers get involved. 🔹Real-time code explanations– Understand functions, dependencies, and logic instantly.🔹Implementation guidance– Get AI-driven suggestions for improving pull requests.🔹Instant PR insights– Forky reviews PRs and suggests improvements before submission. 1️⃣Handling large repositories efficiently– We optimized retrieval methods to extract key insights quickly.2️⃣Balancing AI accuracy and usability– We fine-tuned Forky's responses to provideprecise yet concise explanations.3️⃣Creating meaningful issue classifications– AI needed proper training to distinguish between ""easy"" and ""hard"" issues.4️⃣Ensuring real-time interactivity– We improved response times to make Forky feel more conversational and engaging. Throughout this journey, we gained valuable insights into:✅How AI can improve open-source onboarding and code comprehension.✅The importance of user experiencein AI-assisted development tools.✅Optimizing LLMs for structured repository analysis and interactive feedback. Most importantly, we learned thatopen-source contribution should be easy, accessible, and fun!🎉 🔹Expand Forky’s capabilitieswith AI-poweredcode refactoring suggestions.🔹Improve issue-matching algorithmsto help contributors find the perfect tasks.🔹Build a browser extensionfor inline GitHub assistance.🔹Launch Forky as a GitHub appto integrate directly with developer workflows. 🚀 Forky is here to make open-source collaboration smarter, faster, and more accessible. Join us in shaping the future of open-source development!",,,,,,,"fastapi, gcp, gemini, github, gitingest, jinja, posthog, pyviz, tailwind","https://forky-364607428894.us-central1.run.app, https://github.com/AdonaiVera/forky"
RevolutionUC 2025,Leo's Kitchen,https://devpost.com/software/leo-s-kitchen,False,"Our team was inspired by food, social media systems, and the mascot of Revolution UC 2025, Leo.","We created Leo’s Kitchen, a website that is like social media for sharing recipes. The user can access the website and create a profile, post and view the homepage of posted recipes, like and save recipes, access a chatbot, and view our about us page.","We created our code with python and used streamlit framework to format our site. We first created the homepage and then added the sidebar with 7 more pages: About, Chatbot, Login, Meal feed, Profile, Recipe details, and share your meal. We each individually developed each page with the respective different aspects and then combine all of the codes and debugged from there.","We ran into many challenges along the way, namely with creating and accessing a database. The database was what we spent the majority of the time trying to understand and debug, we struggled getting it to recognize and display pictures as well as collecting and properly storing data. We also ran into some struggles using new software like GitHub.",,We all gained more knowledge about coding from both one another and a plethora of YouTube tutorials. We learned some of sqllite3 and know a little more about databases. We all now know just how much time and effort goes into creating a website and have gained a newfound appreciation. We also got to experience working in a team and collaborating on a project that we are all proud of.,,"openaiapi, pandas, python, sqllite3, streamlit",https://github.com/alexisamlin/Leo-s-Kitchen-main-main
RevolutionUC 2025,Social Media Content Moderator,https://devpost.com/software/social-media-content-moderator,False,"The growing need for improved moderation in online communities served as the inspiration behind The Social Media Feed Moderator.
The rise of misinformation, harmful content, and the overwhelming workload on individual moderators highlighted the urgency for innovative solutions.
Harmful content such as hate speech, abuse, racism, and threats spread on social media, affecting users' mental well-being and safety.","With a strong commitment to transparency and fairness in moderation, we sought to harness advanced technologies like Language Learning Models (LLMs) to tackle these challenges effectively.
An AI-based content moderation system that flags offensive text, ensuring a safer online environment.",We built using Google Gemini API to analyze the user posts and categorize them,"We faced challenges in accurately evaluating the context of user posts using Gemini, as language understanding can be complex and nuanced.
Ensuring accurate AI classification
Avoiding false positives
Maintaining real-time response
Handling large volumes of content",We're proud to have built an app that not only allows users to save posts but also leverages the Gemini API to analyze content and flag potentially harmful or misleading information. This achievement represents a significant step toward enhancing moderation processes and ensuring a safer online environment.,"Throughout this journey, we gained valuable insights into content moderation, the capabilities of the Gemini API, and the integration of AI-driven analysis. Additionally, we learned the importance of balancing automation with human oversight to create an effective and ethical moderation system.","Implement real-time moderation for live streams
Improve AI accuracy with better training data
Expand to multi-language content analysis","gemini, mongodb, python, streamlit",https://github.com/sundhar2025/TrustFeed
RevolutionUC 2025,Ever-Mind,https://devpost.com/software/ever-mind,False,I was inspired by a Vietnam veteran who was affected by Agent Orange. This helped me to decided to build an application for people like him with Parkinsons,,"AWS and Javascript, HTML and CSS",AWS,It works,We need to learn CLI better,AWS integration and improvement of existing features.,"css, html, javascript",https://evermind.netlify.app/
RevolutionUC 2025,SustainGenius,https://devpost.com/software/rizz_up_my_social_score,False,"We always wanted to address the global issues of food wastage and sustainability. And at the same time, we wanted to implement our own NFT generation tool. Furthermore, we as a team, always looked forward to develop something substantial and useful. Finally, our efforts manifested in the form of SustainGenius.",SustainGenius has two powerful sections!,"We built SustainGenius using the Flask framework and the HTML-CSS-JS stack. The recipe generator uses Google's Gemini API to produce the recipe in a specified format. The NFT generator uses python libraries to identify and validate the trash images, uses 6 random pixels as coefficients in complex mathematical functions, and produces the unique images.","As a team, we ran into many challenges. We used git branching for the first time ever and messed up our repository over five times. In addition to this, we changed our topics twice since we couldn't use the technologies at hand to address the issues and had to abort two projects in the development phase. Lastly, we skipped sleep to work on our project and debug.","We are proud of numerous things! Firstly, we are confident with git as we can now confidently perform branching and can use GitHub collaboratively. Our second feat is the generation of NFTs using images and using pixels as coefficients in mathematical equations. Lastly, we are proud completing our project and moving on from two aborted ones. We spent 22 hours for the first time in a hackathon and are proud of having finished a project!","We learnt how to manage time and stress through RevolutionUC 2025. In addition, we learnt to use git collaboratively and uses branches in our repository. We also learnt how to stay focused and address our problems at hand practically and not frantically.","We look forward to implement the generation of credible NFTs, develop a marketplace for NFTs, and deploy SustainGenius online to help people make an impact and address sustainability and food waste.","css3, html5, javascript, jupyter, python",https://github.com/AdityaKulkarni2706/RevUC-Tool
RevolutionUC 2025,Beachies,https://devpost.com/software/beachies,False,,Beachies will teach kids about beach safety and saving the beaches through minigames and interactive experiences.,"We used C# scripts in Unity to build our projects. Each one of us took on programming a minigame on our own and then combined it all in the end. One of us found royalty-free/copyright free assets and another one made quite a few on the iPad. So, with a combination of a ton of facepalming and celebration, we made Beachies.",One of our biggest challenges was combining all our projects into one. We didn't realize that half of us were using a different version of Unity and it caused us to run into some issues when combining it all. Another challenge was the steep learning curve to using C# and Unity. It led to a ton of debugging and teamwork to get a line of code working.,Managing to make four different minigames in the twenty-four hour period was pretty impressive on our end given that we had little to no experience in Unity prior with a relatively bug-free code. We are also proud of the fact we made an interactive map with four finished minigames.,We learned quite a bit. We had to learn how to integrate multiple projects into one. We were forced to learn how to script with C# which a majority of us had little to no knowledge of. We learned how to debug as a team and walk through troubleshooting why certain features didn't work as intended. Consistency was a major thing we had to learn how to implement throughout all our individual projects for things such as the dialogue boxes and pause menus.,"In the future, we will add many features. Some of which would include a currency system and a beach house that you can upgrade. We would also polish up the art and animations a lot more. In the way future, Beachies will become a brand that can release educational materials such as books and shows teaching children of all ages how to be green and safe.","c#, procreate, unity",https://izy101-idk.itch.io/beachies
RevolutionUC 2025,replies.ai,https://devpost.com/software/replies-ai,False,"Communication has been an integral part of human survival for the longest time. This world completely changed with the introduction of a new tool called electronic mails. Humans had started working towards fast and efficient communication. Another step to this race has been the introduction of AI. Leveraging exactly this, we present replies.ai.","replies.ai is a powerful tool that accumulates all your unanswered emails at one place, not only that, it gives you personalized suggestions and draft reply templates. Now you can use our AI-powered suggestion and send it automatically, edit it yourself or regenerate a new template!","Our web application leverages the React framework supplemented by Google OAuth and Gmail APIs alongside Gemini APIs. When the user onboards using OAuth, their emails from the last 60 days are parsed through the LLM which picks the top 5 referencable emails and divides them between 3 formality categories, including Casual, Professional, and Formal. When the user opens an unanswered email on the application, the email's body is parsed through the LLM again to write a suggested reply based on the sentence structure and vocabulary referencing the previously sent emails.","Creating dynamic end-to-end projects comes with a variety of challenges and endless debugging. We are proud to say that our team handled bugs very efficiently and overcame challenges as a team in collaboration. One of the major challenges we ran into was with connecting OAuth authentication to Gmail authentication, as well as Firebase authentication. Initially, we researched dynamic and optimized alternatives, however, with the time constraint, we decided to remove the database functionality and store data on local servers to achieve our minimum viable product.","With the changing world and new technical introductions every day, we are proud to keep up with the artificial intelligence phase of things. We believe we did a great job leveraging large language models specifically Gemini APIs, and put it to a use that has very high social impact. Our product can potentially save multiple hours and make tedious jobs easier. While spending the last 24 hours together, we also got time to learn more about each other and also our own selves. We started to build a great team and started finding motivation and happiness by accomplishing short-term goals.","This weekend was not only an opportunity to improve on our soft skills but also gave a platform and a team to grow technically. The technology stacks that our team researched on and implemented are Gmail API, Firebase OAuth Authentication, Gemini API. Our team had very diverse technical backgrounds, which helped us share our experiences and increase our breadth of technical knowledge, as well as become better engineers.","replies.ai is an extremely powerful tool with very high potential to compete in the market. We aspire to scale this project and allow the user to train the model on their own by feeding it data in the form of articles, blogs, or manually written email templates, and fine tune the API calls for a better and more accurate response. We also rejected the idea of a chrome extension during the start of the project due to time constraints. We aim for an extension that pops up when the user opens their Gmail/Outlook and provides suggestions directly on the email provider's user interface. We also aspire to create our machine learning model and train it to better satisfy our need with increased efficiency and accuracy.","css, firebase, gemini, git, github, gmail, google-gmail-oauth, html, javascript, node.js, react",
RevolutionUC 2025,UniCircle,https://devpost.com/software/unicircle-x4gyi0,False,"As students, we noticed a gap in collaboration across different university departments. Many talented individuals struggle to find the right team members for their projects, whether in tech, arts, music, or research. UniCircle was born out of a need to connect students beyond their classrooms and help them build something great together.","UniCircle is a platform that enables students to find collaborators based on their skills, interests, and project needs. It helps students connect across universities, form teams, and work on startups, research, or creative projects efficiently.",,,Successfullylaunched a working prototypewith real-time updates and authentication.,,,"edge-functions, email/password-signup, frontend:-react-18-with-typescript, jwt, otp-verification-via-twilio-sendgrid-api, react-hook-form-+-zod, react-router, sendgrid, supabase-storage-authentication:-supabase-auth, tailwindcss, vite, zustand-backend:-supabase-(postgresql)-with-row-level-security-(rls)","http://startupunicircle.com, https://github.com/SaiRohithVennu/UniCircle"
RevolutionUC 2025,Bad USB Detector,https://devpost.com/software/bad-usb,False,"With the increasing risk of malware and malicious USB devices, we wanted to create a simple yet effective tool to scan and analyze USB files for potential threats. Our goal was to provide an extra layer of security using VirusTotal's API, helping users identify and remove dangerous files before they can cause harm.","Bad USB Detector scans files on a USB drive for known malware signatures. By calculating file hashes and checking them against VirusTotal’s database, the tool can quickly determine if a file is safe or potentially harmful. If a malicious file is detected, the user is given the option todelete it instantlywith a dedicateddelete button. If a file is unknown, it can be uploaded for further analysis. The tool also generates a QR code linking to the VirusTotal report for additional details.","We developed the application using Python and Tkinter for the graphical user interface. File scanning is performed by computing SHA-256 hashes and querying VirusTotal’s API. If a file is found to be malicious, adelete button appears, allowing the user to remove it immediately. We also integrated QR code generation to provide an easy way to access VirusTotal reports.",,,,,"bash, hashlib, linux, nx, python, qrcode, raspberry-pi, tkinter, virustotal",https://github.com/mPyle05/RevUC2025
RevolutionUC 2025,SatSignal Seeker,https://devpost.com/software/satsignal-seeker,False,We wanted something open source that wouldn’t cost a bunch of money and we wouldn’t have to tire our arms holding a beam antenna,Calculates your current position and the position of a satellite and uses the azimuth and elevation to point the directional antenna toward where the satellite is to in order for us communicate with it,"With some stepper motors, a raspberry pi, and a box of scraps",Getting one stepper motors to work at all,Getting both the stepper motors working,Don’t use a raspberry to power the external stepper motor driver board and to be more prepared,Finish the current prototype and refine the algorithm to track satellites,"i2c, python, raspberry-pi, stepper-motor",
RevolutionUC 2025,Modern Wellness Companion,https://devpost.com/software/modern-wellness-companion,False,"We were inspired by the increasing demand for tools that seamlessly integrate wellness into our daily digital routines. Observing the growing awareness of mental and physical health, our team envisioned a companion that not only reminds you to take care of yourself but also makes self-care fun and interactive. We wanted to build something we truly believed in and would use ourselves, which is the main reason we built this easy-to-use and integrate Chrome extension.","Wellness Companion is an innovative Chrome extension designed to integrate wellness into your daily routine seamlessly. It helps you monitor mood, track your finances and stay productive with a built-in Pomodoro timer—all while offering a unique plant care game that visually grows as you nurture it. With a user-friendly interface and smart notifications, Wellness Companion transforms everyday self-care into an enjoyable and interactive experience, nurturing your well-being, one click at a time.","We built Modern Wellness Companion using HTML, CSS, and JavaScript, leveraging the Chrome Extensions API for seamless integration into your browser. Our approach involved creating modular components for each feature, mood notifications, productivity timers, finance trackers, sticky notes, and gamified wellness challenges. We integrated dynamic UI elements to ensure a smooth and responsive user experience and maintained a clean and aesthetic look.","One of our primary challenges was ensuring smooth functionality across multiple integrated features while keeping the extension lightweight. Managing Chrome’s extension limitations, and optimizing the user interface for both performance and ease of use were significant hurdles.  We overcame these by iterative testing and refining our code based on user feedback. We learnt all of git right from the basics to foster efficient collaboration and teamwork, and make sure that everyone could contribute to the code and extension. We did a lot of learning and growing through this process, learnt how professional teams communicate and work together through Git, and also gained valuable communication skills.","We are proud of creating an extension that not only encourages better wellness practices but also makes the process enjoyable. Successfully merging productivity tools with engaging gamified elements, like our interactive plant care game, has been a key milestone. Our project demonstrates that technology can make self-care both effective and fun. We are first and foremost most proud of our product, the chrome extension. However, we are extremely proud of ourselves for achieving this milestone and being able to put out such a great product within 24 hours. We feel accomplished with what we are putting out and excited for our chrome extension to be used by hundreds of people and be seen by all the judges!","This weekend was an incredible learning experience as we dove into building a web Chrome extension from scratch. We enjoyed designing and implementing features like a Pomodoro timer, an interactive plant care game, financial tracker and other motivating features that promoted wellness, which really highlighted the potential of browser extensions to enhance daily productivity. Additionally, we refined my Git collaboration skills by working closely with our team—learning how to manage branches, handle pull requests, and resolve merge conflicts efficiently. The hackathon not only pushed our technical abilities with Chrome extension development but also strengthened our teamwork and version control expertise, making it a truly rewarding experience. It felt super rewarding working on something that can bring about social good and help battle mental health struggles. Our focus was building an accessible and easy solution that we would personally also use in our daily lives.","Looking ahead, we plan to expand the capabilities of Modern Wellness Companion by incorporating personalized wellness insights and advanced analytics. Future updates may include integration with wearable devices, additional gamified wellness challenges, and more customization options to help users track and improve their overall well-being. We hope to revolutionize the chrome extension space especially in the acessible mental health tools sector.","css, html, javascript",https://github.com/adhvika07/revolution-uc-chrome-extension
RevolutionUC 2025,PhysiCube,https://devpost.com/software/physicube,False,First we tried something else. Then it didn't work. So we brain stormed and one of them had a good idea.,Lets you see the forces acting on each axis of a cube in a 3D space.,We made it using Unity's XR toolkit,"This wasn't our first idea and a lot of time was put into the first one, resulting in a time crunch. We also couldn't get the physical hardware working because the Arduino wasn't recognizing the SDA and SCL lines. We ditched the hardware in favor of fake values to show off the capabilities.",Having a completed project and also getting AR to work in general is very cool!,"Making an AR app isn't the hardest thing in the world, unlike making a bot that makes up complete languages.","More physics like adding rotation, torque, showing the values of each force.","ar, c#, unity, visual-studio",https://github.com/BoomBoomMushroom/PhysiCube/blob/main/Builds/finished.apk
RevolutionUC 2025,Breakfast Bar,https://devpost.com/software/breakfast-bar,False,Struggles with balance in everyday life as an engineering student (school vs friends vs health vs...),"It provides a user-friendly interface dashboard that connects anything and everything you need in everyday life, while also being able to categorize those into the specific aspects of your life that you want to balance.","We utilized a React js base, using mainly Javascript to compose the source code. Within the javascript files, we build the website pages. AWS provided authentication, authorization, and hosting through Cognito and Amplify.","The first challenge we encountered was creating a database that would help us retain user information for their dashboard, this is probably because we were all new to creating databases on AWS and it turned out that we didn't have the necessary permissions to create an Aurora DSQL database. 
The next challenge was hosting it on the .tech domain, turns out that feature was broken, so we had to shift to AWS Amplify, we had no trouble hosting the webpage on AWS but we had some trouble re-routing the webpages.","Since it was a first proper hackathon for most of us, we were really proud of just getting a somewhat good product. We were able to build the application pretty close to what we envisioned in the beginning. 
Another thing is that we learned a lot of new stuff about different services involved in creating a web-application.","I would say we learned a lot about AWS services and how they can help streamline our processes. We also learned a lot about how APIs work, transfer data, and answer to requests.","Adding in AI-powered module to the website to empower users to compare their desired balance between their aspects and their actual balance that they input. Additionally, more customization options like user-customized styling for their dashboard, variable modules, and user-input fields for their desired websites.","css, html, javascript",https://master.d124y5ym8jm7vo.amplifyapp.com
RevolutionUC 2025,ByteBlox,https://devpost.com/software/byte-by-block,False,"Our first-year computer science course challenged us to create a project that would help everyone learn code. We settled on our idea since it is accessible for anyone due to the flutter framework being deployable to desktop, web, and mobile. It also is a scalable idea that could be expanded to include many more languages and to potentially become a long-term project and possibly even a business model.","It takes in files, and outputs them into different lessons the user can do. The lessons consist of blocks of code (only Python right now with a hope to expand).","We used Dart, a google programming language, with Flutter as our Framework.","None of us knew flutter coming into the hackathon, so we all had to learn and to figure out how to debug our many, many errors.",We build our app completely to the specifications we laid out for ourselves at the beginning of the hackathon.,We all learned the basics of flutter and dart.,We hope to expand the app to include user data and numerous other courses to introduce people of all backgrounds to programming in an easy and accessible way.,"dart, flutter",https://github.com/Logan-Heitkamp/ByteBlox
RevolutionUC 2025,Financial Future,https://devpost.com/software/financial-future,False,"I've made several google sheets in the past to help myself and friends budget, but keeping them up to date is a time consuming process, and fairly boring","Takes account statements to show in a user-friendly manner gross income, expenses, net income, and planning to roth contribution (all of which Fifth Third has access to)",First time using svelte,First time using svelte,Getting a working app,svelte,Make graphs that work with d3,svelte,"https://github.com/Jasmine-Mogadam/FifthThird-FinancialPlanner-RevUC2025, https://revuc2025.netlify.app"
RevolutionUC 2025,Secure Sight,https://devpost.com/software/secure-sight-pom2s4,False,"In this hackathon, we wanted to develop a solution that improves public and personal safety using AI. We are aware that increasing security concerns in public areas cause a lot of disturbance and we saw an opportunity to use real-time AI detection to help prevent threats. We all want to feel safe, and by using our AI system that quickly detects danger and alerts the proper people provides a strategic approach to security. Our SecureSight platform leverages AWS's powerful cloud infrastructure and AI services, particularly AWS Bedrock LLM, to create an integrated safety ecosystem. By combining real-time weapon detection, pose estimation for fall detection, smoke and fire monitoring, and incident reporting with instant Twilio alerts, we've built a comprehensive security solution that serves law enforcement, military defense, fire services, and first responders. The scalability and reliability of AWS EC2 and S3 services enabled us to build a robust platform capable of handling critical security operations in real-time. This project demonstrates how AWS's advanced AI capabilities can be harnessed to create practical, life-saving applications that contribute to our vision of a ""Threat and Theft Free World.""","We created Secure Sight, an AI platform that analyzes live video to identify potential threats and safety violations. The system detects dangerous objects and sends instant alerts to assigned contacts. That is, it can call for help even before a person realizes the danger or cannot do so themselves. We also implemented a fall detection system that identifies the pose of a person in a real-time video streams and automatically detect if the person is in the process of falling or has already fallen. The system will generate alerts along with supporting evidence to notify security admins. In addition to already mentioned threat detection, Secure Sight also includes fire and smoke detection, an AI-driven chatbot that gives information about the safety queries regarding specific area, and an incident form to report an suspicious activity or safety concerns in your area.","In this project we used an AI-driven object detection to process live video and identify potential threats with moderate accuracy. The backend was developed using Flask, while frontend was built with React. We leveraged multiple AWS services to create a robust and scalable solution: AWS S3 buckets for secure storage of surveillance footage and incident evidence, AWS EC2 instances to host our application and handle computational workloads, AWS Lambda functions to trigger real-time alerts based on detection events, and Amazon Bedrock to power our intelligent safety chatbot with advanced language capabilities. Twilio was integrated to send real-time notifications allowing relevant authorities to respond immediately to detected threats, falls, or fire hazards. For frontend dependency management, NPM ensured efficient package handling. Our architecture demonstrates the power of AWS's comprehensive cloud ecosystem, showing how services like EC2, S3, Lambda, and Bedrock can be orchestrated to create an end-to-end safety solution that processes video streams, analyzes content for threats, stores critical data, and communicates alerts—all within a seamless, responsive system.","Throughout the development of the SecureSight platform, we encountered several significant challenges. The integration of WhatsApp messaging via Twilio presented particular difficulties, including content type compatibility issues with uploaded media and WhatsApp Sandbox restrictions requiring recipient opt-in. We also faced API configuration complexities when connecting our React frontend with both Node.js and Python backend services, especially when transitioning from local development to production deployment on EC2. Cross-origin resource sharing (CORS) and proper handling of multimedia content in form submissions required careful implementation. Additionally, ensuring real-time geolocation functionality while maintaining user privacy presented both technical and ethical considerations. Despite these obstacles, we successfully implemented core features including the Travel Safety Assistant chatbot and the Incident Reporting system, creating a comprehensive security platform that effectively addresses travel safety concerns.","We are proud that we have successfully built a real-time AI-powered security system that works efficiently and provides instant alerts. The AI model performs effectively in identifying not only threats but also safety risks. The user interface is intuitive and ready to use, so users will be able to navigate through the system without any problems. We think it was a great accomplishment to develop and build a fully functional security system in a short period of time.","In this project we learned a lot about optimizing ML models for real time use and effectively managing cloud based storage. By combining our ML models (weapons detection, smoke and fire, and fall detection) we learned how to combine everything in one model without loosing performance and accuracy.","We see great potential when it comes to expanding our project to large scale environments such as schools, office buildings, and industrial sites. Additionally, this technology could be applied to Medical Imaging Analysis like advanced cell nuclei detection and segmentation or machine learning-based disease classification. Another option is to use it as an AI Wildlife Camera to identify and track species in real-time. To count populations and analyze movement patterns. Thus, this project has the potential to be adapted to various solutions across multiple industries.","amazon-web-services, aws-bedrock, aws-ec2, aws-lambda, flask, machine-learning, npm, python, react","https://secure-sight-revuc.vercel.app, https://github.com/Akash346/secure_sight_revuc"
RevolutionUC 2025,JobLens,https://devpost.com/software/joblens,False,"College students, especially UC students, or any job seekers have to keep track of their application for later update and contact. However, the process of inputting application information into Excel manually is really frustrating, our team has developed a web application to solve this problem.","The web application allows users to apply jobs on external board such as Handshake, LinkedIn, and our model will automatically update that application in Excel.","Express.js for backend and HTML, Tailwind CSS, JavaScript (used to fetch API)
MongoDB for user database
Microsoft Azure AD for OAuth login using Microsoft account, Google's Gemini API and Nodemailer
Power Automate for automatic update in Excel","Most of our challenges came from working with and setting up APIs. The first challenge we faced was how to utilize OAuth to let users log in using Microsoft account. Secondly, we had to figure out how to extract key data from user-input links to job postings that they applied to by calling Gemini API. Besides, we needed to find out how do we send those data to user's Microsoft email (which could be extracted from the user information as they log in with Microsoft credentials). We also faced difficulties working with new tool (Power Automate). But eventually, we all learned a lot.","We are proud that even though we did not have much experience at first, we tried our best to learn and finish the project together. And that we can contribute to make our life better.","Through this project, we learned how to use OAuth to let users log in with a third-party account (like Microsoft). We also got to discover how to effectively extract important data from website links to get key information that we want to look for. It's also valuable for us to get a better sense of calling third-party APIs while developing this project. Besides, we learned how to use Power Automate, and how to integrate data in Excel.",Keeping up with the purpose to make the campus life even more effective and easier. We are going to continue to develop this application to spread the convenience to more student so that they can save time in job searching and spend it to build up wonderful projects.,"azure, css, express.js, gemini, html, javascript, node.js, powerautomate",https://teams.microsoft.com/l/message/19:969e4f6bc5e14829b642fd4375a02782@thread.v2/1740929647839?context=%7B%22contextType%22%3A%22chat%22%7D
RevolutionUC 2025,Visual Black Jack Game,https://devpost.com/software/visual-black-jack-game,False,"Our inspiration came from Balatro, a unique deck-building poker game with an intriguing twist.","We aimed to create a Black Jack game with various options to give players an edge, where winning rewards you with exciting prizes but sadly didn't get to the rewards part due to timing.",We developed the core game logic in Python and utilized Pygame for the visual elements.,"We faced challenges in making the twist and managing the time constraints, as perfecting the game logic was time-consuming. Additionally, learning Pygame presented a learning curve.",We're proud of successfully implementing some visuals and effectively collaborating as a duo to distribute the workload evenly.,"We improved our graphic design skills by creating card designs and backgrounds using an editor; the visual game is stuff we all made(i.e the cards, background, buttons, etc). Additionally, we enhanced our game design abilities by developing a meaningful twist and challenge for the players. We also learned to work efficiently within a tight timeline, ensuring we included all crucial details in a short period.","We plan to continue this as a side project, aiming to develop a fully playable game. Achieving this would be a significant accomplishment for both of us and pretty cool if we could actually play it.","pygames, python",https://github.com/Nick-Rigg/Hackathon_project_spring_2025
RevolutionUC 2025,Emotional AI Chatbot,https://devpost.com/software/emotional-ai-chatbot,False,,,,,,,,"bert, css, elevenlabs, flask, gemini, html, javascript, python",https://github.com/Tinkershlok/Pathos-GPT.git
RevolutionUC 2025,Speak Sign,https://devpost.com/software/speak-sign,False,"The need for accessibility in digital communication has never been more critical. With remote work and virtual meetings becoming the norm, individuals who are deaf or hard of hearing face challenges in participating in meetings, especially when using sign language (ASL). Interpreters are often needed to bridge the gap, but finding one can be difficult, expensive, and time-consuming. The idea behind SignSpeak is to create a solution that helps ASL users communicate effectively in Zoom meetings by automatically translating their signs into English and announcing them aloud. By doing this, we can make virtual communication more inclusive for the deaf community.","SignSpeak is designed to recognize ASL signs and translate them into English, using voice output to announce what has been signed. The current functionality supports the translation of basic ASL alphabets, converting them into spoken English. As the project progresses, more words and full sentences will be recognized. The goal is to eventually incorporate the technology into Zoom meetings to offer a seamless, real-time ASL-to-English translation for participants, breaking down barriers in virtual communication.","SignSpeak relies on machine learning models that have been trained using large datasets of ASL images. The model recognizes the signs by analyzing the images captured from a webcam and then maps them to their corresponding English letters. For now, the system only handles basic ASL alphabets but has the potential for more advanced translation as we expand the dataset. We also use TensorFlow for the model’s training and inference, and integrate it with a Zoom widget that would eventually allow real-time translation within meetings.","One of the biggest challenges I faced was training the model with a large set of reference images. Since ASL is a visual language with many gestures that look similar, the model needs a substantial amount of data to differentiate between letters and words. Currently, I am using 100 reference images per letter, but some letters are difficult to distinguish due to their visual similarities. This creates the need for even larger datasets to improve the accuracy of the model. Additionally, running the model locally means that the system must handle large amounts of storage for image references. As the ASL library expands to cover more words, the storage requirements will increase significantly.","Despite these challenges, I was able to get the basic alphabet recognition up and running, with the system accurately translating ASL letters and speaking them aloud. The model is currently able to recognize individual letters, which is an exciting first step toward full ASL translation. This initial success shows the potential of the project to grow into a powerful tool for communication in virtual meetings.","Here's the revised version: Through this project, I learned image processing and gained an understanding of how it impacts the training of machine learning models. I also developed skills in working with webcam input and integrating models into applications.","While the current version only supports basic ASL alphabet recognition, the next step is to train the model to handle full sentences and a wider vocabulary. This would require collecting more data and refining the model to improve accuracy and differentiation between similar signs. Once the model is robust enough, I plan to implement it into a Zoom widget that would allow for real-time translation during meetings. The ultimate goal is to make SignSpeak a fully functional tool for ASL speakers to communicate seamlessly with hearing participants in virtual environments, making online meetings more inclusive for the deaf community. As I continue developing the project, I also plan to explore other features such as the ability to recognize and translate common ASL phrases and gestures beyond the alphabet, providing even more context for communication in meetings.","opencv, python, sklearn",
RevolutionUC 2025,"Feather | Ethical Investing, Made Easy",https://devpost.com/software/feather-ethical-investing-made-easy,False,"I became inspired to create Feather when I began trading stocks with some of my spare cash and getting involved in investment. I came to realize how many companies I see as unethical, and wanted to create a way for people like me to find investments they can beproudof.","Feather uses cutting-edge AI technology to enable socially conscious investors to both quickly research stocks and find stocks that align with their values. At current, the main two features of Feather are to analyze a list of stocks (given by the user), and suggest stocks to the user, both based off of a short, quick questionnaire designed to map the user's moral universe.","Feather originally began as a full-stack web application utilizing node.js and the Google Gemini API, but quickly changed into a static website implementing theDeepseek API. It was built from the ground up in HTML5, CSS, and Javascript.","As mentioned before, Feather began it's life as a node.js application, but it became clear within a few hours that the team (myself) lacked the skills to implement a full-stack web application. The choice at this point was to stick to our guns and try desperately to use Gemini (which required node.js,) or bite the bullet and use a different AI model. Ultimately, due to time constraints, the switch was made, which is why Feather is built on Deepseek.","Firstly, getting a project complete at all is something to celebrate! But aside from that, we're proud of getting an API to work, of making a functional AI app, of making something with the potential for genuine positive impact if developed upon, and most of all, we're proud of how much we accomplished with such a small team (1 person).","The team (again, me) learned a lot about API usage in the process of implementing the Deepseek API. I learned many functions of CSS and HTML in the process of making this web app as well. Additionally, despite ultimately abandoning the full-stack build of Feather, we learned a lot about how a frontend communicates with a backend, and vice-versa.","A feature we toyed around with including in the RevolutionUC build of Feather that we would love to include in the future is implementation with theRobinhoodAPI. We're not familiar with how extensive it is, so this might be a fool's errand, but we want a feature for investors to be able to import their own investment portfolios into Feather.  Other key features we plan on including are the ability to save a user profile and values (either via cookies or a proper backend) to expedite the process, and mobile support with a reactive design.","css, deepseek, html5, javascript, surge",http://selective-bomb.surge.sh
RevolutionUC 2025,WebAble,https://devpost.com/software/webable-pxjsz3,False,"The internet should be inclusive, yet many websites unknowingly create barriers for users with disabilities. Inspired by this idea, we set out to buildWebAble, a Chrome extension that not only identifies accessibility issues but also makes fixing them intuitive. By integrating AI, we aimed to provide deeper insights and actionable solutions, making web accessibility effortless for developers and designers.","WebAblescans web pages for accessibility issues and highlights problem areas in real time. It detects issues such as WCAG violations, missing or broken links, color contrast issues, and buttons with no text. Additionally, we leverage Google's Gemini API to generate a concise accessibility summary, providing users with a high-level overview of the site's accessibility score, key issues, and improvement recommendations. This AI-driven analysis helps developers prioritize fixes and gain a better understanding of their website’s accessibility compliance.","While buildingWebAble, we read through the WCAG documentation and designed a variety of flags to facilitate our extension. Inserting these many elements into a website can become performance-heavy very quickly, so we also had to be mindful of how efficient our code was to make the usage of our extension viable. For the front-end, we designed a sidebar that gives users an at-a-glance view of all the errors on their webpage, a description of all the various flags and what they mean, and an AI summary powered by Gemini of the website that provides meaningful recommendations rather than just issue detection.","Integrating theGemini APIfor the first time brought several challenges, from understanding how to structure API requests to ensuring that the responses were both relevant and actionable. Communicating between thebackend and frontendalso proved to be tricky, as we initially struggled with data flow and synchronization, leading to delays in rendering accessibility summaries. Additionally, we had to navigateChrome’s Content Security Policies (CSP), which restricted how we could load external scripts and handle API calls. To comply with these security policies, we had to refactor parts of our architecture, ensuring that all requests were properly routed through the extension’s background service while maintaining efficiency. Overcoming these hurdles made WebAble more robust and secure, ensuring seamless performance without violating browser security protocols.","We successfully built a lightweight yet powerful Chrome extension that combines real-time accessibility scanning with AI-powered summaries, making it easier for developers to understand and act on issues. Our intuitive UI visually marks accessibility violations and provides clear, step-by-step recommendations. By integrating AI,WebAblegoes beyond simple issue detection and offers meaningful insights, helping developers improve inclusivity in their designs.","Through buildingWebAble, we gained hands-on experience in developing Chrome extensions, from understanding how to interact with theChrome extension APIto optimizing real-time page analysis without affecting performance. We also learned how to effectively integratethe Gemini API, refining AI-generated insights to provide meaningful and actionable feedback. Additionally, diving deep intoweb accessibility standardsexposed us to many overlooked WCAG violations, such as improper ARIA attributes, focus order issues, and subtle color contrast problems that significantly impact users with disabilities. This project not only expanded our technical skills but also deepened our understanding of how small accessibility improvements can make a big difference in creating a more inclusive web.",Multi-browser support: Expanding to Firefox and Edge. Deeper AI integration: Improving the Gemini API’s recommendations by incorporating real-time accessibility data. Automated Fix Suggestions: AI-powered code snippets for quick issue resolution. Integration with DevTools: Embedding WebAble into Chrome DevTools for a seamless developer workflow. Custom Accessibility Reports: Exportable reports for teams to track and improve compliance over time.,"css3, figma, gemini, html5, javascript, jquery",https://github.com/Kaserollinss/RevUcExtension
RevolutionUC 2025,SmartTrack AI Health SOlutoin,https://devpost.com/software/smarttrack-ai-health-solutoin,False,"Improve Clinical Process for Diabetes & Blood Pressure Patients WHY:
To develop an AI-driven solution that enhances the clinical process by enabling early detection and proactive management of chronic kidney disease (CKD) in patients with diabetes and high blood pressure. The system will continuously analyze patient data, predict individual CKD risk, alert patients and clinicians, and provide personalized education and prevention strategies, helping to reduce late-stage kidney failure and improve patient outcomes. Learned:
SPRINT solutions are the best solutions for hackathons. Last-minute changes in implementations may or may not result in an intended outcome. Challenges:
It is difficult to interpret what another person wants. It is difficult to work on code as a singular person without an additional team member who can support and assist, likewise.",,,,,,,"python, scikit-learn, streamlit",https://github.com/dylandamiano/medpace-revuc-hack
RevolutionUC 2025,OptiLine,https://devpost.com/software/optiline,False,"The inspiration for OptiLine came from the frustration of long meal wait times during events like hackathons. Seeing everyone miss out on valuable networking and project time, we set out to create a solution that enhances dining efficiency and reduces queue frustration.","OptiLine optimizes meal service by dividing attendees into dynamically assigned wristband groups. Using data-driven strategies, it effectively staggers meal access to minimize wait times and crowd congestion, ensuring a streamlined dining experience for all.","We built OptiLine using Pyomo, a Python library for optimization. The model takes into account factors like service rates and student numbers to determine the optimal grouping and timing for meal service. We iterated on various group sizes to maintain linear constraints while achieving the best possible queue reduction.",One of the main challenges was ensuring our model remained linear to fit within the capabilities of available solvers. We also faced the complexity of balancing group sizes against service constraints to achieve the desired wait time reduction.,We are proud to have developed a functional model that reduces wait times through smart optimization. Our solution not only addresses a common event issue but also showcases the real-world application of operations research techniques.,"We learned the importance of iterative problem-solving and the intricacies of optimization modeling. The challenge highlighted how powerful data-driven optimization can be in creating practical, efficient solutions to everyday problems.","Looking ahead, we plan to enhance OptiLine by incorporating real-time data to adjust the model dynamically during events. We also aim to expand its application to other queue-heavy scenarios like conferences and festivals for a broader impact.","collab, ipynb, jupyter, optimize, py, pyomo, python, pytorch","https://colab.research.google.com/drive/1db-MA9nALUnsB45ExcHNNVbRUYofReIi#scrollTo=0cJK2lZWT2N7, https://uct.tiiny.site/"
RevolutionUC 2025,Click Mr. Whiskers,https://devpost.com/software/mrwhiskers-exe,False,Our inspiration for this game were popular idle clicker games such as _ Cookie Clicker _.,"This is a 2d point-and-click game where you can click Mr. Whiskers. Each time you do, you get a WHISKER! With the WHISKERS you can buy different buildings suck as a Cat Tree, Treats, and Toys!","Noah made the code of the game using Game Maker Studio 2, while the art by Faith used ReSprite for the iPad.","One huge challenge we ran into was idea making. We originally had another idea that we worked on for about 16 hours, but it ended up not being feasible to finish. So, we were able to move onto this.","Something that we are proud of was being able to pivot from the original idea very fast, and come up with a new idea without losing too much time.","Noah was able to learn even more about GameMaker, and will be taking much more experience onto his personal project. Faith was able to learn a lot about sprite making, and making it look incredible!","With more time given, Click Mr. Whiskers could get more buildings, one-time upgrades, click power, and so much more. The possibilities areendless!","gml, resprite","https://github.com/noahrider/clickwhisk, https://noahrider.com/retro/clickwhisk/play.html"
RevolutionUC 2025,CPASS: Cincinnati Public Accessibility - System Survey,https://devpost.com/software/cincinnati-public-accessibility-system-survey-cpass,False,"More than anything else, we were inspired by the results of Project Sidewalk. A initiative that has combined AI and public data to map accessibility of 1700 thousand kilometers of public infrastructure across 7 cities. Building on their work we chose to tackle a similar but novel problem. Utilizing two stages of vision AI: one to identify accessibility features and another to identify necessary infrastructure we extended previous work in the field to examine a unique combination of features supporting accessibility.",,,,,,,"amazon-web-services, api, artificial-intelligence, deep-learning, git, github, java, json, machine-learning, opencv, pandas, python, roboflow, vision-ai, yolo",https://prezi.com/v/view/NaKpxpNSEK6FC16iQ6v3/
RevolutionUC 2025,8-Bit-Beats,https://devpost.com/software/8-bit-beats,False,"8BitBeats is a terminal-based 8-bit music generator that allows users to create, customize, and replay chiptune-style music. The tool generates random, yet reproducible, tracks using a deterministic algorithm based on user-defined inputs. It also provides a simple way to manage and replay previously generated tracks via track keys. The development team for 8BitBeats are all passionate about music creation. Each of us play instruments in our free time in addition to being full time Computer Science students. This project is a way to unite our passions and create an exciting new application. To ensure reproducibility, each track's unique ID is derived from user inputs and a hash function. The process follows these steps: Note: these inputs can also just be random, to generate random songs completely Create a String Representation: Use the Track ID to Seed Music Generation: This project uses several Rust crates to handle music generation, playback, terminal UI, and data persistence. Below is a breakdown of the dependencies by their purpose. TUI Music Gen Data Passing (TUI inputs -> randomize -> Music Gen) Playing the Music [Stretch] Create a playlist",,,,,,,"crossterm, ratatui, rodio, rust, rust-music-theory",https://github.com/DerekCorniello/8BitBeats/tree/main
TreeHacks 2025,HawkWatch,https://devpost.com/software/hawkwatch,True,"In an era where security cameras are everywhere but meaningful surveillance is scarce, we saw an opportunity to transform passive recording systems into intelligent security guardians. Our inspiration came from real-world incidents where crucial moments were missed despite having camera coverage, and the overwhelming challenge security personnel face in monitoring multiple video feeds simultaneously. We wanted to create a solution that doesn't just record but understands, analyzes, and acts, whether it's for local businesses like grocery markets to bigger organizations like hospitals and shopping malls.","HawkWatch is an intelligent video surveillance platform that detects crime, suspicious activities and life threatening events such as fainting and choking and sends phone alerts to alert security of the issue. Our intelligent model generates time-stamped incident reports with video evidence. It has 4 main features:","Our tech stack combines modern tools for a robust, scalable solution:",,,,"Future enhancements we're planning: Our vision is to make HawkWatch the go-to platform for intelligent video surveillance, making security monitoring more efficient and effective for everyone.","chatgpt, gemini, mp4, nextjs, react, resend, supabase, tensorflow, typescript, vlm",https://github.com/Grace-Shao/Treehacks2025
TreeHacks 2025,OmNom,https://devpost.com/software/omnom-hg16v3,True,,,,"Motor controllers:
Our decision to use the roboclaw motor controllers and do a custom interface made our work quite hard - the USB interface made it a challenge to do simple things like set a power to a single motor. We had quite an ordeal just getting the motors spinning. GNSS Drift:
The RTK module we used advertised precision to within 1 foot, but we found it drifted quite a bit. This was one of the key things that made our navigation unreliable outside - preventing us from having a reliable demo which did the whole sequence of events. Power Management:
Making a custom 12 volt to 5 volt converter was a time sink which proved to be fruitless - since it was too inefficient it melted its own solder. We ended up having to switch to AA batteries for our 5V servo power after enough attempts to get it to work all off of our 12V batteries. Langchain DeepSort Depth Estimation","Taking on this project was extremely ambitions. Building a robot of this complexity, and programming it to effectively interface with the world in only 36 hours was an uncertain task, and at many points we weren't sure if we were even going to have a moving robot. The hardware. Learning new hardware platforms and tuning PID loops for drivetrain and stable movement of the pulley took an extensive amount of effort and time.
Coming up with creative solutions on the software to overcome hardware blockages was quite rewarding as well - we originally were planning to implement SLAM and generate a 3D visual ming of the environment but had to pivot our indoor navigation solution after realizing SLAM was not performant due to vibrations. After some research and talking with other hackers, we came up with a solution completely based on stereovision and depth perception.","Arjun:I learned about mechanical design in creating this type of robot. Lessons I learned include the age old lesson of measure twice cut once, as one wrong measurement in my CAD resulted in half an hour of rework on a metal plate. Nils:I learned a lot about distributed systems while integrating the different sensors together on the robot. I didn't have much experience in robotics before, so I also learned a lot about PID loops and the long and arduous process of tuning them. Robert:Coming in I wanted to do something blending both hardware and software for a more interactive experience. Only after our team spent the first 14 hours building the
robot did I realize the vast challenges of learning new frameworks and integrating different hardware together. While hardware definitely needs to be quality and
tuned for effective use, I see significantly more impact and potential of software enabling hardware than I did before. TreeHacks this year has inspired me to pursue
more experiences across both hardware and software to build incredible software that has real impact through effective hardware. Johnathan:I learned a lot about the mechanical side this year, which I didn't expect, especially coming in as a software person. Working on the web app taught me a lot about websockets and RESTful APIs. Finally, designing the ipad interaction state machine was a good lesson on thinking step-by-step, as well as implementing python image libraries.","OmNom is just getting started. We were focused on food delivery during TreeHacks - trying to solve a relatable problem for Stanford students. This technology has applications beyond just food delivery for students - most directly connected is delivery for peaople who can't go to restauraunts, but the technology test bed and algorithms we've implemented will work for other applications that require a physical presence - being able to interface with the world almost as well as a human. In the future, OmNom will have improved hardware for more diverse interactions and much better navigation capabilities. By reinforcing
our linear rail system and building a stable vertical gain tree, we will install an IMU and LiDAR enabling SLAM 3D mapping and navigating novel environments. We also 
would like to work more with VLMs and vision models to generate robot navigational instructions based on user navigation prompt and retrieval of destination floorplan
images. We would also combine tracking, recognition, and localization and mapping to map objects to specific locations within buildings for detailed object retrieval 
and interaction. Say you forgot your charger in a classroom, OmNom could remember where you left it and retrieve it for you, then deliver a Valentine's message on its
way back to you.","arduino, clip, dpt, fast-api, google-maps-places-api, jetson-nano-c++, lasercutting, matplot-lib-v0.dev, metal, next.js, open-cv, open-route-service, openai-api, python, react, roboclaw, soldering, tailwindcss, typescript, vercel, vit, vite.js, websockets-api","https://github.com/NilsIrl/ActuallyAHuman/tree/main, https://om-nom-ordering.vercel.app/"
TreeHacks 2025,Hearti,https://devpost.com/software/hearti,True,"Congenital heart disease (CHD) is the most common birth defect in the United States, and thanks to advances in medicine, more children with CHD are surviving into adulthood. But with this improved survival comes a lifelong need for careful monitoring - as symptoms can change over time. Recognizing this challenge, we set out to build an AI-powered web application that helps both doctors and patients navigate the complexities of CHD. Our vision is simple: provide a tool that offers clear, reliable insights for global healthcare professionals and their patients while ensuring that patients receive a smooth transition into their medical journey.",,Surface Intensity and Segmented Heart Models,"While the dataset we found online was extremely useful and new, it was very limited (59 hearts only). To train a truly accurate model, we need a much larger sample size.","-We successfully built and rendered 3D representations of 2D heart CMRI scans. Besides looking extremely cool,it is both new and valuable for the ways in which we treat people with CHD.
-Optimized the models to prioritize surface pixels, smooth representations, and segment-specific viewsto make them more accessible across devices and browsers.
-We built a web-based AI agent that can not onlypredict a user’s CHD conditionsbut can also act as asource of medical information for decades. If inputted with enough patient data, it can even more effectively provide accurate information that doctors can approve.",,,"elevenlabs, fastapi, lumaai, meta-quest, next.js, numpy, nvidia, plotly, python, pytorch, react.js, tailwindcss, typescript, vespaai","https://heart-ai-seven.vercel.app, https://github.com/ethan-tam33/heartAI"
TreeHacks 2025,Edith,https://devpost.com/software/edith-bvh014,True," With our software stack, we introduce a powerful any-3D-object segmentation architecture backed by several SoTA machine learning models. Many object segmentation models are trained on highly specific datasets such as COCO and hence lack generalizability, whereas VLMs like ChatGPT-4o lack a rich understanding of the 3D world. By parallelizing AI inference on 4 NVIDIA A100s, we computed and streamed accurate, realtime depth maps, segmentation maps, and point clouds to contextualize the richness of the 3D world for those visually impaired. With the live Gemini multimodal API, we prompt Gemini with the object heading and depth, from which it is able to obtain the precise location of any desired object from where the user is. The low latency communication between devices was a huge part of getting this to work in essentially real time. Our initial networking setup was built from a series of complicated websocket servers and clients on our various devices, including a Raspberry Pi, our laptops, and an NVIDIA A100 server. This setup enabled more granular communication and flow of data between devices, allowing us to pursue more ambitious ideas with our glasses. However, we ultimately decided on a much simpler architecture involving porting forward files and video streams across the network and simple HTTPs API routes. On the embedded software side, we wrote several scripts to program LEDs, and vibration motors and feed the camera stream to other devices. We programmed an IMU on the glasses to calculate the yaw, pitch, and roll to detect when someone has fallen. To elevate the user’s sense of spatial awareness, by coordinating our IMU with the vibration motors, we programmed a haptic feedback algorithm using complementary filtering to get the motors to vibrate with a decaying amplitude the closer they approach the desired object of interest, indicating to them when they are facing the object. To address the complex challenges faced by visually impaired individuals in navigating their environments safely and autonomously, we developed Edith—an actionable, adaptable, accessible assistant translating the visually rich world into forms that the blind can experience. Edith facilitates spatial awareness and object recognition, enabling users to interact with their surroundings with greater ease and confidence. Given the system’s technical demands, including real-time object detection, haptic feedback, and low-latency data processing, a robust hardware framework was imperative. The decision to implement a smart glasses-based system, rather than handheld or mobile alternatives, was driven by the need for an intuitive, ergonomic, and non-intrusive user experience. Smart glasses provide a seamless integration into daily life, avoiding the conspicuousness and inconvenience of handheld devices such as smartphones or external cameras. Our primary engineering objective was to optimize the functionality and efficiency of the system while maintaining a lightweight and comfortable form factor. The hardware architecture centers around a Raspberry Pi Zero 2W, supplemented by an inertial measurement unit (IMU) for motion tracking, a high-resolution camera for real-time video processing, and haptic feedback motors to provide tactile sensory input. Additionally, an LED strip was incorporated to enhance night-time usability by acting as an auxiliary illumination source. To ensure prolonged operation, the glasses integrate lithium-polymer (Li-Po) batteries, managed by step-up voltage boost converters to maintain stable power delivery across all components.
Modular Framework and Material Selection
To facilitate rapid prototyping, adaptability, and scalability, we employed a modular design paradigm for the hardware assembly. The structural framework consists of 3D-printed components, allowing iterative design refinements and seamless modifications. The integration of heat-treated brass elements enhances durability and structural integrity, ensuring the device remains lightweight while offering sufficient rigidity to support embedded electronics. The modular design also enables the repositioning of different hardware modules, allowing iterative optimization of weight distribution and system balance. Through multiple design iterations, we identified the optimal layout for component placement, ensuring both ergonomic comfort and efficient power management. In addition to the CAD models used to develop and implement Edith, custom-built PCBs were designed to support the embedded hardware. Using perf boards, we developed small, compact tactile circuit boards that effectively integrate haptic feedback motors while efficiently utilizing the limited space available within our compact glasses. An example of the PCB we designed and implemented is presented below. While similar designs exist, our approach is particularly exciting as it was developed to be as small and compact as possible using only resources available at a university hackathon or makerspace. We entered this competition as four driven individuals with one goal in mind: to walk out with the greatest creation we could build in 36 hours. But beyond the project itself, what made this experience truly special was being surrounded by our closest friends—watching each other work with enthusiasm, dedication, and relentless determination was nothing short of inspiring.
For 36 hours, we were confined within the walls of the David Packard Electrical Engineering Building, yet in that space, we found countless moments of growth. Each of us came into this hackathon with different backgrounds—whether in embedded systems, cutting-edge machine learning, or CAD design—but together, we played to each other’s strengths and, more importantly, pushed ourselves to learn.
The challenges we faced were often outside our comfort zones, leaving us no choice but to adapt and grow. From interfacing with a Raspberry Pi for the first time to soldering custom PCBs on a perf board, every obstacle became an opportunity to expand our skill sets. By the end, each of us walked away not only with new technical expertise but also with memories that will last a lifetime.
Oh, and about five empty Monster cans each!",,,,,,,"adafruit, gemini, numpy, pytorch, raspberry-pi, react, tailwind, typescript",https://github.com/jotalis/Edith
TreeHacks 2025,BrailleBot: Make Every Document Accessible!,https://devpost.com/software/readable-ai-braille-printer,True,,,"Hardware:
The “punch” mechanism is comprised of a series of 3D printed parts and powered by the printer’s extruder motor, making the modification incredibly fast and inexpensive. A snap action mechanism converts a quarter rotation of the extruder motor into a load and then rapid punch motion. A screw at the top allows the user to adjust the spring’s compression, thereby changing the depth of the punch. The stock extruder can create up to 10 punches a second. Software
On the backend, we leverage the Groq inference engine and Zoom API, using Llama3-8B and Llama3-11B Vision to transcribe documents, generate text descriptions of images, and summarize Zoom lecture transcripts with low latency. Then, we use our Python Flask backend to convert the ASCII PDF representations into braille letters and GCode instructions for the Marlin firmware (used by most 3D printers).
Commands are sent to the printer through USB serial.
We use a JS canvas to display a live realtime preview of the print.
The frontend is built with Next.js, Typescript, Chakra, and React.","Finding the right combination of punch geometry, travel distance, and impact material was a challenge on both the software and hardware side because of the limited resources on hand and criteria that the product be cheap and easy to retrofit. We tried a variety of foams, cloths, and papers, eventually landing on a cork sheet which can be purchased at hobby stores for a few dollars. This had the right amount of compliance to make a clear mark without punching through the paper. We also created a mechanism to iteratively test spring compressions and travel distances by having an adjustable spring and head offset, allowing us to efficiently explore a large design space. We 3D-printed multiple punch heads to ensure the best tactile experience.",,,"We plan to open-source our work, so anyone seeking to build a braille printer can do so at low cost. We will include detailed documentation, files, and backend code logic, 3D printed prototypes, such that anyone can replicate our build.","cad, chakra, claude, groq, llama, nextjs, openai, python, typescript, vercel",https://github.com/cathyzbn/braille-printer
TreeHacks 2025,BlinkAI,https://devpost.com/software/blinkai,True,"For millions of people living with Parkinson’s, ALS, and other mobility challenges, something as simple as sending a message can feel overwhelming. Most digital assistants rely on voice or typing, but what if those barriers didn’t exist at all? That’s why we created Blink AI—not just another assistant, but an intelligent system powered by a groundbreaking translation algorithm that converts blinks into text, opening a world of possibilities for those who need it most. Because communication should never be limited by physical ability. Everyone deserves a way to express their thoughts, ask questions, and stay connected—and now, they can do it with just a blink.","Blink AI detects intentional blinks, measures their duration and sequence, translates them into Morse code, converts that Morse code into text, and then sends the final text to an AI assistant to generate a response. Blink AI doesn’t just detect blinks—it translates them into a language AI can understand, creating a seamless bridge between human expression and technology. No voice. No keyboard. Just a blink.",,,,,,"css, flask, html, mediapipe, opencv, python, react",https://github.com/FuzionEcho/BlinkAI
TreeHacks 2025,Iris,https://devpost.com/software/iris-vlnzp0,True,"Headset Banner Headset Demo Have you ever wondered how blind people cross the road? How imminent hazards like a ""wet floor sign"" or a pothole to their right might inflict harm? Here are some scary statistics:  This hackathon we decided to build Iris. Iris is a sleek headset built for the blind and visually impaired, designed to provide real-time guidance through the world. By attaching an iPhone as a camera to an affordable 3D-printed headset, users can navigate their surroundings with confidence. Iris doesn’t just describe the environment – it actively guides the wearer through A.I. agent specialists. Using real-time models such as captioning, classification, image segmentation agents, and depth estimation agents, Iris detects hazards in under 100 milliseconds. When a danger is identified, it instantly provides a haptic buzz, signaling the user of danger. From there, Iris delivers step-by-step spoken instructions via TTS, directing the wearer on how to safely navigate around obstacles, cross streets, or reorient themselves in complex environments. Taking a VLM model, we were able to tune it for real-time workloads via aggressive PyTorch compiler optimization, cudNN utilization, specific matmul precision, and usage of optimal attention kernels like flash / paged attention. To achieve high network throughput, we developed a custom userspace protocol to stream tokens to clients, and to reduce agent latency, we quantized models to architecture-specific dtypes. Built using Expo, the mobile app is compatible on both Android and iOS devices. Iris is also able to take STT commands and decode it into agentic workflows such as hazard detection, scene similarity scoring, and object detection (via Groq LPU APIs) which are carried out in real-time. Want to know when the pedestrian sign turns white for you to walk across? Want to know where your water bottle is? Iris has your back. Even the slightest delay in detecting a hazard could mean the difference between safe navigation and serious injury for a blind user. Latency was a critical factor, so we spent a significant amount of time tuning our models to minimize Time to Detection (what we call TTD). We implemented attention optimizations, leveraged cuDNN for accelerated inference, and fine-tuned every aspect of our pipeline. But no amount of optimization could make up for the fact that we were hitting a hard wall – our compute resources simply weren’t enough. That’s when we reached out to Brev.dev, which gave us access to the compute power we needed to push our models further, drastically improving both training and inference speed. Compute wasn’t our only bottleneck. Network bandwidth became a serious constraint. Our initial protocol relied on JSON, which, while easy to work with, was bloated – filled with unnecessary delimiters and whitespace that slowed down transmission. Every millisecond mattered. To eliminate this overhead, we designed a custom datagram protocol, stripping out redundant characters and ensuring that every byte carried meaningful information. The result was a lean, high-speed data stream that helped us shave off critical delays. Finally, there was the challenge of hardware. One of our core goals was affordability – if our solution wasn’t accessible to blind users, it wasn’t a solution at all. Off-the-shelf headsets were too expensive, so we designed our own. We developed a custom STL model for 3D printing a headset that could securely hold an iPhone, ensuring a low-cost but effective hardware setup. This way, users could leverage the sensors and processing power of their existing devices without needing expensive proprietary hardware. Iris started as a simple idea: an intelligent headset that could actively guide blind users through the world. But for us, it was also personal. One of our team members has a blind relative, and hearing firsthand about the daily challenges of navigating city streets without reliable assistance pushed us to make Iris as fast and reliable as possible. What we ended up with was a fully functional, multimodal A.I. system capable of real-time hazard detection and step-by-step navigation. Here are some of Iris' capabilties: This project pushed us deep into the weeds of HPC optimization, hardware, and networking in exciting ways. One of the biggest lessons came from wrestling with the PyTorch compiler. We thought we understood it – until we started hitting obscure bottlenecks that weren’t obvious at first glance. Some layers weren’t fusing correctly, some optimizations were actually slowing things down, and cuDNN didn’t always behave the way we assumed it would. We spent hours profiling CUDA graphs, attempting optimal Tensor core and register usage, tweaking VRAM memory layouts, and rewriting parts of our model just to squeeze out those last few milliseconds. On the hardware side, we learned that designing an STL file isn’t just about making a model that looks right – it has to print right. Our first 3D print warped mid-print. Through trial and error, we figured out how to design for material constraints, adjust print settings for strength, and optimize for minimal waste. Eventually, we had a sleek, durable headset that could securely hold an iPhone – something that felt incredibly rewarding after our early failures. Iris has proven its ability to provide real-time hazard detection and navigation, but we’re just getting started. Here’s what’s next:",,,,,,,"cuda, expo.io, groq, nvidia, python, pytorch",https://github.com/RoyceAroc/iris
TreeHacks 2025,SurviveX,https://devpost.com/software/survivex,True,,,,,,,,"executetorch, finetuning, github, llama, mixedreality, nvidia-brev, python, pytorch, swift, torchtune, vision, visionos, visionpro, vscode, xcode","https://github.com/rockuutree/SurviveX-Treehacks/tree/main, https://drive.google.com/drive/folders/1Pntxog05MBqzbzW5uoAEGDyOuOXghBv9?usp=sharing"
TreeHacks 2025,PUSHPA - Pyro UAV System for High-risk Patrol and Alerting,https://devpost.com/software/pushpa-pyro-uav-system-for-high-risk-patrol-and-alerting,True," Firefighters operate inresource-constrained, high-stakes environments where reliable connectivity can be scarce. In many disaster zones, as seen in the recent LA fire,Wi-Fi and cellular networks are often unavailable. Despite these challenges, these brave men and women stand associety’s protectors, risking their lives to save others and preserve our communities. In the U.S. alone, there were an estimated 374,300 residential building fires in 2022, leading to 2,720 deaths and \$10.82 billion in property loss. Wildfires can be equally catastrophic: in 2024, over 64,000 wildfires burned ~8.92 million acres, with human activity responsible for about 85% of them. Having grown up in California, our team has witnessed firsthand the intensity of wildfires and the need foron-device intelligenceanddrone reconnaissance for robust backend support. Our goal withPUSHPA: Pyro UAV System for High-risk Patrol and Alertingis to empower these everyday heroes by providing them withAI-driven, on-site decision supporteven whenno Wi-Fi or cloud connectivityis available. PUSHPA(PredictiveUAV andSmartHeadset forProactiveAssistance) is a multi-component system thatintegrates autonomous drones,wearable edge computing(via Meta Glasses), and anAI agentic layer(LangChain): Wearable Meta Glasses Autonomous Drone Integration Agentic AI (LangChain) On-Device Edge Compute Backend analytics for reconnaissance Overall, PUSHPAautomatesmany of the manual processes frontline responders face, so they can focus on saving lives and containing the emergency. Hardware / Drone + Glasses Hacking On-Device AI and Edge Compute 20x Optimized Convolution Pipeline Agentic AI with LangChain Front-End and UI No Official SDKs Android Restrictions Model Integration Security & Robustness Real-Time Edge InferenceAchieving near-zero-latency AR overlays and object detectionwithoutrelying on Wi-Fi or external cloud APIs. Fused Winograd and 20x SpeedupDemonstrating a robust on-device CNN pipeline tailored for local hardware, critical for disaster scenarios with minimal connectivity. Unified UI + Synthetic TestingWe designed an intuitive interface and used Gemini to generate synthetic data for validating our system under pseudo-wildfire conditions. Agentic AI OrchestrationOur LangChain-based system seamlessly coordinates instructions—from “count forested areas” to “locate potential hazards”—across drones, glasses, and the user’s smartphone or browser. Edge AI is CrucialIn many disasters, Wi-Fi is completely absent. On-device inference ensures continuous operation and real-time insights. Workarounds Are Sometimes InevitableOfficial SDKs might be nonexistent. We learned to mirror video, hook into hidden AR pipelines, and build custom libraries for local inference. Complex Systems Demand Modular DesignBy keeping subsystems (drone logic, glasses inference, AI agent, UI) separate, we could iterate quickly without breaking the entire platform. Prompt Security & Input ValidationLarge Language Models can be manipulated by malicious prompts. We employed strict input checks and user authentication to thwart unauthorized access. Multi-User AR CollaborationLet entire squads of firefighters share real-time overlays—e.g., each vantage point combined into a global map. Advanced Predictive AnalyticsExtend our system to forecast fire spread, structural collapse, or toxic gas pockets—again without needing a live internet connection. Further Hardware CompatibilityOfficially integrate with more drone manufacturers and AR devices, removing the need for “hacks.” Live Field TrialsWe plan to partner with fire departments and emergency services to test in real-world conditions, collect feedback, and refine. Industrial and Agricultural ExtensionsBeyond firefighting, we can adapt the same architecture for warehouse inventory scanning, industrial inspections, or crop health mapping—particularly in remote areas lacking internet. Ultimately,PUSHPAkeeps first responders safer and better-informed, combiningdrones, edge compute, and a powerful AI agentinto a cohesive system that saves time, protects property, and—most importantly—helps save lives. Most Creative On-Device AI DeploymentWe deployed a specialized on-device CNN pipeline on a Samsung Galaxy with fused Winograd transforms for near-zero-latency inference. This 20x speedup enabled advanced hazard detection without relying on Wi-Fi or cloud services. Edge Operator Optimization ChampionOur custom kernel-level optimizations minimized overhead and maximized throughput for real-time hazard detection. By leveraging a highly efficient local inference engine, we delivered robust, high-performance edge compute under disaster conditions. Best Use of GeminiWe harnessed Gemini to generate synthetic data under pseudo-wildfire conditions, helping us rapidly iterate and refine our drone and AR detection pipelines. This approach ensured our system’s resilience and accuracy when deployed in real-world disaster scenarios. Best Use of Vercel@___ in Edge AI trackWe built a Next.js + TypeScript telemetry dashboard and deployed it on Vercel, streaming live drone feeds and AI-generated bounding boxes. Vercel’s serverless architecture let us scale effortlessly for real-time analytics and edge AI workloads. Health Command CenterWe integrated wearable Meta Glasses that offer spoken alerts on structural weaknesses or obstacles, acting as a personal health and safety command center. By giving responders continuous hazard awareness, we reduce the physical risks they face in the field. Waterproof Beach WindSurf HackOur UAV system and AR overlays are designed to operate in wet or windy conditions, enabling first responders to handle coastal emergencies without losing critical data. Waterproof enclosures and robust drone controls ensure uninterrupted rescue efforts in challenging environments. Replit BoostWe leveraged Replit for streamlined, collaborative coding, allowing our team to rapidly prototype and share updates on the AI pipeline. Replit’s integrated environment made it easy to debug and iterate on new features. DevOps Genetic WorkflowWe implemented an end-to-end CI/CD pipeline to automatically test and deploy AI model updates and drone firmware. This DevOps approach minimized downtime and ensured quick delivery of new capabilities to the field. Harder, Better, Buiding: Best Use of Data HackWe combined drone video, AR sensor data, and real-time analytics to train and refine our local detection models. This multi-modal data integration gave our system the depth needed to proactively identify hazards and provide actionable insights. Lap the lines of LangChainOur LangChain-based agent interprets user commands and coordinates tasks between the drone, Meta Glasses, and on-device AI. This agentic layer seamlessly executes complex processes—like building scans for hotspots—and delivers alerts in AR or via text channels.",,,,,,,"executorch, gemini, langchain, llama.cpp, pytorch, vercel","https://pushpafire.vercel.app/, https://github.com/chandrasuda/nextgen-firewatcher"
TreeHacks 2025,MediLedger,https://devpost.com/software/mediledger,True,"Hospital stockouts—the unavailability of essential medications and medical supplies—present a major crisis in healthcare. Patients suffer delays in critical treatments, emergency rooms lack life-saving drugs, and hospitals are forced to ration care. Despite medical advancements, hospitals still rely on outdated, fragmented inventory systems, leading to supply shortages that compromise patient outcomes. The biggest challenge? Hospitals don’t want to share their entire inventory with competitors. Right now, checking stock requires manual calls between hospitals, which is slow, inefficient, and unscalable. The result? Wasted time that directly exacerbates preventable patient harm. Our project is uniquely ambitious in the way we are applying Web3 to a deeply centralized system like healthcare, but we want to reinvent how cybersecurity works in this important industry. We care about using blockchain for social good, so a detailed problem statement highlighting the inspiration for our product is linked below.","MediLedger is all about collaboration: providing a decentralized platform on which hospitals can more securely and confidently transmit sensitive data. Our system enables hospitals to securely check drug availability at other hospitals without exposing full inventory details on both ends. Instead of storing a simple list of drugs, each hospital stores its own inventory in a Merkle tree, where each leaf node contains a cryptographic hash of a drug name and quantity. The Merkle root (a compact summary of the entire inventory) is then submitted to EigenDA, a decentralized data availability layer. EigenDA allows hospitals to store inventory commitments on-chain without revealing individual stock details and ensures data integrity—any change in inventory changes the Merkle root, which prevents tampering. When a hospital needs a drug, the AI agent first checks the local hospital’s inventory. If the drug is out of stock or the supply is trending low, the AI agent fetches Merkle roots of nearby hospitals from EigenDA using Zero-Knowledge Proofs (zkProof) and the provided Merkle trees. The hospitals that have the requested drug generate a Merkle proof, which extracts the leaf node (hashed drug and quantity) from the Merkle tree, includes all necessary sibling hashes to allow recomputation of the Merkle root, and ensures the proof matches the stored Merkle root on EigenDA. The system then outputs a list of hospitals whose Merkle proofs have been validated, thus proving they have the drug in stock.","The backend server was built with Express.js, Node.js, and TypeScript. For the hashing algorithm, we used SHA256. While configuring the adapter for EigenDA, we used EigenLayer’s extensible agent framework for verifiable AI capabilities and data availability logging. The frontend was built with FlutterFlow, with an initial POC frontend built using Next.js.","The biggest challenge was integration—the backend deployment was not Much of what we worked on also involved repositories and tools with limited documentation, where we underwent educated debugging in a field we had little prior experience in. We initially had two ideas (this one and another more fintech focused on price manipulation) that we pivoted twice between on Friday, so defining the initial feature set for both with a blockchain focus took some time, especially with the crunch time of 36 hours. The biggest challenge was integration as well with FlutterFlow.","For most of the team, TreeHacks 2025 is our first hackathon ever. We could have played it safe using familiar Web2 tech stacks from classes, we decided to challenge ourselves by diving headfirst into Web3 and cryptography. In a span of 36 hours, we overcame the steep learning curve of understanding and implementing Merkle proofs and trees, which are notoriously complex but incredibly powerful for verification. Despite occasional roadblocks, we were determined to not give up on our ambitious ideas (where else has blockchain been integrated with healthcare?). Our team wore multiple hats—from testing backend deployment to graphic design and marketing. In the end, we are proud of how we built a fully decentralized inventory system for hospitals to use.","One of our largest takeaways from this weekend is how accessible hacking can be—even to students with limited technical exposure. With the right background and a willingness to learn new tools, we became a determined team that collaborated well on an interesting idea. Blockchain can be used for security and social good, and we hope we proved that through intersecting these technologies with healthcare. For a more technical side, we learned how hash-based data structures allow for selective proof generation. Storing data off-chain in EigenDA was a major shift from Web2 thinking, and now we understand that different decentralization techniques come with trade-offs in performance.","If we had more time, we would love to implement more features related to Web3:","data, eigenda, express.js, flutterflow, merkle-proofs, merkle-trees, node.js, sha256, typescript, web3","https://github.com/shayaansultan/treehacks-final, https://app.flutterflow.io/project/exchange-crypto-app-template-u-i-kit-jlnp6p, https://docs.google.com/document/d/1T2YmsUn6v2e3QweyVUE5JKmtD1eC_nNfvqhIwLeXnAo/edit?usp=sharing"
TreeHacks 2025,AgentZero,https://devpost.com/software/agentzero,True,"As a stock-market investor, I deeply understand emotions are one of the greatest obstacles to success in financial markets. Humans have all felt the fear of uncertainty and loss, the temptation of impulsive choices, and the long period to react and make decisions. But what if we could remove human fragility and the reaction time from the equation? By using a fullyautomated,verifiableAI agent, we utilize technology to makeprecise, unstoppable, and data-drivendecisions. Our fullyautonomous agentis more than just a tool—it is a revolution in investing. With real-time analysis, user-defined customization, and adaptive intelligence, it ensures that trades are executed efficiently, rationally, strategically, and safely. The future of finance is not dictated by emotions, but byinnovation. It is time to embrace the next generation of investment intelligence.","Our system consists of two specialized machine learning models: We combine these models by allowing users to assign custom weightings to each—giving them control over how much influence price trends versus news sentiment have on the agent’s final decision-making.
Once the weighted predictions are generated, our fully automated AI agent can execute trades and market reactions in real-time, ensuring optimal strategies based on user preferences and live data. This level of customization and automation ensures a strategic, data-driven approach that removes emotional bias and maximizes efficiency in ETH trading.","We built our system by integrating EigenLayer for fetching real-time news data and Dune for pulling Ethereum price data. By combining these elements, our AI-driven system provides a fully customizable andverifiabletrading agent, allowing users to optimize their strategies with real-time, data-driven decisions while eliminating emotional bias.","At the beginning, we had limited knowledge of blockchain technology, making it challenging to deploy our agent on-chain. However, by learning from online resources and extensive research through Google, we gradually gained the coding skills needed to integrate agent. Another major challenge was designing a verification process to ensure that the model was trained on the intended dataset. Verifying the authenticity of training data in a decentralized environment is inherently complex. We addressed this by reducing the model's size, making it more efficient while maintaining transparency in the training process. Through persistence and problem-solving, we overcame these obstacles to build a trustworthy, on-chain AI trading agent that is bothverifiable and efficient.","We started from scratch and converted our innovative and crazy idea into an applicable real-world project. Through effective collaboration, extensive problem-solving, and actively seeking help from sponsors, our team built an impactful and inspiring AI agent that can own and manage on-chain data (current price), integrateverifiableoff-chain services and Web3 APIs (Dune and Infura testnet faucet), and enforce policies or rules that guarantee safety, trust, and transparency.",,"Although we have successfully deployed AgentZero on-chain, there is still room for improvement to enhance its performance and accuracy.","eigenlayer, gnark, go, python",https://github.com/2pir2/TreeHack2025
TreeHacks 2025,ECGo,https://devpost.com/software/ecgo,True,"Both our team members come from families working in the medical device industry, and we know firsthand how important critical medical devices are for saving lives around the world. Our project aims to solve the lack of portable ECG machines on hand in developing regions or crisis-stricken areas, where such devices (and the doctors needed to operate them) are incredibly expensive or simply unavailable.","ECGo takes a three second sample of your heartbeat using attached electrodes. It then forwards this sample to a remotely hosted database, from which an AI model is able to run an arrhythmia diagnosis on the test data. This information is then sent back to the user, along with a sample ID they can use to access their data in the future from our web application.","The heart (get it?) of the hardware on this project consists of an ESP32 microcontroller, which uses its built-in ADC to read values from an AD8232 heartbeat filtering IC. This data is then displayed on two 0.96"" OLED screens controlled over I2C. The user can also use a joystick to pan and zoom the ECG waveform displayed on the screen. For safety, both the microcontroller and IC are powered by AA batteries to avoid any potentially dangerous power connections. On the software side, the ESP32 must be connected via Wi-Fi to some network which allows it to make HTTP requests. We use these requests to write data to an InfluxDB database, and also read the AI model's diagnosis out from the same database. Each sample is randomly assigned an ID from 0-10000 for use with the web interface. The front-end software is a simple streamlit app, which uses Python to query the database for user data and displays the resulting data. The server uses a CNN Transformer which we pretrained on open-source ECG data. It also interfaces with the InfluxDB database by checking it periodically for a new series of data, and writes its inference to the database when it finds new data.","One early challenge with the hardware bringup was the fact that my OLED screens were manufactured with a single I2C address, meaning that there would be no way to use two screens to display different data. To fix this we had to desolder a small surface mounted resistor and shift it to a different pin, which was quite a challenge. Otherwise, noise was (and is) a big issue with the project; we do our best to avoid being in areas with tons of electromagnetic interference when taking detailed samples.
Testing the model was also a challenge, since we don’t have a good way of measuring an irregular heartbeat with our electrodes. The model did report a positive diagnosis of arrhythmia for incredibly noisy environments, which have more irregular signals, and a negative diagnosis for a regular heartbeat. We explored the web3 track and experimented with placing an AI-powered agent on the application also assists with interpreting their test results and giving medical suggestions. However, this was rather difficult to integrate in the limited time we had left and so in the final product we left it off.","As a team of only two (one hardware/firmware and one software/AI), we are very proud of having a successful final product with so many different features and complexities.
We’re very happy with the way the hardware fabrication went, from prototyping to the final PCB soldering without any major hiccups. Furthermore, the entire data flow of chip -> HTTP -> Database -> Model -> Database -> HTTP -> chip is surprisingly reliable and effective.","This project was a new experience for both of us when it comes to integrating our areas of technical expertise with each other, as interactions between embedded systems and AI on such a close level are less common in our normal work.
Technically, we both learned a lot about working with time-series data and InfluxDB. Working through the aforementioned integration gave us both a better understanding of how to work with diverse inputs and outputs that we don’t usually see.","ECGo is not just about arrhythmia diagnosis - this paradigm of an inexpensive, mobile device for use in the field in conjunction with more powerful AI diagnostics and support in a remote server has great potential to us in bringing healthcare to areas around the world with less access to technology or medical personnel. The next future steps are to further develop the diagnostic capabilities and improve the ECG filtering, and then adapt the device to be even more technology-free by moving the patient query onto the ESP32 as well, making the relatively inexpensive hardware component the only item we need on-hand to perform the whole suite of diagnostic, data storage, and recommendation.","c, esp32, influxdb, opacity, python, pytorch, streamlit",https://github.com/fightingj305/ecgo
TreeHacks 2025,Therms - Thermoregulation Wearables with AI-powered Insights,https://devpost.com/software/therms-thermoregulation-wearables-with-ai-powered-insights,True,"Our project is inspired by the fascinating thermoregulation research conducted by Professors Craig Heller and Dennis Grahn at Stanford University. Their work captivated us (papers linked below), and we had the opportunity to take Professor Heller’s class, where we directly experienced how temperature affects physical performance, health, and disease. For some scientific context, our body'sglabrous skin(hairless areas rich in specialized blood vessels called arteriovenous anastomoses, or AVAs) plays a critical role in heat exchange. The soles of our feet and the palms (made of glabrous skin) of our hands act as thermal windows, regulating heat dissipation and conservation.When the body needs to cool down, AVAs dilate, increasing blood flow to the skin’s surface for heat loss. Conversely, when warmth is needed, AVAs constrict, reducing heat loss. In Professor Heller’s class, we were actually able to test this research by experimenting on ourselves. We pushed our aerobic and anaerobic limits with and without a cooling stimulus applied to our palms. The results were mind-boggling: we could do more reps and sets of strength exercises, and we nearly doubled our running time in a 100°F room with cooling applied. Experiencing this research firsthand fueled our interest in its potential as an incredibly useful healthcare-engineering project. However, the cooling setup we used in class had some serious drawbacks. It relied on a liquid cooling pump attached to tubes which were bulky, messy, and far from portable. Wearing large cooling gloves made using our hands impossible, which was impractical for real-world applications. Accordingly, we set out to solve this problem by shifting the cooling mechanism to the soles of the feet and designing a compact, self-contained system capable of both heating and cooling. Our goal was to create a wearable thermoregulation device that eliminates the need for a liquid cooling pump while maintaining efficiency, portability, and usability. We also saw an opportunity to integrate healthcare analytics, leveraging data from the device to provide valuable insights for both medical and performance-based applications. In short, we had to create a miniaturized HVAC system that seamlessly fits in the sole of a shoewithout the bulk, without the mess, and without interfering with daily activities.","Therms is a wearable thermoregulation device designed to actively regulate body temperature by targeting the glabrous skin on the soles of the feet. Therms usesthermoelectric Peltier modulesto provide rapid, reversible heating and cooling. Our design uses areversed-polaritymodel to switch the system between heating and cooling states. An embedded sensor continuously monitors foot temperature, ensuring the system responds in real time to thermal needs.AI-powered analysesof temperature data alongside health metrics (e.g., activity levels, medical conditions) help predict andpreemptively adjust stimuli responses, optimizing comfort and performance. Therms also connects to APIs (Terra, Scrapybara, OpenAI) to incorporate additional physiological data, such as heart rate, sleep patterns, and chronic conditions. This enables personalized thermoregulation strategies and early detection of anomalies. For example, if a diabetic user’s foot temperature rises abnormally, Therms can cool the area to prevent tissue damage and AUTOMATICALLY book an appointment with a healthcare professional. Therms has medical applications, supporting individuals with thermoregulation dysfunction, such as those with spinal cord injuries, diabetes, or neurodegenerative disorders, by stabilizing foot temperature and preventing complications. Therms is also focused on performance-based applications to enhance recovery and endurance by optimizing thermal regulation during and after intense activity (soldiers, athletes, manual laborers). Finally, Therms has immense geriatric applications by protecting elderly users, who are particularly vulnerable to hypothermia and hyperthermia, with preemptive adjustments based on real-time data. Overall, Therms combines advanced thermoelectric technology with a deep understanding of human physiology to create a wearable system that actively supports thermal homeostasis. By targeting the glabrous skin on the feet, Therms achieves efficient,non-invasive thermoregulationwhile integrating AI-powered insights to improve health outcomes and performance.","After hours of whiteboard sessions and squabbling with one another, we designed a modular wearable system that efficiently manages heat transfer while leveraging AI-powered analytics for personalized thermal regulation. Miniaturizing an HVAC-like system into a shoe sole required replacing traditional liquid cooling with Peltier boards for precise heating and cooling. These boards were placed on top of a heat sink that were connected with a copper foil so that any heat accumulated on the Peltier after the current was turned off was dissipated away. A motor controller dynamically adjusts polarity and current flow to switch between heating and cooling states. A copper foil was also placed on the top of the frame to connect the Peltier boards and distribute heat transfer evenly among the surface area of the foot, while a temperature sensor was placed next to the top copper layer, but insulated from it, to provide real-time thermal feedback. A microcontroller coordinates these components and processes sensor data, while an IMU sensor tracks movement to provide additional activity insights. The entire system is housed in a modular sole frame (durability, extendability, and ease of integration into different footwear). The software ecosystem enables real-time data processing and automation. Firebase handles seamless communication between the device and the web application, while Terra API integrates external physiological data from other wearables. ChatGPT processes the integrated thermal and health data, identifying anomalies and optimizing thermoregulation strategies. Scrapybara automates tasks such as booking medical appointments and managing prescriptions based on detected trends. A Next.js frontend and Node.js backend ensure a responsive and scalable platform for users to monitor and personalize their Therms experience.","A major challenge came from Peltier technology, which can prove to be incredibly inefficient energy-wise and temperature-wise. When turned off, the boards retained and radiated heat on both sides, causing unwanted warming immediately after cooling. To counter this, we created an optimized heat sink system with our modular sole frame that captured as much heat as possible and dissipated it through a copper layer; this significantly reduced the risk of overheating. The modular design we keep talking about was one of the most challenging parts. We had to design a system that allowed for easy upgrades and customization and incorporate several different hardware modules in a very small space. After multiple iterations of our 3D models, we refined the design to maximize space efficiency while ensuring easy assembly and part replacement. On the software side, we built a pipeline incorporating three different data sources and three third-party APIs: shoe data + user data + Terra API activity data → OpenAI insights → Scrapybara automation. Our streamlined process looked great in theory but proved far more complex in execution. Ensuring data consistency across different technologies was a tedious task, with unexpected bugs that took considerable time and patience to debug.","We are incredibly proud to have successfully created one of the first completely pump-less, battery-powered heating and cooling systems in a wearable format. Several reports said Peltier technology is inefficient and that using a liquid cooling/heating system that wasn’t mobile was the only way to effectively thermoregulate. We also had no design blueprints to reference, so it was exciting and also gratifying to see our hacky model come to life. Getting the entire design, code, and actual working device done within 36 hours was incredibly rewarding and helped us each hone and develop new skill sets that will serve us for life. Beyond hardware, we integrated incredibly varied technologies like Terra API, Scrapybara, and OpenAI and created a fully working web application portal that can pair shoes, collect user data, provide insights, and display professional data visuals. We are also proud of the research-first approach we took, where we used mathematical and biological models to quantify stimuli responses, healthcare insights, and have a solid grasp of why this technology is so important to human performance and health.","Throughout this project, we gained hands-on experience in developing scalable software and hardware. Our software required multiple APIs and a plethora of data being transmitted to different endpoints, which acted as a forcing function to make our application lightweight and scalable. We can expand the number of shoes, user metrics, wearables integrated through Terra, and much more without any latency using our Firebase + NextJS architecture. We refined our skills in CAD modeling and 3D printing, iterating through multiple designs to achieve a compact and efficient wearable. On the hardware side, we learned how to integrate super niche hardware modules, from thermoelectric modules to motor controllers, while balancing thermal physics, power efficiency, and real-world usability. Additionally, we explored IMU sensor modeling, developing a system that extracts motion data to enhance user tracking and inform advanced healthcare analytics. Finally, working with Terra API and Scrapybara taught us how to streamline data integration across multiple platforms, ensuring seamless communication between hardware, software, and AI-driven automation.","We plan to refine and optimize Therms with the goal of commercializing it for users who need to condition/train heavily (athletes and soldiers) or users who have thermoregulation dysfunction symptoms. Our next steps include enhancing energy efficiency, ensuring that heating and cooling functions operate with minimal waste, and redesigning the form factor so the entire system fits nicely within the shoe. From this business and research perspective, we aim to scale Therms into a consumer-ready product that enhances human performance and recovery in athletes, workers, and everyday users by reducing fatigue and optimizing thermal regulation. Additionally, Therms has critical healthcare applications, particularly for senior citizens, providing a continuous, non-invasive way to monitor body temperature and mobility, reducing the risk of temperature-related health issues. By integrating advanced AI-driven insights, real-time health tracking, and an ergonomic design, we envision Therms becoming an essential, wearable thermoregulation solution for both performance enhancement and preventive healthcare.","firebase, nextjs, node.js, openai, scrapybara, tailwindcss, terraapi","https://github.com/arnavnc/therms-platform, https://pubmed.ncbi.nlm.nih.gov/9804564/, https://pubmed.ncbi.nlm.nih.gov/22076097/, https://pubmed.ncbi.nlm.nih.gov/19640130/, https://pubmed.ncbi.nlm.nih.gov/15879169/"
TreeHacks 2025,Project AVIA,https://devpost.com/software/project-avia-autonomous-vigilance-insights-for-aviation,True,"Homepage Command Center for Squad Analytics Active Pilot Monitoring Mission Biometric Analysis Recommendations for Pilots Our amazing team <3 Fighter pilots are among the Air Force’s most critical assets—each one represents a $10.9M investment, and every mission puts both lives and high-value aircraft at risk. Yet, 98% of fighter pilots report debilitating fatigue and pain, impacting their performance and operational effectiveness. This isn’t just a pilot problem—it’s a leadership challenge. Without real-time visibility into pilot health and performance, decisions are being made with limited data. Biometric monitoring changes that. By leveraging wearable technology to track key health and performance metrics, leadership can deploy the right pilot for the right mission, refine training cycles, and enhance overall force readiness. With the power of real-time data, we can make smarter, faster, and more strategic decisions to optimize pilot performance, health, and mission success. Project AVIA is an AI-driven command center that consolidates real-time biometric data—heart rate, heart rate variability, oxygen saturation (SpO2), sleep quality, stress levels, exercise, and estimated blood pressure—sourced fromTerra APIandPPG recordings. Our command center has comprehensive insights into a pilot’s health and performance, enabling leadership to make informed, data-driven decisions. We offer insights during: Throughout working on this project, we learned a lot regarding how biometric data can actually directly influence military operational readiness. Namely, it was fascinating reading about how there are clear correlations between physiological signals and combat performance. We also learned about the nuances and complexities that went into integrating PPG waveforms into predictive health analytics, and understanding how each metric is correlated with each other. A unique thing we learned was how important it is to optimize API efficiency by separating biometric requests into modular API calls as that greatly improved our performance. We also explored the different metrics that could be estimated from PPG alone, especially blood pressure by utilizing careful signal processing and peak detection. We're incredibly excited about AVIA's future potential to revolutionize how the Air Force approaches pilot health and mission readiness. Here's where we're heading: Ultimately, AVIA isn't just about keeping individual pilots healthy – it's about creating a smarter, more resilient Air Force through the power of data. We're excited to keep pushing the boundaries of what's possible when you combine cutting-edge biometric monitoring with AI-driven insights. The future of military aviation might just be healthier, safer, and more effective than we ever imagined.",,,,,,,"cors, express.js, flask, mistral, ngrok, python, react, tailwind, terra, vercel",https://github.com/Harsh-Karia/TreeHacks
TreeHacks 2025,VisionGuard: smart vision for smarter driving,https://devpost.com/software/visionguard-smart-vision-for-smarter-driving,True,"Driving in extreme conditions likeicy roads and blizzardsoften leads to accidents due topoor visibility or slippery surfaces. Having personally experienced car accidents caused by vehicles losing control on ice, it’s clear how critical it is to ensure that drivers are cautious and aware. Similarly, my teammates from thecrowded Bay Areaface challenges withcomplex road systems and traffic, where crashes are just as frequent. This inspired us to developVisionGuard, a system that leverages AI to analyzedriver behavior and road conditions, ultimately helping drivers make safer decisions. Our goal is to buildsmarter driving tools that reduce accidents and save lives.",,,,,,,"apis, colab, gpt, python, reinforcement, vscode","https://github.com/JayYeung/tesla_kaggle, https://docs.google.com/presentation/d/1I6-hDtITWCSTrmS5IM57R79F7E6Rf0M57GbeiiAiLEo/edit?usp=sharing"
TreeHacks 2025,HiveMind,https://devpost.com/software/hivemind-18cula,True,"The idea for HiveMind was born from observing the disparity in student learning experiences, and have a framework based system to collectively improve everyone in the class week by week, leaving no one behind. We created a system that fosters collaborative learning environment, collective growth and collective improvement until everyone in the class is on same level of understanding and comprehension.","HiveMind is an AI-powered educational platform that enhances student learning through continuous collective growth through peer learning sessions. By seamlessly integrating with online learning environments, the platform analyzes student responses and categorizes them into different levels of understanding. This classification enables the system to provide tailored support, ensuring that students who need extra help receive it, while those who have mastered concepts can reinforce their knowledge by guiding others. The platform assesses student performance through AI-driven quizzes and assignments, determining their comprehension levels. Based on this analysis, students are placed into one of four learning hubs: those requiring foundational support, those needing additional clarification, those who have a strong grasp of concepts, and those who have achieved mastery. This hub is made up of a right balance of 4 students and this matching is done by the custom algorithm. The system then facilitates peer matching, allowing students with higher scores to assist those in lower scores. Additionally, Its seamless integration with existing learning platforms ensures that students receive real-time feedback and targeted support without disrupting their educational workflow.","HiveMind was developed using a combination of advanced web technologies, machine learning frameworks, and 3D visualization tools. Our backend, built with Python, manages data processing, user authentication, and AI-driven assessments. We implemented custom algorithms to evaluate student performance, generating meaningful insights that adapt to their learning needs. The frontend, developed with TypeScript, Three.js, and Next.js, provides an intuitive interface where students can track their progress through an interactive 3D brain visualization. To enable real-time peer matching and vector-based assessments, we incorporated vector embeddings using IRIS Vector database, and Perplexity Sonar API reasoning that analyze student responses and categorize them accordingly.  These embeddings are stored efficiently to facilitate quick retrieval, ensuring smooth transitions between different learning levels. Once the peers are matched, they will go through a Zoom session which is monitored by an AI agent which evaluates the progress of the student as well as the hub. Throughout the development process, we optimized the platform for scalability, ensuring that HiveMind could accommodate a growing number of users without performance bottlenecks. The integration of AI, real-time analytics, and peer-to-peer networking allowed us to create a seamless experience that enhances student engagement.","One of the biggest challenges we faced was merging the 3D dashboard into our existing system, which already included vector embeddings and a peer-to-peer network. This led to numerous Git merge conflicts, causing unexpected issues that disrupted our workflow. With time running out, we had to make a critical decision—to start fresh with a new GitHub repository in the final hours of development. This shift required us to carefully migrate existing functionalities while ensuring that all components remained fully functional. Rendering the 3D brain visualization was another major hurdle. Achieving a balance between performance and visual appeal proved difficult, as real-time interactions required optimized rendering techniques. We experimented with various configurations in Three.js, adjusting parameters to ensure smooth animations and intuitive navigation. Additionally, maintaining compatibility across different devices added another layer of complexity, as rendering performance varied based on system specifications. Despite these challenges, our team remained persistent, debugging issues, restructuring code, and finding creative solutions to ensure that HiveMind functioned as intended. Overcoming these obstacles reinforced our teamwork and problem-solving skills, making the final product even more rewarding.","Building HiveMind was an ambitious challenge, and we are proud of what we have achieved so far. One of our biggest accomplishments is successfully implementing an AI-driven assessment system that evaluates student comprehension in real-time. Our machine learning models accurately categorize students based on their understanding, enabling tailored learning experiences that adapt dynamically to individual needs. We also developed a structured peer-matching system that facilitates collaboration between students at different knowledge levels, fostering a strong sense of community and support within the platform. Another major achievement is our intuitive , which provides students with a clear and interactive representation of their learning progress. By integrating AI-driven insights with visually appealing analytics, we have transformed the way students perceive their academic journey. Additionally, our backend infrastructure is optimized for scalability, ensuring that HiveMind can handle a growing number of users without compromising performance. Successfully integrating the platform with online learning tools while maintaining a seamless user experience was a significant milestone that highlights our technical expertise and problem-solving abilities.","Throughout the development of HiveMind, we encountered numerous challenges that pushed us to think critically and innovate. One of the most valuable lessons we learned was the importance of real-time processing in educational platforms. Immediate feedback is crucial for student engagement, and optimizing our AI models to deliver quick yet accurate assessments requires extensive experimentation and fine-tuning. We also gained a deeper understanding of the complexities involved in integrating AI-driven tools with existing educational platforms. Ensuring compatibility with learning management systems, designing efficient data pipelines, and maintaining user privacy were all critical considerations that shaped our development process. Furthermore, we realized that while AI can enhance learning, human interaction remains an essential element of education. This insight reinforced our belief in combining AI-driven recommendations with peer-based learning to create a balanced and effective system. From a user experience perspective, we learned the significance of designing interfaces that encourage participation without overwhelming students. Keeping the UI simple yet powerful was a challenge that required iterative testing and feedback. Understanding the psychology of learning and motivation helped us refine our platform to make it more engaging and beneficial for students.","HiveMind is only at the beginning of its journey, and we have exciting plans for its future. One of our key next steps is integrating the platform with Zoom, enabling real-time transcription and AI-powered suggestions during class discussions. This feature will enhance virtual learning by providing students with relevant questions, summaries, and recommendations, making online education more interactive and insightful. We also aim to enhance our adaptive learning models to provide even more personalized learning paths. By refining our AI algorithms, we can ensure that each student receives targeted resources and exercises tailored to their specific needs. Another major focus is incorporating gamification elements, such as achievement badges, leaderboard rankings, and interactive challenges, to keep students motivated and engaged throughout their learning journey. In the long term, we plan to expand HiveMind’s reach by forming partnerships with universities, online learning platforms, and EdTech companies. By integrating our system into large-scale educational environments, we can impact a wider audience and help bridge learning gaps across diverse student populations. Our vision is to make education more personalized, collaborative, and accessible, ensuring that every student has the tools they need to succeed.","api, chromadb, flask, gemini, intersystems, nextjs, perplexity, python, sonar, streamlit, superbase, three.js, typescript, vector-embedding, vespa, zoom","https://github.com/rajashekarcs2023/HiveMind, https://github.com/rajashekarcs2023/knowledge-balance-network, https://github.com/AahilAliCodes/Hivemind2"
TreeHacks 2025,ERWIN: Enhanced Rock Weathering Impact Navigator,https://devpost.com/software/erwin-enhanced-rock-weathering-impact-navigator,True,"Our TreeHacks team formed due to common career motivations in tackling climate change. We’re particularly interested in how technology can shape global finance toward climate solutions, hence our interest in the Carbon Removal Assessment challenge. We spoke with Stanford Earth Systems Professor Kate Maher to learn about pain points in Enhanced Rock Weathering (ERW), an increasingly popular method of carbon removal, where certain species of rock with high CO2 sequestration rates are ground up and distributed to maximize reaction rates via surface area. Forecasting the carbon removal potential of an ERW project is an esoteric process with a steep learning curve - estimates used by investors and project planners in carbon credit markets have vast uncertainty, and more scientific approaches require PhD-level knowledge and access to poorly-documented methods of accessing computational models running on Fortran and text files. There are few people in the world with the resources and experience to scientifically forecast carbon removal rates for ERW projects.","We’ve developed a visual platform democratizing ERW carbon removal assessment for any user, focusing on carbon credit markets and ERW researchers. Now, users can select a target region for an ERW project and input simple parameters of feedstock (rock) type project area, feedstock surface density, and time series length for the output. After clicking ‘Run’, the user is presented with graphs of the CO2 removal rate per meter squared, and the pH, over time in years. The user is also presented with an estimate of the total CO2 removed over the total project area over the inputted period of years. Accommodating our two main categories of stakeholders, we have a ‘Basic’ and ‘Advanced’ mode on the User Interface. Those less familiar with the science of ERW, such as potential investors, can choose the Basic mode to be provided with the simple data needed to observe Carbon Removal rates in selected regions. For those who are better-versed in ERW, the Advanced mode serves to provide more options and detailed information regarding the science of ERW and our calculations.","After the user clicks ‘Run,’ ERWIN automatically retrieves a comprehensive set of soil and environmental data for the selected location and integrates it with user-provided inputs to generate and refine the data required for underlying computational algorithms. To do this, we took the latitude and longitude of the center of the user’s region of interest and used soil and weather APIs to fetch data relevant to soil carbon exchange geochemistry, such as soil composition, cation exchange rate, and mean annual precipitation, to name a few. As an additional functionality, before the user selects 'Run,' they have the option to explore quarries recommended for their bounded location. After the user chooses a type of rock and a selected area, the toggle switch on the top left outputs n (set to 50) points with the best quarry options to obtain that rock type. This data comes from the National Mine Map Repository, pulled using Selenium and BeautifulSoup4, and stored in CSV files on a public GitHub repository. These spreadsheet-like files are named with the latitude, longitude, and rock type that they represent, and are accessed as such within the script in JavaScript. One of the greatest challenges of carbon removal projects lies within random sampling and generating probability distributions across vast datasets. Powered by Qiskit in Python, our quantum computing system leverages quantum superposition and entanglement to significantly enhance sampling efficiency and accuracy. Likewise, our quantum processes can run on quantum computers in real-time with IBM Quantum Cloud in the present day, and these processes will become more scalable over time. Identifying the relevant time series outputs across the many output files, based on CO2 removal relevance and scientific interest, we generated distribution curves of these prioritized datasets (ion concentration and soil pH time series), used stoichiometry to convert chemical data to CO2 removal rates, and graphed the mean curves for CO2 removal rates and pH over time with one standard deviation of uncertainty. By integrating the data under the curve, we found the total CO2 removed per m^2 across the time series and multiplied this by the project area to get the total CO2 removed over the provided project timeframe. Overall, our key tasks were:",,,"We’re particularly proud of learning carbon removal geochemistry on the fly, and teaching ourselves to understand complex academic methodologies to the extent that we were able to build a simplified workflow around the necessary computations - to build a simplified model, we had to understand not only inputs A and outputs B, but also what calculations are required to get from A -> B. Within 36 hours, we built scalable digital infrastructure, a robust UI interface, and a backend interfacing with numerous APIs to fetch soil and climate data, while learning ERW science, formulas, and data representations, and meeting user needs.","We seek to continue our collaboration with Professor Maher and other ERW stakeholders in academia and the private sector, to receive more user feedback and better serve stakeholder needs. We also want to expand our offering of recommending ERW project locations for maximizing carbon sequestration and minimizing distance from feedstock quarries, likely supplemented with better feedstock quarry spatial data through computer vision analysis of satellite imagery. The carbon removal project market, and particularly the ERW sector, is rapidly growing, and we see this tool having great potential for the basis of a venture accelerating a zero GHG future.","beautifulsoup4, crunchflow, docker, flask-(python), javascript, mapbox-api, open-meteo, qiskit-(python), selenium, soilgrid-api","https://github.com/Jackal-Studios/rock-thing, https://www.canva.com/design/DAGfPeW-cJw/De_TRIO2zEPGTqJQVsqDMw/view?utm_content=DAGfPeW-cJw&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h2678110d5b"
TreeHacks 2025,BAS Climate Action Matcher,https://devpost.com/software/bas-climate-action-matcher,True,"Mitigating climate change is only possible through a patchwork of collective action. The future of our planet will be determined by our ability to change previous destructive corporate processes radically. However, companies often struggle to find concrete solutions and actions to reduce their carbon footprint while advancing their business goals. We partner with Race to Zero to create aclimate action matcher. Through a user-friendly interface and an agentic workflow grounded in tool usage, we match THE USER COMPANY with relevant UN-catalogued Cooperative Climate Initiatives and provide them with sustainability reports from similar companies and peer corporate actions to inspire concrete action. Ultimately, we want to transform how we approach climate change – from apathy to collective action. By connecting companies and their initiatives, we want to show that action is possible, popular, and influential – especially when done together as an industry, nation, and planet.","Our tool matches companies to relevant climate initiatives. We provide an agentic system with tools like RAG embedding search on a custom database of company sustainability reports, and UN-cataloged Cooperative Climate Initiatives, web scraping on websites likehttps://zerotracker.net/andhttps://nzdpu.com/home, and more to discover corporate climate actions.","DAIN Butterfly: We use the DAIN Butterfly agentic workflow with tool usage as our central orchestrator for user interface interactions. We built custom tools that find companies similar to THE USER COMPANY based on industry sector and country, match companies to UN-catalogued Cooperative Climate Initiatives, and find relevant climate actions from hundreds of sustainability reports from a custom database we built. We provide the agent with the initial context of its foal (e.g., it is trying to write a report that should reference its sources). Still, the agent can choose which tools to use and autonomously decide its tool strategy depending on the outcomes of previous actions and details specified by THE USER COMPANY. To ensure responsible usage, we instruct the agent to include sources to its information (which it can do as our tools return the href links they got their information from) in its findings, allowing THE USER COMPANY to confirm and dive deeper into the sources. We used the DAIN UI components to format the responses engagingly and professionally. InterSystems Embedding Database: We use InterSystems as our database. We collected and embedded 172 UN-catalogued Cooperative Climate Initiatives with descriptions and over 17,000 paragraphs from scraped sustainability reports. NVIDIA Llama Embeddings: We use Llama-3.2-nv-embedqa-1b-v2 embeddings for our embedding database and query embedding in our RAG vector search. LangChain: We use LangChain to load sustainability PDF reports directly from the web and recursively split the text for subsequent chunk embeddings. Google Gemini Scoring and Classification: We implement company sector classification using Gemini Flash Experimental 2.0. Moreover, we use Gemini to score corporate actions based on their reproducibility and return on investment for action ranking and matching. Scrapybara: We implement an agent to find concrete PDF links on corporate websites that may be deep in the link structure of the page. Selenium Web Browser: We implement web scraping using Selenium.","Finding relevant climate actions first proved tricky since sustainability reports can be pretty vague, and embedding similarity search works best if we try to match the target report structure as closely as possible. We solved the problem by having the DAIN agent brainstorm climate initiatives the companycould be doingand thenverifythese ideas by findingactual climate actions by companiesin their sustainability reports. Another challenge was to have the agent perform enough actions to take advantage of all our tools. We ended up spending some time on prompt engineering and writing clearer tool descriptions which had a clear boost in performance.",,,"Extend the initiatives into a dynamic knowledge graph to track the impacts of climate actions.
Extend scoring to include nature-based solutions, collaborations, estimated impact, and cost.
Create a dashboard to standardize climate reporting for easier comparison.","dain, gemini, google, intersystems-db, langchain, nvidia-llama",https://github.com/Suzehva/bas_labs
TreeHacks 2025,Lemon,https://devpost.com/software/lemon-7gn5hq,True,"Reducing food waste is one of the most effective ways to combat climate change. In fact, food waste has an environmental footprint 248x bigger than plastic!  🌍
As we researched the root causes of food waste, we discovered that some 1.2 billion tonnes of fruit and vegetables are discarded annually at farms before they ever reach the market. But why?
In the U.S., over 50% of produce is thrown away because it’s considered""too ugly""to sell well — that’s about 60 million tons ofperfectlyedible food gone to waste. One of our team members has seen firsthand how healthy and edible produce gets discarded just because it doesn’t look perfect. This issue is REAL and needs to be addressed immediately.","To solve this problem, we built Lemon. 🍋🌱
Lemon is an app that connects farmers, businesses and individual customers to reduce food waste and fight climate change. By signing up as a customer, you can access discounted, surplus produce from local farmers that would otherwise go to waste. Lemon offers a fully transparent shopping experience; each product is listed with details about its source, and the user has full control over when and what products they buy, like traditional online grocery stores. Producers (farmers) can create listings for their surplus or ""wonky"" produce, allowing them to sell items that are perfectly good but not deemed aesthetically pleasing enough for traditional markets. Businesses can also shop for discounted produce like customers, but Lemon offers business accounts additional support by enabling them to request in bulk, advertise to nearby users and and form long-term partnerships with local farmers. Lemon’s goal is to raise awareness about the impact of food waste, making it easier for everyone to make sustainable shopping choices and contribute to a greener, more connected world. Painpoints❓Many consumers want to make sustainable choices but don’t know how. Lemon makes it easy by offering access to eco-friendly food at discounted prices. 
🍏 Farmers with surplus & 'imperfect' produce get to sell their produce, creating a new source of income and increasing their positive impact on the environment.
👉 Businesses get to access locally-produced crops at much lower prices, search for and partner with producers and benefit from advertising perks, like sustainability spotlights on the app's main page that are shown to customers in their area. Broader contextLemon’s focus on reducing food waste aligns with environmental regulations. The U.S. government has set a goal to cut food waste in half by 2030 through various initiatives and consumer education. Some states are even passing legislation to restrict the amount of food waste going to landfills.
Lemon has strong word-of-mouth potential, driven by eco-conscious consumers as well as farmers who are profiting out of Lemon. We also incorporated share buttons in the app so that users can share information about this initiative with their friends. Market & ImpactWith 60 million tons of wasted produce every year in the U.S., the opportunity for a sustainable solution is enormous. The demand for eco-friendly food options is growing rapidly. Stakeholder IncentivesFarmers: sell produce that would otherwise go to waste; a new source of income.
Customers: get fresh, discounted produce; contribute to sustainability.
Businesses: get fresh, discounted produce in bulk; get recognition for sustainability efforts.","We implemented our vision for Lemon using Flutterflow. After drawing a lo-fi prototype, taking user experiences into account, we started familiarizing ourselves with Flutterflow and got to building. We used Supabase/PostgreSQL as our database and also added customized functions to our Flutter app. Using Flutterflow's sponsored special access plan, we were able to collaborate real-time on the app and distribute tasks between team members.","Familiarizing ourselves with a completely new environment, Flutterflow, was challenging at first. This is the first hackathon experience of all team members, making the process extremely exciting but challenging at the same time. Some things on which we spent more time than we probably needed to were 🙃","We are super proud of all the progress we made! Starting with only an interest in contributing to sustainability to landing on an idea to building it out with Flutterflow, we accomplished so much that we are proud to showcase.","We are amazed to see how fast we were able to pick up how Flutterflow works and our ability to familiarize ourselves with a new tool showed us the importance of transferable skills, like logic. It also demonstrated that 'hackers' should be quick to adapt and fast learners.","We are looking forward to getting feedback on our app, its design, and functionality. We hope to make iterations based on the feedback. We will have access to Flutterflow for two more months and who knows---maybe we'll even get to publish our app! [NOTE: Only MOBILE view]","dart, flutter, supabase","https://lemon.flutterflow.app, https://github.com/twosodium/lemon, https://app.flutterflow.io/run/DdNU9oq49bbWYsMZY3xf, https://app.flutterflow.io/project/thackssss-jvfor7"
TreeHacks 2025,Ranger,https://devpost.com/software/ranger-6jkv5s,True,"Our wild (west) setup - Jetson Orin Nano on the left, power bank on the right, and a mic array in the middle, all hooked up to a Meta Quest. ODAS audio spatial visualization - one of the tools we used for analysis and debugging during development. Sometimes helpful ideas come from the silliest places. One day, as our team was playing Fortnite instead of doing our homework, we realized how helpful it was to have a ""sound ring"" in game. Noises like enemy footsteps or gunshots are displayed with a visual indicator to show what direction they came from. Luckily, we aren't trapped in Moisty Mire shotgunning ChugJugs, but wedidrealize people hard of hearing could benefit from this in real life. Both innocuous situations (dropping your wallet in the street and a parent calling you down to dinner) and intense situations (scary noises that would make anyone want to pick up the pace a bit walking home at night, an ambulance screaming by) are much more difficult to navigate for those who can't process auditory input. Visualizing sound helps you enter and understand conversations easily, avoid dangerous situations, and generally be aware of important goings-on in your surroundings. We wanted to build portable, wearable technology that would bring our vision from our colorful video game screen to the real world. We see our product as a proof-of-concept for something that could truly - cheaply, portably,fashionably- transform the lives of deaf people. Ranger is an edge-based AR solution for audio visualization that uses a Meta Quest frontend to indicate to the wearer where sounds are coming from. The user wears a hat, which has a microphone array to capture omnidirectional audio and send it to our processing unit, a Jetson Nano. Ranger classifies all different sorts of real-world sounds using a classifier model and displays them as icons on a circular grid, placing markers according to distance and direction. For speech, we live-transcribe conversations using Whisper which allows those hard of hearing to immediately parse what's happening, even if speech comes from behind them. The sound visualization does not interfere with your real-world view, only enhancing the information already available. It's a real-life HUD, enriching the wearer's experience and using edge computing to bring them into the wonderful world of sound. Hardware: Software: Ranger runs completely on the edge! All heavy computation is done on a Jetson Orin Nano; all communication is done with direct wired USB-C connections. For a TL;DR: using the Jetson Orin Nano, we developed an audio processing software that takes in a 4-channel microphone input, triangulates the direction, amplitude, and type of the loudest sound occurring at any time step, transcribes any detected speech, and sends all this information to the Meta Quest 3 using network-over-USB. For those interested in a more in-depth overview: Real-time Voice TranscriptionWe use Whisper Mini running on the Jetson Nano for real-time voice transcription. To accomplish real-time voice transcription, we capture the last 10 seconds of a user’s audio and process it immediately. Although the Jetson Nano has the ability to run larger Whisper versions (up to the recently released turbo one with 800M parameters), our priority was reducing latency. Sound triangulationTo determine where sound comes from relative to the user, we use a ReSpeaker microphone array. We set it up to triangulate the audio channels to pinpoint where audio comes from (an example of this technique below). This approach gave us a vector with angle and volume, which allows us to position our classified sounds as relatively positioned icons in our 3D scene. Sound classificationSound classification is done using a convolutional neural network (CNN). The model runs on the Jetson and classifies audio in a probability distribution of up to 500 candidate labels. Using this class along with the latest source of noise from the sound triangulation, we’re able to accurately pinpoint what a noise is and where it comes from relative to the user! This was our first time doing a hardware-based project, and our inexperience showed upimmediately. We found ourselves sifting through mountains of cables, walking back and forth to the hardware booth every 20 minutes, and flipping between ""ITSSOOVER"" and ""WERESOBACK"" faster than the GPU fan on our Jetson Nano. We all loved Operating Systems, Concurrency, and Computer Architecture in school, but building a project completely from scratch, with very little electric or audio engineering knowledge, through largely un-trekked territory was an uphill battle. We had several significant challenges: 1) Parsing raw input data intelligentlyWe planned this out without a pretty cursory understanding of auditory science, so we had to spend a lot of time understanding hardware synchronization concerns, channel mixing, and in-built audio driver configurations. Spending a lot of time thinking and diagramming though, rather than just spurting out poorly written code, was incredibly helpful later during integration. 2) Interfacing between backend and frontendWe originally wanted to use bluetooth to communicate between the Jetson Nano and Meta Quest but ran into a ton of issues with trying to get low fidelity bluetooth communication schemes working. After a lot of tinkering, we decided to connect them together with a USB-C cable and used Android Debug Bridge to treat the wire as a network connection through a server socket. 3) Developing a fully on-the-edge systemThe Jetson Nano relies on a DC power supply, which we tried to get around with a USB-A adapter and a power bank. Our lack of electric engineering know-how showed here, though, as we didn’t realize the USB-A adapter was inherently capping our voltage. We want to give a huge thank you to Mr. Chitoku Yato at Nvidia for saving us on this with a custom USB-C to DC cord. The thing we’re all the most proud of is that we actually built what we set out to build! For our first hardware hack, with three different, distinct devices, across different frameworks, operating systems, and modalities of energy, this was an incredible feat. In the trenches of every challenge - Unity running slower than molasses on our rundown Intel Macs, bugs with multithreaded audio device access, bizarre audio sampling configurations provoking questions that not a soul on StackOverflow seemed to ask - we pushed past it and got things working. Each of us had a different moment we started jumping for joy: “I started going crazy when we first saw the visualization of the DOA (direction of audio)“ - Tyler “When I got to see a radar - like white dots on the circle - and the dots started to move when I did, I almost teared up“  - Samuel “After I spent 4 hours straight on our second Jetson Mini getting the Whisper model working with Cuda“ - Charan “I will never again feel happiness like I did when I saw the Android Debug logs on the Meta Quest print the first packet we sent“ - Sarvesh Hardware is called hardware because it’s hard and you can wear it. Getting through our first hardware hack gave us a lot of confidence both for building on this idea and pursuing new ones. Hardware:
Nuances of DC power conversion and portability
Mechanics of audio input processing 
Remote usage and sharing of graphic/audio driversSoftware:
Supporting machine learning for edge devices 
Cabled network communication 
Unity scripting and scene visualization The Meta Quest is the best affordable AR wearable right now, but eventually, we'd want the lightest-weight solution so people would be happy to use Ranger for long periods. We fiddled around with some AR glasses, but many smaller companies focus on treating the glasses as an external monitor, and the Meta RayBans do not have a display. This year, though, the new Meta RayBans will have a visual display, so we could easily swap our (lovably) bulky Quest for a sleek, non-invasive pair of shades. Our team has been chewing on this idea for a long time, and we want to develop this beyond our 36-hour sprint here at TreeHacks. During our development, we thought of a million insanely cool stretch goals to upgrade our current version, and each could be a project on its own. Special audio software that can perform Single Source Separation would let us transcribe multiple voices at once. A more advanced beam-forming location and tracking algorithm would let us intelligently classify objects over time. Porting over multilingual Voice Language Models to our brave little Jetson could expand this project globally, unlocking a new world of interaction for deaf people. This idea has a remarkable depth that we only scratched the surface of, and our intention in the future is todive deep.",,,,,,,"adb, edge, jetson-nano, linux, machine-learning, python, unity",https://github.com/YuanSamuel/Treehacks2025/
TreeHacks 2025,WiiWork,https://devpost.com/software/wiiwork-8u1x62,True,"Agent successfuly interpreting and fulfilling complex requests Agent handling large amounts of visual information WiiWork home screen WiiWork external device pairing screen WiiWork companion app audio input screen Everyone's excited about AI agents interacting with the world. Companies are pouring billions into development. But there's a problem: websites weren't built with AI in mind. Over this weekend, I sought to answer: what would it look like to build a website from the ground up to be AI-native? I built WiiWork solo over the weekend, drawing inspiration from web accessibility patterns. Just like screen readers use ARIA labels, I created a context-rich system where AI agents can understand and interact with web elements naturally. The core innovation is a Higher Order Component (HOC) system that wraps around UI elements and communicates their purpose, appearance, and location to the agent. This ""Agentic Web Component"" system is paired with a comprehensive global state manager (using Zustand) that synchronizes user and agent interactions in real-time and prevents errors from getting out of control. The agent system is especially interesting. Instead of using a one-size-fits-all approach, I built a multi-model router that classifies user requests into categories: Each category has its own carefully engineered system prompt, optimizing for both performance and cost. I prompt engineered all the models using Theo, and prompt engineering tool I built over winter break:https://trytheo.dev I believe this pattern of AI-native web development is the future. In a few years, I expect many websites will use similar patterns to make themselves agent-friendly. WiiWork is just the beginning, a proof of concept showing what's possible when we make websites with Agents in mind from the start. Built solo at TreeHacks 2025 using React, TypeScript, Convex, Zustand, and working about 25/36 possible hours. Check it out athttps://WiiWork.dev",,,,,,,"anthropic, convex, deepseek, elevenlabs, framer-motion, nvidia, openai, porkbun, react, shadcn, tailwind, typescript, vercel, whisper, zustand","https://wiiwork.dev, https://github.com/Alezander9/WiiWork"
TreeHacks 2025,Portable Braille,https://devpost.com/software/portable-braille-cdq8v5,True,"Literacy level of Blind and Visually impaired community even in United States is lower than 10%. And the reason is simple - Braille (tactile text) is not affordable and not accessible today. I followHaben Girma- the first deaf-blind person graduated Harvard Law school. She is a brilliant person, wrote books, influencer and advocates for people with disabilities rights. I saw how efficient she is in communications with people. However, she needs a special person who assists her by typing speech of her interlocutors, so it can be converted to Braille format and Haben can read it in tactile format. I believe that she could do even more if there is a device that could do handle it, keeping her communication private and without relying on assistance.",,"We 3d-printed an enclosure of the device on the 3d printer. Soldered Li-Po battery to battery-management-System module for correct charging/discharging and powered the microcontroller and Step-up boost DC-DC converter to power up braille module, which needs 200V for operating. Braille module gets data from microcontroller. There is a keypad of at least 6 buttons for typing braille combinations. Keypad also connected to the microcontroller. Microcontroller syncs with a phone over Bluetooth Low Energy.
We hacked RayBan|Meta glasses in analog way to connect in to Jetson Orin Nano.",,,,,"arduino, bluetooth, braille, c, esp32, ios, rayban|meta",https://github.com/Adil-Jussupov/PortableBraille
TreeHacks 2025,CyberDrive,https://devpost.com/software/cyberdrive,True,"I've always been inspired by Tesla's Full Self-Driving technology and its potential to revolutionize transportation safety. The prospect of having vehicles that can navigate autonomously, eliminating stress and enhancing safety, is incredibly exciting. I believe this future is extremely close, and Tesla is leading the way in making it a reality. The development of CyberDrive has been an enlightening journey that revealed how hard and complex computer vision and self-driving tasks really are. While I initially faced significant challenges implementing CNNs and other low-level image neural networks from scratch, I found success by pivoting to vision capabilities in large language models, which demonstrated superior performance. The final implementation leverages multiple LLM calls and incorporates advanced techniques such as mixture of experts, majority decision systems, and reasoner reward models to achieve robust driving scenario analysis. Below is a paragraph that goes more into detail about my final project. The system employs a sophisticated multi-stage pipeline that begins with cv2 extracting 5 equally-spaced keyframes from each video, overlaying frame numbers for reference, and storing them with semi-transparent labels using alpha blending. These frames feed into an LLM-based analysis process that leverages prompt engineering to focus the model's attention on specific visual elements relevant to the question at hand (like road signs, lane markings, or potential hazards) while enforcing a structured frame-by-frame analysis methodology. The system makes three independent GPT-4V attempts at analyzing the same frames, implementing concurrent processing with asyncio and rate limiting via semaphores to efficiently handle the API calls. Each attempt generates both an answer and detailed reasoning, which are then aggregated and evaluated by a separate LLM stage using a reasoner model. This meta-analysis stage systematically compares the observations and logic across all three attempts, looking for consensus, unique insights, and potential discrepancies. By having multiple independent attempts and a dedicated reasoning layer to synthesize them, the system can triangulate more reliable answers, catch details that might be missed in a single pass, and critically evaluate the strength of different logical pathways - ultimately leading to more robust and well-justified conclusions than would be possible with a single analysis attempt.",,,,,,,"amazon, async, gemini, google, openai, opencv, python, pytorch",https://github.com/KoaChang/TeslaMCQTreehacks
TreeHacks 2025,HorizonX,https://devpost.com/software/horizonx,True,"GPT 4o sucks!! It is built for the visually impaired,but only with wifi!!Inspired by the potential ofVideo Language Models (VLMs), we hope to bridge accessibility gaps. Over 2.2 billion people globally live with vision impairments, we builtHorizonXan AI camera,  a privacy-first real-time ""sight partner"" on mobile phone that help the blind individuals navigate crowded places. At the same time,  our AI allows people to report hazards (potholes, broken signage) to government simultaneously improving public spaces for everyone, which facilitate government's reconstruction efficiency and transparency.",HorizonXis a two-part ecosystem: ForUser- Mobile App for Visually Impaired Users: ForGovernments- Centralized Platform for Public Safety,"Running a VLM on a mobile device is a hefty task. After trying multiple approaches with MLX and WebGPU, we failed over and over to overcome the bandwidth and memory limits of our phones. We found a way to run a Linux emulator on the Galaxy S24 and mount it to local images, then installing core pytorch and tokenization libraries in a python environment to host a streaming API. We have a local endpoint hosted on the network to send the anonymized reported descriptions on a live map. This map also has information sourced from Sonar Research Pro calls to source live citizen complaints and create a centralized view. All of this is brought together with our locally hosted app, which—without connection—can navigate individuals around complex areas.",Extreamly difficulty system integration,Getting a multi-billion sized VLM to run on a mobile device!This is a significant milestone in local-first and privacy-focused AI inference. Since mostly it's impossible to run a 2B model on mobile phones.,Vim on a phone is easier than on a laptop,Speed up model on edge devices by further quantization or better integration with the current software. And provide more humanistic service to the special community and better verification experience for government.,"docker, javascript, nextjs, python, pytorch, react, webgpu",https://github.com/bsflll/treehack2025
TreeHacks 2025,WingNote,https://devpost.com/software/wingnote,True,"Nurses spend over4 hours per shifton documentation, leading to burnout, high turnover rates, and reduced patient care time. At the same time, most patients don’t understand their own medical notes, creating a gap in healthcare accessibility. We set out to buildWingNote—a fully integrated hardware + AI-powered solution that automates nurse documentation while ensuring patients receive clear, personalized summaries in their preferred language and at their reading level. With atap of an RFID badge, a nurse can start recording a patient interaction, and AI does the rest—transcribing, analyzing, structuring, and presenting the information in anEHR-ready format, while also generating apatient-friendly summary.","WingNote is asmart badge & AI-powered software systemthat automates clinical documentation for nurses and improves patient understanding. WingNote fully automates the documentation pipeline, improving workflow efficiency and patient care.","From hardware to AI-powered documentation, we built WingNote from the ground up.","Overcoming these challenges led to a fully functional, AI-powered clinical documentation system.","WingNote is a fully operational, AI-enhanced documentation system that works end-to-end.",This project pushed us to solve real-world hardware & AI integration challenges.,WingNote is just getting started—our vision is to make documentation completely seamless.,"arduino-nano, deepgram, gemini, li-po-battery, mistral-ai, next.js, openai, perplexity-api, python, rfid-reader, supabase, typescript, vercel, vercel-v0, xiao-esp32","https://github.com/bencullenn/wing-note, https://gamma.app/docs/WingNote-Smart-Hardware-for-Seamless-Nurse-Documentation-ggxl8whe2cff0lg"
TreeHacks 2025,Lumora,https://devpost.com/software/lumora,True,"Lumora Landing Page Lumora Dashboard Lumora Library Lumora Flashcards in Action Lumora Research Overview Diagram Your AI-Powered Collaboration Hub Lumora is a Personal Assistant Marketplace that transforms your Zoom meetings and chats into actionable workflows. By integrating with Zoom's powerful APIs, Lumora orchestrates intelligent workflows to automate tasks, boost productivity, and enhance collaboration—whether you're in education, enterprise, or beyond. Lumora represents illuminating collaboration—bringing clarity and energy to your workflows. In our day-to-day lives, we constantly encounter small tasks that add up to consume significant time—scheduling meetings, creating study materials, or researching topics. When these tasks accumulate, they can overwhelm our productivity. Lumora was born from the vision to automate these routine tasks, allowing people to focus on what truly matters. Within 36 hours of hacking, we created a powerful collaboration hub featuring: We've built 7 specialized tools across education, productivity, and social media: Lumora is open-source under theMIT License. Made with ❤️ by the Lumora Team | Revolutionizing the way we work, one task at a time ✨",,,,,,,"groq, next.js, perplexity-api, supabase, tailwindcss, typescript, v0.dev, zoom-api",https://github.com/ShaneYokota72/Lumora
TreeHacks 2025,"Noteworthy: Visualize, Capture, Remember.",https://devpost.com/software/noteworthy-visualize-capture-remember,True,"  We've all been in lectures where the professor quickly sketches a vague diagram, expecting instant understanding, then erases the board while moving onto a completely new topic. Most students struggle to keep up, especially in technical subjects wherevisualizations are crucial. While we have had many text-to-speech and text-to-image integrations in educational resources, we rarely seespeech-to-image transformations. Our team wanted toredefine this learning experienceby building anAI-powered lecture companionthat dynamically generatesreal-time visualsbased on lectures. This tool also provides students with a comprehensive learning platform where their notes are efficiently stored and organized, as well as allowing students to access summaries, Q&As, chatbots, and other useful resources. Throughout the development process, we discovered several key insights about multimodal communication, efficient information transmission, and integration of AI models: Our project utilizescutting-edge AI technologiesto create anintelligent, real-timeassistive educational application. The development process unfolded through these innovative stages: We engineered aspeech recognition systemby: We createdlecture visualizationsby: We implemented aversatile digital workspaceby: We enhancedlearning opportunitythrough: We created a collaborativelearning environmentby: Building an advanced AI-powered tool in a limited timeframe posed several challenges: Education should beaccessible, engaging, and tailored to every student’s needs. Our tool: Our AI-powered lecture companion aligns perfectly withZoom’s Education Grand Prize criteria: This is just the beginning. Our AI-powered lecture companionredefines how we learn, connect, and grow—ensuring that every student, no matter their learning style or background, has the tools they need tosucceed in the classroom and beyond. 📹 GitHub:https://github.com/arushisharmaa/stanford-tree-hacks-📹 Youtube Video:https://youtu.be/m_1f7-KvGmQ",,,,,,,"bash, gemini, html/css, javascript, ngrok, node.js, openai, p5.js, perplexity, playwrightapi, postman, python, react, zoomapi",https://github.com/arushisharmaa/stanford-tree-hacks-
TreeHacks 2025,FlowPilot: Your Agentic Co-Pilot,https://devpost.com/software/flowpilot-zoom-agent-that-automates-your-workflows,True,"FlowPilot listens, understands, and executes, transforming Zoom meetings into Slide decks and Notion workflows. No more lost action items - just approve and watch it execute. FlowPilot provides amazing collaboration opportunities for sales, engineering, and product teams alike. Having been apart of workflows at fast-growing startups and large corporations, our team grew increasingly frustrated with the significant friction caused by meetings. This includes the amount of action items discussed in meetings that often get forgotten, tediously written, or executed. We noticed that while tools like Notion excel at documentation and Google Workspace handles scheduling, there wasn't a seamless way to transform meeting conversations into automated workflows. FlowPilot bridges this gap by creating an intelligent layer that turns verbal commitments into automated actions.","FlowPilot transforms Zoom meetings into an autonomous productivity engine. By leveraging DAIN's advanced agent framework, our system:",,,,,,"dain, docker, express.js, node.js, rtms, websockets, zoom","https://github.com/cyu60/dain-agent, https://github.com/CarterCote/flow-pilot, https://flowpilot.us/, https://v0.dev/chat/fork-of-presentation-app-PJSbvsry9Gc, https://ai-generated-presentation.vercel.app/?id=83dc8131-c0a6-4863-a2b2-c9147c1d3588"
TreeHacks 2025,ZoneOut,https://devpost.com/software/zoneout-atz5pe,True,"We've all been in classes or meetings where we ZoneOut, even if for a few seconds, and came back to see that the cure to cancer has been invented! Inspired by attending online lectures this Friday (we're dedicated students), we found out that this isn't as uncommon as you'd think. As a matter of fact, the average retention rate of a student after just 45 mins of online learning is just61%, which isHIGHERthan the average person. Moreover, the average drop in engagement is87% (scaling exponentially!)on average, in meetings with larger sets of participants.","ZoneOut utilizes textual, visual & audio contexts from meetings, enabling our AI assistant to teach, revise & explain any concept,In-Depth & in Real-Time, to users to maintain higher levels of retention, productivity & engagement.","We built ZoneOut with a complex, yet execution directed architecture, developed completely onWindsurf. We used theZoom APIto connect the client to a Real-Time Media Server (RTMS) via a Handshake protocol, and consequently sampled data at microintervals as well as based on when a sentence/section of an idea being discussed finished. We collected textual data from the chat, audio data via live transcripts and visual screensharing/camera data with help from Zoom's API. We then usedOpenAIfor embedding the text & images with Chain-Of-Thought (CoT) Reasoning to keep context well-fitted and connected, independent of context window sizes and to keep images & text associated with one another. We also used parallel computation to allow us to index this data usingChromaconcurrently, associating images with concepts in both audio & textual formats in different timestamps. Finally, we used a similarity search RAG system with ChromaDB for the audio/transcript & textual data, and a vision-based RAG system onColPali (VLM), which we accelerated using a caching system that we developed, allowing us to use it without reloading it into memory again & again. The outputs of both RAG systems are then passed throughOpenAI's APIto format it nicely. We also optimized sampling parameters to avoid hallucinations caused by excess external information or misunderstanding contextual information. We then send this data back to the client, who's now back in the loop of everything that's happening!","Originally, the RTMS faced issues with streaming audio & video. After a lot of debugging & troubleshooting, we found & cured the error by handling edgecases through intensive vision programming back & forth, sending our sample code to the Zoom team so they can debug other teams. Then, our VLM workflow turned out to be too slow as the VLM was being loaded into memory repeatedly. So again, after coding a lot of reacharounds, we finally implemented our own caching system to supercharge our VLM, which now works with various forms of handwriting effectively. We also faced hallucinations wherein the model knew information it should not, and misinterpreted information it had. We cured this using indexing & CoT, to reach the product we have today!","This hackathon has been a proud technical moment for all 3 of us. Our achievements stem from our challenges. We very quickly figured out the edge case of professors writing on whiteboards, both virtual & real, instead of explaining things like equations. So we developed a multi-language model workflow to work around. Another proud accomplishment was improving the Zoom RTMS repo, as we were the first people that figured it out, turning our curiosity into open source contributions in Zoom's repos. Next was integrating a complex parallel workflow to interpret & contextualize images, text & audio data altogether, particularly because of how LLMs & VLMs can be very funky sometimes. After that, was when we implemented our own caching system to boost our VLM system, after having faced a barrage of vision problems. Finally, was our creative use of prompt engineering, context windows & frontend-backend structuring for Windsurf to swap between entirely different frontend frameworks (HTML/CSS & React) & even simple backend worflows without breaking the frontend or the backend, letting us build very quickly, despite initial samples & software not being completely compatible, causing issues in the webSockets & handshake protocol, amongst other incompatibility issues.",,,"chromadb, colpali, openai, rtms, vlm, zoom-api",https://github.com/NxtGenLegend/TreeHacks
TreeHacks 2025,BEF,https://devpost.com/software/beef-xk5pif,True,"Workflow of part 1 of project PPG WARNING SYSTEM EXAMPLE(Classification trained model, yellow indicates warning region) PPG ECG RECONSTRUCTION EXAMPLE(Reconstruction trained model) Classification metrics beautiful mockup (overestimated ability to turn into frontend) B.E.F. streamlines multi-user wearable data—such as workout sessions, daily activities, and sleep logs—into a single, queryable dashboard. Its primary users include leaders in high-stakes environments, like military officers or coaches, who need insights on team fitness and risk without being medical experts. By integrating Terra’s HIPAA-compliant APIs, the framework continuously pulls health metrics, then leverages an LLM for natural-language querying. This allows a colonel or coach to ask, for example, “Who’s most fit and who’s at highest injury risk?” and receive an immediate, actionable summary without parsing raw data. Beyond readiness assessments, B.E.F. provides real-time cardiac monitoring: when our 1D CNN flags potential arrhythmias, a BiLSTM model reconstructs a single-lead ECG, offering medical professionals a more familiar signal for follow-up. The entire pipeline—from data ingestion to alert generation—runs securely on edge hardware, ensuring compliance, minimal latency, and fully localized control of sensitive health information. While Terra offered a powerful aggregation tool, several challenges emerged: Due to these limitations, the project pivoted from expecting a fully real-time data stream to usinghistorical and periodic updates. We implemented scheduled data fetches (e.g., every few hours) rather than a continuous stream. This approach was sufficient for generating meaningful summaries and answering questions based on past data, but it also meant that some real-time use cases (like instant alerts on abnormal metrics) could not be fully realized. TheMistral AI 7Bmodel is a 7.3-billion-parameter large language model that we chose as the core AI engine for the Health Command Center hosted on a singular GPU in Rayan's poor overworked GPU. This model was selected because it offers strong performance relative to its size, is open-source, and is designed for conversational, instruct-tuned tasks. Our system aim to use aRetrieval-Augmented Generation (RAG)approach by integrating the Mistral 7B model with our Terra dataset stored in a Pinecone vector database. Here’s a breakdown of the process: Data Indexing in Pinecone: Query Embedding: Retrieval from Pinecone: Response Generation with Mistral 7B: Our system addresses two primary goals: We trained a1D CNNon a PPG dataset (70% split for training) to determine whether a given PPG segment indicates an abnormal event—such as frequent premature ventricular contractions (PVCs) or atrial fibrillation. Key highlights: Once this model flags an abnormal segment, the system triggers the reconstruction step described below. When the CNN identifies a suspicious arrhythmia, we reconstruct asingle-lead ECGfrom the same time-aligned PPG segment, offering a more familiar signal for clinical review: Workflow: Architecture: A two-layer bidirectional LSTM (hidden size = 64) with a linear layer outputting a single channel at each time step. Loss Function: Mean Squared Error (MSE) between the reconstructed ECG and the actual ECG ground truth. Efficiency: Early stopping and Adam optimization keep training fast and prevent overfitting, making it practical to fine-tune per user on consumer-grade hardware.Note-paired-PPG-ECG database sourced from here By combining arrhythmia detection (1D CNN) with on-the-fly ECG reconstruction (BiLSTM), our approach ensures: When an anomalous event is detected in the health data from Terra—such as irregular heart rhythm patterns—the system immediately initiates an in-depth analysis by reconstructing the ECG signal. This process involves several critical steps: Event Detection and TriggerThe system continuously monitors incoming wearable data for any irregularities or predefined warning signs. Once an event is detected, it triggers a signal that activates further analysis. Dedicated GPU-Powered EndpointAn endpoint hosted on Rayan's desktop, equipped with an NVIDIA GPU, is designated to handle the computationally intensive tasks like Fortnite. This setup ensures that the ECG reconstruction and subsequent classification are performed quickly and efficiently. AFIB Classification InferenceAdvanced machine learning algorithms (ground-up made deep neural networks optimized for time-series data) analyze the signal. These algorithms are specifically trained to identify patterns associated with atrial fibrillation (AFIB) as well as other cardiac anomalies. The outcome is a classification that indicates whether the detected patterns correspond to AFIB or another condition. ECG ReconstructionInstead of relying solely on raw wearable sensor data—which may be incomplete—the system reconstructs a high-fidelity ECG signal when an alert is processed. This process integrates multiple data streams to generate a detailed view of the heart's electrical activity, providing a clearer understanding of potential abnormalities. Actionable InsightsWith a high-quality ECG and a corresponding classification result, the system is able to offer actionable insights. If AFIB or another serious cardiac condition is detected, the system generates alerts for further review by medical professionals, ensuring timely intervention. This integrated, GPU-accelerated approach enhances the overall reliability and depth of our health monitoring system, transforming raw sensor data into critical, actionable medical insights. Throughout the development of the B.E.F., we encountered several significant technical challenges, which led to valuable learning experiences and informed our future directions. The design of the user interface (UI) aimed to create an intuitive, conversational experience where users could query their health data as if talking to a personal health assistant. Query:""What was their average heart rate this week?""Response:""This week, their average resting heart rate was about 72 bpm. It remained stable throughout the week, ranging from 70 to 74 bpm, which is consistent with their usual readings."" Query:""Did they meet my step goals recently?""Response:""Over the past 7 days, they met or exceeded your daily step goal on 5 days. Their best performance was on Thursday with 12,345 steps, while two days fell slightly short at around 8,000 steps."" Query:""How has the teams sleep been in the last month?""Response:""In the last month, they averaged approximately 6 hours 45 minutes of sleep per night. There were about 10 nights where they achieved the recommended 7–8 hours, though a few nights dropped to around 5 hours. This suggests a trend of slightly below-target sleep on many days."" Query:""Give me a summary of the teams workout performance.""Response:""In the past two weeks, they completed 5 workout sessions. They ran a total of 15 miles with an average pace of 9 minutes per mile, burning roughly 2,300 calories. Notably, their longest run was 5 miles on Tuesday, and their pace has improved by about 30 seconds per mile compared to two weeks ago."" https://www.mdpi.com/1424-8220/21/1/156",,,,,,,"cuda, python, pytorch, react-native, terraapi, vercel, vercelv0",https://github.com/shafink11/ppg_project
TreeHacks 2025,WatchDog AI,https://devpost.com/software/watchdog-ai,True,"Crypto AI - especially AI agents - has been a hot topic within the industry with many players trying to identify new ways to incorporate AI agents into their builds. However, it seems that the most promising fits are still within that of memecoins - another hot topic this cycle. Memecoins are fun. But rugs are not. Yet, it seems like there is no way to stop rugs from happening since it is financially advantageous to do so... or is there? ps another inspiration is the web3 track ""prompt"" to use AI agents + AVS to do something crazy So why is this crazy. Because this essentially is a way for a person to do good, make money and stop memecoin rugs at the same time. You get to make money of the rugger (be it hard rug - liquidity removal or soft rug - sell offs). In this project, we align fun, profitability and security.","WatchDog AI in its essence is a memecoin anti-rugger, solving the problem by removing the financial incentives. The AI agent is designed to sit on top of a potential AVS, obtaining information from 1) DEX to analyse the riskiness and the possibility of rugs, 2) mempools for transactions and 3) operates as an MEV trading bot. 1) Memecoin evaluator
The AI agent itself possesses the ability to evaluate the potential of future rugs based on past transactions and other data about the memecoin. It provides an evaluation on a risk scale. If the memecoin is deemed to be ""risky"" WatchDog AI would start listening to the mempool for potential malicious rugs. 2) Mempool listener
WatchDog taps into the mempools of coins to identify suspicious transactions in real time. It could be able to detect rug attempts and react instantly by sacrificing gas fees to frontrun the rug attempt. 3) MEV attack Autotrader
Identifying the rug attempt (for instance a large sell off), the agent could decide to frontrun the transaction with a large short and buy back after the the rug attempt has taken place. (Or even worse, find a way to obtain a flash loan to create a further leverage on the trade. ) This would greatly reduce the monetary gains of rugger. 
Else if the rugger attempts to remove liquidity, the bot could remove liquidity ahead of the rugger such that there are insufficient liquidity for the rugger to remove resulting in their loss before adding liquidity back into the pool.",,,,,,"python, solidity, typescript",https://github.com/BdNWoG/watchdog-ai-final
TreeHacks 2025,Cipher Shield,https://devpost.com/software/cipher-shield,True,"The inspiration for our project came from a problem based approach to ideation. We as students fill out countless job applications, and each one asks for demographic data. Most simply opt out as there seems to be no tangible benefit. In fact, there seems to only exist a potential downside: that it may be illegally misused to negatively affect your application. And yet, demographic collection does serve a large social purpose for society, specifically for detecting trends that may expose discrimination. In numerous cases, including employment, healthcare, loans, and credit cards, collecting demographic data is actually mandated by the federal government. We figured that in the era of decentralization and advanced encryption schemes, there is no reason for misuse of this information to even bemathematically possible.","Cipher Shield acts as a trusted third party, providing completely secure data aggregation services via the blockchain. We empower companies to extract aggregate statistical insights from homomorphically encrypted data, thus making it impossible to profile individuals based on their protected data.","Blockchain: The chain is a critical component of our architecture and business model since it accelerates trust in our services. Every aggregation request is stored on the chain, creating an immutable, public record. Before any homomorphic computation is initiated, our smart contract performs critical on-chain validation - a transparent verification that ensures enough records exist to make reverse engineering mathematically impossible. If there aren't enough data points to maintain statistical anonymity, the contract automatically rejects the request. Once the request is validated and the computation is complete, the final aggregated result is also stored on chain, creating an audit trail of our services that companies can trust - we can't nullify data because every step is permanently and publicly verifiable on the blockchain. Dealing with advanced encryption schemes and blockchain operations, our business logic was written entirely in C++ and solidity. We leveraged OpenFHE homomorphic encryption operations and key generation. As a B2B product, our demo needed to include an entire demo business to use our product, which required a whole other backend+frontend pair.","We ran into a number of challenges while building the product: 
Implementing our design of the homomorphic encryption in OpenFHE library, specifically with the dual private key setup which did not come as a out of the box solution.
Writing the implementation for the backend responsible for the data aggregation in C++ and facilitating the key exchange with correct serialization, deserialization.
Managing the large payloads of ciphertext in the communications between company and server and storing ciphertext.","We started off as 4 complete strangers with incredibly varying interests and skillsets and we were able to collaborate seamlessly. Together, we created the first ever implementation of a split key homomorphic encryption scheme that’s also verified through blockchain technology. We learned a lot about homorphic encryption as well as blockchain. We were able to intergrate the full stack of the application together and make all of the parts of the system working locally, with all of the Customer Data, Company/Organization, and Authority.","We learned about how to implement Homomorphic Encryption libraries and packages and alter their fundamentals to be able to work with split keys. We learned about the different ways that data security and insurance is important in B2B SaaS solutions, as well as potential business market ideas that could benefit both business and consumer at the same time. We also learned about the integration of Web-2 with Web-3 technology as well as integrating visualizations on the full-stack end despite coming in with very limited knowledge about blockchain. Moreover, we also learned about creating front-end web applications as well as a balance of creativity, inspiration, and feasibility when implementing a large scale and ambitious project. We also learned about the importance of Web-3 in web-application security.","We believe Cipher Shield should mediate the demographic collection and protected data collection processes of every company in America, and we believe this will have transformative societal outcomes in mitigating discrimination. The data aggregation capabilities of our scheme extend far beyond single company aggregation, but can even be used to securely combine hospital datasets. To encourage companies to adopt our tech, we would also want to create an easy to use SDK for organizations, companies, and industries less knowledgeable on the prospect and understanding of Blockchain and encryption. Furthermore, the ability to directly create a form or application directly on Cipher Shield and embed it into any company or organizations’ website directly through a Web Embedding that CipherShield uses would be essential to seamless adoption.","c++, node.js, openfhe, react, solidity",https://github.com/zoraizmohammad/treehacks-2025
TreeHacks 2025,The Giving Tree,https://devpost.com/software/the-giving-tree-x0a5hk,True,"As natural disasters and social crises fill our news feeds today, the need for humanitarian aid is at an all time high. With charitable donations sometimes being the only thing many charities rely on to combat their respective issues, for causes such as natural disasters or international conflicts, timely donations make a real time impact. However, not all types of donations are helpful. Without knowing what a disaster-stricken community might need, donations of material goods can quickly cause problems, classified as the “Second Disaster”. Our team discussed what allows rapid monetary transactions between a donor and a charity, through borders and roadblocks that material or cash donations could face, cryptocurrency. Cryptocurrency, once sent from one wallet to another is immediately a liquid asset one can use. Cryptocurrency is essential in areas where it is currently impossible to physically access banks and their services. With its speed, for charitable causes that require immediate reaction, crypto donations could contribute in helping hundreds to thousands and even millions of people. For example, just within seven days after the horrid Earthquakes in Turkey and Syria, AHBAP, a local non-profit, managed to raise 4.9 million dollars through crypto donations. It is also reported that around 20% of non-state aid to Ukraine was done in crypto currency. But nobody has time to constantly check the news, find a charity related to a topic, and donate. We wanted to come up with the most efficient way to assist users with crypto charity donations while being responsive to urgent news that relates to causes that they wish to donate to.","The Giving Tree is the most efficient and intelligent charitable donation portfolio manager. It takes the user’s mission statement (topics that they are passionate about) and creates their category portfolio. With live RSS feeds from news websites, our app tracks every article that it deems relevant to charity. It then sorts these articles into categories of impact. With these articles, we find charities that we could donate to and update our user’s charitable donation portfolio in real time to ensure that the user is making the most efficient and intelligent donations.","The Giving Tree is built with a Flutter front end, Python FastAPI backend scripts, a database layer including both Postgres and ChromaDB cloud databases, as well as a series of AI agents to help us cater a collection of charities to most effectively handle user’s crypto. Our chroma databases use similarity search with the embeddings to provide the framework in sorting and filtering out news articles and charities. We scraped charity data from a series of websites, gathering wallet addresses, names, and descriptions to help determine donation portfolios. Additionally, we are constantly scraping RSS feeds for new articles, which are filtered by relevance and further processed through a similarity search to find the most relevant charities. This information, along with user data, is passed to smart contract functions to execute and record transactions on the block chain.","Coming into the project, none of us had any experience developing with Flutter before nor have tried Web3 on Flutter. Because of this, there was a relatively steep learning curve when it came to our frontend, especially when integrating with AppKit, a package that was relatively unused in this context. We struggled with authentication of crypto wallets due to the limited documentation and community suggestions as well as generating and tuning prompts for the AI agents that would lead for us to get the responses that we thought were “correct”. Because our project had so many different parts to it, it was also a struggle trying to piece them all together. While we had mapped out our system layout, it was still a struggle bringing the full integration to life and we worked close to the deadline to connect everything.","Having never really done Web3 development in the past, creating an application involving blockchain and integrating it with a frontend that none of us have worked with before was a difficult yet fulfilling challenge. We also had a relatively complex workflow involving our data processing and filtering, and being able to not only scrape everything but smoothly integrate our results with AI agents and vector databases was something we were proud to accomplish. Trying to wrap our heads around how everything ultimately tied together as well as highlighting the necessary relationships and interactions required collaborative system design effort from everyone, and we had very productive conversations trying to optimize our structures.","As a team, we learned about developments in Flutter and creating our own API endpoints to support integration of backend development with Flutter. We learned how to utilize software IDE’s that integrated AI flawlessly to power our development process. We experimented and challenged ourselves to learn more about agentic workflow, letting AI make its own decision with tool calling, and classifications evaluations in order to fine tune our prompt generation. We also had the opportunity to dive deeper into Web3 and blockchain development, taking time to experiment with smart contracts and crypto wallets.","We want to continue integrating other cryptos into our app so that crypto donations become accessible to anyone who has ever owned a crypto wallet. We also want to expand the number of charities that will accept crypto donations, maybe once they find out how effective and powerful crypto donations could be, they’ll join us for our mission!","beautiful-soup, chroma, chromadb, flutter, openai, perplexity, postgresql, python, rss, web3",https://github.com/emilyjsun/treehacks25/tree/main
TreeHacks 2025,Moods and Metrics,https://devpost.com/software/moodmap-ai,True,"In a world where stress and anxiety are increasingly prevalent, we were inspired to create a tool that not only helps individuals understand their emotional states but also empowers them to take control of their mental well-being. Mood and Metrics was born out of a desire to bridge the gap between technology and emotional health, offering a way for people to easily visualize and comprehend their emotional levels through AI-driven insights.",Mood and Metrics offers three analysis modalities:,,,,,,"d3.js, fastapi, gemini, huggingface, python, pytorch, react, tailwind, three.js",https://github.com/corey-shen/stress-level-audio-analysis
TreeHacks 2025,Vigilante,https://devpost.com/software/vigilante-xkf7s9,True,"Today, misinformation spreads quicker than you can say “retweet.” Staying informed has never been more crucial—or more challenging. Social media platforms can be powerful tools for connection, but they’ve also become battlegrounds for accuracy, where misleading claims and manipulated information thrive. We built Vigilante to change that. In addition to being driven towards working on projects that solve real-world problems, each of our teammates are incredibly passionate about human-computer interaction—how technology and innovation overlaps with the human experience. We came across a paper published in the journalSciencewhere researchers leveraged AI chatbots to combat belief perseverance (DOI: 10.1126/science.adq1814). As solution-centric engineers, we saw endeavors like these as validation that AI can be a powerful tool to fight entrenched misinformation and cognitive biases. We wanted to bring that power to the masses, leveraging AI to combat misinformation and improve media literacy in doing so.
We chose to focus on misinformation on social media because it’s one of the defining challenges of our lives. Social media has the power to shape public discourse and individual belief systems. When placed in the wrong hands, it can be used as a tool for propaganda, sowing division and manipulating public opinion. For a demographic like ours, these points feel especially poignant and relevant. We were motivated by the idea of building a tool that could not only combat misinformation but also help foster media literacy—empowering users to think critically about what they read and engage with online.","Vigilante provides users with areal-time fact-checking workflowthat allows them toeffortlessly identify potential misinformation. The user’s entire Twitter feed is anonymously ingested and, using agentic AI workflows, verifies in real-time every single post. Posts are stripped of evocative or emotional language and are broken down into their individual claims; these claims are then verified using Perplexity's Sonar API to ground our analysis in up-to-date sources. We do not flageverytweet you see as misinformation—rather, we carefully filter outopinionsfromstatements,satirefrommisinformation. We also highlight when posts (and even specific claims) are, in fact, accurate and supported by evidence. Using Vigilante, in addition to staying informed through your feed, you also help contribute to a crowdsourced database of tweets, their metadata, and analyses. This data lives on a dashboard that visualizes ongoing trends in misinformation on social media using data science and NLP techniques. This publicly available dashboard provides its audience with insights into what topics to be wary of online, all the while helping to tune your media literacy and stay on top of misinformation.","Vigilante exists in 3 parts: a Chrome extension, the fact-checking engine, and a data visualization dashboard. The fact-checking engine, as well as routes for interfacing with our database stored in Supabase, is exposed via a FastAPI backend. The FastAPI backend supports data queries and analysis for the dashboard, and the model routing and plumbing for our verifier. We deploy and expose our backend for production on Koyeb. The Chrome Extension is built using Plasmo as a framework, with React/Tailwind for frontend. We implemented details like caching tweets client-side and using Plasmo messaging to bypass standard Chrome popup styling. The dashboard uses Next/React/Tailwing as scaffolding, and D3 for data visualizations. We also support real-time updates on the dashboard using a combination of technologies. For regular updates to overall statistics (i.e. number of tweets and claims analyzed), we leverage the Stale-While-Refresh technique through the useSWR hook in React. For live updates on new tweets flagged by our system, we loop in Supabase real-time messaging and listen for table insertions in the database. The fact-checking engine is built on top of multiple AI technologies. To ensure the best UX as possible, it was imperative the system exhibited the lowest amount of latency as possible. To accomplish this goal, we leveraged Groq’s blazing fast inference API. We use 3 different language models across the system, each of which excelled in different tasks:gemma2-9b-it,​​llama-3.1-8b-instant, andllama-3.2-11b-vision-preview. We specifically use the vision model to endow the system with multimodal capabilities, allowing it to handle posts with text, images, and/or video, since each of these modalities are critical to a post’s core meaning. To evaluate a post’s claims, we leverage the new Perplexity Sonar Pro model to both search the web for evidence and provide an explanation for whether a claim is accurate or misleading. This process is also completely parallelized for each claim embedded within the text of a post, allowing for as fast of a system as possible.","Our team is all super passionate about this problem space, but the initial hours of our work on this project was somewhat fragmented as we weren’t aligned on specific details. We went on a variety of fruitless goose chases, and didn’t settle on a specific vision for our project until we were deep in the weeds of it half-way through. (For instance, we toyed around with the idea of incorporating datasets of tweets from past election seasons and had even built out some visualizations, but ultimately chose not to pursue that path.) Ultimately though, we were able to rally and refocus on our vision—moving away from disorganized prototyping to a specific actionable gameplan. One specific issue we ran into was brainstorming visualizations for the dashboard. We all had different ideas for how we envisioned the dashboard, and we initially found ourselves working independently on designing visualizations and developing data processing pipelines. However, when we synced up, we realized our visions had all largely diverged from each others’. Though it isn’t inherently bad to have competing ideas, it was difficult for us to reconcile a common vision once we had all individually put so much thought into our own. Once we put our heads together, however, we developed a cohesive plan for not only this dashboard, but our project altogether. This taught us the importance of early alignment and continuous collaboration—syncing frequently and creating a shared vision early on can save time and reduce duplicated or divergent efforts. A more technical issue we encountered was not being overly skeptical of (what we thought were) good results from our system. The fact-checking system went through numerous rounds of iterations (archived as a Python notebook in our repo) and we had a working prototype that we implemented into our FastAPI backend. However, after performing a refactoring for additional features, we accidentally removed the call to the Perplexity API, a critical functionality, without us noticing. We took for granted that the system was still flagging tweets as “misleading” without actually checking to see whether its explanations were up to our standards. Thankfully we were able to catch this issue and resolve it in time before it got buried even further. We learned to not take surface level results for granted and to look deeper behind the scenes—which is quite fitting for this project altogether.","One of our biggest accomplishments was our ability to iterate rapidly and improve upon multiple aspects of our system within a short timeframe. We took on significant technical challenges and addressed performance bottlenecks in our backend with creative, scalable solutions. Each iteration brought meaningful improvements, and we’re proud of how we managed to adapt and refine the system as we encountered new challenges. Another accomplishment were particularly proud of is that we were able to incorporate multimodal fact-checking capabilities. “An image is worth a thousand words” is particularly true in the context of social media, so it was important that our system wasn’t restricted to only text input. However, this was not a simple challenge to solve. We chose to integratellama-3.2-11b-vision-previewinto our system to analyze claims that extend beyond simple textual information. This is a feat we consider a major leap forward in combating misinformation in multimedia-driven social platforms.
On another note of personal fulfillment, we’re proud of how we built a project that aims to address an incredibly pressing issue in today’s world. Navigating misinformation is shaping up to be one of the defining challenges of our lives; as social media platforms explore stripping away existing safeguards to prevent the spread of this sort of content, individuals have to be extremelyvigilantwith what they engage with online. Vigilante was built with this in mind, and we’re proud of how effective of an initial solution we’ve come up with to this problem.",,"After we recover from our sleep-deprived past 36 hours, we plan to publish the Vigilante Chrome extension to the Chrome Webstore. We also aim to continue to maintain the crowdsourced dashboard to effectively present the anonymous data we collect to the general public. Beyond what we currently have built out, we plan to extend Vigilante’s functionality to mobile platforms. Apps for iOS and Android would provide fact-checking on-the-go and bring our technology to another form factor. Moreover, many users solely engage with social media on their phones, so it is important we can reach this audience as well. We are also very interested in how we can better leverage the data our users provide for personal and community benefit. Meta and others have begun to cancel their fact-checking programs, and users often report negative experiences with features like “community notes” on X/Twitter. We suspect that we can leverage the data we’re collecting to reveal larger trends in misinformation and hopefully begin to prevent it before it spreads. Altogether, we envision Vigilante as a tool that not only combats misinformation but also fosters a culture of critical thinking and informed engagement online. We’re excited to continue building after Treehacks, refining and scaling this platform to make a meaningful impact in the fight against misinformation.","d3.js, fastapi, groq, next, perplexity, plasmo, postgresql, python, react, supabase","https://github.com/dav1dbai/Vigilante, https://www.usevigilante.com/"
TreeHacks 2025,Superclassroom,https://devpost.com/software/superclassroom,True,"Every since I could remember, I've happily dedicated countless hours to helping my brother, Zac, digest content and prepare for his exams. Zac was very naturally smart, but always struggled with learning the way that the US education system wanted him to. As it turns out, he wasn't alone -- an overwhelming 74% of students nationwide are dissatisfied with means in which they are provided to learn and absorb concepts, and are proven to perform upwards of 200% better when introduced to more adaptive ways of learning. But it gets bigger than that -- Zac was also diagnosed with dyslexia, making his journey towards academic success an even steeper climb. The support for dyslexia in the US remains a frightening concern, as over 95% of diagnosed dyslexics claim that they do not receive the appropriate accommodations in school, amongst the rigid education system. Inspired by my brother's struggles and the infinite curiosity that lie in the different ways that human beings love to learn -- I wish to create a platform that is able to adapt to each student's individual needs.","Superclassroom is a student-adaptive exam preparation and learning material generation tool designed to adapt to each student's needs. Students using the platform will upload course material relevant to their upcoming exam or topics that they want to study, establishing a personalized knowledgebase for each course the student is taking. Superclassroom then uses this information in a RAG pipeline to generate highly-relevant course study material, such as comprehensive notes, topic summaries, adaptive flashcards, and practice exams. There is also a space to leave notes about the student's profile, which is used in the RAG pipeline to generate more personalized results and accommodate disability.",,,Getting a tool that is functional and something that my younger brother can actually use is a huge accomplishment for me.,,Keep building and eventually develop a platform that will change the lives of many students worldwide!,"intersystems, iris, mistral, rag, vectors",https://github.com/airjlee/superclassroom
TreeHacks 2025,Verify,https://devpost.com/software/factchex,True,"As young, first-time voters, almost all of our political awareness and understanding comes from our engagement with social media. Although information access is easier than ever, attaining unbiased information has become nearly impossible in today's highly divisive political environment. From charged language that misrepresents reality to outright lies, a solution that could provide an accurate and unbiased assessment of online content could remove this misinformation fog currently clouding social media.","Verify is a Chrome web extension that can analyze a tweet with just a click of a button, presenting an unbiased assessment of the post's subject matter, while citing key sources and presenting an accuracy score. Although intended primarily for political awareness, the tool can be used to assess the accuracy of any tweet, from debunking false sports trades to fact-checking history fun facts. We were motivated to add this on X specifically, as the platform only has community notes as a check against misinformation, which is not available on most tweets.","Our team built Verify by building a Chrome-integrated UI with HTML and CSS, and a JavaScript backend that connected to Perplexity's API for accurate searching and tweet language processing. Our team segmented work between front and back-end development based on our personal interests and experience.  At a high level, Connor built our front end, learning and implementing key web design principles on the fly. Meanwhile, Jacob built out our back end in JavaScript, ensuring accurate tweet processing and API calls. Jonas handled complexities related to API calls and ensured well-formatted, balanced, and customized prompt responses with Perplexity. Kevin focused on connecting all the pieces, and integrating the back and front end while supporting UI/UX development.","The biggest challenge we faced was developing the backend, particularly the messaging between service workers and Chrome extensions. To resolve this problem, we all got hands on deck to resolve it and pushed through several hours of debugging line-by-line, learning a ton about threading and multiprocessing.","We're incredibly proud that we were able to come up with and implement a tool that we would personally use in our day-to-day lives. As first-time hackers, we loved seeing our work turn from just an idea into a product that we believe can make a genuine impact on misinformation. We're also very satisfied with the technical progress we achieved.","We all gained significant technical experience, specifically improving our ability to develop in a full-stack environment and to integrate API calls to leverage the immense impact of LLMs. We also learned how severe the misinformation gap is on social media while implementing our product. When testing our tweets on popular creators on both sides of the political spectrum and even on history fun fact pages, we realized how many social media users digest misinformation without thinking deeply about whether the information they're reading is verified.","We are all-in on this project idea and its impact. We will expand upon Verify to integrate the product on other social media platforms like Reddit, Instagram, and YouTube. We believe the biggest opportunity will be to analyze and summarize video content on platforms like YouTube, reducing both misinformation and time spent. Ideally, we would also be able to implement Verify on iOS and Android to further expand its reach by integrating it within apps.","css, html, javascript, json, perplexity",https://github.com/jcub-gold/TreeHacks_25_Project
TreeHacks 2025,SynapseAI,https://devpost.com/software/synapseai,True,"Driven by a shared passion for healthcare, our team sought to tackle this track. When further research was conducted, we had the shared consensus that there was a disproportionate level of emphasis on soldier’s productivity during service, but rarely on catalyzing their transition back to civilian life. Our veterans are twice as likely to develop PTSD compared to the rest of us, with 87.5% experiencing at least one traumatic event during their service. To make matters worse, nearly 20% have suffered traumatic brain injuries in the past two decades. These invisible wounds often leave them struggling to communicate, manage stress, or reintegrate into everyday life. Current rehabilitation methods have proven to be ineffective. Up to two-thirds of veterans still meet the diagnostic of PTSD post-treatment, and dropout from treatment ranges from 25 to 48%. A recurring pain point for those undergoing rehab is the inability to express themselves, which can lead to frustration, avoidance, and disengagement from rehab. Heightened stress levels is further exacerbated by cognitive impairments from conditions like Aphasia that make verbal communication difficult. This inspired us to innovate in the intersection of empowerment and expression. A tool that both empowers and catalyzes veterans to better express themselves and regulate their stress levels. Introducing,SynapseAI","SynapseAI transforms brain waves and biometric data into vivid visual representations in real time, enabling non-verbal communication and emotional expression for veterans with PTSD, TBI, or aphasia. Here's how it works: Pre-Flow: Veterans are equipped with a Muse EEG headband and a Terra-enabled wearable that sends stress level data. Brainwave patterns and biometric data are captured and transformed into embeddings using machine learning. These embeddings are stored in a vector database as ""thought categories."" User Flow: In real time, SynapseAI captures brainwave activity and stress levels. The system uses cosine similarity to match current brainwave patterns with stored embeddings. Based on the match, a multi-agent AI system (LangChain) expands, refines, and synthesizes a prompt, generating a video via Luma AI, dynamically modulated by the user’s stress levels. The result? Veterans can express thoughts and emotions visually, enabling communication and emotional regulation even under high stress.","EEG Signal Processing → Processed EEG Brainwaves and extracted features using FFT, PSD, and Wavelet Transforms. Thought Embedding Model → Trained a CNN-based triplet loss model to create embeddings for thoughts based on 5 main categories. Doctor Helps Patient - Thoughts of care, empathy, concern;
Sister argues with brother - Thoughts of conflict, frustration;
Fire burns house - Thoughts of emergency, fear, rapid reaction;
Child cries for mother - Thoughts of sadness and distress;
Null - To account for no thoughts Muse Headband Testing → We collected 50 10-second brainwave readings, changing between the different emotion clusters Vector Search Database → Stored thought embeddings in ChromaDB for real-time retrieval. Multi-Agent Thought Processing (LangChain):
Expansion Agent → Adds vivid details to enhance thought clarity.
Reasoning Agent → Breaks down complex thoughts into a logical sequence.
Emotional Agent → Adds emotional layers to thought. Adapts thought representation based on stress and biometric feedback from Terra API using the Garmin Watch. LLM-Powered Thought Refinement → Uses OpenAI-based agents to structure thoughts before video generation. Adapts thought representation based on stress feedback from Terra API using the Garmin Watch. AI Video Generation → Converts structured thought output into an AI-generated cinematic experience using Luma AI","EEG Data is Noisy → Required extensive filtering and preprocessing to extract meaningful patterns. Mapping Thoughts to Meaningful Outputs → Training an EEG-to-thought model was difficult due to limited labeled datasets and the complex/unstructured nature of EEG data. We had to collect data on our own using the Muse Headset and train on that. Vector Search Optimization → Fine-tuned cosine similarity in ChromaDB to improve thought-matching accuracy. Biometric Adaptation Complexity → Integrating real-time stress data into the thought expansion process required careful balancing. We did extensive research on how stress can affect characteristics of thought, like its imagery. LLM Constraints & Rate Limits → Had to optimize API calls to avoid delays and costs. Despite these challenges, we successfully built a working pipeline that converts raw EEG signals into structured AI-generated videos.","We are proud of bridging research in neuroscience, AI, and social impact. This bridge has spurred an innovation that will empower affected veterans to more effectively express their chain of thought, allowing healthcare professionals to better engage with each individual's unique situation. This ultimately catalyzes the transition back to civilian life for post-service veterans. Additionally, we are likewise proud of hacking and building moving pieces that we have never worked on before like training a model","We learned a lot in this hack. For the majority of us, this is our first hackathon. We primarily learned how to integrate different components of a large project, from the ML model, to the pipeline, and the frontend. Additionally, collecting data from the EEG was substantially tricky, and we learned that EEG-based communication is promising but complex.","We’re excited to bring our project to life! We plan to talk to our Ideal Customer Profile(s) (ICP) to run pilot studies, assessing product-market fit. Additionally, we want to:Expand EEG Model Training → Collect more data to refine thought embeddings and improve accuracy. Fine-Tune Real-Time Thought Adaptation → Enhance biometric-driven AI reasoning for better personalization. Incorporate more biometric variability like heart-rate or perspiration to our agent pipeline. Enable Continuous EEG Streaming → Allow real-time, uninterrupted thought-to-video processing. Optimize AI Video Generation → Improve coherence, storytelling, and realism in generated videos.","chromadb, google-cloud, langchain, luma-ai, muse-sdk, numpy, openai-api, pandas, python, pytorch, scipy, tensorflow, terra-api, typescript, vue.js",https://github.com/angad-the-coder/synapse
TreeHacks 2025,Conductor AI,https://devpost.com/software/conductor-ai,True," our team      Enterprises run a large number of agents, and being able to choose which agent to run a query on is a complex and expensive task. Routers exist today that can handle O(10) agents, but what about O(1000). Salesforce has O(10000) agents and — in a future where agents seem poised to replace many jobs — organizations may come to rely on the orchestration ofO(1,000,000) agents with O(100,000,000) prompts/day. If we want a future driven by agents, with companies formed of agents, and to maximize performance/$ for inference, we need to solve the large-scale routing problem.The routing space is large and continuing to explode, and there exist no systems that can scale to the size that is needed. Fortunately, our team (check below) is adept at handling matching problems at scale. Taking inspiration from the recommendation system at TikTok, ConductorAI matches queries and routers through a two-stage embedding approach, learning prompt and agent embeddings from exploring the interaction between the two and without manual encoding or intensive processing of either. Check out the technical details in our slides! ConductorAI provides unheard of speed and cost reduction at reasonable accuracy for an agent-driven future. Conductor implicitly learns a mapping between a semantic-embedding space and an agentic-embedding space, where coordinates correspond to features such as problem difficulty, tools required, and context that may be relative when deciding between agents. Additionally, adding agents to the system requires no hard-coded rules or descriptions, Conductor can naturally learn agent embeddings that exceed human performance. With A agents and P prompts, a traditionally LLM based router has inference scale on the order of O(PA), as each agent needs to be referenced in context, a classification-based router scales on the order of O(PA), and the theoretical perfect router scales at O(P). We scale at O(P log A) amortized, being the only neural-network based approach to do so. With the Intersystems vector search system, this O(log A) term is practically unnoticed. To showcase the power of ConductorAI routing on practical problems, we have orchestrated an Agentic Suite around ConductorAI, incorporating agents for: Here is our github repo containing our router, completely open-source: https://github.com/shloknatarajan/ariadne-routing For our code generation service, we extended it to provide a dev tool for Codegen users to run SWE-Bench on. We've made it very easy to run, test, and evaluate on SWE-Bench using Codegen's SDK, and included our own Codegen Agent that works on SWE-Bench. Here is the pull request containing the addition: https://github.com/codegen-sh/codegen-sdk/pull/521",,,,,,,"agents, ai-agents, contrastive-learning, knn, python, pytorch, recommendation-systems","https://docs.google.com/presentation/d/1SJoZzRauls_bHquv1L-HxrW5x9swVYLvnOsvtVu0Tj8/edit?usp=sharing, https://github.com/shloknatarajan/ariadne-routing, https://github.com/codegen-sh/codegen-sdk/pull/521"
TreeHacks 2025,CodeInsight,https://devpost.com/software/code-insight,True,"Both of us have worked at big corporate companies where we have had to be onboarded to a certain application very fast. It is often hard as an intern to automatically come in and understand the code base just like that. So we decided to create a tool that allows you to analyze a code base and see the function dependency which is nicely visualized and also chat to mistral AI to learn more about the code base. In terms of UI, we had some inspiration from Obsidian.","CodeInsight is an interactive tool that analyzes a codebase to generate a dynamic visualization of function dependencies. It extracts function definitions and their call relationships from a selected repository and displays them as an interactive network graph. Users can click on a node to view the corresponding source code in a draggable, resizable panel, with animated arrows that show the direct call flow—both outgoing (caller to callee) and incoming (callee from caller). In addition, CodeInsight integrates a chatbot powered by an AI model to answer questions about the codebase, helping developers quickly gain an understanding of complex systems.","We built CodeInsight using a combination of Python and JavaScript. On the back end, Python’s AST module performs static analysis to extract functions and their call dependencies from a codebase. This data is then served as JSON via a Flask API. For the front end, we used D3.js to render an interactive, force-directed graph of the function dependencies, adding features like zooming and dragging. We integrated Interact.js to allow code panels (that display the source code for each function) to be draggable and resizable, and Highlight.js provides syntax highlighting. To further enrich the user experience, we integrated a chatbot powered by Groq’s Mistral model (accessed via our configured API client) that lets users ask questions about the codebase. Finally, we packaged the whole thing into a standalone application using pywebview so users can simply select a repository without needing any command-line interaction.","One of our biggest challenges was ensuring accurate static analysis in a way that filtered out built-in or external functions while capturing the true internal dependencies of a complex codebase. We also had to balance performance with interactivity—making sure our visualization remained responsive even for large repositories. Integrating multiple libraries (D3.js for visualization, Interact.js for UI controls, and Highlight.js for code formatting) and ensuring they all worked harmoniously presented additional hurdles. Finally, deploying the app in a consistent manner across different environments and handling subtle differences in runtime behavior was a significant challenge.","We’re proud that CodeInsight not only generates a clear and interactive visualization of function dependencies but also provides an intuitive interface that helps users explore code in a dynamic way. The integration of real-time animated call flows (with directional arrows and moving dots) gives users an immediate sense of the underlying code structure. Additionally, the ability to open detailed code panels and interact with an AI chatbot for further insights makes Code Insight a comprehensive onboarding and analysis tool. Seeing our tool help new developers understand complex codebases quickly is a real win for us.","Working on Code Insight taught us a great deal about static code analysis and the nuances of parsing complex codebases. We gained valuable experience integrating diverse technologies—ranging from Python’s AST for analysis, to D3.js and Interact.js for building a rich, interactive UI, and even leveraging AI via Groq’s Mixtral model. We learned how to manage cross-environment deployments and how subtle differences in runtime or browser behavior can affect the end-user experience. Most importantly, we discovered the critical importance of effective visualizations in speeding up developer onboarding and code comprehension.","Looking ahead, we plan to extend CodeInsight with dynamic analysis features that capture runtime call flows, further enhancing our static analysis results. We’d like to integrate advanced filtering and search options—such as grouping functions by modules or highlighting high-complexity areas—to provide deeper insights into the codebase. Additionally, we aim to expand the chatbot’s capabilities so that it can answer even more nuanced questions about the code. Ultimately, our goal is to create a comprehensive tool that not only accelerates onboarding but also aids in ongoing code maintenance and refactoring, making it an indispensable resource for developers.","flask, groq, html, javascript, mistral, perplexity, python",https://github.com/evanng07/ej-treehacks
TreeHacks 2025,Swarm AI,https://devpost.com/software/swarm-ai-s6lhd0,True,"Picture this: there we were, running our AI restaurant waiter, feeling confident about our automated system taking phone orders. But then came the testing phase, and reality quickly set in. Our only option was to manually call our AI repeatedly to validate each conversation flow – a process that’s extremely bottlenecked by a major limiting factor. Not exactly scalable when you're trying to ensure your AI can handle a million different things, ranging from simple reservations to complex menu customizations.
We quickly realized we weren't alone in this challenge. From customer service to healthcare, everyone adopting voice AI was facing the same bottleneck. These AI agents needed to be thoroughly tested before deployment, but the testing tools hadn't caught up with the technology. In an age of automation, we were still relying on manual testing processes. And that didn’t sit well with us.
That's when the idea for Swarm AI clicked: what if we could just create a platform that could spawn thousands of virtual callers, each with their own characteristics, accents, and conversation patterns? Just as load testing transformed web development from guesswork into a science, we believed voice AI testing needed its own revolution. By enabling developers to uncover edge cases and identify potential issues before they reach real customers, we could help ensure more reliable AI interactions across every industry.",,,"Our biggest technical hurdles centered around real-time communication and scalability. The WebSocket connection, important for maintaining live conversations between AI agents, proved particularly tricky – every disconnection meant a failed test and lost data. We overcame this by implementing robust connection handling and retry mechanisms. Call management also presented unique challenges. What seemed straightforward – ending a call – became complex when dealing with thousands of concurrent conversations. We had to carefully orchestrate call termination to ensure clean exits and proper resource cleanup. Our batching system underwent several iterations before we found the right balance between system load and testing throughput. One of our most interesting challenges was running simultaneous speech-to-text and speech-to-speech processing. This required careful stream management and precise timing to prevent feedback loops or processing delays. Figuring out how to transfer speech-based audio chunks efficiently and quickly proved was a major obstacle as well. After numerous debugging sessions and architecture revisions, we developed a stable solution that could handle both streams efficiently. Finally, our database architecture evolved significantly throughout development as we better understood our data needs. What started as a simple call logging system grew into a complex but efficient structure handling test configurations, real-time analytics, and detailed conversation transcripts.","The both of us poured our expertise into crafting a robust testing platform that exceeded our initial vision. The clean, beautiful interface we designed masks the complex orchestration happening behind the scenes – something we take pride in. We're especially proud of our system's reliability. Through persistent debugging and optimization, we created a platform that can handle many concurrent test calls while maintaining stable performance. But beyond the technical achievements, what stands out is how well we worked together. The both of us brought our strengths to the table and stepped up when needed, allowing us to build something substantial in such a short timeframe.",,"While we initially set out to build a voice AI for restaurant ordering, developing this testing platform opened our eyes to a much bigger opportunity in the voice AI ecosystem. We're now pivoting our startup to focus on Swarm AI as a comprehensive testing platform for voice AI developers, with plans to expand our testing capabilities and add features like custom scenario builders, advanced analytics, and integration with popular voice AI development frameworks.","fastapi, javascript, next.js, openai, python, render, supabase, vercel","https://github.com/phombal/swarm-backend-new, https://github.com/sidjavvaji/swarm.git"
TreeHacks 2025,PULSE.AI,https://devpost.com/software/pulse-ai-0oafm5,True,"Developers constantly search the internet for solutions while coding—whether it's project planning documents, API documentation, recent research papers, or Stack Overflow discussions. We wanted to seamlessly integrate browser tab context into an AI code editor (Windsurf), making coding more efficient by surfacing relevant information automatically.","PulseAI captures browser context (screenshots, URLs, and titles), performs extra searches for adjacent topics via Perplexity, and stores & retrieves semantically relevant data using ChromaDB and Retrieval Augmented Generation (RAG) to bolster AI-assisted coding on Codeium's Windsurf-Cascade editor. We have two main workflows, a Passive Workflow for building up the browser context vector embedding ChromaDB and an Active Workflow for handling real-time user queries to the AI Code Editor. In addition to the main tool, we built a frontend where hackers can view and delete database entries, for added flexibility and control.","We built a Chrome extension to extract browser data, an agentic backend to process our information and store it in ChromaDB, a callable tool (MCP) that is callable by Cascade, and a visualization tool that lets the user view and edit the context generation process in real-time.",,"When we first decided on our project, we acknowledged that there were going to be several moving parts, and it's great to see that we were able to integrate them perfectly. We're also quite proud of our idea, as it expands a coding agent's ability to learn about its project outside of its IDE and at the same time minimizes context switches for the user. Furthermore, our product works almost exactly as intended and is a great proof of concept; with more work, we're confident it could be a move towards the next level of AI coding assistance.",We learned about...,We aim to improve overall latency and implement a more automated browser + AI code editor experience that is even more hands-off while still respecting users' privacy.,"cascade, chroma, codeium, css3, fastapi, html5, javascript, mistral, mpc, perplexity, python, rag, react, sonar, structuredoutputs, windsurf",https://github.com/ekang7/PulseAI
TreeHacks 2025,traider,https://devpost.com/software/traider,True,"Finance isn’t a subject commonly taught thoroughly in schools, yet it plays a crucial role in everyone’s lives. Many people, especially students, grow up without exposure to financial concepts, making it challenging for them to navigate real-world financial decisions. This lack of financial knowledge can lead to overwhelming debt, poor savings habits, and financial stress. In our lives, we have seen that these knowledge gaps in finance can cause enormous hurdles when entering college, internships, or the corporate world as young adults struggle to manage their growing debt and earnings.","traideris an educational platform that provides beginner investors with mock trading simulations and instantaneous AI-powered feedback for their trades. Scraping REAL historical and current stock data from Yahoo Finance, traders can engage with accurate market data and place their trades over a user-specified simulation duration beginning at a specified start date. Unlike models trained on outdated datasets, our product offers real, relevant market and economic data based on the stock ticker and current date in the simulation, which we then synthesize with NVIDIA to supplement young traders with more robust information to fuel their financial decision-making. This allows users to make informed trades using accurate and up-to-date market information. Our product stands out from other trading simulations in that it allows students to pick any timeframe in the past to put their trading skills to the test. Further, our supplemental market events information provides a more detailed painting of the financial landscape during the simulation, encouraging informed investing. Our dynamic and colorful data insights allow students to examine takeaways, while our AI-powered feedback for each executed trade promotes healthy learning and detailed mistake analysis. The leaderboard and financial calculator features offer a social and educational playground for students to explore the world of trading while gamifying finance like never before.",,,"We were excited to be able to integrate and leverage AI models, such as Perplexity and Llama 70B, NVIDIA Brev, and cloud computing, for numerous features of our trading simulation. These included providing young investors instantaneous feedback on their trades, parsing through and summarizing market data with Google Search API and Llama 70B, and offering an AI-powered chatbot that answers users' questions and concerns regarding finance and trading. We were also proud to utilize a database technology that was new to us, Convex, to store user information. Through Clerk, we could also set up secure user authentication through Google sign-in. Additionally, we leveraged the Yahoo! Finance API to consolidate and display financial stock data for any time frame the user needed, allowing the user to see detailed stock information, such as high, low, open, close, volume, and more. These complex calls and technology integrations allowed for a seamless user experience and analytical trading dashboard. We are proud of our dynamic homepage UI and the detailed and analytical simulator page UI. Their seamless and intuitive interfaces make them easy to use for our young target audience. We paid close attention to the color scheme, ensuring that it aligns with our brand identity and creates a visually cohesive and inviting interface. The result is a user-friendly design that is both functional and appealing, providing an enjoyable experience for users of all experience levels. Additionally, the components we are most proud of are the Homepage for its nifty abstract animations, which make the page more exciting. The home page’s color palette is also bright, giving it a more vibrant feel since it is marketed towards students to encourage them to learn more about finance. Another page we are proud of is the trading simulation and portfolio analytics page. The UI is attractive and enhances the theme. Many of the boxes and the pop-out form offer interactive and dynamic components. We were also proud to be able to offer various features on our page in just 36 hours of coding, from financial calculators to a leaderboard to data visualizations regarding portfolio analytics. We thoroughly enjoyed building this engaging product and integrating the technical backend with an appealing front end.","Through developing this project, we learned about the complexity of designing scalable backend architectures, handling real-time data processing, and optimizing system performance. We learned the importance of efficient database structuring to maintain data integrity while ensuring fast retrieval. Additionally, we also deepened our understanding of financial market mechanics, from order execution to portfolio management, and the challenges involved in simulating a fair and realistic trading environment. Most importantly, we gained valuable experience working collaboratively and iterating design choices to build the most reliable user-centric platform.","Next mission for traider is to bring it to the real market with users of all age groups who are students to try our product out. We also plan on implementing more robust features, such as fine-tuning an LLM on trades and historical stock data to provide more tailored insights to users, developing a more social aspect of the platform to allow young investors to share their accomplishments with their networks, and integrating voice-based AI-powered chat support for our younger client base. We would also aim to build AI-generated weekly quizzes and maintain performance metrics and streaks to keep our users rewarded and engaged on a daily. Additionally, we would love to implement cryptocurrency in our platform.","fast-api, figma, google-search-api, nvidia, openai, perplexity, v0, vercel","https://traider-omega.vercel.app/, https://github.com/ajagtapdev/traider, https://drive.google.com/file/d/1jR0J82edGx1O9fPfPDKvBWEco9wBvq2L/view?usp=sharing"
TreeHacks 2025,TheraMind,https://devpost.com/software/theramind,True,"Our inspiration for TheraMind stems from the growing mental health crisis and the challenges therapists face in meeting patient demand. Our guiding question,""How do we help therapists help their patients?"", emphasizes the dual focus of TheraMind: empowering therapists while enhancing patient care. By addressing the challenges therapists face—such as administrative burdens, burnout, and time constraints—TheraMind enables them to dedicate more energy to their patients. Through tools like data visualization, sentiment analysis, and streamlined meeting preparation, TheraMind bridges the gap between therapist efficiency and patient outcomes.",,,,,,"Our vision for TheraMind is to expand its reach and impact by catering to new user groups who experience high levels of stress and emotional burnout. In the near future, we aim to tailor our AI-driven mental health support for medical professionals, project managers, educators, and more. By refining our AI’s ability to provide industry-specific emotional support and wellness insights, we can help these professionals navigate their daily lives more effectively and allow them to achieve peace of mind.","dain, python, scrapybara, typescript",https://github.com/cherylwu834/treehacks25
TreeHacks 2025,Agent Ricky,https://devpost.com/software/jarvis-mhfy78,True,"Our team wanted to use this opportunity at TreeHacks to create something bigger than ourselves, something that serves others. Who better can we serve than the first responders and the military personnel who have devoted their lives to protect and serve us every day? These first responders and military personnel must respond quickly because they are placed in very delicate situations. Thus, we realized that, we needed an AI that can be proactive and create real-time decision-making. Unlike rule-based automation or passive AI that waits for user input, agentic AI goes beyond this traditional AI by enabling autonomous decision-making in critical situations where speed and efficiency are very important.","Agent Ricky is acustom AI agenttailor-made for first responders and military personnel to monitor the status, safety, and health of soldiers, squadrons, and first responders in training and in action. It is essentially another member of the team. Live biometric and geolocation data collection enables Agent Ricky to detect abnormalities and dangers in real time by providing data for a series of health metrics. These health metrics are then utilized by a custom DAIN-driven AI agent to provide timely alerts, statistics, and information that could prove to be the key to save many lives.",Languages:Python · TypeScript Frameworks and Tools:DAIN · Terra API · Nvidia Brev.dev · TensorFlow,"One area that we spent a significant time in was learning how to use DAIN and all its possibilities. Although DAIN draws on the power of LLMs to create AI agents, DAIN's functionality and SDK is quite different from any LLMs we had previously worked with. However, after poking and prodding our way through documentation and experimentation, we discovered the interesting potential of DAIN. Learning through the demo services, the UI manipulation, and background processes took a lot of experimentation, but also offered a novel learning experience in creating and launching AI services.","Creating our own Agentic AI that has the potential to make a real impact. Coming into the Hackathon, agentic AI systems were novel to all our team members. Yet, hackathons are the best places to explore new topics, new ideas outside of comfort zones. As such, we are really proud that we, 4 strangers, not only met each other, but synergized into a cohesive unit that collectively explored agentic AI through DAIN's products. In addition, together we were able to build Agent Ricky, an agentic AI system that has the ability to provide first responders and military personnel and their leaders with the crucial information to keep them safe in training, service, and combat.","The next powerful wave of AI may come from Agentic AI that leverages the abilities of LLMs to more effectively and efficiently perform specific, precise, and detailed tasks.","The potential of Agent Ricky only grows the more data there is available. With an expanded user base and many more data points, we see Agent Ricky as having real potential in greatly improving the safety of first responders and military personnel.","brev.dev, dain, python, tensorflow, terraapi, typescript","https://github.com/nipunbatra8/healthbot, https://github.com/nipunbatra8/mapagent, https://docs.google.com/document/d/1iCNAtqf1H5ELC9MBTEaelI2FQOK2W6TXy2yYBOtoppM/edit?usp=sharing"
TreeHacks 2025,Motherly.ai,https://devpost.com/software/ai-mama,True,"In the past year, 74% of people have felt overwhelmed by stress, and 30% of young adults in the U.S. experience loneliness multiple times a week. We wanted to create a large-scale solution and turned to what helps us most with both stress and loneliness: our moms. While AI companions like Character AI provide emotional support, they can’t truly take care of you. Our goal was to build an AI agent that deeply understands your emotions and takes action to help you.","Stress management is deeply personal - what works for one person might not work for another. That’s why we built Motherly.ai, an AI-powered agent that understands how you experience stress and takes real-world actions to support you. Using biometric data like EMG, heart rate, and blood pressure, it detects your mental state in real-time and responds with personalized interventions, whether it’s scheduling a break, ordering comfort food, or adjusting your environment to help you relax. Unlike traditional AI companions, Motherly.ai doesn’t just listen – it takes care of you.",,,,,,next.js,https://github.com/MingkuanY/ai-mommy
TreeHacks 2025,ProtestLink,https://devpost.com/software/protestlink,True,"In times of social unrest, the ability to communicate is as vital as the cause itself. Protesters across the world face increasing challenges when it comes to coordination, safety, and information sharing. Governments have been known to shut down cellular networks, limit internet access, or monitor online communication, making it difficult for protesters to organize effectively. Even when networks remain online, they often become overloaded, rendering them useless at critical moments. Protesters struggle with locating medical aid, distributing supplies, and avoiding high-risk areas where clashes may occur. Beyond digital suppression, protesters also struggle with navigating volatile environments, distributing supplies efficiently, and requesting medical assistance when situations escalate. How can protesters stay connected and informed when traditional communication systems fail? That was the problem we set out to solve. We asked ourselves:How can we create a communication system that is secure, decentralized, and resilient enough to function even under extreme restrictions?With this goal in mind, we built ProtestLink, a Wi-Fi mesh-based protest coordination tool that provides secure messaging, real-time resource allocation, and risk assessment without relying on internet access or centralized servers. ProtestLink ensures that protesters stay connected, protected, and prepared, even when traditional communication methods fail.","ProtestLink is a decentralized communication and coordination platform designed to keep protesters connected, informed, and protected even in the face of network disruptions, internet blackouts, and active surveillance. By leveraging Wi-Fi networking, live location sharing, and AI-driven speech analysis, ProtestLink provides an autonomous protest network that helps protesters navigate safely, request critical resources, and broadcast urgent alerts without relying on traditional service providers. With ProtestLink, protesters can:","At the core of ProtestLink is a self-hosted web server that facilitates location sharing, encrypted communication, and live protest heatmaps. We built a custom Wi-Fi network to allow devices to connect and interact without relying on traditional cellular infrastructure. Protesters can open the web app, share their live location, and send alerts—all without requiring a constant internet connection. To enhance real-time protest intelligence, we integrated Whisper AI, an edge-based transcription and NLP system capable of converting live protest audio into actionable data. This means that if a protester yells for help, reports police movement, or requests medical assistance, the system can analyze and categorize the request, ensuring it reaches organizers or other protest members in real time. We also used sentiment analysis to detect potential conflict zones, allowing ProtestLink to generate a live map of risk areas. The protester map is powered by WebSockets and real-time geolocation tracking, ensuring fluid updates without relying on centralized data sources. Protesters can pin their locations on the map, mark areas as safe or dangerous, and request water, medical aid, or evacuation support—creating a distributed protest intelligence network that operates independently of government-controlled systems. Security was a critical concern throughout development. We implemented end-to-end encryption to ensure that messages, locations, and alerts remain private and tamper-proof. The zero-trust authentication model allows protesters to join the network anonymously via QR codes, eliminating the need for phone numbers, email, or personal data. This ensures that no centralized entity can track or compromise protesters through ProtestLink. By combining Wi-Fi-based communication, AI-powered speech processing, and encrypted location sharing, we created a robust and decentralized coordination system that empowers protesters to act swiftly, securely, and strategically.","One of the biggest technical challenges we faced was working with Bluetooth on iOS. Initially, we planned to leverage Bluetooth mesh networking for decentralized communication, but Apple's restrictive CoreBluetooth API made this extremely difficult to implement. Unlike Android, which offers flexible background BLE (Bluetooth Low Energy) support, iOS heavily limits background Bluetooth communication, making it nearly impossible to maintain persistent message relays between devices. After multiple failed optimizations, we ultimately pivoted to Wi-Fi-based networking, which provided greater reliability, better message throughput, and cross-platform compatibility. Navigating Apple’s strict development ecosystem also posed a challenge. Integrating Swift-based networking protocols with React Native required bridging complex native APIs while ensuring that location tracking and WebSockets functioned reliably across both iOS and Android. Debugging Bluetooth issues in Apple’s sandboxed environment further complicated development, as maintaining persistent connections in the background was heavily restricted. Despite these challenges, we successfully transitioned to a more scalable Wi-Fi-based approach, built a real-time protest tracking map, and integrated speech recognition AI into an autonomous protest coordination system. These obstacles forced us to rethink our architecture, leading to a more powerful and adaptable solution.","Building ProtestLink required pushing beyond conventional networking and security models, and we are incredibly proud of what we accomplished. One of our biggest milestones was successfully implementing a live protester map that provides real-time protest intelligence without relying on external services. The ability to share live locations, report threats, and visualize protest dynamics makes ProtestLink a game-changer for secure protest coordination. We are also proud of integrating Whisper AI’s real-time transcription engine, which allows ProtestLink to analyze protest audio, detect emergency phrases, and generate dynamic risk assessments. Protesters can now transmit critical alerts without needing to type, ensuring faster responses in high-risk situations. Additionally, we successfully implemented a zero-trust authentication system, ensuring that protesters can join securely and anonymously without exposing personal information or metadata. By making ProtestLink fully decentralized and resistant to surveillance, we created a communication tool that protesters can truly rely on.","This project reinforced just how fragile centralized communication systems can be in times of crisis. Governments actively shut down networks, throttle internet speeds, and exploit surveillance laws to suppress protests. ProtestLink has shown us that truly decentralized, peer-to-peer communication is not just a convenience—it’s a necessity. We also gained extensive experience in Wi-Fi networking, encrypted communication protocols, and real-time AI processing on edge devices. Working with Whisper AI and NLP-driven event detection expanded our knowledge of how machine learning can be used to enhance real-world safety. Most importantly, we learned that technology has the power to protect human rights. ProtestLink is more than just an app—it’s a lifeline for those who need a secure way to communicate, mobilize, and protect themselves in uncertain times.","ProtestLink has the potential to redefine protest communication, and we’re excited to take it even further: ProtestLink isn’t just a project—it’s a movement. We’re committed to building censorship-resistant, decentralized technology that empowers people to speak, organize, and protect their rights—no matter what.","axios, bluetooth, esp32, flask, leaflet.js, numpy, openai, openstreetmap, python, react, sentence-transformers, sqlite, stadia, tailwindcss, transformers, typescript, web, webrtc, whisper","http://protest.morelos.dev/, https://github.com/wyang563/protest_app"
TreeHacks 2025,CodeForge Pages,https://devpost.com/software/codeforge-pages,True,"We love to build, but design is hard. To make something visually appealing, it requires a lot of experimentation and research into what needs to be added. Landing pages are often the first thing a customer sees about a business. We thought current AI tools fall short of our needs. They were either too rigid, didn't integrate well into our workflow of Next.JS, or didn't have enough complexity.  So, we thought, why not automate it?","CodeForge Pages asks users to enter information about their company along with the name of the company. Based on this information, CodeForge generates a multi–component landing page in React that contains beautiful layout, text, and style. Users can view the rendered code in either the web-view or a simulated mobile view. When pleased with the website, they can export it–which will download a zip of all the react components. All users are also authenticated – meaning when you create a project, we associate it with your profile and it can be accessed at any time through the dashboard.","CodeForge Pages is built in Next.JS. The AI pipeline is quite intricate. We first recorded the data of existing landing pages and then extracted images of every single component of that page (header, hero, feature, etc). Every landing page was then added to an Elasticsearch serverless vector database with a corresponding description generated automatically. Now, when the user prompts the app, it finds the most similar company and generates components through a section-by-section workflow. This backend operation is all coded in Python and utilizes FastAPI. To render the generated component, we use Sandpack from CodeSandbox. In order to authenticate each user, we use Supabase – allowing GitHub, Google, and verified email log-in. To save the data about each project for every user, we use Firebase. Specifically, every component is stored as a String and is rendered every time the user opens the project.","It was quite difficult to render the generated React components on the screen. At first, we created an implementation using Babel. This nearly took 5 hours and was quite complicated as there were many issues with UI libraries we were using such as Tailwind. Later, we experimented with some other features and asked those at the booths for help. After some more research, we found CodeSandbox. Additionally, the model was glitching – generating white text on white background or dark text on dark backgrounds. To fix this, we utilized Tailwind to ensure the model was accurately placing classes on elements. Additionally, we improved the prompting through iteration.","The app generates beautiful and aesthetic components as shown in the images. These designs can be implemented at a real business with a great level of acceptance. Additionally, the code also displays code and an accurate preview. Preview at first seemed difficult to generate accurately, but now it is quite accurate and rarely errors.","We gained many insights about UI / UX automation through AI and what it takes to generate such components. Additionally, we learned more about how code rendering really works in React projects. In the future, we should check if there are any libraries to simplify our tasks.","We plan to add more features to the app and ship it to the public. Some of the features we plan to add are chat to add features, an interactive no-code editor, on-demand hosting, and image generation / implementation.","beautiful-soup, elasticsearch, fastapi, firebase, next.js, python, react, supabase","https://github.com/KshitijTeotia06/frontendTreeHacks, https://github.com/KshitijTeotia06/backendTreeHacks"
TreeHacks 2025,theo,https://devpost.com/software/theo-k1enau,True,,,"For this hackathon, we tried incorporating a myriad of new and exciting frameworks into our tech stack. We relied on a React-Native frontend as well as Firebase’s Cloud Functions for our backend, as we wanted our users to be able to journal on the fly. We used Firestore as a general-purpose no-SQL datastore and ElasticSearch to run vector search on our embeddings. Finally, we used Mistral’s fast models for our agentic behavior. This combination of frameworks enabled us to achieve flexible functionality when it came to meeting our app’s functional requirements.","The architecture of the foundation – the full stack app itself was smooth sailing. However, as many developers who work with AI agents know, we quickly learned that AI agents are unreliable, with LLM output outright hallucinating inconsistent behavior. In particular, we struggled with querying elastic search for semantically similar journals using a vector search, as Elastic search’s semantic search function returned to us different output every time we called one of our HTTP endpoints. Getting the annotations working proved an enormous difficulty as well, requiring us to design an entire mental module associated with indexing the characters on screen.","We’re proud of building an AI tool that doesn’t just optimize for efficiency but also for depth of thinking. Seeing theo make successful connections between separate thoughts and building valuable insights was a major win. We also prioritized user experience, making sure theo feels like a natural extension of the user’s mind rather than an overwhelming AI assistant.","We gained deeper insight into how AI can facilitate cognition rather than just automate tasks. Through testing, we saw firsthand how prompting users with the right reflection questions can unlock new insights, giving us a glimpse of how AGI might go about functioning. On the technical side, we learned a lot about embeddings, vector similarity, and how to design an effective note-retrieval system.","We see theo evolving into a more personalized and adaptive cognition copilot. Future iterations will include improved contextual understanding, allowing theo to better detect a user’s current mindset and tailor its prompts accordingly, and orienting user within their complex web of thoughts by visualizing connections between different journeys. theo is more than a productivity workflow — it’s a thinking partner for a clearer mind.","agents, ai, elasticsearch, embeddings, firebase, google-cloud, mistral-large, rag, react-native","https://github.com/RALS-TreeHacks-25/backend, https://github.com/RALS-TreeHacks-25/frontend"
TreeHacks 2025,<PriorityQueue>,https://devpost.com/software/priorityqueue,True,"      The U.S. interconnection queue faces a massive backlog, with two terawatts of mostly clean energy projects waiting for grid access, more than the 1.25 terawatts currently online, causing delays of three to five years or more. High interconnection costs and uncertainty lead to frequent project dropouts, forcing restudies that further slow the process and subsequently the clean energy transition. The current transmission planning system is inefficient, requiring the next project in line to bear upgrade costs, discouraging development and creating cascading delays. While FERC Order 2023 introduces reforms like batch processing and penalties for delays, the lack of proactive regional transmission planning remains a major bottleneck. Driven by genuine climate interest and frustration about the lack of progress in the energy transition, our team was drawn by the unique challenges that the grid interconnection queue offered, an often overlooked reason for lack of progress in clean energy transition. Like many other climate-adjacent issues, it all comes down to regulation. In a brief published by the Federal Energy Regulatory Commission, the commission detailed the great need for an efficient and transparent queue management system. Highlighted customer insights can be foundhere. Without one, American energy goals cannot be met. The US is not the first to think about this issue. France, for example, has taken strides to streamline its grid connection processes, leveraging a centralized system to integrate renewable projects faster and ensure grid stability. Similarly, parts of Texas have piloted innovative approaches, such as distributed energy resource integration and priority queues for renewables, to accelerate clean energy deployment. These efforts reflect growing public and institutional support for systems that not only simplify bureaucracy but also unlock the potential for cleaner, more reliable energy. It’s clear that people want this change and believe it will help bridge the gap toward a sustainable energy future. Our solution is a user-centric platform that visualizes geospatial data of generation stations, manages each developer’s projects, and organizes said projects into clusters using proprietary algorithms. Each developer has access to current energy generator information, energy generators in the grid interconnection queue, and their own projects. Developers can take part in a “bidding system”, which will be pooled towards fees associated with connecting to the grid as a cluster. Developers also have access to a RAG agent, Link the Energy Expert, who is contextualized by all of PQ’s data and understanding of the grid interconnection system, and can consult with Link to plan which clusters to join and more. Another user of our platform is the ISO manager, or government-affiliated project manager. This user has access to all project proposals, and has the ability to grant or reject said projects. Our infrastructure is centered around a web app model through NextJS that is both scalable and can easily work with large amounts of data as we use MongoDB as our database. We made extensive use of Elastic for our RAG model and geospatial analysis, along with OpenAI for open-ended location interpretation based on user prompts. As beginner hackers, we were inexperienced in working with the tools we had access to at Treehacks, but we were eager to try out new software. Unfortunately, after spending a great deal of time working through a particular sponsor’s product, we realized that the product was not compatible with our web app, and the information could not be embedded into our final product. A drastic pivot had to be made and we tackled steep learning curves during the latter half of the hackathon. Many of our teammates have never built out a project of this scale at the speed we worked at during Treehacks. Moreover, we had varying degrees of experience with working with complex project files, so many of us learned an incredible amount about web app files and planning for larger project structures. Due to the research intensity of our project, we now know a significant amount about the nuances of the electric grid problem that we didn’t before. When working with niche pain points such as the interconnection queue, intuition for what our clients need will be built on a lot of painstaking research and prototyping through methods such as speaking with users. PriorityQueue is far from finished. We envision a future where PQ works alongside government and regulation to promote transparency inter-developer and between the ISO and generators. Moreover, PQ currently sets up an environment in which developers can accurately assess the benefits to joining a cluster. However, by incorporating AI-facilitated decision making and analytics, PQ’s consulting service abilities could be bettered by tenfold. Imagine a platform that vertically integrates the energy project planning process, scrapes all the data that exists on which projects are currently being implemented, and can help developers make informed decisions about their business! We are inspired by the potential of PQ to: Pitch deck can be foundhere.",,,,,,,"elastic, mapbox, mongodb, node.js, openai, react, tailwind",https://github.com/shrenikpatel17/treeHacks2025
TreeHacks 2025,CodeCrack,https://devpost.com/software/codecrack,True,"Want to ace that upcoming technical interview for your dream job? LeetCode is a great place to start for developing good problem solving skills, but it lacks the interactive communication demands of a live coding interview where interviewers assess your problem-solving, debugging, and communication skills in addition to your ability to code. It’s fun to recruit your friends to help conduct a mock interview but these resources are not available to everyone. From under-resourced students to experienced builders, interview preparation materials and access to experienced coders can be an obstacle. As engineering students who have undergone Software Engineering recruiting, we wanted to build a tool that bridges this gap - allowing coders to improve their skill set while simulating the pressure of a live interview with real-time feedback. We built CodeCrack to empower anyone to crack their interview with confidence.",,"We built CodeClack on a Vercel web platform with functional modifications made for integrating the AI agents to execute their client tasks, keep track of state, and upstream information to the voice. On top of this, we built on the user interface to reflect the various components of our project. For instance, we’ve integrated an IDE with Monaco, the code editor behind VScode, and Pyodide, a Python interpreter so users can run and execute their code. Leveraging ElevenLabs voice agents, CodeCrack offers both behavioral and technical interviews. To simulate a Software Engineering coding interview as closely as possible, In the technical coding challenge, the AI agent will introduce themselves, give an overview of the interview, give the user a moment to introduce themselves, and then will dive into the coding challenge, populating the user’s screen with the challenge and reading it outloud. The agent is designed to take on the persona of a more senior level Software Engineering interviewer, processing how the user breaks down the problem and explores various paths before diving into an approach, offering an open-ended hint if the user indicates they are stuck or asking a follow-up question if they are on the right track. Within our agentic system, there are two main flows of information sharing: agent to client and client to agent. For the coding challenge question, we retrieved leetcode questions and solutions from a HuggingFace dataset, and in the first agent to client stream, we developed a client tool, sendNewQuestion, that randomly retrieves a question within the user’s difficulty preference, upstreaming it to the voice agent to read aloud. The other mode, client to agent, is activated when the user clicks “Run Code”, so the user’s code submission can be tested for functionality and the agent can generate feedback on code style. The agentic system is keeping track of the state of interactions, making decisions to pivot the conversation to get the user closer to a functional solution without revealing too much information. After the user ends the interview, an API call is sent to ElevenLabs’ criteria evaluation feature to get the agent’s feedback on communication, problem-solving, and debugging based on the overall state of interactions.","With no prior experience building with AI agents, the primary challenge we initially faced was understanding how agentic frameworks are built, and the tradeoffs between multi-agent frameworks and single agent frameworks. The next challenge we faced was learning how to influence single agent frameworks, and specifically, how to send information to it and retrieve information from it for various components of our project’s functionality. For instance, after retrieving a leetcode question we wanted to upstream this information to populate the user’s screen and have the AI agent read the problem out loud, but we faced difficulty understanding and building on the information streams that come with client tools. Expanding on this, all the builders on the team are most comfortable with Python and have no experience building extensive frontends. After realizing our Python client tools were not easily compatible with the AI agent codebase, we converted that entire part of our codebase to lay within Next.js and React, with some help from the ElevenLabs sponsor with only hours left before the deadline.",,"Over the last 36 hours we learned how to build our first tools with AI agents. In particular, we designed an AI agent for our interviewer persona and built various client tools to handle retrieving leetcode questions, evaluating the user’s code on run, and populating the UI with the coding challenge and comprehensive feedback. Specifically in terms of new technologies, we had no prior experience working with Node.js and React, so it was an exciting challenge to focus on user interface design.","We look forward to building a more comprehensive system including System Design, Python trivia, and resume-based questions!","elevenlabs, javascript, node.js, python, vercel",https://github.com/kkarenvoo/TreeHacks2025
TreeHacks 2025,The Duck You Mean?,https://devpost.com/software/the-duck-you-mean,True,"We were motivated by the power ofrubber duck debuggingand the wisdom of Richard Feynman’s quote: Too often, AI is used to give answers, buttrue learning happens when we explain concepts ourselves. We wanted to flip AI on its head. Rather than asking the duck for answers,you teach it, and it pushes back with evaluations, scores, and follow-up questions to deepen your understanding.","The Duck You Meanis an interactive web app that lets users pick a topic and engage in a back-and-forth conversation whereAI plays the role of a curious student. Users must explain concepts, and the AI: We also created zone called ""The Pond"", where users can create Zoom Meetings to study with friends or chill out after completing a lesson.",,,,,,"elevenlabs, luma, mistralai, openai, zoomapi","https://github.com/joycechen721/duck-you-mean/, https://docs.google.com/presentation/d/1wbRdoGqOnnMw5rVU0nRgBMmOErweNsQdrtsrA7q-214/edit?usp=sharing"
TreeHacks 2025,EcoBite,https://devpost.com/software/ecobite-qcujae,True,"In the USA alone, food waste makes up 22% of landfilled solid waste, and “globally, food loss and waste represent 8 percent of anthropogenic greenhouse gas emissions” (EPA 2021). Our inspiration for EcoBite was an interest in this incredibly relevant issue within our world, as well as theStanford Sustainability Challenges, specifically Challenge #3: the development of a program for determining the weight of food in an image. From a technical standpoint, this challenge aligned greatly with our interests:","EcoBite is a mobile app meant for users to take pictures of their leftover food for the main purpose of estimating the food’s weight. After this action on the user’s end, the app will analyze the weight of the food in the image along with classifying its type. On top of being able to record and display this data to users, EcoBite keeps track of individual progress when it comes to food waste, allowing for milestones to be reached when individuals to a good job not creating as much food waste. This information is recorded and held in a backend to be analyzed for generalized food waste trends, all of it location-based. This process of recording images is also great for businesses that produce much food waste in their day to day in order to better keep track of food costs as well as intercept food waste.","EcoBite utilizes a variety of sponsor technologies to complete this project. For a professional responsive frontend, we leveragedFlutterFlowto rapidly develop a polished mobile application that enables users to track their food weight, view detailed analytics of their consumption patterns, and participate in an engaging points-based reward system. FlutterFlow's intuitive low-code platform streamlined our development process, enabling us to rapidly create a polished mobile app with professional UI components and functionality that would have otherwise required significantly more development time. Various API endpoints were created to retrieve any necessary information the user may need. For the backend, we usedFlaskas the AI APIs perform best in a Python environment.Google Geminiwas used to perform essential functionalities such as food classification and segmentation, volume estimation, and weight calculations. Our team learned prompt engineering principles to optimize these AI interactions, ensuring accurate and consistent responses while minimizing token usage and processing time. For obtaining the density of different food items, we used the Food and Agriculture of the United Nations'Official Density Databaseas our primary source. If the database did not contain a food item, we relied onPerplexity's Sonar Promodel to determine density values.","On top of using Google Gemini for estimating the density of food via prompt engineering, our team wanted to use a food volume estimator deep neural net trained on many food images to assist with the classification of food items and estimation of their weights. There were many great examples ofolder workbeing done in this field, however a lot of the code was incredibly difficult into modern python / usability standards to this day, so our team ran out of time when it came to implementing this model alongside the AI model. This neural net would have been trained on100100 images of food, usingResNet50for image classification.","Our team is most proud of our development of EcoBite using a tool none of us had worked with previously: FlutterFlow. Thanks to TreeHacks for hosting the company as a sponsor, we were given a perfect opportunity to pick up the tool and became highly motivated to try our best to master it over the weekend. Overall it was an amazing to learn how to use, all employees present helping us out greatly throughout countless steps of our app development process; end-to-end! Thanks to all of the assistance, as well as FlutterFlow’s user friendliness, we have a great looking mobile application that was incredibly straightforward to learn how to build.","Thanks toKeegan Cooke, the head of Stanford's Sustainability Challenge #3 and Director of Stanford Ecopreneurship, our team gained valuable insights about the challenge and received a food scale for our preparations. Through our research, we discovered previous attempts to estimate food volume (rather than weight) using image classification. Along the way, our team quickly picked up and mastered several technical tools, including the Perplexity API, Google Gemini API, FlutterFlow for app development, and Flask for backend implementation.","We believe a great product is one that listens to its users. We look to gain as much feedback from users on their experience using EcoBite and what features have been useful and those in need of improvement. Through our beta testing phase, we will actively collect user feedback through in-app surveys, user interviews, and analytics tracking to understand usage patterns and pain points. This data will guide our development roadmap, ensuring we prioritize features that provide the most value to our users while continuously refining the app's functionality and user experience to better serve our community's needs. Some future goals include expanding to helping out businesses interested in keeping track of their food waste for the sake of improving sustainability and helping with professional costs. We would also love to expand the backend model used from AI tools to an actual existing estimation model trained on real images, both ones found in existing databases and those taken by users of the application. A big goal of EcoBite would be to create a data expansion pipeline wherein which the food density estimator could continuously train on new datapoints. EcoBite aims to counter food waste and instill better practices in how we prepare and consume food. Through our gamified approach and real-time food tracking, we empower users to make more conscious decisions about their portions and develop sustainable habits that benefit both their personal well-being, community, and environment.","flask, flutterflow, gemini, perplexity, python","https://github.com/SirKentut/EcoBite, https://app.flutterflow.io/share/eco-bite-9o3s7v, https://eco-bite-9o3s7v.flutterflow.app/"
TreeHacks 2025,Sprout,https://devpost.com/software/sprout-io,True,"It all started with a TikTok video: a teacher shared how she turned her kid’s doodles into adventure books, which sparked curiosity and made the excitement in the classroom undeniable.
✨ What if we could bring that magic to every child, everywhere? The Challenge: The Solution:Sprout is the future of storytelling, interactive learning, and self expression. Through Sprout kids can: With Sprout, children aren’t just reading stories—they’re living them. They upload their own drawings, personalize their adventures, and build language skills through play.
Teachers can tailor stories to lessons, integrate vocabulary, and expand beyond just reading into science, history, and more. Sprout.io is a win-win.","Sprout starts with your student’s drawing. From there, we build an engaging, choose-your-own story, with generative visuals based on your input. The adventure is filled with grade-level vocabulary to hit comprehension goals, with a quick comprehension check at the end.","Sprout.io was developed using Flutterflow, for a seamless user experience. We integrated several AI-driven APIs: By combining these resources, Sprout creates a high value learning experience with personalized and interactive storytelling. 
Project link withcode",,,"We learned a TON about FlutterFlow. Not knowing much about frontend design, FlutterFlow was a great tool that we spent the majority of our hackathon working in. We managed to get our app to work seamlessly cross-platform and in sync with our dynamic backend endpoints, which took a ton of time, but was a great learning experience for creating a full-stack app from nothing but an idea. Special shoutout to the FlutterFlow team for staying late into the night with hackers to help debug ❤️. We learned a ton about front-end and had great conversations.
We also developed a ton of API routes in FastAPI, which we then interfaced within FlutterFlow, adding actions to route data across the frontend, and sending and receiving information from our Generative AI models. Working with these generative AI models was very new to us as well, and we picked up a lot on how to use Gemini’s Multimodal 2.0 Flash API and Luma’s API to generate the content we need, doing a ton of prompt engineering along the way. 
We also learned the value of discussion and diagraming our workflows and API connections using visual tools like excalidraw for a better design process.",We are planning on expanding Sprout.io’s functionality to better benefit teachers when creating personalized learning plans for students!,"dart, fastapi, flutterflow, gemini, genai, google-cloud, imgur, luma, lumalabs, multimodal, python","https://treehackssprout.flutterflow.app/, https://app.flutterflow.io/project/story-8owtoa, https://github.com/sdoshi4/Treehacks2025"
TreeHacks 2025,Caren AI,https://devpost.com/software/caren-ai,True,"Every year,850 millioninsurance claims are denied in the U.S., leaving patients with unexpected medical bills and financial stress. Shockingly,less than 1%of these denied claims are ever appealed not because they lack merit, but because the appeal process is frustrating, time-consuming, and complicated (Wall Street Journal). As a result, nearly 1/3 of Americans contribute to$220 billionin medical debt, often due to denied or mishandled claims (Consumer Financial Protection Bureau). We wanted to level the playing field by putting AI in the patient’s corner. What if an AI negotiator could read the fine print, fight back against denials, and handle everything seamlessly? That’s how Caren AI was born—to empower patients, recover their rightful coverage, and eliminate unfair medical debt.","Caren AI automates the insurance claim appeal process and negotiates with insurance providers on behalf of patients. Our AI reads insurance denial letters, cross-references them with Evidence of Coverage (EOC) documents, and calls insurance companies to dispute denials using legally backed arguments. Key Features: Caren AI features two AI agents dedicated to simplifying healthcare management: Sam (Scheduling AI)– Helps users book and manage doctor appointments based on availability and medical history.Noa (Negotiation AI)– Calls insurance providers, disputes denied claims, and negotiates fair settlements to reduce costs. Result: Patients save thousands on medical bills without the hassle of navigating complex insurance policies themselves.","We designed Caren AI as a full-stack AI-powered assistant, ensuring seamless automation of insurance claim disputes and healthcare scheduling. Frontend: FlutterFlow (Dart) – A clean, user-friendly interface for patients. AI Backend: ElevenLabs Conversational AI – Trained on insurance policy documents for intelligent claim analysis. Fastest AI Model: Gemini 2.0 Flash – Parses complex medical and insurance documentation. Live Call Automation: Twilio API – AI negotiates claims directly with insurers via automated phone calls. Authentication: Firebase Authentication – Secure user logins and data protection. By combining AI, automation, and real-time negotiation, we eliminated the need for manual claim appeals, making the process fast, accurate, and effortless for patients.","Training AI on Complex Insurance Policies – Extracting accurate insights from dense, jargon-heavy documents required custom model fine-tuning. Real-Time Insurance Call Handling – Ensuring AI could hold a live, logical conversation with insurers using Twilio without sounding robotic was a key challenge. Using Flutter for the First Time – Since our team was new to FlutterFlow, we had to learn its UI-building framework and state management quickly to build a seamless frontend experience. Through iterative testing, API optimizations, and AI refinement, we successfully overcame these hurdles to create a seamless patient experience.","Built a Fully Functional AI-Powered Claim Negotiator – Our AI successfully disputes insurance claims in real-time. Implemented End-to-End Automation – From claim upload to resolution, everything happens without manual intervention. Integrated Live Twilio Call Handling – Caren AI makes phone calls to insurers, delivering legally sound counterarguments. Designed a Scalable, Patient-Friendly Platform – Built using FlutterFlow, making it accessible to all users, regardless of tech expertise. Our solution doesn’t just solve a problem it revolutionizes patient advocacy in healthcare.","The Healthcare System is Stacked Against Patients – Insurance denials are often not justified, but most patients don’t have the knowledge or time to fight back. AI Can Level the Playing Field – With intelligent claim analysis and negotiation, AI can recover thousands of dollars for patients. User Experience Matters – Many users aren't tech-savvy, so creating a seamless, intuitive interface was crucial. Real-Time Communication with Insurers is Game-Changing – Automating insurance negotiations was far more effective than just document-based appeals. Caren AI isn’t just an app—it’s a movement. By leveraging AI negotiation, we’re fighting for patient rights and bringing fairness back to healthcare. Join the revolution, and let AI handle the hassle, so you can focus on your health.","Callback Feature for Dropped Calls– If an insurance call disconnects, Caren AI will automatically redial and resume the negotiation. Multilingual Support– Expanding beyond English to support Spanish, French, and more to help non-native speakers navigate insurance issues. Family Insurance Management– A family plan feature that allows users to track and manage multiple insurance claims in one place.","ai, ai-agents, dart, elevenlabs, firebase, flutterflow, gemini, perplexity, twilio","https://github.com/Tap2Call/flutterflow, http://tap2call.flutterflow.app/, https://app.flutterflow.io/project/translateusing-groq-streaming-a-p-is-046jc0, https://app.flutterflow.io/run/1c0qLm3zk9jT4WiNtcHJ"
TreeHacks 2025,Navi,https://devpost.com/software/navi-7sp5mz,True,"Given the substantial lack of accessibility resources and compliance in modern website UI—over 96% of websites fail to meet accessibility requirements—we wanted to develop an AI-driven copilot agent, Navi, for human browsing, in addition to creating an easier web infrastructure for AI models to interact with. 🌐🤖","In short, Navi makes web browsing easier for human users, with accessible voice navigation 🎤, intuitive browsing with user intent prediction 🔮, instant feedback to questions 📝, and personalized further reading suggestions 📚. In terms of serving AI models, Navi extracts web pages more efficiently compared to traditional VLMs. ⚡","To integrate visual context, we captured screenshots of the user's navigation, which were then processed by visual language models (VLMs) to inform Navi's AI. This enabled Navi to understand and interpret website layouts in real time 🖼️. Simultaneously, we harnessed Context, a robust context fetching engine, to analyze user interactions, predicting browsing intent and tailoring recommendations. In the background, Scrapybara autonomously suggests related websites 🌐, while Groq converts audio commands into prompts for OpenAI's 4-o LLM, ensuring rapid and accurate responses ⚙️. Additionally, we leveraged Mistral for fast, simplified website renderings that highlight key information, making navigation even easier for the user 🏃‍♂️.","Balancing rapid AI responses with efficient web page extraction was just one layer of our challenge 🏔️. A deeper complexity arose from integrating multiple AI components into a unified, accessible system. Each component—whether it was the Context engine, VLMs, or voice processing with Groq—had its own processing speeds, data formats, and dependencies. Merging these disparate systems required careful orchestration to ensure they communicated seamlessly in real time ⏱️. We designed robust interfaces and error-handling protocols to bridge differences in performance and data structure ⚖️. This meant synchronizing outputs from slow VLMs with faster processing modules, ensuring that delays in one area wouldn’t disrupt the overall user experience.","We're proud to have built Navi as a truly integrated, AI-driven copilot that enhances web accessibility and usability 🌟. Navi not only improves browsing efficiency through context-aware predictions and voice navigation but also serves as a high-performance data extractor for AI models. This project sets a new benchmark for creating accessible digital experiences and bridging the gap between human and AI interaction 🌉.","Throughout this project, we gained invaluable insights into prompt orchestration, context fetching, and the nuances of designing for accessibility. We learned how critical it is to balance technical sophistication with user-centric design, ensuring that advanced features translate into real-world usability for diverse audiences 🎓💡.","Looking ahead, we plan to expand Navi's compatibility with more websites and enhance its voice and intent recognition capabilities 🔧. Our next steps include optimizing the system for even faster and more accurate responses ⏩, refining personalized recommendations, and exploring additional accessibility features to further empower users with disabilities ♿.","context, elevenlabs, gemini, groq, javascript, mistral, next.js, openai, perplexity, scrapybara, typescript, windsurf","https://navi-browser-agent.vercel.app/, https://github.com/danieltyx/navi-browser-agent"
TreeHacks 2025,Spheroid Tumor Invasion Kinetics (STIK) GUI,https://devpost.com/software/tumor-invasion-kinetics-gui,True,"Last summer one of our teammates worked in a lab studying glioblastoma, an aggressive strain of brain cancer. The project involved knocking out genes of interest to better characterize tumor invasion patterns at a molecular level. Such a project necessitated a copious amount of data; the lab produced well-plates upon well-plates of treated cells and subsequent images, all of which required tiresome analysis involving manual hand tracing the images and lots of coffee. It was the kind of work for an unassuming undergrad. Thus, our teammate traced hundreds of photos on an unreliable, finicky platform. Somewhere in between image number 250 and 255, our teammate thought: no way this can't be automated.","STIK_GUI (Spheroid Tumor Invasion Kinetics GUI) is an AI-powered tool designed to automate the tracking and prediction of tumor spheroid invasion. Instead of manually tracing hundreds of images, researchers can upload their data and let STIK_GUI handle segmentation, analysis, and visualization. The intuitive graphical interface makes it easy to process well plate images, extract invasion kinetics, and generate meaningful insights—saving time and reducing human error.","STIK is developed using PyQt, a set of Python bindings for the Qt application framework, which enables the creation of arobust and interactive graphical user interface (GUI). This user-friendly interface allows researchers to seamlessly import datasets, visualize segmentation results, and analyze generated plots with ease. The platform leveragesinstance segmentation through the CVAT(Computer Vision Annotation Tool) framework, providing precise image annotation and labeling. For model training, STIK utilizesYOLOv8for both segmentation and bounding box detection, ensuring high accuracy and You-Only-Look-Once speed in identifying and delineating objects. With a best precision of 99.739% and a mean Average Precision (mAP50-90) of 97.369%, STIK delivers state-of-the-art segmentation performance, making it an invaluable tool for researchers working with complex image datasets. Moreover, STIK utilizesmulti-threading in PyQt, leveraging its signals and slots mechanism to efficiently manage parallel processing. By running the GUI and the model on separate threads, the platform ensures a smooth and responsive user experience without delays or freezing. Multi-threading allows the GUI to remain interactive while the model performs computationally intensive tasks, such as loading large datasets, running segmentation models, and generating plots. This separation enhances performance, prevents UI lag, and enables real-time updates using PyQtGraph.",,,,"The first version of STIK GUI is particularly well suited for spheroid invasion assays, a common wet lab experiment that mimics the 3D environment of human tissue to study how cancer cells invade. Naturally, we are curious to see if we can extend our model beyond spherical subjects to automate invasion patterns in, for example, patient scans, tissue slices/histology data, animal models, etc. From the user interface standpoint, we plan on developing a feature that visualizes the images continuously by displaying invasion patterns in a time-lapse format. Additionally, we aim to expand the data capacity to analyze multiple wells, lanes, or even entire well plates at once. This would enable us to perform sub-comparisons between past and current experimental treatments in real time. We believe that STIK_GUI has the potential and versatility to enhance the efficiency of wet lab analysis pipelines across a range of applications.","conda, cvat, pyqt, python, yolo",https://github.com/sonnyloweus/STIK_GUI/
TreeHacks 2025,Plot,https://devpost.com/software/plot-3pi8c4,True,"This project was born out of our own personal pain points as students. Like many others, we’ve faced the challenges of searching for housing at school, for summer internships, full-time roles, and traveling. When planning my trip to Rome, I had no idea which area was safe to stay in for an Airbnb. I ended up spending hours on Reddit looking for reviews and suggestions, which felt inefficient and time-consuming. This made us realize that finding the right places to stay could be far more seamless with the right insights from the right sources. We saw a gap in the market for a platform that could quickly and accurately provide sentiment data on locations, helping people make informed decisions about housing, whether for short-term stays or long-term investments. What if we could make this process faster and more efficient?",,,,,,,"langchain, mapbox, mistralai, next.js, openai, perplexity, react, recharts, typescript, zustand",https://github.com/toni-akintola/jats
TreeHacks 2025,The Anachronist,https://devpost.com/software/the-anachronist,True,"When students hear the subject “history,” they are likely reminded of a textbook marred with numbers, dates, and timelines; when in fact history consists of real people and real events, which serve as a powerful tie that conjoins even the most dissimilar individuals through strong ties. Driven by a vision to bridge this rift, we devised a game that promises the development of a deeper, more immersive understanding of history by placing players directly into pivotal moments of the past. By warping through time and stepping into the shoes of people who lived through transformative events, players can experience history not as a collection of facts, but as a living, breathing narrative shaped by human resilience, emotion, and decision-making. Our game takes place at the heart of the Manhattan Project—Los Alamos, 1943—and allows players to bond with eminent historical figures and pedestrians, vicariously reliving the diverse ethical dilemmas, scientific breakthroughs, and political tensions that permeated the age. Through these interactions, players develop empathy and a nuanced appreciation for the complexities of historical events, turning history from a mere paragraph between pages to a dynamic and sentient being, that breathes through us today.","The Anachronistis an educational mystery game that “time warps” players into a historical period that is unknown to them prior to completing the game. Their task? Identify the time and place by carefully observing surroundings, interacting with characters, and analyzing historical details, with a slight twist: Don’t get caught! While learning hints and gaining further context through every conversation, the player must also be conscious of the fact that they should not arouse suspicion, that they are potentially from the future. Figures within our game range from children, who were brought to Los Alamos as their parents were involved in the Manhattan Project, to figures who symbolize the Manhattan Project to us today. Navigating this world is not just knowing the “timeline” or “facts” of the events, which we grade through a quiz; but instead understanding the very fabric of the community that they are teleported to.","We builtThe Anachronistusing the Nitrode game engine, custom-made Blender models, and advanced triggers, interactions, and animation loops. Our 20+ NPC characters were designed through reconstruction of historical figures (and others) based on VLMs, and tailored to our purpose through mesh texture rigging. LLMs, such as Open AI and Gemini’s models, allow players to engage in conversations not only with historical figures but also with everyday people from the past, bringing history to life. We implemented LLM-powered dialogue systems, leveraging retrieval augmented generation. As such, we were able to prompt 20+ NPC based on the historical context and agent-specific profiles, which allowed for conversations that were both accurate and consistently “in-character” that performed robustly across diverse conversational inputs of the player, which could range from questions and comments about everyday life, technical questions tailored to the historical protegé in their presence, to conversations which engage with deep ethical questions of the age. Furthermore, we leveraged LangChain to manage our agent workflow, which involved both synchronous generation of “evaluation” of the player’s ability to blend in with their environment, as part of our game objective and generating fruitful interactions, fulfilling our education purpose. Additionally, we integrated the LumaLabs AI API to generate realistic videos, which both open and close our game with an immersive storyline and stunning visuals, which immerse the player in the gaming environment. We also used the ElevenLabs API for high-quality voice synthesis, which enhanced the storytelling experience with authentic-sounding dialogue while providing oral guidance to the player regarding the game-playing rules.","One of our biggest challenges was balancing historical accuracy with engaging gameplay—ensuring realism without sacrificing playability. On the technical side, building an entire 3D world from scratch was a major hurdle, especially since it was our first time developing a 3D game. Creating lifelike characters with unique traits was complex, but we leveraged VLM and an AI-powered pipeline to generate, rig, and animate 3D models from text-to-image inputs. This pushed us to refine our design and animation processes, making the world feel more immersive and authentic.",,"Achieving the immersive gameplay ofThe Anachronisttaught us extreme attention to 3D modeling, developing complex programmatic interactions, and learning all the technologies of Godot game development from scratch. The challenge taught us how to learn quickly, experiment iteratively, and come up with novel solutions to problems like building 3D interactive AI NPCs with unique personalities, movement patterns, and levels of comfort with sharing information. Moreover, we gained a deeper appreciation for historical nuance. We discovered how small details, such as speech patterns, architecture, and cultural practices, can reveal a great deal about a historical era. Most importantly, we realized the value of player agency in learning. When users are empowered to make their own independent discoveries in the world and inquire openly about tricky issues, they develop a stronger connection to the material. This is the power of immersive worlds in technology – it heightens their ability to think critically about not just the past, but also the present and the future.","Moving forward, we aim to expand our historical settings to include underrepresented periods and cultures, ensuring a more diverse and comprehensive learning experience. We are also exploring the possibility of adding multiplayer or collaborative modes, where players can work together to deduce their location and time period. Another exciting development is the introduction of VR / AR support, which would create an even more immersive time-travel experience. All in all,The Anachronisthas the potential to get into classrooms, museums, and cultural institutions as an interactive history-learning tool. By blending history with interactive problem-solving, we hope our game redefines how we engage with the past—making learning not just informative, but truly thrilling.","elevenlabs, gemini, godot, langchain, lumalabs, nitrode, openai","https://www.nitrode.com/games/the-anachronist, https://github.com/rgu0114/the-anachronist"
TreeHacks 2025,Mirror,https://devpost.com/software/mirror-lmd54a,True,"Remembering things is hard – and staying on top of your self improvement and reflection while juggling a busy schedule is even harder. As a team of busy students, one of whom recently started working a full time job, we decided to build Mirror as a tool to leverage unique smart glasses technology to revolutionize the way people handle self-improvement and reflection.","Mirror is an AI-powered app compatible on allAugmentOS-enabled smart glasses which transcribes your conversations and generates actionable insights and summaries tuned for reflection in real time. Our processing agents will automatically update on the Mirror web interface as you speak, intelligently picking out the most appropriate points to give a well-rounded overview of your day, extracting todos, a general snapshot of your day, and important reflection questions. Just as important as understanding the specific details of your day is being able to look back in time at past memories and see how they fit into the context of the rest of your memories. Mirror offers full-text semantic search through all memory content to allow for quick and easy retrieval and recall of specific details even if you only have a small idea of what you’re looking for. Furthermore, visualizing the relationships between memories is made easy and beautiful via Mirror’s 3-Dimensional memory map which displays latent connections between memories. Each edge and connected component tells a story and provides insights into the relationships between your experiences. The best part – all of this is generated via our processing pipeline in real time as you talk! Throughout the entire day, Mirror will always be up to date with your latest memory insights.","Mirror is built in 3 parts: The AugmentOS Smart Glasses Client is an Android application written in Java which handles the interface with the smart glasses – handling both the voice transcription and relaying text back to the user via the display. Shoutout to theMentrateam for providing mentorship and lending us theEven Realities G1Glasses that we built Mirror to work with. The backend is written in TypeScript via NextJS and handles interactions with both the smart glasses client and the web interface. Using ElasticSearch, Mistral AI, Vercel’s NextJS, and Codeium’s Windsurf, we created an intuitive interface to interact with your thoughts throughout the day.",,,"We learned that working with experimental hardware is hard – and persistence is key. Wearethe R&D for the software for these glasses, testing the limits of what’s possible with them and trying to push the field forward.","We would love to continue expanding Mirror’s interaction with our users, especially in terms of active reflection throughout the day. We envision a world where our agents can prompt you during conversations about related past conversations, remind you of your past reflections, and help guide you towards a more productive, social future.","augmentos, elasticsearch, java, mistral, nextjs, react, typescript, windsurf","https://github.com/kevinzhu12/treehacks-glasses, https://github.com/sarahjsu/tree_augmentos"
TreeHacks 2025,Sherlock,https://devpost.com/software/sherlock-n4fikj,True,"Sherlock Sherlock Web App Investigative journalism is one of the most powerful tools for transparency, but it’s also one of the hardest jobs in the world. Reporters juggle huge amounts of unstructured data, struggle with limited resources, and, in many cases, risk their own safety to uncover the truth.
The reality? That’s why we built Sherlock. Sherlock is an AI-powered investigative assistant that connects the dots for journalists. Instead of dumping a long list of search results, Sherlock maps information into an interactive, evolving graph, showing how people, events, and documents are connected. Key Features: Sherlock is built on a multi-layered AI system combining LLMs, retrieval-augmented generation (RAG), and knowledge graph structures. Frontend: React + TailwindCSS + Vite Backend: In the backend, connecting Sherlock to multiple APIs, handling different file formats, and making sure everything talked to each other seamlessly was a massive task. Every new integration introduced new failure points and debugging nightmares. Some APIs also have strict rate limits, which meant we had to get creative with caching, batching requests, and optimizing queries to avoid bottlenecks. For the frontend, there was a steep learning curve in customizing React Flow for real-time graph updates and smooth interactions. In addition, settling on a type system that balanced developer sanity and runtime safety took way longer than expected. We started actually hacking around Saturday at noon, and in just over a day, we pulled off a fully functional AI-powered investigative tool with a complex backend, real-time graph UI, and multi-source data integration—a huge feat given the scope. Sherlock dramatically speeds up investigations, cutting hours of manual research into minutes by automatically organizing and expanding leads. Beyond just functionality, we’re proud of how usable and intuitive the final product feels—Sherlock was built with journalists in mind, prioritizing clarity, speed, and seamless interaction over complexity. Sherlock isn’t just an AI tool—it’s a revolutionary assistant that empowers investigative journalists to work faster, safer, and smarter. We're excited to see how it can change journalism.",,,,,,,"python, react, vite",https://github.com/Honyant/TreeHacks2025
TreeHacks 2025,Fable,https://devpost.com/software/fable-br8e1d,True,"1 2 3 4 5 Fable began with a simple question: What if a story could adapt to your state of mind? As our team explored EEG technology, we became excited about the idea of measuring brain waves and uncovering hidden patterns of thought. What if a narrative could respond to your level of focus and relaxation—your shifting mental state? With EEG headsets reading brain activity, Fable dynamically shapes its plot, characters, sound effects, and visuals, crafting a living, breathing story that unfolds uniquely for you. This isn’t just storytelling—it’s an experience powered by your mind. Fable reads your brainwaves to craft a living story that responds to your every thought. As your mental state shifts, the plot dynamically evolves—changing tone, pacing, or twists to reflect how you feel. Simultaneously, the visuals adapt to match the unfolding narrative, immersing you in a real-time, brain-driven adventure where each scene is shaped by your mind. Even the moving gradient color background gradually fades and shifts, mirroring your inner state and heightening the immersive feel of your journey. We built Fable by combining a Muse 2 EEG headset with a dynamic storytelling pipeline, powered by a Python FastAPI backend and a Next.js frontend styled with Tailwind CSS. The headset reads beta, alpha, theta, and gamma wave patterns—interpreted as relaxed, neutral, or focused states—using Mind Monitor and streams this data in real time through OCS. Our main story loop runs every 30 seconds, checking the user’s current state and using the OpenAI API to generate story scripts (and corresponding sound effect prompts) based on those EEG readings. We map each state to a specific narrative direction—for neutral, we steer the story toward fresh discoveries; for focused, we introduce more details or challenges; and for relaxed, we invite ease and wonder. To enhance immersion, we integrated 3.js and GLSL shaders to create an EEG-based dynamic gradient background that shifts in real time with the user’s mind state. We then feed both the text-to-speech lines and OpenAI-generated SFX prompts into ElevenLabs (streamed via WebSocket), allowing us to seamlessly produce both TTS dialogue and ambient sound effects. Finally, we bring the story to life visually with the Luma Labs API. Throughout this process, we employ a multithreading approach to process text, voice, and video asynchronously, ensuring smooth, parallel generation of each element. We faced several hurdles bringing Fable to life. Hooking up OpenAI’s story generation so that each paragraph was generated sentence by sentence required careful orchestration within our webapp. Integrating ElevenLabs’ text-to-speech to ensure that audio and subtitles streamed seamlessly in real time was another challenge. Working around lengthy inference times on video generation models forced us to asynchronously and concurrently queue and segment videos to keep up with the script and audio. Our hacking led us to benchmark at least 10 different video generation APIs against each other for speed, including text-to-image and image-to-video generation. We also developed our own text-to-image and image-to-text pipeline to improve efficiency and control over the entire process. We’re proud of creating a seamless, real-time storytelling platform that translates brainwave data into dynamically shifting narratives, audio, and visuals. By integrating multiple APIs—Muse for EEG, OpenAI for generative text, ElevenLabs for audio, and Luma Labs for visuals—we managed to build an immersive, multi-sensory experience that feels both personalized and technically robust. And most of all, we love the experience of listening to the stories that come out of our product! Through developing Fable, we gained a deeper understanding of real-time data processing, from parsing EEG signals to synchronizing audio and video outputs. Integrating diverse tools like OpenAI, ElevenLabs, and Luma Labs taught us the value of modular design and clear communication between APIs. We also discovered how critical it is to balance technical complexity with user experience, ensuring that the shifting storyline remains both immersive and coherent. We’re excited to broaden Fable’s capabilities by refining our EEG interpretation for an even wider range of emotions and deeper engagement tracking, exploring additional wearable sensors beyond the Muse headset, and advancing our storytelling techniques—potentially introducing multiple branching storylines, co-op experiences, and VR integration.",,,,,,,"3.js, eeg, elevenlabs, glsl, javascript, javascript/typescript-(next.js), lumalabs, mind-monitor, muse, ocs, openai-api, perplexity, python-(fastapi), tailwind-css, websocket",https://github.com/StiopaPopa/fable
TreeHacks 2025,TreeTrash,https://devpost.com/software/treetrash,True,"During TreeHacks, we noticed a significant number of items being placed in the wrong waste bins, which reminded us of a broader problem with tracking and understanding waste patterns, so that they could be effectively mitigated in the future. This sparked our idea to expand upon challenge #3 on the sustainability track and leverage AI to not only identify and address this issue but also take it a step further. We wanted to go beyond simply classifying waste and analyzing the weight — we aimed to develop a program that could analyze images of trash, recycling and compost and generate a report that could provide organizations like Stanford with valuable insights into food waste patterns, enabling them to implement more effective sustainability initiatives and reduce unnecessary waste.",,,,,,,"adafruit, gemini, openai, perplexity, vercel, vespa",https://github.com/sonnetx/treehacks-2025
TreeHacks 2025,Zue Research,https://devpost.com/software/zue-research,True,"56% of students in the world do not have internet access at school [1]. Students learn best through multi-sensory, hands-on, structured experiences that are tailored to their interests. Yet, the primary method of learning for them continues to be textbooks. This is even more pertinent to the 129 million students globally who have ADHD [2]. We built in our product with neurodivergent and offline learners in mind. Drawing on bleeding edge research, we proved that multimodal applications can be deployed entirely at the edge using distributed inference using heterogeneous compute [3] [4].",,,,,,,"elevenlabs, groq, javascript, langchain, lumalabs, mistral, openai, perplexity, rust, typescript",https://github.com/zcsabbagh/zu-lm/
TreeHacks 2025,Lattice: Orchestrating end-to-end web workflows at scale,https://devpost.com/software/lattice-data-at-your-fingertips,True,"Design End to end workflow Individual sheets Over the past few months, there've been a lot of really cool agentic research products that've been released (Exa, OpenAI/Perplexity Deep Research, etc). In general, these are point solutions and relatively sequential in manner (ie, the next action is conditioned on the previous actions). But AI much more exciting if we can: We want to build something that takes advantage of the parallelized nature of research agents to solve end-to-end workflows en masse (we can run hundreds of agents at once!). Lattice allows you to orchestrate complex end-to-end research workflows at scale, with complex/structured dependencies. Consider a sales representative sending cold outbound: This involves: Today, Lattice solves the vast majority of this workflow – and we do this at scale, across hundreds of companies in parallel. We first find potential companies, and then for each company, find a set of defined attributes. https://imgur.com/a/bsq1S3i Then, for each company, we can find the key people we want to reach and augment this information. https://imgur.com/a/7b0FhLY Then, for each companies' list of people, we can qualify them based on attributes we defined + have researched. https://imgur.com/a/dl6gfzL Finally, we can pipe all the research information to a language model to produce a final work product. https://imgur.com/a/fAdaElA We do this at scale and save a considerable amount of time over manual research – but also existing tools that only do this for a single part of the workflow (ie, Clay)! As a page one end-to-end workflow. We let you build any workflow like this and chain spreadsheets together. Anything that involves scaled web research! But concretely: Start with a provided list of companies and find company-level information. Then for each company, find all their products, and for each product, create a grid of if they flag certain policies. Then aggregate all the violations across the products, run some risk/modeling calculation, and compile into a per-company report. Find a list of recently funded startups (Series A), then for each startup catalogue all of their rounds, how much they raised, and who they raised from at each round. Across the rounds of each company, count up the number of T1 VCs. Draft emails to send outbound to companies with more than 3 T1 VCs on their cap table. Find a list of film festivals in the past year. Then for each festival, find which films won the top prizes, the names of the movies, and the authors. Then, choose the film across with the funniest title from each of the festivals. For each of those films, then find other films created by those artists. For each of those films, then collect the Rotten Tomato and IMDB scores. Then, choose the film for each artist with the highest aggregate scores and write a congratulations emails! We know that all three (maybe two) and millions of other similar use cases are very important and significant workflows conducted by humans today. We treat spreadsheets (or really tables) as a core primitive. This is how data is displayed, edited, and passed between different steps in a workflow. Most of the heavy lifting is on the frontend (Next/Typescript/Tailwind) – we build a primitive spreadsheet interface from scratch, added multiple dimension to it, then created a chain of dependencies between the different spreadsheets, and then added a bunch of agent orchestration on top of it! The cool thing is this all actually works, and works quite well! Frankly, we thought Paradigm's challenge was really cool, and were inspired in part by what they do. We think spreadsheets are a fun primitive to build around: It's a surprisingly challenging task. Spreadsheets are one of the best ways to visualize bulk agentic outputs and enable a lot of cool things. We started with what we thought were an interesting limitation of 2D spreadsheets – we can't do a nested series of ""Find a list of XYZ, then for each find a list of ABC""-style inputs. There's a lot of powerful workflows though, that depend on being able to serve multiple layers of these kinds of queries – so we designed a ""3D spreadsheet"" where each sheet is mapped from a row in a 2D spreadsheet. Our spreadsheet workflows, in some ways, are essentially a directed graph with dependencies. This is roughly how we treated the orchestration / execution of individual agents within our system. At any point, we can have a one-to-many relationship between incoming and outgoing edges, or many-to-one relationship. This allows us to manage state/context passing between different steps within a workflow. For example, when going from a single row into a spreadsheet -> a sheet in a 3D spreadsheet -> a single row in a spreadsheet again, we pass parts of the initial row through the entire chain so our agents have the right context. Another cool thing that comes out of this is that we provided verifiable + auditable traces into how exactly the end to end workflow was executed. As any step, you can look up the full table of data that the end considers when making decisions, all the raw attributes collected, and how they flowed in the workflow. A lot! Here's a few:",,,,,,,"agents, apis, llms, typescript",https://github.com/WeltonWang02/spreadsheet_hackathon
TreeHacks 2025,Spielberg AI,https://devpost.com/software/spielburg-ai,True,,"Spielberg AI is an AI-powered video editor that allows creators to edit videos using only natural language. Instead of manually cutting, trimming, and applying effects, users can describe their desired edits in plain English, and Spielberg AI—powered by the agent, our AI agent—handles the rest. There is a strong emphasis on developing and editing viral videos for the users.","The core of our project is built around FFMPEG, the industry-standard video encoding and decoding backend. We started with a simple real-time preview system using FFMPEG to encode frames and allow for precise, frame-focused cuts. From there, we integrated AI-driven enhancements to streamline and automate complex video editing tasks. To push performance to the extreme, we custom-compiled FFMPEG with NVENC encoding and CUDA hardware acceleration. This optimization massively reduced processing times, enabling near-instantaneous rendering of complex edits. What once took minutes now executes in mere seconds (on our gaming computer). Spielberg is driven by the agent, a highly advanced AI agent designed to interpret natural language commands and execute sophisticated editing tasks seamlessly. The agent's intelligence is built upon a multi-layered NLP and processing pipeline, leveraging: The backend is structured into multiple Python layers, each optimized for efficiency, with distinct modules handling inference, video processing, and audio synthesis. The agent seamlessly works across these layers. Our project takes full advantage of NVIDIA GPUs and CUDA acceleration to handle the most computationally intensive workloads. Using OpenAI APIs, we generated LUT-based color filters from natural language prompts. Our CUDA kernel applied these filters nearly instantly, outperforming traditional GPU-accelerated FFMPEG operations. Beyond video processing, we integrated several key APIs to enhance the intelligence and automation of the editor:","The biggest challenge we faced was the sheer computational demand of our backend operations. Web GPUs are significantly weak for video processing, which is incredibly demanding, and we quickly realized we needed a proper GPU to achieve this. To solve this Reilly drove down to UC Santa Cruz, convincing a friend to lend their high-end gaming PC for the project. This borrowed GPU became the foundation of our CUDA optimizations, allowing us to crank out the custom CUDA kernel that enabled real-time AI-driven edits. We wanted to use a GPU that someone's actual computer has so real editors could leverage features in Spielberg and not need to use brev.dev to rent an H100. Additionally, this only optimizes the best features of Spielberg, but the average person can still use Spielberg, but they will just wait for longer. With the newfound power, we tuned our CUDA kernels, implemented multi-threaded AI inference, and reduced processing times by orders of magnitude. What started as an impossible challenge turned into our biggest breakthrough—a GPU-powered AI editor that processes in seconds what used to take minutes. Also, we found version control when we had a sloppy codebase was very difficult. In the end, ""adeng"" is our working branch as the main branch has been slightly corrupted and is no longer functional.","Building a video editor in 36 hours? That's pretty cool and something we are proud of doing. Working with videos is extremely difficult and processing them with LLMs and having agents work with them and interact with them is very difficult.
Successfully built an AI-powered video editor that edits using only natural language.
Achieved real-time AI-driven video editing using custom CUDA-accelerated processing.
Developed a Multimodal RAG system that learns from YouTube creators to optimize viral content.
Integrated Google Gemini, OpenAI Whisper, Luma AI, and ElevenLabs to create a seamless editing experience.","As corny as it sounds, we learned that we can literally do anything if we put our minds to it. We learned so many new APIs, tools, and tricks and accomplished so much stuff we thought would be impossible without an army of engineers like Netflix or Adobe.",We want to expand the agent's capabilities and finally take the time to deploy this project for the world to use. We know this is a serious problem for creators and we want to democratize content creation so everyone can tell stories about their lives. We would love to talk to Neo about turning this into a company.,"cuda, elevenlabs, fastapi, ffmpeg, gemini, googlegenai, huggingfacetransformers, langchain, lumalabs, next.js, openai-gpt, openai-whispr, opencv, tailwind",https://github.com/adeng27/treehacks-25
TreeHacks 2025,NeuroPilot: Brainwave Computer Control,https://devpost.com/software/mindcontrolpc-control-computer-with-brainwaves,True,"We explore the world through motion and words; yet 5.4 million people in the United States alone have some form of paralysis, and over 15 million people worldwide have spinal cord injuries. Imagine a world whereall individualswith limited mobility can navigate a desktop or generate files – simply by thinking. Whether it’s a student attending online classes, a software engineer working their job, or an artist bringing ideas to life, the ability to seamlessly control a computer without physical limitations opens countless opportunities for independence, productivity, and creativity. Our team was driven by the challenge faced by individuals withlimited motor/speech function, with personal experience of stroke-induced paralysis in the relative of one of our team members.NeuroPilot, the accessible autonomous digital assistant, was born from the idea that thought alone should be enough to control a computer.","NeuroPilot translates neuralEEG signals(brainwaves) into natural language commands and allows users tocontrol computer functionssimply by imagining speech; it is a real-time, end-to-end EEG-to-action pipeline. Put simply, it allows users to control computers with their minds.","NeuroPilot integrates neural and language data modalities with agentic workflows powered byScrapybara. The foundation of NeuroPilot is its brain-to-text component, which uses aBrain-Computer-Interface (BCI)hardware (OpenBCI electrode headset) to translate neural EEG signals into natural language commands. We used BrainFlow to extract real-time data from the headset’s Cyton chip. Our system interprets activity detected from OpenBCI EEG electrodes through a multi-step system. First, it uses anRNN model to identify phonemes(basic linguistic units) in imagined speech signals. To enhance performance in limited-resource settings, we applied transfer learning by first pretraining on high-quality invasive data to leverage relevant neural features. We then fine-tuned the model using our own dataset, which we collected and labeled with our non-invasive EEG equipment. Our dataset focuses primarily on phonemes found in words commonly used for computer control, such as “search” and “Google,” ensuring that the system is optimized for real-world task execution. Next, alanguage model-based decoder is used to determine the most likely sequence of phonemes according to the probabilities assigned by the RNN and its knowledge of n-gram patterns in language. Finally, we useprompt engineeringthrough the OpenAI API to clarify and isolate a command from the thought stream. We iteratively tested and refined our prompts to align well with Scrapybara’s agentic framework. For example, when a user intends to open an application, the system recognizes the intent and modifies the command accordingly (e.g., instead of just interpreting the phrase “manage file,” the model refines it into a structured directive like “Open file manager in applications”). We useFirebaseas our database to communicate in real-time between the BCI, the model, the resulting natural language command, and the client end of the program. Once the natural language command is determined, it is fed into an automated system interaction framework powered byScrapybara. The system translates the natural language commands into system actions on a virtual desktop and enables agents to execute user intent.","Brain-to-text and other works exploring using EEG signals have become a field of interest recently and have achievedvery high performancewith the introduction of more advanced deep learning techniques. However, theyoften rely on invasive methodsto obtain the signals. In order to make these systems more accessible, we faced the challenge of using relatively weaker and lower-quality signals from much less channels. We innovatively applied atransfer learning approachby leveraging high-quality and extensive invasive datasets to pretrain our model and then fine-tuning to data collected from the electrode cap. This allows us to make use ofrelevant features and patternsthe models can learn from these other cases and apply them to perform better in low-data andlimited resource settings. This increases theinclusivityof these technologies while minimizing the sacrificed effectiveness.","In bringing together our brain-to-text and agentic workflow components, we implemented an LLM-drivenprompt engineeringsystem that would optimize the very simple extracted natural language commands into effective commands that couldmake the most of the full powerful potential of Scrapybara’splatform’s powerful workflow automation.","A huge learning from the process of working on NeuroPilot was the experience ofintegrating a network of many different componentsinto a single, streamlined pipeline. From streaming inputs from our hardware and processing them live through our multi-step algorithm to communicating their outputs with our database and executing agentic workflows with Scrapybara, we learned how to build a complete and multifaceted product.","NeuroPilot is the future of digital accessibility and effective human-AI workflows with the power of your mind. A next step is exploring the potential of further modalities, such as tapping into a webcam to include limited facial movement visuals to improve the performance of the model. There are always constant developments in the field of BCI and neuroprosthesis, such as the recent Meta paper released this past week on novel non-invasive brain-to-text methods, and we hope to improve NeuroPilot to become more robust and accurate, bringing tomorrow into the now.","api, brainflow, firebase, huggingface, openai, openbci, python, pytorch, scrapybara",http://github.com/nicolesplaining/treehacks/tree/main
TreeHacks 2025,Miru,https://devpost.com/software/mapdash,True," Some fun photos from our build process. Haptic feedback system mounted on a wearable belt. Pathfinding algorithm given CV-processed map. More than 20 million Americans experience visual impairment, with 1 million classified as legally blind. For many, navigating indoor spaces independently is a significant challenge, limiting daily activities and self-sufficiency. While outdoor navigation solutions—powered by GPS and mapping services like Google Maps—are widely available, indoor navigation remains a relatively unsolved problem due to the lack of reliable spatial data and GPS limitations. Given that people spend most of their time indoors, the status quo is the reverse of what we need. We set out to create an intuitive, lightweight, and accessible indoor navigation aid for the visually impaired. With the rise of advanced video capture technology—such as the Apple Vision Pro and Meta Ray-Ban glasses—we saw an opportunity to harness computer vision and wearable technology to make indoor spaces more navigable for those with visual impairments. Miru is an indoor navigation tool for the blind, effectively serving as an indoor Google Maps. Miru provides real-time, step-by-step guidance to help visually impaired users navigate complex indoor environments with ease. A picture of the floor plan of the building—readily available on fire escape plans near elevators or fire extinguishers—is uploaded to Miru, which automatically generates waypoints and pathfinding routes. The user wears a pair of Meta Ray-Ban smart glasses for real-time visual input and localization, and a servo-fitted belt provides gentle haptic feedback to guide movement, ensuring a non-intrusive and intuitive experience. By integrating wearable technology with intelligent navigation, Miru empowers visually impaired individuals to move confidently and independently in indoor spaces. To convert fire escape plans into functional map data, we implemented Canny edge detection, extracting key architectural features such as walls and doors. Analyzing the map, we then generated waypoints, followed by A* pathfinding to compute optimal navigation routes. Live video footage was acquired through Meta’s Ray-Ban glasses. We tried to use streaming through Instagram to acquire the footage, but realized there was around 30 seconds of latency which wouldn't work. We found that streaming footage to WhatsApp provided data much faster, with less than 1 second of latency. We also had the challenge of extending sensory capabilities to the visually impaired; in particular, a system that could reliably provide direction. We ended up deciding on creating a wearable belt, created using servos as a tactile feedback mechanism, and it has five servos capable of generating directional instructions (left, right, forward, left-forward, right-forward). As the user turns around, we use our orientation mechanisms to provide feedback in the correct direction. Localization—identifying the user's real-time position indoors—was one of our most challenging tasks. Specifically, because it is impossible to access GPS, no indoor navigation system that relies on GPS will work. Our innovative approach involved two key steps: Besides localization, determining orientation was also crucial for our project’s success. We built our own iOS compass app that could interact with our websocket system using the iOS compass data to determine orientation and integrated it with a wearable utility built to guide user interaction. We found a bug in the iOS compass app, where the compass would drift randomly for no apparent reason. This resulted in even more pain as it meant that we had one less source of truth to trust. We immediately ran into latency issues with Meta’s Ray-Ban glasses. Previous projects we’d looked at involving Meta Ray-Ban glasses streamed the Ray-Ban footage to Instagram Live, because Meta Ray-Ban glasses are incapable of natively streaming to a laptop. Instagram Live, however, had a stream delay of ~30 seconds. We resolved this issue by streaming to WhatsApp and mirroring the phone screen onto a laptop, providing latency of <1 second. Parsing through vector embeddings via OpenAI’s vision model meant high latency, which was undesirable for real-time use. As a result, we opted to use Vespa AI’s API for efficient updates and queries. The most difficult challenge we ran into was localization. Determining where a user was proved extremely challenging, and we opted to use computer vision and similarity detection to determine the location of the user. Orientation was also an issue. Initially, we tried using tri-sensor IMU fusion, but sensor drift rendered IMUs inaccurate. We managed to pull iOS compass data as an effective and simple method of determining user orientation. We’re extremely proud of the progress we were able to make in just 36 hours. Prior to this hackathon, none of us had extensive experience with remote sensing, and we’re proud that we were able to develop a real-time, GPS-free indoor navigation system that required only a floor plan and live video feed. This serves as a simple alternative to beacon-based positioning. We’re also happy that we were able to create a functional and intuitive haptic feedback system that allows users to “see” through feel. We think that localization is one of the most difficult challenges within robotics and we think that the new age of AI has so much potential and ability to help robots better understand the world around us. We learned how to work extensively with computer vision, vector embeddings, basic hardware, and software integration, all of which were extremely challenging but rewarding to work with. We also learned how to integrate software with hardware for an intuitive and powerful experience. A few ideas we hope to implement:",,,,,,,"arduino, clip, edge, embedding, groq, healthcare, llms, meta-rayban, metaraybans, navigation, python, typescript, vespa, vlms",https://github.com/flatypus/miru
TreeHacks 2025,CodeFusion,https://devpost.com/software/codefusion-v9eq2x,True,"We were inspired by the thought of, ""what if Cursor could do this?"" Cursor composer, a powerful tool in it of itself, how much crazier would it be for Cursor to have access to video and audio, along with our terminals so not only could it write code, it would verify it's capabilities and more. You start with screen recording and talking to describe the issue or enhancement needed. We then process this using gemini flash 2.0 and turn it into a structured schema organized by timestamps. We extract relevant files and urls and then extract html from the web pages and turn them into embeddings. We extract relevant text using a RAG pipeline with Intersystems-IRIS. Once we have all this info we use codegen to edit relevant code and verify correctness by running the application. Streaming using the gemini live API was very difficult. We were able to stream video and audio and get real time responses asynchronously, but the text inputs stopped working. We instead processed the video before sending it to gemini flash 2.0 with a specific schema enforced. Setting up the VS Code extension to have the UI in the side panel instead of the tab was challenging. In addition, the extension was a web view and restricted display capture so we had to find a workaround to send video to the backend. We are proud of getting a fully functional backend with a multifaceted backend. We were able to get 3 distinct components (along with the frontend VS code extension) working: the video processing into a structured schema by gemini, the RAG with intersystems, and the agentic flow and tool calling with codegen.",,,,,,,"flask, gemini, intersystems, openai, opencv, visual-studio-code",https://github.com/vsahni3/CodeFusion/tree/main
TreeHacks 2025,Supernova,https://devpost.com/software/supernova-p1tum5,True,"The creator economy is evolving at lightning speed. With platforms like TikTok and Instagram fueling the rise ofshort-form user-generated content (UGC), influencer marketing has become a powerful way for brands to connect with their audiences. Yet, despite breakthroughs in generative AI—like dynamic video creation and realistic talking avatars—few platforms (including notable startups like Suno) have truly tapped into these tools to automate UGC ads from start to finish in a truly methodical (and not AI-sloppy) manner. We saw an exciting opportunity to leverage generative workflows and advanced AI research toradically streamline influencer-driven contentand make it more accessible as a channel. That vision led us to buildSupernova: a fully end-to-end, AI-powered ad generatorwith true access to the world’s knowledge.",Supernova takes a single product link as its input and handles everything else:,"We builtSupernovausing a multi-agent orchestration framework guided byOpenAI’s agent-first principles, enabling asynchronous operations and tool-based model calls for efficient task execution. Each major workflow was designed withspecialized tools, running concurrently to maximize speed and accuracy. For market research, we integratedPerplexity’s APIandFirecrawlExtract pipelines with a custom search schema builder, enabling recursive queries and nested searches for audience trends, pain points, and competitor insights. Real-time data aggregation fromBrave SearchandeCommercereviews feeds into a vectorized knowledge base, which is continuously updated during the monitoring process (as news updates occur) to power downstream workflows. During influencer profiling, we usedOpenAImodels with tool-calling capabilities to execute searches, scrape data from social platforms andFamousBirthdayswith Firecrawl, and extract insights from articles and interviews. Psychographic profiles are generated by scoring influencers on audience fit, controversies, and value alignmentusing weighted embeddings. ALlama 3.2 11B via Groq vision modelmatches these profiles against staticHeyGenavatars to produce the closest visual likeness or, for celebrities, directly using SyncLabs on real video clips of the personality. For B-roll generation, we prompt gpt-4o to generate structured shot lists based on the influencer profile and market insights. Simultaneously, a reference search pipeline collects contextual images to steerRunway ML’s Gen3Alpha Turbo image-to-videogeneration model in generating accurate scenes with and without product placements (depending on b-roll requirements). In the voice pipeline, we first expand psychographic audience insights using agents with web scraping and search tools. Using avoice embedding search over indexed profiles, we select a voice that best matches audience preferences and fine-tune it for tonal accuracy and emotional resonance, properly create the new voice clips withCartesia’s voice-to-voice API, and then match the new voice clips to the corresponding A-roll and B-roll segments for timestamps usingOpenAI Whisper. Finally, video assembly is fully automated usingFFmpegfor clip merging and synchronization. AZapCapAPI layer adds captions and carefully selected music generated from the script, ensuring style consistency and platform readiness. This orchestrated, tool-based architecture allows Supernova to generate high-impact, data-informed video ads with minimal human intervention and maximum creative precision.",,,,"We envision a future where marketing becomes seamless and effortless—yet every frame of an advertisement is purposeful, delivering meaningful value to sellers, creators, and viewers alike. We'd love for you to check out our gallery of public-facing ads that we generated at the link below:https://supernova0.vercel.app/gallery","agents, cartesia, groq, heygen, nextjs, openai, perplexity, runway, supabase","https://github.com/pr28416/ai-ugc, https://supernova0.vercel.app/, https://supernova0.vercel.app/gallery"
TreeHacks 2025,Runway,https://devpost.com/software/runway-x0ca3y,False,"We work on a ton of projects, and a lot of them gravitate towards fine-tuning to get the deliver the best product to users—most recently,https://opennote.me. As a result, we've had our fair share of run-ins with vague cloud service providers and tricky paywalls for building the datasets, and doing the training and deployment needed toactuallywork with them at scale (being broke students does not help either). So we decided to makeRunway, our version of Vercel for everything AI models.Now anyone can build rich datasets, select a model, train on production-grade cloud hardware, and deploy for easy external API use and sandbox use directly on Runway—all using natural language. We even give you next steps and advice throughout the process, from model selection foryouruse case to potential applications after deployment.","Runway lets anyone access and build AI models that they want—from non-technical to 10x engineers. Using solely natural language (and no ML background), you can:",,,,,,"browserbase, fastapi, groq, lambda, nextjs, perplexity, python, sagemaker, supabase","https://runway0.vercel.app, https://github.com/abhi-arya1/runway"
TreeHacks 2025,Trailhead,https://devpost.com/software/trailhead,False,"Many nights, my teammates and I have lied awake, driven by the urge to solve an interesting problem or learn something new. A general societal shift towards increased reliance on AI as a replacement for knowledge and overconsumption of short-form ""brainrot"" content threatens the magic and curiosity of our education. That's why we've decided it's time to go back to our roots...literally. IntroducingTrailhead✨","Our project,Trailhead, generates developer maps for any skillset you can imagine. Whether it's learning how to cook an omelet or becoming an expert at chess,Trailheadwill converse with you in order to figure out your learning objectives, current knowledge, and preferences for sources, creating a personalized workflow for you to achieve your goal.Trailheadstores your learning plans as ""hikes"", allowing you to revisit any of the sources you've already studied. You can then mark your ""vistas"" as completed to ensure you receive new, advanced content in your tree. Our web app has several features, including tree generation, tree interaction and viewing, and an RLHF and vector embeddings-based recommendation system representing user learning preferences. We also have a side-quests feature to recommend topics and learning path which the user is most likely to be interested in learning but hasn't though about.","We built this web app using FastAPI. There are two main components: a client side (frontend) and backend. We built the backend mainly using Python scripts, which manipulate and analyze data from the SQL tables on Supabase. We create customized GPT agents which can extract information from user queries. We have three main kinds of agents: an agent that extracts user intent, one that obtains user resources, and one that orchestrates resource deployment.","One of the main challenges we ran into was perfecting our prompt engineering to achieve optimal user outcomes. Orchestrating the AI agents in our workflow was quite difficult; we were thoroughly involved in type checking and schema validation. In seeking out solutions, we were able to address these issues by using Pydantic, which offers a clean and concise way to define data models using Python classes and automatically generates JSON schema.",,"From brainstorming our initial idea to building out the entire product, we were able to build a complete scaffold for our project with basic functionality relatively fast. However, we wanted to expand it to be much more useful with features that we, as students, could actually use. This project taught us to consider our own experiences, as well as the experiences of the average user, while building a fully integrated app.
Our team learned how to collaborate through stressful situations, hone our creativity by personalizing unique features, and expand our app from an initial vision into something that is more robust and even larger than our original plan.",,"css, fastapi, github, html, javascript, luma, next, openai, perplexity, pydantic, python, sql, supabase, tailwind, typescript, vercel","https://trailhead-eight.vercel.app, https://github.com/womogenes/trailhead/tree/interactive"
TreeHacks 2025,StoryBridge,https://devpost.com/software/storybridge-z5nrht,False,"As of 2020, the foreign-born population makes up about12% of the total U.S. population. These second-generation immigrants grow up with the unique struggle ofbalancing two culturesand reconciling the clashes between them. As children of immigrants ourselves, we know what it feels like to think and speak in a language different from our parents and relatives. This is why we createdStoryBridge—to help children connect with their heritage while embracing their new language.",,,,,,,"javascript, lumalabs, node.js, openai, perplexity, python","http://treehacks2025-xi.vercel.app, https://github.com/getina/TreeHacks2025/tree/0a03963078bc1c19598467dd76514388d0e45d8a, https://docs.google.com/presentation/d/1vWmL8ppZVggz-Z5oS4cKkZxZeeaYDmXhIpEh4gbcTdw/edit?usp=sharing"
TreeHacks 2025,ArmI,https://devpost.com/software/armi,False,"The inspiration came from the need for personalized health advice to help maximize the potential of those in the army. Throughout my own experience with workout recommendation apps, I have had a lot of experiences with apps that take the ""one-size-fits-all"" approach. Without being able to receive that individual advice from top to bottom, there is the risk of injuries and negative impacts on health.",ArmI is an application that through data extracted from wearable devices provides workout advice based on individual biometrics through the use of a powerful reinforcement learning model. This allows for an adaptive and self-improving model that does not generalize for a population but tailors for the needs of every soldier.,,,,,"ArmI plans on expanding this past further than just an army application but one that can benefit the general population's health. Additionally, the model can be enhanced to be able to choose from a larger and more specific range of exercises. Also, if ArmI's model were to have access to a larger dataset, then potentially, a pre-trained model can be implemented that is then fine-tuned to fit a person's biometrics and best workout plan. This could speed up the model and not require the model to require long training times before outputting a precise workout plan tailored to one's needs.","artificial-intelligence, cloudflare, dart, flutterflow, machine-learning, python, reinforcement-learning, terraapi","https://github.com/karammasad/ArmI/tree/main/api, https://www.linkedin.com/in/karammasad/"
TreeHacks 2025,Tradelingo,https://devpost.com/software/tradelingo,False,"The financial market is broken: 95% of retail investors lose money, vulnerable older adults are left behind by complex platforms, and over 80% of college graduates feel unprepared for real-world investing. Platforms like Robinhood Gold charge up to $1,000 per year, while market articles are lengthy, confusing, and inaccessible for beginners. Meanwhile, outdated college financial textbooks leave students without practical experience, and older investors face confusing tools that cost them their savings.
We built Tradelingo to fix this. Powered by quantum computing and DeepSeek AI, our platform delivers 97% accurate stock predictions, real-time market insights, and personalized reports tailored to each user’s profile. From students gaining real-world experience to retirees making informed investments, Tradelingo makes high-quality insights fast, simple, and affordable. With cutting-edge technology and human-centered design, we’re making Wall Street-level intelligence accessible to everyone because smart investing shouldn’t be a luxury.","Tradelingo is a cutting-edge stock intelligence platform that leverages quantum computing and real-time market insights from the web via DeepSeek AI to generate 97% accurate stock forecasts and portfolio optimizations with built-in risk management. Our goal is to make top quality financial literacy more accessible to everyone. Our platform delivers personalized, easy-to-understand reports based on user preferences, such as age, interests, and selected stocks—offering actionable investment recommendations tailored for all experience levels. With quantum algorithms, Tradelingo provides hyper-accurate predictions at any interval (per second, minute, day, or month), while DeepSeek AI compiles top market articles and financial reports into concise, insightful summaries. Designed with accessibility in mind, Tradelingo empowers everyone, from students building financial literacy to older adults seeking simplified tools, to navigate the stock market with confidence and clarity.","THE CORE: Quantum Computing: Tradelingo uses Quantum Phase Estimation (QPE), a powerful quantum computing technique, to achieve 97% accurate stock price predictions by analyzing the hidden patterns within stock movements. QPE estimates eigenvalues from stock price data to uncover topological features called Betti numbers, which represent market structures such as trends (b₀: components), cycles (b₁: loops), and voids (b₂: gaps). Our quantum circuit, built with Hadamard gates, phase rotations, and the Inverse Quantum Fourier Transform (IQFT), extracts these features from quantum-embedded market data, leveraging quantum superposition to process multiple outcomes simultaneously. When tested on Microsoft (MSFT) stock, QPE-trained models accurately predicted future prices, outperforming traditional methods. This success comes from our quantum approach to topological data analysis (TDA), where stock prices are converted into quantum states on the Bloch sphere, revealing deeper patterns for AI-driven forecasting. By combining quantum computing with AI, Tradelingo delivers faster, smarter, and more reliable market predictions—empowering users to stay ahead with cutting-edge technology. Our unique approach to interpreting data combines Topological Data Analysis (TDA) and Graphical (Node-Edge) Analysis, enabling deeper insights from complex datasets. Through TDA, we analyze multi-dimensional data structures, making it easier to identify hidden patterns and trends within large datasets. Meanwhile, our graphical analysis maps data relationships using nodes and edges, allowing us to compute correlations and coefficients to uncover connections between different market factors. By merging these methods, we gain a more comprehensive understanding of market behavior, enhancing our predictive models and delivering more accurate insights. Portfolio Optimization & Risk Management: How do we achieve the optimization in portfolio design?This project leverages quantum computing to revolutionize portfolio optimization and risk management by integrating Variational Quantum Algorithms (VQAs) and Quantum Imaginary Time Evolution (varQITE). Using parameterized quantum circuits (PQC), we encode financial data into quantum states, enabling efficient exploration of optimal asset allocations beyond classical approaches. The optimization process is further enhanced by Max-Cut-based graph partitioning, which is an NP-hard problem in classic computing,  allowing for structurally optimized risk-aware investment strategies. By harnessing quantum-enhanced heuristics, this framework demonstrates the potential of quantum computing in transforming financial decision-making. Back-End: 
    We scraped, embedded, and tokenized over 2,160 articles based on criteria such as relevancy, novelty, and bias to any given company within our selected companies. Utilizing Retrieval-Augmented-Generation (RAG), we gave our chatbot the ability to collect knowledge about how relevant and recent events are likely to impact any given stock trade. By collecting as much publicly available data as possible, we can effectively provide insights that can be used to assist our users in making a decision in light of world events. Front End:
Our frontend is more than just a stylish wrapper. At our core, our platform is designed for anyone, and of any age, to use. This is because finance is already hard enough – what are ETFs, options, technical analysis?? Nobody has time for that. Therefore, before even touching a line of HTML, CSS, or JS, we dove deep into user journeys to tackle how different user bases might approach our app and issues they may run into, all to provide the smoothest user experience. The most important part we focused on was the signup. Users want to focus on the markets, not on filling out forms. Therefore, we designed an appealing, snappy, and frictionless auth flow that takes users less than 30 seconds to fill out, while providing us with all of the information we need for our services. After that, we cracked down on the dashboard itself, making it as intuitive as possible. Users are automatically given a “for you” suggestion of stocks we think they should invest in (based on their profile), so they don’t have to worry about picking dangerous or random stocks they don’t know about. Beyond that, simplistic graphs help users understand what articles they want to analyse at first glance. Lastly, our chat agent (Aaron!) is super personable and easy to talk to. Users will have a good experience, not only with finance, but scrolling as well.","Problems that we faced integrating the quantum computing model to achieve 97% prediction accuracy required extensive fine-tuning and optimization of complex quantum algorithms. But we are able to successfully implement the quantum computing model in the end. 
When collecting articles and documents, we hit a number of rate limits that prevented us from developing our database. However, through team-work(and many email addresses), we were able to overcome this obstacle. In the end we are also balancing accuracy with speed, therefore we need to ensure real-time predictions without compromising accuracy pushed us to optimize our backend infrastructure.","We are proud to have implemented cutting-edge quantum computing algorithms that outperform current market prediction strategies, coupled with an AI-powered model delivering comprehensive market forecasts and risk management insights. Our team successfully built a seamless front-end and back-end integration, ensuring real-time performance, and crafted a user-friendly interface that presents complex market data in a clear, actionable format. This achievement merges advanced technology with practical usability, making financial insights smarter and more accessible for everyone","Through this project, we learned that real problems need real solutions—the gaps in financial literacy, especially for beginners, students, and older adults, are more critical than ever. We discovered that user-centric design matters, with simplicity and personalization being key to delivering clear, actionable insights rather than overwhelming data. We saw firsthand that technology is a game-changer, as quantum computing and AI integration unlocked new possibilities for solving real-world investment challenges. The experience taught us that the market craves innovation, with a strong demand for affordable, accurate, and real-time insights beyond what traditional platforms offer. Most importantly, we learned that collaboration is everything—combining technical expertise with market research, user feedback, and educational partnerships helped us build a stronger, more impactful solution.","2025: Develop platform, launch beta testing, and roll out with a major marketing campaign.
2026: Build university partnerships, launch mobile apps, and introduce educational licensing. End the year with a major update featuring AI-powered portfolio optimization.
2027: Expand globally, launch advanced analytics tools and Premium Plus Plan. Conclude the year preparing for IPO or Series B funding.","alphavantage, elasticsearch, fastapi, google-colab, groq, natural-language-processing, next.js, openai, python, qbraid, qiskit, rag, youtube-data-api",
TreeHacks 2025,VisionMate: the AI agent to help visually impaired people,https://devpost.com/software/orion-pcwlg3,False,,,"We knew coming into TreeHacks of our project idea, and we got right to building in minute 1. Jeet worked on all things front-end, Pratham worked on the object detection, Anaïs worked on text-to-braille conversion and image-to-audio conversion, and Eric worked on training the watch motion detection using a convolutional neural net.
The object detection was done to detect objects in a one-foot-radius of the app. We use an augmented reality kit which helps us get in-depth footage of objects around us. Once we detect an object, we send a signal to the watch app to trigger a response as haptic feedback.
We used the Stanford Product Lab to create the hardware component of the project with the braille.",,,"We learned about a variety of frameworks and how to integrate them together, including how to train convolutional neural networks using motion training data. We also learned about more Apple Watch development and iOS development.",We are excited to expand this product and hopefully take it to become a real AI Agent that can be used for sale. We are looking into speaking to accelerator programs about our product and/ or venture capital firms.,"c++, openai, python, swift, typescript, watchos","https://www.vision-m8.com/, https://github.com/anaiskillian/treehacks25"
TreeHacks 2025,Mustard Seed,https://devpost.com/software/mustard-seed,False,,"AI Journalling app to use LLM to categorize entries in specific areas useful to note one's progress as they journey through their Christian life(areas of sin, thanksgiving events, people events etc).",Open AI and Flutter Flow,New to Flutter Flow,,,,flutterflow,https://app.flutterflow.io/project/mustard-seed-9niir2
TreeHacks 2025,Om: Optimized Meditation,https://devpost.com/software/om-optimized-meditation,False,,"We developed an AI-driven system that personalizes meditation in real-time by integrating biometric data and visual analysis: Biometric Wearable Data:
Tracks heart rate variability, respiratory rate, electrodermal activity, blood volume pulse, and body temperature to assess physiological responses. AI-Powered Visual Analysis:
Uses video to monitor facial muscle tension, eye movement, head positioning, postural sway, and micro-expressions, providing insights into cognitive engagement and emotional states. AI-Powered Personalization & Feedback:","To enhance accuracy and effectiveness, we:","Developing Mathematical Models for Facial Recognition – We wrote custom mathematical formulas to analyze facial expressions and movements: Optimizing Real-Time Processing – Running API calls concurrently with audio playback in Streamlit required efficient request handling for Eleven Labs, Gemini, and Amazon S3 to minimize latency. This integration of mathematical modeling and real-time processing ensures accurate, adaptive meditation guidance.",,,"Deploy and test the platform on a team member's app, Novuslink. Have access to 250+ users. Wedge into the market going B2B with companies that have existing meditation apps, and eventually deploy a B2C application.","amazon-web-services, deep-learning, gemini-flash-2.0, opencv, perplexity, python, relational-databases, streamlit, terra-api, visual-language-models, vlm",https://github.com/rahulrejeev/treehacks-2025.git
TreeHacks 2025,Turwin,https://devpost.com/software/turwin,False,"API security testing is stuck between two broken approaches. Automated scanners quickly find basic vulnerabilities but miss critical logic flaws, while human security researchers can catch these flaws through intuition and understanding but take weeks. Companies need continuous testing but can’t scale human analysis or rely on basic automation. We built the Turwin to solve this problem. Turwin is an agentic workflow that integrates novel LLM-based and conventional solutions to finding critical vulnerabilities in web and mobile APIs. This enables our hackbot to operate just like a skilled security researcher on applications. Through the electronic healthcare system (EHR) simulations we have made, we show that Turwin can scan for and find vulnerabilities in seconds, eliminating the need for the painstaking manual analysis that security researchers typically go through.","Turwin is a hackbot that can detect unauthorized access vulnerabilities on an API. In particular, it focuses on access control vulnerabilities that currently no open source cybersecurity automation can detect. Below is an example that showcases Turwin's capabilities. In our case, we use an EHR simulation, where users can register as either patients or doctors. Patients can update their profile details and view their own test results, while doctors can create and access test results for specific patients. To make Turwin modular and easy to maintain, we have made our EHR simulation plain and simple, implementing only fundamental features of modern EHRs. Let’s say Alice, a patient, logs in and requests her test results through an API call like:GET /api/test_results?patient_id=123But what happens if Alice manually changes the patient ID to Bob’s (456) in the request?GET /api/test_results?patient_id=456 If the API doesn’t properly enforce access control policies in their codebase, this endpoint could return Bob’s private medical records to Alice upon request, which would have caused a serious security breach. Turwin detects this vulnerability long before it can be exploited by malicious parties. Preferably, Turwin would be run in development and staging environments by companies, long before products meet the end users.","Turwin uses two important files to make an initial LLM call: The OpenAI spec is crucial because it defines the request syntax and intended behavior of different calls to the endpoint, allowing us to restrict our search of vulnerabilities to valid requests that fit the format that the API expects.  For the request-response pair file, we use a custom crawler built through Playwright which takes advantage of the fact that the ui is the intended behavior for the apis. Turwin uses an LLM-powered analyzer which can find the “objects” in a website. We define “objects” as elements which allow for the interaction and manipulation of webpages. Our crawler feeds the “objects” and two files above into an LLM, which creates what we call an initial permission model. This permission model is updated as new requests reveal updates to objects in the web application and show objects that have not been seen before. To test Turwin, we developed a custom vulnerable application, Turwin Medical Center, which simulates a patient-doctor portal. This application is containerized using Docker to allow for easy deployment and controlled testing environments. To provide a user-friendly interface, we built the Turwin application using Windsurf, which enables users to input an OpenAPI specification file and visualize the API structure in a tree format. This interactive visualization allows users to hover over different nodes to scan for vulnerabilities.","One of the biggest challenges was defining what an “object” is to an LLM. While humans can easily determine interactive elements from request-response pairs (e.g., user, patient, doctor), conveying this understanding to an LLM is complex. Parsing the OpenAPI spec and network logs in a way that allows the LLM to extract meaningful access control rules required extensive experimentation.
Building the crawler also presented significant difficulties. We needed a mechanism that could not only navigate complex frontends but also capture interactions that reveal the true behavior of the underlying APIs. Handling dynamic content and authentication mechanisms while ensuring accurate request-response pair extraction was a major hurdle.
Additionally, designing an effective permission model that adapts as new interactions are discovered was non-trivial. Ensuring that Turwin could update its model dynamically without requiring excessive retraining was crucial to making it scalable.","We successfully created a modular framework that can be expanded to automatically detect security vulnerabilities in websites. Unlike existing solutions that rely on either static analysis or human-driven penetration testing, Turwin bridges the gap by combining automation with intelligent reasoning, reducing the time needed to identify access control flaws significantly.
Another major accomplishment was building a realistic and controlled testing environment using our custom EHR simulation. This allowed us to validate Turwin’s capabilities against real-world use cases in a way that is both reproducible and extensible.
Finally, we’re proud of the usability of Turwin. By integrating an intuitive visualization interface with automated security analysis, we’ve made it easier for security teams to test APIs without requiring extensive manual effort.","We gained a deeper understanding of the complexities of API security testing, particularly around access control vulnerabilities, which remain one of the hardest categories to automate. The process also reinforced the importance of balancing LLM-based automation with traditional cybersecurity techniques to achieve meaningful results.
We also learned a lot about the limitations and strengths of using LLMs for security analysis. While they can infer logical structures and patterns efficiently, ensuring accuracy requires high-quality input data and clear contextual guidance.
Additionally, our experience with building a crawler that accurately extracts real-world interactions showed us the importance of grounding automation in realistic use cases rather than synthetic or theoretical data.","Moving forward, we want to refine the object identification process further. Instead of treating objects statically, we aim to implement a more dynamic approach where objects and their associated permissions are continuously refined based on new interactions observed in live applications.
We also plan to expand Turwin’s capabilities beyond detecting unauthorized access vulnerabilities. Future iterations will include detection of business logic flaws, race conditions, and API misconfigurations that are often overlooked by conventional scanners.
Additionally, we want to improve integration with existing security tools and CI/CD pipelines, allowing organizations to seamlessly incorporate Turwin into their development workflows. This will help ensure that security testing happens continuously and at scale.
Ultimately, our goal is to make Turwin the go-to security assistant for API testing, combining the speed of automation with the depth of human-like analysis.","burpsuite, kubernetes, llm, openapi, playwright, python, react, windsurf",https://github.com/Turwin1729/turwin.git
TreeHacks 2025,SquadPulse: A Military Healthcare Analytics Platform,https://devpost.com/software/squadpulse-a-military-healthcare-analytics-platform,False,"As our team originally dove into the reality of frontline operations, we discovered that the statistics are sobering:25+% of military casualties occur during non-combat situations, with cardiovascular events being a significant contributor. Yet somehow, in 2025, we still lack a unified, real-time health monitoring system for our armed forces. Over60% of military healthcare data remains siloed, never integrated into a unified command center platform. This insight actually hit particularly close to home, as one of our closest roommates back in Cambridge has a family member in the military who suffered a sudden cardiac arrest in the field, nearly losing his life before receiving proper attention. We realized a seamless, real-time data pipeline could have flagged his deteriorating vitals and alerted medics much earlier. That personal scare lit a fire in us to build a solution ensuring no one else’s loved one slips through the cracks. And as a result, we truly believe SquadPulse stands ready to safeguard every heartbeat in our armed forces. Because every soldier’s life is a mission worth protecting.","SquadPulse leverages advanced data analytics to classify each soldier’s cardiac signals into categories—regular, irregular, or AFib—based on photoplethysmography (PPG) data and broader metadata. We utilized deep learning models (trained in Python, PyTorch, TensorFlow, and Excel pivot-based data orchestration) to ensure the system and model pinpoints anomalies. Commanders can view squad-level health or deep-dive into individual soldier stats in real time. SquadPulse at its core is a centralized dashboard that provides real-time insights. But where it truly shines is its multimodal analysis engine. We've integrated our PPG analysis with external datasets including EEG signals, stress biomarkers, and performance metrics to provide unprecedented insights into: We’ve integrated a multimodal chatbot that can ingest external datasets to deliver on-the-spot insights. Through natural language queries, leaders can ask questions like, “How is stress level trending across Platoon Bravo?” and instantly see correlated data—like how sleep deprivation might impact decision-making speed, or whether certain squads are close to critical limits in blood oxygen levels or vital cardiac health-adjacent resources. Such an approach, we genuinely believe, can help shape strategic decisions to maintain a healthier and fully aware fighting force.","Our development process focused on three key areas: First is the Data Processing Pipeline, where ... Second is the Machine Learning Architecture, where ... And finally, we can't forget about the Interface Development, where ... Our team is proud to have made a robust pivot-based ETL pipeline in Excel that aggregates thousands of rows from multiple subsystems. This “Excel Core” was shockingly efficient as it let us quickly transform data into training-ready format without specialized data lake solutions. Additionally, the ML-driven insights we’ve been able to derive has been fascinating; we successfully combined deep CNNs, attention-based LSTMs, and standard anomaly detection techniques to create a triple-ensemble classifier with high accuracy on real-world data. And plus, we’ve done all of this while maintaining a scalable architecture. Our microservices (React + FastAPI + TerraAPI) process data quite effortlessly from multiple squads (simultaneously) without bottlenecks. Finally, we love our UI and think it is very intuitive to utilize!",,,,,"fastapi, matplotlib, numpy, openai, perplexity, python, pytorch, react, rshiny, tensorflow, terraapi, typescript",https://github.com/michelleweon/treehacks-2025
TreeHacks 2025,J.A.R.V.I.S. — Augment Human Connection with Infinite Memory,https://devpost.com/software/j-a-r-v-i-s-augment-human-connection-with-infinite-memory,False,"Technology has always been about making life easier, but for many — especially older adults, people with dementia, or those with disabilities — it often remains inaccessible and stands as a barrier rather than a bridge. Remembering past conversations, recalling important details, or even keeping up with social interactions can be overwhelming. What if technology could do more than just provide information, but also help you connect with others on a deeper level, recall cherished memories, and navigate conversations effortlessly? By combining breakthrough AR/VR technology with cutting-edge AI agent networks, we envisioned a system that doesn’t just respond to commands but actively helps users engage with the world around them, without requiring them to remember every detail themselves.","That’s why we built JARVIS—an accessible, intelligent assistant designed to enhance social connection and memory recall. Whether you're having a conversation, trying to remember a name, or looking for common interests to spark a discussion, JARVIS is always there. It scours the internet to find mutual interests, recalls past conversations, and provides helpful context, ensuring that every interaction feels effortless and deeply personal. For individuals with dementia, JARVIS serves as a memory aid, helping them remember people, past experiences, and meaningful connections. For those with physical disabilities, it provides a seamless, hands-free way to interact with the world. By bridging the gap between human memory and technological capability, JARVIS doesn’t just make life easier—it makes life more fulfilling.","At its core, JARVIS gives the Vision Pro the ability to truly understand its environment by combining speech recognition, facial recognition, and memory recall into a specialized agentic AI reasoning system. Whenever you engage in a conversation, JARVIS leverages the Vision Pro’s eye-tracking system to identify the person you’re speaking with. If you've met them before, JARVIS instantly retrieves a summary of your past interactions. If not, it automatically scrapes the internet in real-time to gather relevant information — helping you bring up key details effortlessly. JARVIS remembers everyone you meet, storing conversation transcripts and contextual knowledge it gathers from the web. Using Retrieval-Augmented Generation (RAG), it pulls in accurate, relevant context from this vast memory to enhance interactions.
Beyond conversation, JARVIS responds to custom gestures to activate its agentic system, which intelligently determines the best response by assigning tasks to specialized agents, including: All of this is seamlessly integrated into a custom augmented reality HUD, naturally overlaying information onto the user’s surroundings.
We combined these elements into a low-latency, always-available assistant that understands both spoken and unspoken intent—bridging the gap between human intelligence and technological capability.",,,,,"all-minilm, all-minilm-l6-v2-agentic-ai:-langchain, avfoundation, beautiful-soup, beautifulsoup-facial-recognition:-dlib, cncontact, dlib, face-recognition, face-recognition-frontend:-swiftui, groq, iris, langchain, llama, llama-transcription:-whisper-(multilingual)-web-scraping:-selenium, openai, python, realitykit, selenium, sql, sql-database:-intersystems-iris, swift, swiftgestures, whisper",https://github.com/apple-314/treehacks25/tree/main
TreeHacks 2025,SnapAR3D,https://devpost.com/software/snapar3d,False,"We were inspired by the increasing demand for accessible 3D modeling tools and the incredible potential of augmented reality. Seeing how AR could guide users to capture high-quality photos, we envisioned a tool that bridges real-world imagery and accurate 3D reconstructions.","SnapAR3D uses AR to guide you in capturing optimal photos of an object, while precisely recording the camera’s positions and orientations relative to that object. This data is transformed into matrices and imported into NVIDIA Diffractive Reconstruction (nvdifrec) to generate high-quality 3D models.","We built SnapAR3D by integrating ARKit to capture photos and sensor data, and COLMAP to format the camera parameters. A custom conversion pipeline transforms the captured data into NeRF-compatible transformation matrices, which nvdifrec uses for reconstruction. We also developed a lightweight web host for easy sharing of the resulting transforms.json file.","One of our biggest challenges was fine tuning—adjusting the photo capture process and data transformation parameters to ensure that the final 3D model was as accurate and detailed as possible. We also had to address coordinate system discrepancies between ARKit and NeRF, and seamlessly integrate data from multiple sources.",,"We deepened our understanding in AR app development, gained valuable insights into how 3D models are built using 2D photos, and learned to always push through the challenges because it will be worth it in the end.","Next, we plan to improve the user interface for an even smoother capture experience, expand support for diverse objects and scenes, and fine tune the model further while enhancing real-time feedback during data capture.","arkit, colmap, javascript, opencv, python, realitykit, swift, swiftui",https://github.com/wwangg22/treehcks
TreeHacks 2025,Aegis,https://devpost.com/software/aegis-kz4qnm,False,"In a world where every second counts, especially in modern combat scenarios and emergency response situations, the golden hour—the critical period following a traumatic injury—often determines survival. According to a study of U.S. military casualties in Afghanistan and Iraq, 87.3% of all injury mortality occurred before reaching a Medical Treatment Facility, with 24.3% of these deaths being potentially survivable. Aegis was born from this realization—in high-stakes environments where every second counts, immediate medical intervention can mean the difference between life and death. While medics and combat lifesavers provide invaluable care, they can't be everywhere at once. We envisioned a future where every soldier and first responder carries their own automated guardian—a system smart enough to begin life-saving interventions in those crucial first moments.","Aegis is a wearable device that combines biometrics with computer vision to assess trauma and deliver accurate, life-saving medicine. Using an Apple Watch for vital sign monitoring and a camera system for wound analysis, Aegis is able to make informed decisions about medical interventions. Our current prototype features a dual-reservoir system with peristaltic pumps—one that would contain a hemostatic agent for bleeding control and the other with pain medication (such as Morphine). In most cases, users of Aegis would be in areas without an active network connection, which encouraged us to pursue an edge-based solution with the capability to process complex medical data efficiently. What further sets Aegis apart is its centralization through a command and control center, allowing unit leaders to monitor their team's medical status and location in real-time through a detailed UI and natural language queries like ""Who is most injured and where are they?""","We’ve deployed a biometric processor using an Apple Watch and Terra API in combination with  a Vision Language Model—VILA—directly on the Jetson Orin Nano, leveraging edge computing for real-time wound classification without cloud dependency. To reduce redundancy, we created a flask server to keep both models persistently loaded, enabling efficient, low-latency processing while maintaining accuracy in medical image analysis. Our edge model inputs complex biometric and visual data and outputs a medicinal dosage, which is then passed onto a prototyped delivery system that we've built out using two peristaltic pumps, a motor controller, and an Arduino for precise fluid control. Relevant data, including the dosage amount,","Running a sophisticated Vision Language Model on edge devices proved particularly challenging—we had to balance model performance with the Jetson's limited RAM. Initially, we faced significant latency issues as we were reloading model weights with each inference, which we eventually figured out and optimized. Hardware integration presented its own set of challenges. As software developers, this was our first time working with circuits, jumper wires, and physical components. We even had a Jetson Nano fry during development, forcing us to get a replacement and swap SSDs. The integration between WatchOS's HealthKit and Terra Streaming API proved more complex than anticipated, limiting our access to certain biometric data. Working with Swift, which was new to our team, had a bit of a learning curve as well, leading us to develop a lightweight iOS app focused solely on data transmission.","We are incredibly proud to have successfully worked with components we had never touched in the past. To be able to build a hardware prototype and successfully merge biometric sensors, vision technology, and edge AI into one cohesive system was not easy, and we're glad we took on the challenge. We also came up with the idea the day the hackathon started, so we had to order hardware components that would only come in the next day. Being able to successfully build something under such time pressure surpassed any of our expectations!","Throughout this project, we learned that combining hardware, software, and people's lives is a very delicate balance, and one that is both extremely important and also requires a lot of care and thought. Also, learning to balance performance with power and size limitations on platforms like the Jetson Nano was quite important—compute and energy are not unlimited.","We envision Aegis revolutionizing care for millions of first responders across the globe, who put their lives on the line for others. Beyond the field of military medicine, Aegis can serve as a personalized shield for firefighters, police officers, and paramedics. For military leaders, our product provides insight into where troops and medics can be most effectively deployed based on congestion of injuries.","elevenlabs, gemini, jetson-nano, mistral, next.js, supabase, swift, terra, vercel","https://tree-hacks-2025-aegis.vercel.app/, https://github.com/SpotDylan/TreeHacks-2025-Fullstack"
TreeHacks 2025,ConvoCoach,https://devpost.com/software/convocoach-ipvah3,False,"For over 70 million people globally, speech disorders turn communication—a fundamental human connection—into a source of anxiety. Stuttering, articulation challenges, and social anxiety can erode confidence, limit opportunities, and create isolation. While traditional speech therapy plays a crucial role, it remains out of reach for many due to cost, geographic barriers, and social stigma, leaving a gap that technology has the potential to bridge. The idea for Convo Coach emerged from a personal experience. One of our team members was once criticized in school for overusing filler words like “um”—a small comment that carried lasting emotional weight. This moment revealed a universal truth: even minor speech differences can fuel self-doubt. Many people hold back from speaking, not because they lack the words, but because they fear judgment. Over time, this fear discourages practice, leading to a cycle of avoidance that reinforces insecurities. Speech challenges are more than technical obstacles—they are deeply human struggles. A person might remain silent in a meeting to avoid stuttering or rehearse conversations endlessly due to anxiety. Convo Coach was created to break this cycle. By providing real-time analysis of speech patterns and filler words, it empowers users to practice and improve in a private, judgment-free space. More than just a tool, it’s a step toward making speech a source of confidence, not anxiety, ensuring that everyone has the opportunity to be heard.","Convo Coach helps users reduce filler words in their speech by providing real-time AI-powered feedback. When a user records themselves speaking, the app accurately transcribes their speech, highlighting unnecessary fillers like “um,” “like,” and “uh.” After analyzing the speech, an AI avatar responds dynamically, providing personalized feedback on areas for improvement, such as suggesting pauses before complex phrases or reducing filler words for clearer communication. By offering instant, interactive feedback in a judgment-free environment, Convo Coach helps users refine their speech and build confidence in their communication skills.","ConvoCoach is an AI-powered conversational trainer that helps users improve their speaking skills in real time. By leveraging advanced AI technologies, it listens to speech, transcribes it, analyzes patterns, and provides personalized coaching through a talking AI avatar. The system enhances fluency by identifying issues like filler words, pauses, stuttering, and pacing irregularities, offering actionable feedback in a natural and interactive way. To process speech, ConvoCoach uses AssemblyAI for transcription and OpenAI API for speech analysis, detecting patterns such as overuse of filler words, extended pauses, and inconsistent pacing. Based on this, it generates concise, constructive feedback with practical suggestions like “Try slowing down a bit” or “Reduce filler words like ‘umm’ and ‘like.’” The feedback is then converted into speech by D-ID, which syncs it with a talking avatar to create a more engaging and immersive coaching experience. By integrating AssemblyAI for transcription, GPT-4 for analysis, D-ID for avatar interaction, and Flask for backend processing, ConvoCoach delivers a seamless, real-time speech training tool. This AI-powered system makes conversational coaching more accessible and effective, transforming speech practice into an interactive, judgment-free experience.","Integrating a Flask API for speech processing while maintaining low latency was a major challenge. Speech transcription and real-time sentiment analysis required optimizing backend processing to ensure responses felt instant and natural. Balancing accuracy with speed proved difficult, as deeper speech analysis introduced delays that could disrupt user experience. Developing a real-time AI avatar that delivers personalized feedback added another layer of complexity. Generating dynamic, context-aware responses required fine-tuning sentiment analysis models and ensuring that the AI’s tone and pacing adapted to user input. Creating a seamless connection between the avatar and speech analysis while avoiding response lag was a significant hurdle. Synchronizing the full-stack architecture between Flask and React required careful API optimization. Managing user speech data efficiently, ensuring smooth frontend-to-backend communication, and refining feedback delivery were key technical challenges. Through rigorous testing and iteration, we built a responsive, interactive system that delivers instant, AI-driven speech feedback to help users refine their communication skills.","As newcomers to the hackathon scene, we are incredibly proud of building a fully functional project within such a short timeframe. Stepping outside our comfort zones, we embraced new technologies and tackled the challenges of creating a full-stack AI-powered speech coach from scratch. Beyond just technical achievements, we’re most proud that Convo Coach has real-world impact, providing an accessible tool that can help people improve their communication skills. We also think our Kenna avatar is pretty cool.","Through the development of ConvoCoach, we gained valuable experience in integrating multiple AI technologies into a seamless, real-time system. We deepened our understanding of Machine Learning for speech analysis, learning how to process live audio input, extract meaningful insights, and generate feedback in a way that feels natural and intuitive. Working with AssemblyAI, GPT-4, and D-ID, we navigated the challenges of interfacing different AI models, ensuring smooth synchronization between transcription, analysis, and avatar-based feedback to create an engaging user experience. Building ConvoCoach also pushed us to refine our skills in real-time data processing, minimizing latency while maintaining accuracy. We optimized API calls, synchronized AI outputs, and streamlined backend operations to ensure smooth performance. Beyond the technical aspects, this project reinforced the social and educational value of AI-powered speech coaching, highlighting its potential to help students, professionals, and individuals with speech difficulties improve their communication skills. This experience has shown us how AI can be leveraged not just for automation, but for meaningful, human-centered applications that empower users to communicate more effectively.","The development of ConvoCoach has opened up exciting possibilities for expanding AI-driven speech training. Moving forward, we aim to enhance the system by introducing gamification features, allowing users to earn experience points, level up, and complete speech challenges that encourage continuous improvement. By incorporating progress tracking, users will be able to monitor their speech development over time, fostering a sense of motivation and achievement as they refine their communication skills. Beyond individual speech coaching, we see potential for ConvoCoach to be applied in education and classroom engagement. A possible extension of the platform would be an AI-powered substitute teacher that allows educators to upload lesson plans, which are then delivered interactively through AI avatars. Students would join as avatars, engage in discussions through voice or chat, and interact with AI-driven prompts in real-time. By integrating with platforms like Pear Deck, this system could facilitate structured class discussions and quizzes, making learning more engaging and accessible. The broader vision for ConvoCoach is to expand its impact on accessibility and human-centric AI solutions. The platform has the potential to assist individuals with speech difficulties, improve public speaking skills, and provide an inclusive learning environment for those who may struggle with verbal communication. As AI-driven speech coaching continues to evolve, we are committed to refining ConvoCoach into a scalable, adaptive, and widely accessible tool that empowers users to communicate with confidence in any setting.","ai, ai-applied-sentiment-analysis, assembly-ai, css, d-id, flask, html, javascript, natural-language-processing, python, react",https://github.com/lag-gam/ConvoCoach
TreeHacks 2025,Chronos Calendar,https://devpost.com/software/chronos-calendar,False,"Keeping track of a busy schedule is a frustrating, manual process. Adding, searching, and editing events takes a lot more effort than it should, with the average American spending anywhere between45-50 minutes planning their day. Most calendar apps require a manual configuration of details (specific dates, times, and names) and don't adapt to how people plan their days. What if your calendar could think the way you do? Instead of forms and dropdowns, imagine just typing what you need — ""Block focus time in the mornings"" or ""Find time for dinner with Sarah next week"" — with your schedule update instantly. We're making scheduling effortless with natural language processing and smart event handling.","At a high level, Chronos is a set of intelligent agents working together to find the intent behind your prompt and schedule your event for you in real-time. We have combined this agentic workflow with a beautiful UI/UX to create a truly comprehensive AI scheduling assistant. Prompt Chronos to create, modify, or delete simple events like “Lunch with James at 12:45pm tomorrow”, or even complex queries like “I want to go to the gym withkunal@email.comat 8am every other day for the next 3 weeks.” P.S.--adding your university course schedule for the quarter/semester has never been easier! Preferences act as your “user profile.” This is where you put conditions that don’t often change and that Chronos must always keep in mind, i.e. “I don’t want to have any meetings after 6pm.”",,,"✅ User-Friendly Interface – Our intuitive design makes it effortless to manage schedules, with 35 people already on the waitlist for our Beta launch. ✅ Smart Multi-Event Scheduling – Users can create recurring or multi-event commands in plain English, like:
""Start going to the gym every night."" Chronos understands and schedules it seamlessly. ✅ Agent Transparency – Users can see how our seven specialized AI agents collaborate in real-time to process and optimize scheduling requests.","Agentic Workflows need to be Robust
    …
User Research is Crucial Organization is Key for Success Structured Workflows Keep Teams Aligned Moving Quick","This weekend, we spoke to fellow hackers to learn from their experiences and better understand their pain points. We now have a growing waitlist of 35 students. In the coming weeks, we want to keep iterating. Here’s what’s next:
Speech-to-Text Integration (Groq): Interactive Chat Functionality: Keyboard Shortcuts for Efficiency: Understanding Preference-Backed Goals:","chromadb, figjam, figma, google-calendar-api, groq, llama, next.js, python, react","https://github.com/sfkunal/Chronos, https://www.figma.com/proto/p2P43kxpTnfm2ILd8ZHXff/TreeHacks-2025?page-id=282%3A803&node-id=282-1081&viewport=-681%2C162%2C0.51&t=56PBUtfmvnRULU4y-1&scaling=contain&content-scaling=fixed"
TreeHacks 2025,booksy,https://devpost.com/software/booksy,False,,,,"On the technical side, the core challenge we faced was minimizing application load times as we needed to generate multimodal elements (story text, accompanying illustrations, and our booksy narrator) in real-time. Another challenge was keeping every new dynamically created page linked with the previous pages of a book. (Think: a linked listed with no previous pointer) For minimizing load times and latency, we relied on the ultra-fast Groq inference for Llama-70B and parallel processed creating the booksy’s voice and accompanying illustrations. For keeping pages linked to each other, we generated a custom UUID for each new book and kept passing this token between the server and the frontend to continue creating new story pages. A challenge we experienced on the UI/UX side was crafting a user interface that would resonate with both parents and children while maintaining intuitive functionality. Creating an experience that feels magical and engaging for a child, yet remains practical and navigable for adults, requires careful consideration. We wanted to prioritize playful elements without overwhelming the core storytelling experience. We ultimately achieved this balance through a mascot-guided interface and a familiar digital bookshelf element.","We're proud of how our diverse team of first-time and experienced hackers came together to create something special. Our Stanford first-timers brought fresh perspectives that complemented the technical expertise of our seasoned hackers from UBC and UW, leading to an application that exceeded our initial vision and turned into a project that we could all walk away proud of. The design team crafted an interface that perfectly balances whimsy with functionality. On the technical side, we successfully implemented real-time processing of three different types of media - generating text narratives, creating images, and producing voice output - all while maintaining a smooth user experience. Perhaps most importantly, we're proud of creating a platform that doesn't just tell stories, but helps families create and preserve meaningful moments together.","We quickly learned that effective communication was a powerful tool, being clear and honest about our vision while remaining realistic about what we could accomplish in the hackathon timeframe. Our design team discovered the importance of being responsive, constantly adjusting our UI/UX priorities based on real-time development progress.","Our primary focus is expanding the platform's collaborative features, enabling extended family members to become involved in a story's narrative. We would plan on integrating video calling capabilities, ensuring that distance never stands in the way of storytime. We're also excited about incorporating more interactive elements and diverse genres, while giving our beloved mascot, booksy the mouse, a more prominent role as an engaging guide through the app's features. Another idea we are excited about is having parents share their own childhood memories and experiences, transforming them into personalized stories that create deeper connections.","elevenlabs, groq, javascript, lumalabs, react, supabase",https://github.com/gigabite-pro/treehacks-25
TreeHacks 2025,Phonix,https://devpost.com/software/phonix-w943zc,False,"Worldwide, approximately one in four individuals over the age of 25 will experience a stroke in their lifetime, with an estimated 12.2 million people affected annually. Stroke remains a leading cause of disability across the globe, and this issue hits closer to home than we might realize. In fact, recently, a mentor and friend of mine suffered a stroke, resulting in Broca’s aphasia, a form of language impairment where speech production is severely impaired, while comprehension remains largely intact, due to damage in the brain. Broca’s aphasia heavily affects both stroke patients and the elderly in general, as cognitive decline progresses, presenting as an extremely difficult situation to navigate for both the patient and their loved ones. As a result, many patients that experience aphasia undergo speech therapy in order to regain their cognitive abilities and improve the situation, but several issues manifest. Firstly, speech therapy is costly, with different providers and forms of therapy averaging at almost $200 per session, and this cost is exacerbated by the need for ongoing treatment. Additionally, accessibility may be hindered by the difficulty or inability to see a therapist on a daily basis. Inspired by the personal and statistical impacts of Broca's aphasia, we thought about what we could do to resolve these issues, with a goal to prioritize the user’s experience in terms of accessibility, effectiveness, and personalization. Our solution is an AI-powered speech therapy web app designed to meet all of the goals mentioned above. It serves both healthcare and educational purposes, targeting stroke patients, the elderly, individuals with language difficulties, and students learning English as a second language.",,"The integration of OpenCV, Dlib, OpenAI's WhisperAI and GPT-4o-mini API, and ElevenLabs API forms a crucial foundation for our project. We utilize SQLite to store initial word and sentence data generated by RandomWordAPI Vercel and OpenAI’s API, respectively. This data is then displayed on our user interface, and the user’s pronunciation of the text is recorded. WhisperAI is employed to transcribe the audio into text, which is then compared to the expected text from the database. To enhance accuracy, we employ the english_to_ipa module to convert both the spoken and expected English text into the International Phonetic Alphabet (IPA). This enables us to precisely isolate each syllable and assess discrepancies between the expected phonetic transcription and the user’s pronunciation. Upon identifying the mispronunciation, the particular word and syllable are passed to the OpenAI's GPT-4o-mini API call, which generates a phonetically similar word specifically tailored to the user’s particular mispronunciation. Concurrently, WhisperAI processes the user's audio file through the Open Source Allosaurus function to generate a series of timestamps corresponding to the pronounced phonemes. By identifying the mispronounced phoneme and its timestamp, we match this with the corresponding frame in the OpenCV feed, capturing the precise moment the phoneme was articulated. Using Dlib’s facial landmark detection capabilities, we extract the user’s mouth positioning and compare it to a pre-labeled database of accurate mouth positions for each IPA phoneme. The system computes differences in the relative dimensional ratios to assess pronunciation accuracy, and personalized feedback is then provided based on these findings. This feedback is then coupled with ElevenLabs API’s text-to-speech functionality, allowing the user to hear the correct pronunciation, which is essential for improving performance. Furthermore, OpenAI’s API is invoked to provide a comprehensive response based on the user’s audio input as an additional layer of specific feedback that would be helpful to the user.","The execution of this project proved to be a challenge, especially with regards to the integration and alignment of audio and visual components. Specifically, we realized that aligning the timestamps of the audio file to the specific phoneme, which also had to be matched to the correct corresponding frame of the video, was a critical issue. As a result, we learned varying methods to combine both components, using language-dependent models of phoneme identification through an encoder and an allophone layer via Allosaurus, to align audio and video. Additionally, coming up with personalized feedback according to the user’s lip positioning was another challenge because it was difficult to determine what information we could extract from still frames. Therefore, in order to resolve this issue, we found a library of accurate or expected lip positions, and placed the images into a database labeled in IPA. Then, we used the ratios of lip dimensions using feature landmark detection in order to compare phonemes, and analyzed what each of the potential combinations of ratios implied in order to provide personalized and accurate feedback. Furthermore, the integration between frontend and backend was a challenge due to our many separate components–audio, visual, API calls, database, and facial analysis, as well as JavaScript UI. Therefore, we used React and Flask to facilitate the interconnection between each component in order to put together a pipeline for efficient data exchange and program composition. Accomplishments that we're proud of
TreeHacks 2025 was the first time participating in a hackathon for each of us, so we ultimately felt as though we learned a lot through hands-on experience and turned out to be an enriching and positive learning experience. In addition to learning computer programming-based skills, we also honed our collaboration and refined our ideation-execution abilities, all while having fun. We are delighted that we successfully completed this project within a limited amount of time, and ultimately were able to integrate all components even though we each focused on separate parts.",,"The process of developing Phonix was a valuable learning experience for all of us, from expanding our technical expertise to growing together as a team. To build the frontend of Phonix, we learned to use Javascript, React, and CSS. We developed our backend using Python, through which we learned to research and adopt modules such as allosaurus and english_to_ipa and the power of open-source. We also learned to integrate Python functions with Javascript server using Flask, make and manage API calls including GPT 4o-mini and WhisperAI, construct databases using SQLite, and detect facial landmarks using computer vision with OpenCV and dlib. We also learned how to build our own dataset for use in computer vision and image analysis. This project was also a meaningful opportunity in immersing ourselves in the holistic engineering process from start to finish. From ideating to researching to designing to programming to assembling Phonix, we learned the importance of flexibility, communication, and persistence in building a product from scratch in such a short time period. We found that frequent check-ins and updates are crucial in maintaining transparency on our project progress, especially because we chose to delegate the various components (frontend, audio processing, visual processing, etc.) to our team members. We also discovered that integrating these separate components can prove challenging, so communication and thorough planning is extremely important. Lastly, we learned that a hackathon is a great deal of fun and learning and greatly appreciated this opportunity!","Our hope for Phonix is to achieve widespread implementation, which can be achieved through collaboration with speech therapy programs, healthcare providers, and educational institutions as an accessible form of treatment or learning resource. By reaching a global audience, we will be able to drive meaningful impact via enhanced rehabilitation outcomes. The creation and implementation of Phonix will improve the recovery journey for millions of stroke patients per year, while facilitating more accessibility and personalization for a holistic experience; the product may also be applied for educational purposes and aid in the language-learning process. Our wish is for our product to live up to its name, using phonetics to assist users in regaining their speech skills, much like the rebirth of a phoenix.","allosaurus, css, dlib, elevenlabs, flask, gpt4o-mini, icspeechmouthpositioning, javascript, openai, opencv, python, randomwordapi, react, sqlite, whisperai",https://github.com/MakerofMarkers/treehacks2025
TreeHacks 2025,GenuisAI,https://devpost.com/software/genuius-your-genuine-genius,False,"The last thing you want your tutor to do is lie to you. Yet, 77% of users have been deceived by AI hallucinations when using chatbots for learning (Tidio). Students deserve to study with confidence, accuracy, and trust, but traditional AI tutors often fabricate information, leading to confusion and misinformation. We built Genuis because students shouldn’t have to second-guess and cross-reference their study tools. Unlike general AI models, which prioritize versatility over accuracy, Genuis only provides answers grounded in real course materials and lectures. Whether you're reviewing for an exam or tackling a tough assignment, Genuis ensures that your AI tutor is as reliable as your professor.","Genuis is an AI-powered study assistant that delivers fact-checked, course-specific tutoring by pulling directly from uploaded lecture notes, textbooks, and classroom materials. It never hallucinates answers—it simply doesn't generate responses unless it finds relevant context in your uploaded resources. And for those tired of copying and pasting to feed a chat bot content you want clarification on—we do exactly that! We keep an extensive database of all the prompts you’ve entered previously which is often refactored and holds on to the contents of pdfs, html, and even youtube videos that can deliver your answers and thorough understanding. Ensuring your minimal input, the database considers specific courses, professors, office hours, assignments, lectures, and much more so that you get answers at the click of a button. And the icing on the cake—our voice feature with vocal output option that makes it so you don’t even have to read!",,,,"Optimizing EfficiencyA key challenge was balancing response speed and accuracy. To optimize this, we implemented a confidence-based retrieval system, where the LLM rated its ability to answer a query without additional context. If the confidence was below 8, the system either queried our vector database or performed a web search, reducing unnecessary API calls and improving runtime efficiency. To prevent redundant data retrieval, we leveraged Elasticsearch to store past prompts, relevant links, and vectorized content. Using cosine similarity, we compared new user queries with stored data to determine whether an existing answer was sufficient before scraping new information. This approach significantly reduced latency and minimized redundant external requests. Tools & Design ChoicesWe designed our system using FlutterFlow for the front end and Python APIs for backend processing. Key tools and integrations included: OpenAI API for natural language processing and query understanding
Perplexity Sonar API for selecting the most relevant sources
Text-to-Speech APIs to enhance accessibility
Mathematical methods (vectorization, cosine similarity) for ranking query relevance
Elasticsearch for efficient data retrieval and indexing
These choices allowed us to create a system that prioritizes efficiency while maintaining high accuracy. By combining structured data storage, intelligent retrieval methods, and API-driven search capabilities, we ensured a fast, relevant, and user-friendly experience.","One of our main goals moving forward is to expand how we use the vector database to provide a more personalized and proactive academic assistant. Currently, we store past user prompts and relevant resources, but we want to take this further by organizing entire conversations to track student learning progress over time. By analyzing a student’s search history and previous interactions, our system could: Periodically update conditionals based on their coursework and study patterns.
Suggest study sessions when it detects gaps in understanding.
Recommend meeting with an advisor if it notices frequent queries related to struggling topics or career planning.
Integrate with calendars to automatically set up study sessions, tutoring, or academic advising meetings based on workload and deadlines.","amazon-web-services, api, elasticsearch, elevenlabs, flutterflow, groq, openai, perplexity, python, query, tts, whisper","https://github.com/fabianmolinaEE/TreeHacks25, https://app.flutterflow.io/run/d3DbI94DtyxW5RVApBvE, https://app.flutterflow.io/project/treehackscopy-prmpn4"
TreeHacks 2025,Identity AI,https://devpost.com/software/identity-ai,False,"Every year, 5 million human trafficking victims are transported using counterfeited or fake passports and ID cards. These fake identity documents not only fuel human trafficking but also facilitate identity theft, allowing criminals to assume false identities, evade law enforcement, and expand their illegal operations. The problem is only getting worse—according to The New York Times, a new generation of 'unbeatable' fake IDs is making it increasingly difficult to distinguish real documents from fraudulent ones. To combat this growing crisis, we developed IdentityAI.","Our project can be used to detect even the most perfectly crafted false identity card. We know that at least one of the fields on this ID is falsified, whether it be the photo, name, birthday, address, or all of the above. And, if the fields are all valid, then we can check the similarity of the ID photo to the face of its holder. So, we leverage web scraping, machine learning, and AI agents to find these discrepancies.
We take pictures of a person and their ID using the Meta Ray Bans glasses, process this information, and utilize agents to crawl the internet and verify the validity of the ID. We have a web UI where a user can view the results of our search, which include a face-matching confidence score, address, age, and other identifying information. Our project uses a multitude of strategies to achieve this goal. We built a web scraper to perform reverse image searching. We leverage Perplexity’s Sonar and other online sources to validate the person’s name, age, and address. At the end of this workflow, Identity AI presents results to the user, who will then know the validity of an ID regardless of its quality.","We built the backend using python and FastAPI. These include endpoints to OCR the ID, to gather additional information off internet searches, as well as interact with large language models with tools and agents. We leverage a Yolo model to detect the largest face in an image. This is then sent to our reverse image search tool to find related articles, pictures, and websites that could have correlating images. After receiving this corpus of information, we run the links acquired into crawler instances to retrieve the data from the links. This data is then fed into an Anthropic LLM to congregate the data into meaningful leads. Meta Ray Bans glasses do not have an API for us to use, so we got creative in order to take advantage of this cool technology. We created a facebook account to send images to from the glasses and had a chrome extension monitor the FaceBook Messenger DOM in order to then send these images to our server. Additionally, we leveraged OCR to compare the ID owner’s face and the photo on the ID, and we determine this similarity by taking an L2 norm of the image encodings. We then utilized a formula to convert this norm into a percentage based confidence score. We also queried Perplexity’s Sonar API to gather more leads on whether the information on the ID card is falsified. Furthermore, we experimented with an agentic workflow to leverage all these tools we created and make qualitative decisions about what information is still necessary in order to come to a conclusion about the realness of an ID. We built an orchestrator agent who interacts with OSINT agents and a decision agent. We provided these agents with function calls to various tools we built.","Creating agents that run together and deliberate with our dedicated tools to find the next best move. Whether that is to gather more information with our tools or if the information is already good enough, determine the final output. Our agents needed to decide whether to gather more information or finalize a decision based on existing data. Striking the right balance between exploration and resolution was tricky because without clear stopping conditions we risked infinite loops or premature decisions with low confidence. To tackle this, we implemented a structured decision loop that iterates through agent deliberation while enforcing constraints on information retrieval. Each iteration, the DecisionAgent assesses available data, determining whether to request additional OSINT queries or finalize its output. Other challenges that we ran into were the quality of the meta glasses. In testing, phone cameras capture images in high resolution allowing us to manually enlarge and crop the image. We also encountered difficulties in web scraping. Websites we interacted with had bot detection such as reCAPTCHA, and we got creative combining available tools to bypass these measures (e.g. mocking a real user’s chrome profile, randomizing delays when interacting with the DOM). All to allow us to continue our altruistic crawl of the internet!","Based on just an individual's face, we can build a comprehensive profile leveraging smaller pieces of evidence from various sources on the web. We can then cross reference this information with any ID card to validate identification with a high degree  of accuracy.
We truly embraced the hacker mentality, interacting with technologies and tools in ways that are not their original intended purpose, and thus we had to think outside the box to still be able to achieve our goals.
Overall, we are proud of the extent of the functionality we were able to build for this project.","Building complex agentic systems is challenging. Unlike standard software development, where logic is explicit, designing AI agents often feels like recursive programming, where you must construct the right scaffolding and then take a leap of faith, trusting the system to generalize and adapt dynamically. One of the biggest lessons we learned is that orchestrating multiple AI agents requires a different mindset. Instead of writing step-by-step instructions, we had to think about how agents interact, how they handle uncertainty, and how to design feedback loops to improve performance over time and rebound from errors. We also learned a lot about prompt engineering and system design in shaping agent behavior. Small changes in instructions, memory design, or action constraints could lead to vastly different outputs. For example, by adding a simple memory list of previous queries for one of our agents it effectively could learn from its past queries and make better requests as a result.","A potential extension of our project would be to actively match faces against the FBI’s national kidnapping database. By integrating real-time facial recognition, we could help identify trafficking victims who need to be rescued and alert authorities in critical situations.","anthropic, fastapi, fastpersonsearch, firecrawl, groq, openai, perplexity, python, react, typescript",https://github.com/mschubs/IdentityAI
TreeHacks 2025,Atlas,https://devpost.com/software/atlas-7ti0wp,False,"NASA's Exoplanet Watch citizen science project allows everyday people to make tangible contributions to extraterrestrial discovery. However, contributors are currently required to work through a lengthy and complex Google Colab notebook, which has proven to be a great deterrent. To alleviate this issue, Atlas automates much of this process in its backend, allowing its users to instead interact with an intuitive, game-like interface. Our hope is that Atlas becomes a space where kids develop a foundational understanding and interest in astronomy while contributing very valuable data for this collective initiative. As an Exoplanet Watch contributor myself (this is Kailash), I wanted an easier way to contribute myself and also a resource to get kids as young as even 3rd grade or younger interested and passionate about astronomy!",,,,,,,"django, expo.io, javascript, python, react-native, rest, typescript","https://github.com/kailashchanel/treehacks-2025-backend, https://github.com/kailashchanel/treehacks-2025-frontend"
TreeHacks 2025,ChainEscapeLite,https://devpost.com/software/chainescapelite,False,"Inspiration
We wanted to merge AI, blockchain, and cryptographic proofs to create an immersive, verifiable gaming experience. Escape rooms are a natural fit for Web3 because they require trust in game mechanics. By leveraging verifiable AI and on-chain commitments, we built a game where puzzles and solutions are provably fair—no black-box AI, just verifiable logic. What it does
ChainEscape Lite is a Web3 escape room game powered by AI-generated puzzles, where players earn or lose points based on their answers. Every puzzle solution is hashed and committed on-chain, ensuring transparency. Players connect their wallets, attempt solutions, and receive verifiable AI-generated feedback. Correct answers earn points and NFT rewards, while incorrect answers or skips deduct points. How we built it
AI Agent: Uses GPT-4 via OpenAI API to generate puzzles dynamically.
Verifiability: Uses Opacity (zkTLS proofs) to prove AI inferences are unaltered.
Data Availability: EigenDA ensures game state is tamper-proof.
Blockchain: Smart contracts deployed using Hardhat, Solidity, and Ethers.js handle scorekeeping.
Frontend: Built in React.js with wallet authentication for seamless user interaction.
Challenges we ran into
Integrating zkTLS proofs from Opacity into a real-time gameplay loop.
Ensuring AI-generated puzzles had computationally verifiable answers.
Designing an efficient, gas-optimized system for committing and verifying puzzle hashes on Ethereum.
Accomplishments that we're proud of
Fully decentralized AI-generated puzzles that are provably fair.
A seamless player experience integrating Layer 2 scalability.
Secure, off-chain puzzle verification with zero-knowledge proof mechanisms.
What we learned
How to bridge AI and Web3 in a provable, trustless way.
The potential of zk-proofs to enable verifiable AI applications beyond gaming.
Optimizing smart contracts for gas efficiency while maintaining security.
What’s next for ChainEscape Lite
Multi-player mode with competitive leaderboards and smart contract-governed bets.
Generative AI-driven game narratives that change based on user choices.
Expanding to AI-generated treasure hunts or on-chain education puzzles with tokenized incentives.",,,,,,,"ai:-openai-gpt-4-verifiability:-opacity-(zktls-proofs)-data-availability:-eigenda-smart-contracts:-solidity, ethers.js-blockchain:-ethereum, hardhat, layer-2-scaling-frontend:-react.js",https://github.com/tunderulesagain/Chainescape-Lite
TreeHacks 2025,Blacksmith,https://devpost.com/software/blacksmtih,False,"First, we witnessed how Webflow revolutionized web development by abstracting complex coding into intuitive visual interfaces, empowering non-technical creators to build sophisticated websites. This democratization of web development sparked our vision to do the same for machine learning - creating a platform that removes technical barriers and allows users to express their AI needs without dealing with the underlying complexity. Second, Databricks' AutoML platform showed us the power of automated machine learning for classical ML techniques like XGBoost. While their platform serves enterprise users and data scientists effectively, we saw an opportunity to push this concept further. Our vision extends beyond traditional ML by incorporating fine-tuned LLMs and making the entire process more accessible through intelligent agents that handle web scraping and data collection completely automatically, without the user having to lift a finger. Finally, we were captivated by the potential of complex multi-agent workflows to build smaller, more specialized models. The idea of multiple AI agents working in concert - each handling specific tasks like data collection, preprocessing, and optimization - inspired us to bring this vision to life during TreeHacks. This approach not only makes model development more efficient but also opens up new possibilities for creating highly specialized AI models.","Blacksmith is a no-code, text-only ML training and deployment automation platform. Buzzwords aside, all this means is you put in text, you get out ML models :) With just simple prompts like “Create an ML model based on George Washington”, you’ll get a personalized, historically accurate agent for the first president. Not only do you immediately have access to the chatbot just 10 minutes after you request your model, you also have an API endpoint that can then be as part of your larger multi-agent workflow. Traditionally, this process is extremely tedious and impossible for someone who is just getting into machine learning. For finetuning models, data has to be manually scraped and collected, converted into a Q/A format, labeled, uploaded, and finetuned. For training models from scratch, it’s even worse! You need to set up the machine learning codebase yourself and be very very careful when training these sensitive models. We want machine learning to be accessible to everyone, with the power of personalized agents in every person and organization’s hands. Blacksmith makes this possible. Given the text prompt, it first figures out the type of model and data that should be used. Then we use our extremely fast, custom, multi-agent Computer Use system to jump through webpages and extract raw data for our model. The data is automatically organized and labeled, then GPT and/or Mistral is used for the finetuning process. Once the model is fully trained, you have access to the API endpoint and the chatbot, making the world your oyster!","How we built it
To create an end-to-end process that transforms a simple user prompt into a fully functional ML model, we developed a sophisticated multi-agent workflow system operating in three distinct phases: Phase 1: Prompt Analysis Agent: Our initial agent performs deep analysis of the user's prompt through analysing three key aspects of what is required next in the workflow. Phase 2: Intelligent Web Scraping Agents - Our dual-LLM architecture orchestrates sophisticated web scraping: Phase 3: Model Fine-tuning & Deployment Agent The result is a fully automated pipeline that transforms a simple user prompt into a production-ready, domain-specialized LLM, making advanced AI development accessible to non-technical users while maintaining enterprise-grade quality and reliability.","The most difficult part of this process was the web scraping section. Given the unstructured nature of the web, it became near impossible for us to extract data using classical techniques, so we turned to state-of-the-art models like Computer Use for scraping. However, since most of these models use manual mouse and keyboard inputs, the time for webscraping simply was taking way too long. Given this, we decided to build our own Computer Use system that was much, much faster, but also more deterministic. Using a novel multi-agent optimization algorithm integrating Selenium, we enable step-by-step navigation on the web with source code reflection. A Thinker and a Worker work in tandem to create and run Selenium commands on the server, with constant summaries being returned to the frontend. With this, we cut down our time for data collection by multiple orders of magnitude!","Our biggest accomplishment was our novel autonomous web scraping technology described in the challenges section :) We’re also really proud of how integrated and end-to-end our solution is. The first time we were able to get the full system from prompt to chatbot running perfectly was a huge milestone for us, and we want to share the same delights we have training machine learning models with everyone in the world, not just with experts in the field.","We learned so much about the finetuning process and how all of these APIs work together. The orchestration process between multiple agents depending on each other was a huge learning curve at first, since it feels a little like managing a distributed system of really smart toddlers LOL. But this whole process was super fun and the integration with traditional tools like Selenium was a big part of our learning process. Outside of this, our background is largely in machine learning itself and not too much in the web domain, so building a web workflow that seamlessly integrates with our frontend was a big learning process for us!","We have so much planned for Blacksmith! The very first thing we want to work on is diversifying all of the models that we support. Right now, we support text models from OpenAI and Mistral, but we really want to integrate something like the RoboFlow universe into our system, where you can work on large amounts of computer vision models and train them from scratch based purely on a text prompt. For bigger enterprise use cases, we also want users and organizations to allow for training with their own unstructured data that we will automatically label and organize for them. A big pain point in finetuning is the data formatting part, and Blacksmith’s custom Computer Use is really really good at figuring out what data is important and how to format it best for finetuning purposes.","computer-use, fastapi, gpt, mistral, muli-agent-workflow, next.js, openai, python, selenium, typescript",https://github.com/tejasprabhune/blacksmith
TreeHacks 2025,Pyro*AI,https://devpost.com/software/pyro-ai,False,"Not so long ago in January 2025, the Palisades and Eaton fires took over Los Angeles and global headlines, consuming over 37,000 acres, destroying more than 16,000 structures, and tragically claiming at least 29 lives. As this event unraveled, we recognized the need for smarter, more sustainable wildfire management systems that prioritize safety and empower communities to act swiftly. That's why we built Pyro.AI—a centralized wildfire monitoring dashboard designed to protect lives, safeguard ecosystems, and foster resilience against climate-related disasters. Our dashboard extends beyond existing platforms like Watch Duty by offering tools tailored to real-time wildfire detection, risk assessment, and evacuation planning.",,"Our development process began with a critical question: Which helped us devise 3 core hypotheses: To explore these hypotheses, we wanted to quickly build a functional prototype. We acknowledged that one of the major pain points of product development is the gap between ideation and implementation. Because of this, we decided to utilize Lovable.dev due to its 3 core advantages: Next, to thoroughly explore our problem space, we conducted a Competitive Analysis of Watch Duty to identify opportunities and limitations in addressing real-time wildfire detection, risk assessment, and evacuation planning. Watch Duty is a free, nonprofit wildfire alert app that provides real-time updates through vetted data from firefighters, dispatchers, and first responders. Strengths Limitations Opportunities",Our development process faced several technical hurdles:,We successfully developed essential features that make a meaningful social impact:,,"Looking ahead, we plan to refine our core feature--automated evacuation planning by: As well as evacuation zone management:","mapbox, nextjs, python, pytorch, tensorflow","https://pyroai.vercel.app/, https://github.com/EthanSantos/pyroai"
TreeHacks 2025,TeachAI,https://devpost.com/software/teachai-kwfvas,False,"Teachers in secondary education often have to juggle lesson planning, grading, and mentoring—tasks that leave them with little time to provide personalized student support. Meanwhile, students who are still developing English proficiency can easily fall behind in a system where most educational resources are offered exclusively in English. TeachAI was conceived as a tool that would both lighten the grading and administrative burden on teachers, thereby freeing them to focus more on their students’ needs, and level the playing field for learners of all language backgrounds.",,,,,,,"flask, gemini, json, nextjs, openai, python, react, typescript, zoomapi",https://github.com/Laoluog/TeachAI/tree/main
TreeHacks 2025,Bongaloons (Bongo Cat IRL),https://devpost.com/software/bongaloons,False,Our team is inspired by building a product that brings people joy.,"Bongaloons is a project combining Bongo Cats and Guitar Hero, where you get to play with bongos (physical buttons attached to our custom game pad) to beats of many pop songs, meme soundtracks, and any audios of your choosing. We support two forms of gameplay: With this, users can play to beats that we've created or they make themselves. Built-in catalog of songs with varying difficulties (1-5 stars)  Have a song you want to bongo to but don't see it in our list? You can generate custom beatmaps at various difficulties :D . We use a combination of music theory, algorithms, and big brain moves to do this. Want to race your friends at Bongo cat?  Ability to pause/resume gameplay
Maintains accurate timing and score tracking during pauses  Our teammates did a great job with sound design, but that's probably just our own opinions and brainrot vibes at 4am.","Instead of using a game engine, we built our own app on web, which meant we had to build our own: This was difficult given that only one of the team member has experience with full-stack development or frontend, so in hindsight we probably should have used a game engine (lol). But it was fun because we learned a lot and it turned out great! We used real-time websocket communication for:","As expected, we ran into a plethora of challenges, but handled them with much serenity and grace. These include handling multimodal AI model development across various platforms, seamlessly incorporating hardware-software integration and tackling unforseen problems with our bongoes, and creating a cohesive design language and aesthetic ... Not to brag or anything.","We came in looking to make a fun and quirky product, and we did that and much more! By developing our hardware stack in parallel and having clean and quick integration with software, were able to really polish and expand upon our original vision.",,"In the future, we want to incorporate full-body gestures (like Dance Dance Revolution), more unique and intuitive controllers, et cetera. Come visit us at our booth and try out our game!","a100, cuda, hardware, lasercutting, mediapipe, python, redis, stm32, tensorflow, vite",https://github.com/bongaloons/bongaloons/
TreeHacks 2025,Genetic Disorder Detection via cDNA Frequency Map,https://devpost.com/software/genetic-disorder-detection-via-cdna-frequency-map,False,"As medical technology has progressed over the past several decades, genetic diseases have become increasingly prevalent, with many earlier treatable ailments presenting less of a concern. As such, the ability to gain early insights into genetic disorders at an early age, especially fetal, would allow medical practitioners to take preventative action and gain a better understanding of how to care for their patients.","Current fetal genomic examinations involve dangerously invasive processes for extracting amniotic fluid, risking infection and physical hazards. Our approach facilitates minimally invasive cDNA extraction from the mother's blood, which is then put through our extensive model to reconstruct the baby's genome and reveal any concerning anomalies.","Datasets depicting disease-labelled cDNA fragments and genomes were datamined from FinaleDB and PGP Havard respectively. cDNA fragments and locales were compiled into a frequency distribution, normalized, and then used as a probability distribution function to sample indices and associated DNA fragments from the full human genomes. The new DNA fragments and associated disease labels were then tokenized and then put into our LLM as training data.","The availability of high-quality genomic data was the biggest hindrance in this project. Datasets were either protected for patient privacy and of those public, most were either very small or messy, requiring extensive data cleaning. The majority of the time hacking was spent searching for datasets, and then web scraping their sites due to poorly  or nonexistent APIs.","We're definitely proud of powering through the web scraping process, which ended up taking around 8 hours in total, considering the massive size of the sequences, alongside difficulties with data formatting.",,,"juypyter, nvidia, python",https://github.com/mhtruong1031/treehacks2025
TreeHacks 2025,Tesla VLM Track,https://devpost.com/software/tesla-vlm-track,False,"We wanted to build an AI-driven system that leverages vision-language models (VLMs) to enhance Tesla’s capabilities, possibly in autonomous systems, customer support, or AI-powered insights.","The project integrates LangChain with OpenAI’s GPT models (and potentially Groq’s models) to handle conversational AI, maintain session memory, and process vision-related inputs.","We used LangChain to manage LLM interactions, session-based history storage, and potential multimodal processing through Groq’s vision-language models.","Optimizing memory storage for multi-session interactions
API constraints like token limits and response handling
Tuning the model for accuracy in vision-language tasks","Successfully implemented session-based chat memory
Integrated OpenAI’s GPT-4o-mini for fast, efficient responses
Explored Groq’s vision-language models for potential enhancements","How to efficiently manage session-based memory in LangChain
The strengths and limitations of different LLM providers
Strategies for optimizing AI models in constrained environments","Fine-tuning the model for specific Tesla-related applications
Expanding the use of vision-language models for real-time insights
Exploring hardware acceleration for on-device AI processing","chroma, numpy, ollama, openai, pandas, python, pytorch",
TreeHacks 2025,Sync,https://devpost.com/software/sync-79r18p,False,"For female athletes, such as professional competitors and military personnel, workout efficiency is crucial to their performance. Despite this, research has shown that 75% of elite female athletes report their menstrual cycle affects their performance, with over 50% identifying the mid-follicular phase (just after menstruation) as their optimal performance window. However, there is currently no seamless way for women to tailor their fitness plans to their menstrual cycles. Coaches and military supervisors often lack the necessary understanding of female physiology to optimize training regimens, resulting in inefficient workouts and increased injury risk. This gap in sports science and training inspired us to build Sync—a platform that empowers female athletes by syncing their workouts with their menstrual cycles. By leveraging health data and AI-driven insights, we provide an intuitive, science-backed solution that optimizes training for female physiology.",Sync is a comprehensive fitness platform that revolutionizes workout planning for female athletes through several key components:,"From the outset, we knew that integrating multiple APIs and ensuring smooth data flow would be a major challenge, so we carefully designed our approach to maintain both technical feasibility and usability. We started by defining our problem statement: empowering female athletes with personalized, cycle-aware fitness plans. From there, we broke down our implementation strategy into key milestones, ensuring that each component aligned with our overarching goal. One of our biggest decisions was how to handle health data synchronization. Instead of manually integrating with multiple wearable platforms, we choseTerra APIas our bridge to pull data from Oura, Clue, and other wearables -- pulling the data into Supabase to then query from. This decision streamlined our development and ensured broader compatibility with existing health tracking systems. Once we had access to accurate menstrual cycle and biometric data, we focused on integratingGemini AIto generate workout plans based on that information, ensuring our AI-generated insights aligned with scientific training recommendations. We also wanted Sync to be more than just a passive data platform—it needed to engage users actively. This led us to incorporateElevenLabs’AI-powered voice assistant, allowing the app to act as a virtual workout coach, guiding users in real-time through their personalized training regimens. To provide meaningful performance insights, we then integratedPerplexity AI, which allowed us to analyze trends in readiness scores, sleep quality, and workout effectiveness over time. Finally, we built robust export functionality, allowing users to share their health data in spreadsheet format, opening possibilities for integration with platforms likeParadigmfor further analysis. On the frontend, we leveraged Next.js, React, and TypeScript to ensure a seamless and interactive experience. We chose Supabase as our backend due to its flexibility and ability to store large amounts of user data efficiently. Finally, we hosted and deployed Sync on Vercel, ensuring continuous integration and rapid iteration.","We started off with ambitious goals given that this is our entire team's very first hackathon, and faced a lot of challenges along the way. One of the toughest hurdles was ensuring smooth data integration across multiple APIs. The Terra API provided a powerful interface for syncing menstrual cycle and fitness data, but handling data logic across different APIs from Terra to Gemini to ElevenLabs, etc, and reading/writing to the database throughout proved challenging. Another challenge arose with the AI-generated workout plans. Initially, Gemini AI’s recommendations were too generalized and did not fully account for the nuances of different menstrual cycle phases. We had to refine our prompts and implement custom logic to ensure that the AI-generated plans were truly personalized and effective. This took multiple iterations, but in the end, we were able to fine-tune the AI to produce actionable, phase-specific workout recommendations. Another challenge was that ElevenLabs provided an excellent text-to-speech interface, but making it feel like a real personal coach required thoughtful UX design and testing. We experimented with different pacing and voice styles to make the guidance more engaging and intuitive.","We are incredibly proud of building an end-to-end solution that seamlessly guides users from health data collection to actionable fitness plans. By integrating AI-driven insights, real-time coaching, and performance tracking, we’ve developed a tool that addresses a previously overlooked gap in athletic training for women. Beyond that, we’re proud of our ability to leverage multiple advanced technologies and integrate them into a single, cohesive platform. The synchronization between Terra API, Gemini AI, ElevenLabs, and Perplexity AI creates a unique, data-driven approach that truly prioritizes female athletes’ needs.","Throughout this process, we gained valuable insights into health data synchronization, AI-powered recommendations, and real-time voice interaction. We learned how to tune world-class AI models to apply them to real-world use cases and that designing for female athletes involves a level of personalization that is often ignored in mainstream fitness solutions. Additionally, this project reinforced the importance of designing for seamless user experience, particularly in the realm of health and fitness, where engagement is crucial to success.",We see enormous potential in expanding Sync beyond elite athletes to support all women looking to optimize their fitness:,"elevenlabs, gemini, nextjs, perplexity, react, supabase, terraapi, typescript, vercel","https://github.com/isakaii/sync, https://sync-zeta.vercel.app/health"
TreeHacks 2025,eHealth,https://devpost.com/software/r2d2-ltwyxd,False,"Telehealth services are expected to grow at an impressive rate of 39% year over year until 2030, a growth, in part, driven by AI assistants, chatbots, and modern distribution technologies. At the same time, we're facing a growing elderly care crisis. The global population of people aged 65 and older is set to reach 1.5 billion by 2050 – double what it is today. This dramatic demographic shift coincides with a severe healthcare staffing shortage. In the United States alone, we're expecting a shortfall of 86,000 physicians by 2036. This shortage is especially concerning in elderly care facilities, which are already struggling with low nurse-to-patient ratios.
However, we believe there's hope. By strategically implementing AI technology in areas and ways where it's needed most, we can create lasting positive change that touches lives across the world.
Though we had only 36 hours, we wondered if we could build an autonomous robotic assistant which harnessed the accessibility of telehealth, the overwhelming need for elderly care, and the efficiency of AI-powered nursing support? eHealth was born, a robot which we hope can inspire the world to create tools which will stand for the benefit of everyone.","eHealth is a multimodal agentic system which has the autonomy to move itself simply given a request to pick up and drop off anything in its environment. It is powered by two Visual Language Models, or VLMs, hosted locally on an Nvidia Jetson, and uses a Raspberry Pi to make API calls to both secondary devices and cloud computed LLMs. 
Beyond its tech, eHealth was made to be as helpful as possible to as many people as possible.
In the context of disability support, it autonomously identifies and retrieves objects in the environment based on natural language requests or app-based instructions. The agent's ability to be controlled via a joystick provides an additional layer of accessibility for users with different abilities, or for remote home and family safety.
In the field of healthcare, eHealth can help with basic patient care tasks, freeing up healthcare workers for more complex duties or more emotionally impactful encounters. The remote monitoring capability allows healthcare providers to oversee multiple rooms efficiently while maintaining patient privacy and safety. In mental health settings, the agent serves as both a physical assistant and a consistent presence, helping patients maintain routines and complete daily tasks while providing data about activity patterns through its monitoring system.","At the beginning of TreeHacks, our team scrounged up spare parts and within a few hours had built out a basic chassis to hold our robot. Having attended an Nvidia talk where they demoed their impressive Jetson Nano micro AI computers as well as NanoOwl, a live object detection VLM, we realized that this was a perfect opportunity to create what would become eHealth. 
We positioned webcams to act as the eyes of a dual VLM system that gave our robot a complete understanding of its environment so it could be as helpful as possible. We wanted eHealth to be a light in the dark, a helpful friend to anyone who wants to use it.
By combining both VLMs to contextualize its environment through multi-layered natural language processing and calculating the position of objects in 3 dimensional space utilizing hitbox depth software, we apply multiple layers of LLM inference to allow eHealth the ability to parse user inputs as a task or query, going through different decision trees for each of the two inputs. The task utilizes image recognition software string parsing from Gemini, feeds the data from this parsing into the VLM (NanoOwl) to do hitbox detection on “important” objects, wherein the 3d displacement process occurs with both movement and claw tracking.","The first challenge that we ran into was optimizing the latency between the web interface and the physical robot actions as the web interface ran gen-ai image and video inference based off of robotic actions. From more of a UI/UX design perspective, creating an intuitive user interface for different user groups so our product could be accessible in the healthtech sector was of utmost importance. 
A smaller but pivotal challenge was making sense of documentation that consumed tens of hours from our team, as we had to implement robust security for remote control, ensuring reliable object recognition across various lighting conditions, and balancing autonomous operation with human oversight added to the complexity. 
Finally, the cost of development with such a hardware and software intensive project was exorbitant as we had to navigate the challenge of integrating siloed AI backend systems with the frontend project while effectively utilizing Jetson Nano and Raspberry Pi as both computational and physical detection and action objects.","After countless hours of troubleshooting, debugging, and refining, our team finally built the ecosystem that is eHealth successfully, developing a multi-modal control system that adapted to different user needs, which ensured that the robot had a variety of use cases that would help assist the healthcare sector along with elderly care for years to come. 
A team favorite integration was a secure and responsive web interface allowing for smooth remote monitoring, while real-time video streaming minimized latency, making interactions feel natural. We were incredibly excited with such a system as it took over 30 hours of straight coding and hardware development for everyone on the team. The cherry on top was this level of inherent reliable communication between the web application and the physical robot tied everything together. 
At 5 AM, exhausted but exhilarated, we celebrated our success in the only way that made sense—a victorious 10-minute nap before diving back into the next challenge.","This project was more than just a technical challenge: it was a deep dive into the complexities of designing assistive technology that truly works for people. We learned firsthand how user-centered design shapes interactions, with UI/UX playing a crucial role in making systems intuitive and accessible. 
One of our team's biggest learnings stemmed from optimizing real-time communication in robotics, which pushed us to explore AI backend integrations. As a whole, eHealth really allowed us to understand the power of large language models and image detection systems. Balancing automation with human control also came with ethical considerations, particularly in ensuring AI doesn’t interfere with high-risk prompts while also recognizing its potential impact on livelihoods. 
Making complex systems accessible to the diverse user groups who are involved in receiving healthcare required creative problem-solving, as did integrating multiple technologies into a single, cohesive platform. Each challenge taught us something new, reinforcing the importance of designing with both technology and people in mind.","The future of our Robotic Agent is filled with possibilities, with a strong focus on transforming healthcare support. We plan to expand its integration with existing telehealth platforms, enabling seamless remote care. 
More sophisticated healthcare monitoring capabilities will enhance real-time assessments, while specialized routines and behaviors tailored for elderly care will provide personalized assistance. A network of connected devices will allow for large-scale institutional deployment, ensuring consistent support across hospitals, nursing homes, and assisted living facilities. AI-powered predictive care capabilities will further strengthen its ability to anticipate patient needs and improve outcomes. 
eHealth’s vision is to create a scalable solution that not only eases the healthcare staffing crisis but also enhances patient care while preserving the essential human connection through remote monitoring. Designed for flexibility, eHealth’s ecosystem will continue evolving with user needs and technological advancements, ensuring long-term sustainability in the ever-changing healthcare landscape.","gemini, google, jetson-nano, nvidia, python, raspberry-pi, vlm",https://github.com/sohvasa/treehacks
TreeHacks 2025,Roomba Nurse,https://devpost.com/software/roomba-nurse,False,,,"We hacked into a Roomba 692 using a serial port connection and designed a custom-built robot body from foam and vinyl to create an approachable, bedside-friendly form factor. A mounted iPad serves as the interactive face and interface, dynamically changing expressions and responding to patient needs. The system integrates:","Hacking into a Roomba’s hardware and repurposing it for autonomous hospital navigation required low-level serial communication and precise motor control—something not originally designed for this use case. Developing a real-time conversation system that could both understand patient intent and generate appropriate responses was another major hurdle. Additionally, ensuring the robot’s interface felt natural and engaging while remaining functional in a medical setting demanded thoughtful UX design and emotional AI tuning. The most challenging part of the project was making fully autonomous navigation with facial recognition work on the edge. We optimized and deployed a facial-recognition + CV powered navigation system onto a Raspberry Pi, so that all navigation runs locally, which took a lot of experimentation and optimization!","We successfully hacked into a Roomba to create an expressive, intelligent bedside assistant, capable of navigating hospital spaces, engaging in conversation, and autonomously delivering patient needs. Our system integrates speech-to-text, natural language processing, and facial expressions, making it one of the first emotionally aware hospital robots.","We gained a deep understanding of robotic hardware hacking, real-time AI interaction design, and the challenges of building assistive technology in a high-stakes environment like healthcare. We also learned how important expressiveness and human-like interaction are for making robots feel intuitive and trustworthy, especially for vulnerable patients.","We envision a Roomba Nurse at every patient's bedside. We envision a Roomba Nurse for every hospital room, providing seamless, hands-free assistance for both patients and nurses. Future iterations will include enhanced navigation, a broader range of item retrieval, deeper AI-driven patient engagement, and EHR integration for more personalized recommendations. Our goal is to make Roomba Nurse an essential part of every hospital’s care team, reducing nurse burden and improving patient experience at scale.","cnn, gemini, nextjs, openai, raspberry-pi, roomba",https://github.com/KevinWu098/roomba-ai
TreeHacks 2025,Heart-BEATS,https://devpost.com/software/heart-beats-1snate,False,"Heart-BEATS was inspired by the need for a real-time, personalized way to help individuals manage panic attacks and anxiety. Many existing solutions offer guided meditation or generic relaxation music, but we wanted to create a system that adapts dynamically to the user’s physiological state, providing a more immersive and effective calming experience.","Heart-BEATS listens to the user’s bodily signals, specifically their heartbeat, and uses real-time signal processing to generate custom music beats. Measuring several attributes of the heartbeat enables us to estimate a user’s emotions. Heart-BEATS adjusts the music accordingly, helping to restore a sense of calm and stability.",We integrated multiple components to bring Heart-BEATS to life:,,,,,"arduino, groq, openai, pygame, python",https://github.com/shainotshy1/HeartBEAT_TreeHacks2025
TreeHacks 2025,SeedLink,https://devpost.com/software/seedlink-8jrdxe,False,"Many species of plants are going extinct and traditional seed banking isn’t enough to save them. While we initially took inspiration from the Svalbard Global Seed Vault in Norway, which preserves millions of seeds, we learned that simply storing seeds isn’t enough to prevent the extinction of certain plant species. Current environmental challenges, including climate change, habitat destruction, and biodiversity loss threaten entire ecosystems. The removal of just a few key plant species can trigger the collapse of entire ecosystems affecting wildlife, food sources, and human livelihoods. We wanted to create a proactive solution that spreads awareness while empowering individuals to actively plant and nurture endangered species, rather than relying solely on storage systems. By combining AI-driven recommendations with community engagement and incentives, SeedLink makes conservation accessible, impactful, and rewarding for everyone.",SeedLink is an AI-powered platform that guides users in planting endangered species based on their location and environmental conditions. Users can:,"Frontend: React + Vercel for a smooth interactive web app experience and UI/UX design
Backend: Node.js with Firebase and Google OAuth for authentication.
AI Integration:",,,,"Perplexity Integration: We leveraged PerplexityPro to generate detailed agricultural reports, helping users make informed planting decisions based on climate conditions, soil types, and seasonal trends. While we primarily used PerplexityPro, we did not utilize Perplexity's Sonar, Sonar Pro, or Sonar Reasoning APIs, though they could enhance future iterations. Spreadsheet Management & Data Structuring: Our seed inventory is structured in an Excel-like format, ensuring efficient data visualization and easy updates.Demonstrating best practices in structured data management. Mistral AI API Utilization: Mistral AI plays a critical role in analyzing agricultural reports and seed inventory to determine optimal planting recommendations. This integration enhances decision-making accuracy User Experience (UX) Enhancements: We refined login authentication using Google OAuth, making it seamless and secure. Our UI/UX decisions prioritize simplicity, accessibility, and engagement. Venture Viability & Business Potential: SeedLink’s model extends beyond a hackathon project—it presents a scalable business opportunity in environmental conservation, AI-driven sustainability, and corporate CSR partnerships. Our work is, supported by our focus on sustainable incentives, partnerships, and long-term engagement strategies. Broader Context for Sustainability Track: We considered regulatory, financial, and social constraints to enhance venture viability. By addressing ecosystem balance, preventing invasive species, and ensuring equitable access","google-cloud, google-spreadsheets, javascript, mistral, perplexity, react",https://github.com/vicksey/SeedLink.git
TreeHacks 2025,D.E.V.I.S,https://devpost.com/software/d-e-v-i-s,False,"Devin is really annoying to use, but imagine you could just deploy full-stack apps with just your voice - pretty much J.A.R.V.I.S for software engineering.",Generates apps based on voice input and even uses computer use agents to mimic UI designs.,"Scrapybara for computer use to screenshot similar designs, Gemini's VLM to take the screenshot and describe it in natural language, ElevenLabs / Whisper for voice communication, and o1 for all of our software agents.",Figuring out how the APIs worked and stringing everything together. Also a lot of latency issues.,We somehow managed to get something working in this short timeframe.,You just gotta have fun with it tbh.,"A lot more integrations and getting a fully working web app. This could be a decent startup idea too, especially if it controls an army of software agents.","computer-use, elevenlabs, gemini, o1, scrapybara, vlms",https://github.com/jessechoe10/DEVIS
TreeHacks 2025,ArticuAIte,https://devpost.com/software/articuaite,False,"Interviewing is one of the biggest bottlenecks in hiring. Companies spend countless hours conducting first-round interviews, often struggling with standardizing evaluations and reducing bias. Candidates—especially neurodivergent ones—face challenges navigating interviews due to inconsistencies in assessment criteria. Our team, many of whom have experienced these issues firsthand, wanted to build a solution that streamlines early-stage hiring while ensuring a structured, transparent, and adaptable experience for candidates. That’s why we created ArticulAIte, a VR-enabled AI interview automation platform that allows companies to generate, deploy, and assess structured interviews using AI-powered avatars. Recruiters input a prompt, and our system automatically generates interview questions, evaluation criteria, and assessment metrics to ensure consistent and efficient hiring decisions.","ArticulAIte automates first-round interviews by enabling companies to create AI-powered interviewers that dynamically assess candidates based on customizable prompts. The platform provides: Automated Interview Generation – Recruiters input a prompt (e.g., “Assess technical skills for a backend developer. Ask about algorithms and data structures.”), and ArticulAIte generates a structured interview with relevant questions. Customizable Evaluation Criteria – The AI generates a list of assessment metrics based on the prompt and questions, ensuring standardized evaluations across candidates. AI-Driven Interview Deployment – Interviews can be conducted in VR or on a web-based interface, where candidates interact with an AI avatar interviewer. Post-Interview Analysis & Justification – ArticulAIte analyzes responses, extracts key metrics, and cites specific reasoning for each score, providing transparent, data-driven feedback. This allows companies to efficiently scale hiring, reduce recruiter workload, and ensure objective candidate assessments while giving candidates clear insights into their interview performance.","Building ArticulAIte required integrating AI-driven interview generation, automated evaluation, and immersive VR interactivity into a scalable hiring solution. Our system dynamically generates customized interview questions, conducts AI-powered interviews, and analyzes candidate responses to provide structured, data-driven insights for recruiters. We leveraged OpenAI’s API and LangChain for AI-powered question generation and answer evaluation, ensuring context-aware, industry-specific interviews. NVIDIA Omniverse and NVIDIA Cloud (USDCode) enabled real-time AI model processing, optimizing natural language understanding (NLU) and multimodal assessments. Hume AI was integrated for conducting technical phone interviews, allowing for nuanced analysis of speech and nonverbal cues. We deployed a VR-based solution on Meta Quest 3, aligning with Meta’s Most Technically Complex Hack track. Convai powered conversational AI capabilities to enhance real-time interactions with AI avatars. Unity was used to develop immersive and interactive VR interview experiences. Vercel was used for deploying and managing our web-based interface, ensuring scalability and performance. By leveraging these advanced technologies, we built ArticulAIte into a comprehensive AI-driven interview automation platform that streamlines hiring efficiency, ensures structured candidate evaluations, and enhances fairness in recruitment.","We had trouble with Omniverse generating a scene, which required additional optimization and fine-tuning. Additionally, we faced issues with APIs and extensive troubleshooting, ensuring seamless integration between various AI models and systems.","We successfully built a scalable, fully automated interview platform that allows companies to generate, conduct, and assess AI-driven interviews in minutes. Our citation-based evaluation sets us apart, providing transparent, data-backed reasoning for each assessment. Seeing our AI conduct structured, real-time interviews and generate meaningful hiring insights was a huge validation of our work.","This project taught us how to bridge AI-driven automation with human-centric hiring needs. We learned how to: Fine-tune LLM-powered question generation for interview scenarios. Optimize VR interactivity for professional settings. Design assessment models that prioritize both efficiency and fairness. Reimagine hiring processes to be more structured, scalable, and accessible.","We see ArticulAIte becoming an essential tool for hiring teams, with potential expansions including: More advanced assessment models incorporating real-time behavioral analysis. Industry-specific interview templates for specialized roles. Candidate performance tracking over multiple interview rounds. Seamless ATS (Applicant Tracking System) integration for streamlined hiring workflows.","chatgpt, langchain, metaquest, nvidia, omniverse, openai, unity, verecel",https://github.com/MohamedHAbdelnaby/ArticulAIte
TreeHacks 2025,Talk,https://devpost.com/software/talk-6vkz8d,False,"Our personal longing for deeper, more authentic connections fueled the initial spark behindTalk. The real breakthrough, however, came when we spoke with my partner’s father, who is enrolled in a 12-step program. He shared how phone calls can be intimidating, yet he craved daily interactions to help him stay accountable. After hearing similar stories from others in recovery, people struggling with communication gaps, anxiety, and the need for quick support, we realized a simple yet powerful solution could bridge this gap. That realization led us to createTalk.","Talkautomates quick, daily voice or video calls among members of a 12-step recovery group by predicting each user’s availability. The platform cues users when they’re likely to be free, then starts a short, time-capped call to reduce call anxiety and encourage more frequent check-ins. In this way,Talkhelps users maintain consistent accountability and support without the stress of scheduling or the pressure of long, daunting conversations.","We began experimenting withFlutterflow, but later pivoted away once we hit productivity bottlenecks. Rather than a linear build, our process has been a holistic journey centered on user needs and pain points. Along the way, we:",,"We’re especially proud thatTalkis more than a neat idea, it’s a platform we believe will truly serve its users. From the initial spark to our final integration, every step revolved around solving real challenges faced by those in recovery.","Through buildingTalk, we gained deep insight into the challenges faced by individuals with substance use disorders, and how overlooked these struggles often are by mainstream tech solutions. We also learned the importance of empathy-driven design, continuous user feedback, and staying flexible when refining our product vision. Most importantly, we discovered that focusing onhuman connection, not just technical features, leads to more impactful solutions.","We want torevolutionize how people connectby moving beyond text-based interactions toward genuine, real-time voice or video calls. After refining and scalingTalkfor 12-step communities, we plan toexpand to broader social networks and circles, helping anyone, from families to remote teams, stay closely connected without the pressure or friction of traditional phone calls.","figma, firebase, flutterflow, google-colab, openai, perplexity, python, twilio, windsurf",https://github.com/danielodicho/Talk
TreeHacks 2025,Grant Scoper,https://devpost.com/software/grant-scoper,False,"With the new presidential administration, has come frightening uncertainties in sources of funding for research and other academic projects. The current administration has proposed dramatic cuts to the NIH and other government entities that provide critical sources of funding to academic institutions. Additionally, funding opportunities at Stanford and other universities are currently very decentralized across hundreds of web pages and email lists. To address these challenges of funding and decentralization, we built Grants Search, a tool that helps University members find other funding opportunities within the University that are not directly dependent on government resources. Also, in such a large scale research institution, it can be challenging and time-consuming to find faculty leading research that students and postdocs are most interested in. To address this, we also included actively funded NIH projects at the University in our database.","The user inputs a query of a project that they hope to find funding for or work on. Grants Search then searches for funding opportunities (grants, fellowships, scholarships), ongoing projects and research (including NIH funded work), and student and university organizations to find opportunities most closely related to the user’s project of interest. The funding amount and timeline is displayed for all opportunities. The user’s search can be further narrowed down by their academic status at the university (Undergraduate, Masters, Coterm, PhD, Postdoc, Faculty).","We used ReactJS, NextJS, Python, and Tailwind CSS for this project, with the support of Perplexity and Windsurf and Perplexity.",,"Finding, scraping, and categorizing 1.5k+ grants
Building an agent for website scraping across varied websites
Building an intuitive front end user interface using NextJS, ReactJS, and Tailwind CSS. Added actionable information and links (i.e., original grants website, add deadline to google calendar, due date countdown)
Adding filtering and sorting functionality",Semantic search is the best way to find relevant projects to the user’s interests,"We hope to scale Grants Search to other academic institutions across the country and save researchers, student engineers, and faculty thousands of hours every year!","nextjs, python, react, tailwindcss",https://github.com/Bio-Programmer/grants-search
TreeHacks 2025,Engage: Real-time AI Agents for Smarter Classrooms,https://devpost.com/software/engage-real-time-ai-agents-for-smarter-classrooms,False,"As teaching assistants for large courses, we understand the difficulty of addressing individual student needs in real time. With resource constraints and high student-to-faculty ratios, traditional classroom analytics tend to rely on surveys or assessments, failing to capture live student comprehension and causing a disconnect between teachers and their students.
Engage changes that. Unlike previous solutions, Engage leverages real-world physical interactions—body language, speech, and classroom behaviors—to create a proactive AI teaching assistant that provides instant insights into student engagement and understanding. Engage also enables teachers with powerful presentation features capable of evolving to meet student needs during lectures, using agentic AI to dynamically insert information and provide additional resources.","Engage is powered by a real-time multimodal AI, analyzing video, audio, and text to assess student attentiveness, participation, and comprehension, both per-class and long term. It also offers live content generation features to assist teachers by instantly creating learning aids, allowing for evolving presentations which answer students’ questions with increased clarity.
From a live classroom feed, the system:","Engage is built on Google Gemini 2.0 Flash Experimental, optimized for low-latency streaming inference. Engage dynamically calls custom tools in response to observed classroom events (e.g., detecting confusion, tracking student participation). 
Engage’s RAG-based content generation agent uses InterSystems IRIS for its backend, serving as a vector search-enabled SQL database which stores course materials for easy semantic access. SBERT is used to calculate vector embeddings for course content.
The application itself is built using TypeScript, React.JS, and Node.JS, with Python used for some APIs.","One of the biggest challenges was ensuring that each tool for the agent served a distinct yet complementary role within the system. Engagement tracking, event logging, and Q&A assistance needed to operate independently yet cohesively, with each agent serving a specific, specialized purpose, yet being able to interact with other agents effectively. We tested extensively to tune our prompts and injected heuristic logic where possible to ensure consistent output in a variety of classroom situations. We expect that as language models continue to become more accurate and cheaper, Engage will be able to derive further utility from the models it is built on. Another challenge laid in integrating the multiple technologies comprising our stack. Between multimodal models, vector databases, and agent tool calling, we found it difficult to properly link all components even with extensive planning between teammates, especially with time pressure of a 36-hour hackathon. Most of our team was also inexperienced in web development, which made for a challenging but an overall enjoyable experience as we learned new technologies and frameworks while building our project.","We’re proud of building our intuitive user experience within the time constraints. We’ve never previously worked with multimodal language models, especially in an educational context, and it was exciting for us to explore this technology and apply it to an area that we believe benefits from additional data at the physical level.","We learned a lot about orchestrating and planning agent tools effectively, as well as how to work with multimodal data. We also picked up skills in web development as two of our team members lacked prior experience, and combining the various elements of the project involved significant debugging and integration skills which we are sure will be useful for future projects.","We feel confident that real-time AI-powered classrooms will soon become the norm. We hope to integrate Engage with existing LMS systems, seamlessly linking content that students see in the classroom with online sources. We’re also interested in incorporating wearable data into our system, adding metrics like heart rate, fatigue, and readiness to better understand student well-being in the context of education. By aligning real-time data streams, proactive assistance, and personalized analytics, we’re excited to contribute to building the classroom of the future.","gemini, intersystems, multimodal, rag, video",https://github.com/luca-borletti/engage-ai
TreeHacks 2025,SquadPulse,https://devpost.com/software/squadpulse,False,"In high-stake environments like military operations, emergency response teams, and extreme sports, health monitoring is not just a luxury it’s a necessity. SquadPulse aims to empower teams with real-time medical insights using PPG waveforms and metadata, ensuring mission readiness and proactive health management. By extracting key biomarkers like heart rate (HR), heart rate variability (HRV), and SpO₂ levels, SquadPulse can detect early signs of fatigue, stress, dehydration, and cardiovascular risks. AI-powered trend analysis enhances decision-making, allowing squads to respond before minor issues become critical. With SquadPulse, we’re not just tracking health; we’re building a resilient, high-performance future where every heartbeat counts.","In today’s world, preventive healthcare is more important than ever. Fitness tracking has evolved beyond counting steps and it’s about understanding the body’s signals and making informed health decisions. By integrating PPG-based biometrics, AI-driven insights, and personalized health analytics, modern fitness tracking can go beyond generic recommendations to adaptive wellness solutions. Whether it’s detecting early arrhythmias, monitoring recovery patterns, or personalizing training regimens, the future of fitness is precision-driven and data-backed. From elite athletes to everyday users, real-time health tracking bridges the gap between fitness and medicine, ensuring that everyone can train smarter, recover better, and live healthier. 🚀 The future of health tracking isn’t just about numbers. It’s about unlocking human potential.","We started by diving deep into the healthcare and fitness monitoring space, making sure we could truly understand the data we were working with. Using the Terra API, we gained access to critical biometric data, like PPG waveforms, and explored advanced techniques to extract meaningful insights. This allowed us to focus on important biomarkers such as heart rate, heart rate variability, and SpO₂. By training AI models to recognize patterns in the data, we were able to catch signs of fatigue, stress, or potential cardiovascular issues early, helping keep squad members healthy and ready. We also made sure to track trends over time, so we could make proactive decisions about fitness and well-being. For the real-time analysis, we built an intuitive front-end dashboard with React.js to ensure that both squad members and healthcare professionals could easily interact with the system. On the back end, Python Flask helped us process the data quickly and efficiently, so the flow of information was seamless. We optimized our algorithms to handle large datasets without delays, ensuring that actionable insights were delivered in real time during training or missions. We also designed the system to be scalable, so it could be used in various environments, from field operations to high-performance sports. By combining cutting-edge AI, real-time processing, and a user-friendly interface, SquadPulse became a versatile tool to help teams stay healthy, safe, and mission-ready.","One of the biggest challenges we faced was ensuring the accuracy of the biometric data, especially from the PPG waveforms. These signals can be noisy and prone to fluctuations, which made extracting reliable heart rate (HR), heart rate variability (HRV), and SpO₂ measurements difficult. To address this, we had to develop and fine-tune advanced filtering algorithms to ensure that we could accurately detect the key biomarkers. Additionally, synchronizing data streams from different sources like PPG signals, accelerometer data, and metadata was complex. Aligning these signals in real-time was crucial for providing accurate insights, and this required robust data integration techniques to ensure a seamless and synchronized experience. Another significant challenge was adapting our AI models to handle the wide variety of physiological states in high-performance environments. The factors affecting health can vary greatly, such as fatigue, stress, or environmental conditions like temperature or altitude. This made it difficult to create machine learning models that could reliably detect early signs of health issues across different conditions. Real-time data analysis was another hurdle, as we needed to ensure that the system could provide fast insights without compromising accuracy. This required optimizing our algorithms for speed, streamlining data processing pipelines, and refining the system to handle large volumes of data efficiently. Gaining domain knowledge in health monitoring was also a learning curve, as we had to consult with healthcare professionals and experts to ensure that the insights we provided were both actionable and medically sound.","We’re incredibly proud of the proof-of-concept we’ve developed with SquadPulse, successfully integrating cutting-edge technology with real-time health monitoring for high-performance environments. Our system can detect key biomarkers such as heart rate (HR), heart rate variability (HRV), and SpO₂ from PPG waveforms, providing actionable insights that help individuals proactively manage health risks. The AI-powered trend analysis we implemented effectively identifies early signs of fatigue, stress, and cardiovascular issues, enabling smarter decision-making. We’ve also built a seamless, scalable system by combining AI models, a responsive React.js front-end, and a Python Flask back-end, ensuring real-time data analysis and intuitive visualization. This accomplishment reflects not only technical innovation but also our commitment to addressing real-world health challenges, making SquadPulse a valuable tool for squads, athletes, and healthcare professionals.","Building SquadPulse taught us the importance of data preprocessing and synchronization, particularly with noisy PPG signals and aligning multiple data streams like accelerometer and metadata. We gained technical expertise in developing efficient algorithms for real-time health monitoring and learned the crucial role of context in interpreting health data. Understanding variables like fatigue, environmental conditions, and stress response was key to enhancing our AI models. Additionally, collaborating with healthcare professionals and fitness experts broadened our knowledge of health monitoring in high-stakes environments. This experience helped us refine our system to provide medically relevant insights, ensuring our technology is not only accurate but also practical for real-world health management in demanding situations.","Our next steps focus on enhancing the accuracy and adaptability of our AI models, particularly in detecting early-stage health issues like arrhythmias, dehydration, or stress, while providing more personalized insights based on individual health profiles. We plan to refine our algorithms and leverage additional data to improve the system’s ability to respond to complex, real-world health scenarios. Additionally, we aim to incorporate real-time alerts to notify users and teams of critical health conditions as soon as they arise. Looking further ahead, we aim to integrate SquadPulse with additional features like personalized nutrition plans and adaptive fitness regimens, as well as develop early-warning systems for cardiovascular health. By expanding the system’s capabilities and minimizing latency, we plan to integrate SquadPulse with wearable devices and other health tracking platforms to create a seamless, all-in-one solution. Ultimately, we aim to empower individuals and teams to proactively manage their health and performance, making SquadPulse an indispensable tool for high-performance environments.","chroma, delve, perplexity, python, react, terra",https://github.com/jathinsn27/Squad_pulse
TreeHacks 2025,Tradegentix,https://devpost.com/software/agentictraders,False,"Tradegentix Logo Trading Interface Analysis Agent System Diagram Monte Carlo Simulation Stock Data Tesla Challenge Video Analysis Diagram (scroll for FlutterFlow, Perplexity, Rox, Tesla, and Agentic Workflows!) Let's say Stock A goes up at t=0 (where time is t). The idea is that there is a latency between a related/correlated Stock B that may be positively or negatively correlated at t = 3. We would use this tool to predict this correlation and buy at t=2 before Stock B response to some market force affecting Stock A. More specifically, our team developed asemi-high frequency trading systemleveragingCUDA C++ accelerated computingthat allowed for anagent network based chain of reasoningin order to use qualitative factors to explain the quantitative phenomena. Our agentic workflow allows us to benefit from market research and news analysis for a robust report. Our semi-high frequency trading system, powered by CUDA and chain of reasoning, comprises the following components: CUDA-Accelerated ICA:We implemented acustom CUDA-based Independent Component Analysis (ICA)module to extract statistical market forces from mixed signals. This approach is based on the concept that while independent source signals are non-Gaussian, their mixtures tend to exhibit a more normal distribution. For further reading, refer tothis quick guide. Accelerated Monte Carlo Simulation:Using CUDA acceleration, our system runs an implementation of aMonte Carlo simulationthat models variant BM and JP inclusive sequence. The simulations are weighted based on a feedback cycle derived from the ICA process. High-Performance Processing:By focusing on parallelization, the entire system completes computations in just a few milliseconds. Predictive Filtering with HMM:A Hidden Markov Model (HMM) is employed to filter and identify the most reliable predictive paths. This ensures that trading decisions are based on a reward-based estimation of interval delays to set an optimal position lifetime. Together, these components form an integrated system capable of high-speed, data-driven trading decisions. To effectively visualize key components of our trading system—Best Case, Best Path, Last Price, Original Values, and Worst Case—which consists of over 4.8 million computed data points, we used FlutterFlow for a clean, interactive interface. Why FlutterFlow?We chose FlutterFlow for itsseamless Firebase integration, allowing us to easily fetch, store, and display real-time stock data without extra complexity. Additionally, FlutterFlow offerspowerful graph rendering, ideal for our stock trend visualizations. Visualization & Explanation GenerationBy integrating agentic web scraping capabilities, we enhanced our explanations with real-time analytics and relevant news articles, providing a comprehensive, data-backed decision-making tool. We integratedPerplexity Sonarto enhance our decision-making process by fetchingreal-time financial newsrelevant to each stock. This allowed our analysis agent to incorporatemarket sentiment and recent events, providingcontextual explanationsfor each trading decision. By combiningquantitative predictionsfrom our CUDA-accelerated models withqualitative insightsfrom Perplexity, we created a morecomprehensive and transparent trading system. Our project aligns withRox's focus on agentic workflows, leveragingLLMs for context fetching, prompt orchestration, and tool callingto complete a complex financial workflow. Our trading system integratesreal-time market analysis, predictive modeling, and automated decision-makingto streamline stock market insights. More details on our system diagram and architecture can be foundon this document.  Kaggle Link Analyzing video with VLMs is hard. Gemini and Qwen provide decent models that can process video, but in our testing we found them to be lackluster for Tesla's needs. Given the time constraints, we attempted to optimize our challenge approach throughan orchestration of image chunking and prompting. The idea: Instead of processing a video, we split the mp4 into jpeg frames -- the attached Tesla diagram visualizes the following: In chunks of 3, we analyze a subset of frames from the video in parallel. This allows the VLM to see how the video changes within a short interval (for example, 3 frames may show the car moving forward or turning right). If we cut the entire video into frames and run it all through a VLM in parallel, we can quickly gain a holistic analysis of the video contents and how it changes over time. We then combine all of this information and pass it into an LLM to reason on the information. Finally, it works with an output validation LLM to choose an option from {A, B, C, D, E}. ",,,,,,,"agents, c++, cuda, flutterflow, llm, perplexity, python, vlm","https://github.com/danielgao20/tradegentix, https://github.com/assasin2gamer/treehack25, https://docs.google.com/presentation/d/19Kgx6FDDi48enQKZ2quiKMEJlQiiieqsc655y4rNtdc/edit?usp=sharing"
TreeHacks 2025,Driptionary,https://devpost.com/software/driptionary,False,"    Ever had a conversation with your parents or grandparents and felt like you were speaking a completely different language? Slang evolvesfast, and generational gaps make it difficult to communicatenaturally. That’s whereDriptionarycomes in—areal-time speech-to-speech slang translatorthatbridges generations, communities, and culturesby making any slang instantly understandable. Inspired bymisunderstandings in everyday conversationsand the power of AI translation, we builtDriptionarytoseamlessly process, translate, and play back speech—all inreal time. Driptionary is anAI-powered slang translatorthat takesspeech input, transcribes it,translates slang to standard English, andplays it back in seconds. 1️⃣Record & Speak– Press the button and talk naturally.2️⃣Live Transcription– The appinstantly converts speech to textusing Groq.3️⃣AI-Powered Slang Translation– Driptionarydetects slang and converts itusing OpenAI Whisper to standard English.4️⃣Real-Time Playback– UsingAI speech synthesis, the translated phrase is read aloud.5️⃣Face Recognition– The apppersonalizes slang translationsbased on the user’s vocabulary. Whether it’sGen Z slang, regional dialects, or niche terminology, Driptionary ensures seamless, real-time communicationwithout misunderstandings. ✅Live, Real-Time Slang Translation– Afully functional speech-to-speech AIthat translates slanginstantly.✅Sleek, Intuitive UI– Swift UI design that ensures africtionless user experience.✅Personalized Slang Dictionaries– Users cantrain Driptionaryto recognize their unique vocabulary.✅Face Recognition for Personalization– Seamlessly retrieves slang preferences fortailored translations. 🔍The Power of Customization:Slang is personal—allowing users to define their own slang translations made the tool far more effective.⚡Optimizing for Speed:We learnedhow to minimize latencyfor real-time processing while maintaining high accuracy.🗣Balancing AI & User Control:Giving users inputon their slang definitions improved engagement and accuracy. ✅Multi-Language Support– Expand tobilingual slang translations(e.g., Spanglish, AAVE, Pidgin, etc.).✅AR/VR Integration (Oculus Quest 2)– Bringing Driptionary intoVR environmentsfor real-time metaverse communication.✅AI Voice Cloning for Personalized Playback– Generatetranslations in the user's own voicefor afully immersive experience.",,,,,,,"elevenlabs, fastapi, groq, openai, python, socket.io, swift, websocket",https://github.com/rohilvagarwal/Driptionary
TreeHacks 2025,Surgentic,https://devpost.com/software/surgentic,False,,,,,,,,"css, elevenlabs, gemini, javascript, neon, openai, perplexity, python, react, supabase, terra-ai, typescript","https://github.com/SeanZCai/treehacks-agentic-surgeon, https://github.com/SeanZCai/treehacks"
TreeHacks 2025,Shopybara,https://devpost.com/software/shopybara,False,"Finding things that fit your style is hard, especially when there’s an overwhelming amount of options, but nothing that truly feels like “you”. Personal style is reflected in the way we dress, the music we listen to, and the aesthetics we’re drawn to; ultimately, it’s an extension of how we express ourselves. But style isn’t just about how we look-- it’s also about the spaces we live in. The Shopybara team’s vision is to empower self-expression in home design by leveraging your unique tastes and personal inspirations. We believe that personalization in homes extends far beyond traditional decor trends. Traditional interior design services rely on surface-level preferences such as colors or materials; moreover, existing e-commerce platforms lack personalization beyond traditional browsing history. Our team sees untapped potential in unique areas like music and visual inspirations, and we aim to bridge the gap between personal taste and home design by drawing from qualities unique to you.","Our web application curates your personal aesthetic based on your music preferences and visual inspirations through platforms like Spotify and Pinterest. We then interpret and analyze this data to suggest personalized interior design recommendations (i.e. specific products, links, etc.) that truly reflect your individual style. Key features include the following:Taste Analysis Engine (processes personal data from Spotify and Pinterest to understand both emotional and visual preferences), Intelligent Shopping Assistant (leverages LLMs to provide curated product recommendations that match your unique style profile).","The platform is built with TypeScript and React on Next.js, hosted on Vercel for optimal performance. The frontend features Framer Motion for smooth animations, while Supabase handles user data storage and authentication. For data collection, we developed a custom Python scraper using Selenium to gather Pinterest data. The system integrates OpenAI's API for style analysis, processing inputs from various sources including Spotify, Amazon, and Pinterest APIs. This data is then used to power our product recommendation engine, which matches user preferences with relevant items.",,,,"We believe that Shopybara's potential is extensive in terms of scalability; we anticipate that our product’s capability to contribute unique insights to personalization is significant as our world becomes increasingly integrated with AI.
In terms of commercialization, one direction could be transforming Shopybara from an Amazon-focused platform into a comprehensive discovery engine for anything by expanding our scraping technology to target areas of any market.","css, javascript, python, typescript","https://shopybara.vercel.app/, https://drive.google.com/drive/folders/1ydNyIMclDDL6hpqrNir5do2l82qupOj0?usp=sharing, https://github.com/sabrinahu5/shopybara"
TreeHacks 2025,MindShift: AI for Bipolar Care,https://devpost.com/software/mindshift-wpy6re,False,"With the rise of smartwatches and an increasingly tech-savvy generation, physiological data tracking has gained significant attention. However, its application for bipolar patients—who could benefit the most—remains largely unexplored. MindShift aims to fill this critical gap by leveraging biometric data to enhance monitoring, early intervention, and personalized support.","MindShift bridges the gap between bipolar patients and their therapists or support networks. Using biometric data from the Apple Watch, we monitor key physiological indicators like heart rate variability, sleep patterns, activity levels, and agitation—metrics that research has linked to extreme mood episodes. When significant shifts occur, MindShift detects them and initiates an adaptive conversation with the patient. Simultaneously, this data is synced to the Therapist Dashboard, providing real-time access to patient biometrics and behavioral assessments. The dashboard also generates AI-driven recommendations to guide patients back to stability and help therapists make informed decisions. By restoring social connection and emotional balance, MindShift strengthens the support system for those who need it most.",MindShift consists of two core components: the watch experience and the therapist interface.,We faced two major challenges:,"We take immense pride in designing and implementing a fully connected backend and database infrastructure—a significant architectural challenge given the complexity of real-time data processing. Despite the constraints of a hackathon, we successfully built a robust system that combines biometric tracking, AI-powered dialogue, and therapist-facing insights.","Our team gained valuable experience in deploying edge AI solutions and navigating the challenges of integrating real-time physiological and behavioral data. Additionally, through extensive research on bipolar disorder, we deepened our understanding of how digital tools can complement traditional mental health care.","Moving forward, we aim to collaborate with bipolar patients and therapists to refine our system based on real-world needs. We’re also exploring additional behavioral and physiological indicators that could further enhance detection and intervention—potentially contributing to a deeper understanding of bipolar disorder as a whole.","chroma, elevenlabs, flask, gemini, mistral, react, terra",https://github.com/vikram087/Treehacks25
TreeHacks 2025,ResearchLens,https://devpost.com/software/researchlens,False,"As a research student, I know the struggle of sifting through long academic papers, even when the topic is exciting. Sometimes, you simply don't have the time to digest all that information. VectorScholar.AI was created to solve this problem. It helps you quickly find relevant papers, generate concise summaries, and understand complex research topics more effectively. Whether you're looking for similar studies, want insights at a glance, or need to stay on top of a specific field, VectorScholar.AI supercharges your research workflow by providing AI-powered recommendations and even converting text to speech for on-the-go learning.","ResearchLens is an AI-powered platform that helps researchers quickly discover, summarize, and understand research papers. It allows users to upload papers and provides a suite of features such as finding similar papers, generating concise summaries, offering recommendations for related work, and converting text to speech. The goal is to make research more accessible, efficient, and manageable, especially for students and academics dealing with long and complex research articles.","We built ResearchLens using a combination of React for the frontend and Flask for the backend, powered by InterSystems IRIS Vector Search. The backend leverages vector search capabilities to find similar papers and generate relevant suggestions. We also integrated OpenAI's language models for generating summaries and performing question-answering tasks. On the frontend, we designed a user-friendly interface for paper uploads, summaries, and text-to-speech features, enabling a seamless user experience.","Frontend-backend integration: We faced issues connecting the frontend React app with the backend Flask API. The biggest hurdle was configuring proper API calls and handling responses.
Handling complex data formats: Research papers come in various formats (PDF, TXT), which required us to implement proper parsing and text extraction methods for accurate processing.
Vector search setup: Setting up and configuring the InterSystems IRIS vector search database was challenging, especially when fine-tuning its capabilities to return relevant results based on the similarity of research papers.
Time constraints: Given the time limitations, ensuring the integration and deployment of all features without bugs was a constant challenge.","Seamless AI-driven summaries: We successfully integrated OpenAI's models to generate concise and meaningful summaries of research papers, which can help users understand papers faster.
Vector search capabilities: We implemented a vector-based search engine using InterSystems IRIS, which can provide relevant recommendations for similar papers, enhancing the overall research discovery process.
Text-to-speech functionality: The ability to convert text summaries into speech adds a new layer of accessibility, making research easier to consume on the go.
User-friendly design: Despite the technical challenges, we created a clean, intuitive interface that allows users to quickly upload papers and get results.","APIs and integration: The project helped us strengthen our skills in integrating frontend with backend APIs, especially handling file uploads and managing responses.
Vector search technology: Working with InterSystems IRIS provided us with deeper insights into the power of vector-based search engines and their applications in academic research.
AI and natural language processing: We gained hands-on experience with natural language processing, specifically for summarizing and generating insights from large volumes of text.
Time management: Working under tight deadlines helped us improve our ability to prioritize features and manage time effectively to meet deliverables.","Improved AI capabilities: We aim to improve the AI’s ability to generate even more accurate and nuanced summaries and recommendations, possibly by integrating more advanced models or domain-specific data.
Enhanced file parsing: Expanding the platform to handle more file types, such as DOCX or PPT, and improving the accuracy of PDF text extraction.
Collaboration features: Implementing a feature where researchers can share papers, collaborate on notes, or discuss findings directly within the platform.","pgvector, postgresql, python, react, supabase, tailwind, typescript, vite",
TreeHacks 2025,CogniBoost,https://devpost.com/software/cogiboost,False,"Have you ever worried about your grandma after something she said? Don't worry, we have too. Over 7 million Americans don’t realize they have mild cognitive decline (MCI), and studies show that by age 70, two out of three will experience some level of cognitive impairment. Nearly 37% of women and 24% of men are at risk for long-term cognitive issues, making brain health a growing concern—not just for older adults, but for seemingly healthy individuals too. Early diagnosis and proactive care are key to preventing our loved one's long-term damage. For those already showing signs, it's crucial to provide accurate diagnostics and connect them with smart, easy-to-use digital tools that fit into our everyday lives. With the rise of GenAI and advanced visual-intelligence & databases for predictive healthcare, we went out to create a novel multimodal way for individuals to participate in cognitive and neuroscience-backed tasks through their own household objects.","CogniBoost uses your household objects and advanced computer-vision, augmented reality, and RAGs to process multimodal sources of input, mostly visual data, and spatiotemporal marked tasks to generate dynamic short-length tasks for your everyday acute-cognition strength. The goalWe focus on three of the major contributors to growth in sharpness and their useful visuospatial tasks:",See our diagram below.,"We had a lot of core functionalities we were super pumped about building out with some great APIs. However, a lot of my time was spent figuring out how to efficiently route our data processing to our three servers, which all had varying LLM, RAG, or CV pipelines. It was really useful to have cool vector stores + search capabilities through our data that made this process way better and centralized to our case. Definitely challenging trying to get our advanced visual backend to render well in this short 36 hours - but it worked out!","We were able to integrate various types of APIs and create a really immersive application that touched on different pain points we wanted to hit traditional cognitive/physical task systems (hardware costs, not immersive, not engaging).",Merging with so many APIs + pipelines can be hard but its doable!,"Picture this: Grandma steps into her garden, pulls out CogniBoost, and we ask her to take a photo. Our scene recognition system identifies her gorgeous garden, suggests a flower arrangement, and guides her to organize them by flower types. Your grandma probably doesn’t even know it, but she’s doing what she loves with the items closest to her and taking steps towards her long-term neurogenic future.","flask, next.js, openai, react, vercel",https://github.com/rg0727/cogtaskapp
TreeHacks 2025,ASL Live Translator,https://devpost.com/software/asl-live-translator-rqekhp,False,"The inspiration behind the ASL Live Translator came from a desire to bridge communication gaps between the hearing and deaf communities. Many people struggle with language barriers, especially when it comes to sign language. This website aims to make communication more accessible and inclusive.","The ASL Live Translator uses a webcam to capture American Sign Language (ASL) signs, which are then converted into live captions. This allows users to communicate seamlessly with individuals who are deaf or hard of hearing by translating ASL into text.","We built the website using Next.js, incorporating:",,,,,"javascript, next.js, react, tailwind, tensor",https://github.com/nicoleleehy1/asl-live-captions
TreeHacks 2025,Stream,https://devpost.com/software/f-l0643e,False,"Ever since high school, I've been passionate about expanding contraceptive options, particularly for men. I was driven by the belief that everyone, regardless of their sex or gender, should have the power to make informed decisions about family planning and reproductive autonomy. This led me to dive into the development of a non-hormonal male birth control pill. While working in a lab focused on creating a pill that could impact sperm motility, I stumbled upon a sobering realization: the reverse of contraceptive development is infertility. This revelation hit me hard when I learned that 15% of couples in the U.S. today are facing infertility issues—and more than half of these cases are due to male infertility. What's even more troubling is the fact thatsperm counts have halved in the last 40 years, yet the conversation around male infertility remains largely under the radar. We wanted to create a fun and engaging way for males to stay proactive about their sperm health. If so far everything that we've tried to get males to track their fertility has failed, then if the one thing thatwillget them to do so is the male ego, then so be it.","Stream is an AI agent that empowers males to improve their sperm health. Users upload a video of their sperm, which undergoes a YOLO model to identify sperm in different frames and gather their average sperm count and sperm motility score. The AI agent then users their sperm metrics to provide personalized recommendations and award them with NFTs (bronze, silver, gold) based on their sperm health. Users strive to earn the Gold NFT to wear as a badge of honour of their health.","Our approach involved leveraging a YOLO (You Only Look Once) pre-trained deep learning model to analyze microscopic video footage of sperm, which had been captured and developed by researchers at the Simula Metropolitan Center for Digital Engineering in Oslo in 2023. By using this model, we could identify sperm counts and score sperm motility, a key aspect of male fertility. We used the EigenLayer AI agent kit to create our AI agent and verify off-chain data (sperm metrics) and bring on-chain. We used React to build the frontend, so that users can upload their sperm videos, track their history, and see how they compete with others on a leaderboard.","We encountered a lot of challenges with setting up the EigenLayer AI agent kit since the packages were only built for Mac (not Windows or WSL), and there was limited documentation on how to further customize the agent. None of the members on the team have experience with Yolo algorithm and EigenLayer so it was challenging for us to figure out in 10 hours. We didn't have React experience either on frontend, but we managed to build an app overnight!","As a beginner hacker team, we're proud of being able to run the deep learning CV model YOLO on the sperm videos to accurately track sperm metrics, successfully run the EigenLayer AI agent kit, and create a frontend successfully connected with the backend so users can connect their own wallet and upload their own sperm videos.","While these obstacles were tough, they also provided invaluable learning opportunities. We discovered how to integrate deep learning computer vision models with Large Language Models (LLMs), creating a seamless fusion of artificial intelligence that could track sperm motility while also interacting with users in a meaningful way.  We also learned how to work with EigenLayer to create our own AI agents.","We plan on making the AI agent even more powerful by being able to bring off-chain health data on-chain to get a richer health history and make more informed recommendations, integrating wearable tech to consistently monitor biometrics and other health markers, booking telehealth consultations or fertility appointments, and placing orders for sperm health supplements.","eigenlayer, hardhat, opacity, react, solidity, typescript",http://github.com/friendly304010/sperm
TreeHacks 2025,Vuzz,https://devpost.com/software/vuzz,False,,"Vuzz is a dynamic platform designed to connect volunteers with meaningful opportunities in an engaging and interactive way. Through a swipe-based interface, users can effortlessly browse and select volunteering events that align with their interests.",We used React-Native for our mobile app development and used a firestore database for the backend to hold our information and queries.,"Messages was slightly laggy mainly because firestore wouldn’t easily allow chat messages to work that well so it worked sometime. Location services were difficult to implement as well. It was hard to find an API for our purposes, and even when we did, it often caused conflicts with existing support in the profile and events screens.","Within the three of us we had almost no frontend experience nor any prior backend knowledge. Despite our lack of experience, we were able to create a really amazing mobile app in 36 hours which really shows our skills in learning new technologies quickly and being able to adapt in a relatively short time.","This was our first hackathon, so coming into TreeHacks, learning was our number one goal, and one that we feel we have successfully achieved. Firstly, we learned about impact. We knew that we wanted to make something that would make the world a better place, so we looked in the best place we knew - around us. Only 4% of college graduates (over the age of 25) volunteer each year before age 35. As college students, we are starting to experience the constant hustle that tends to leave the genuine fulfilment that comes from opportunities like volunteering behind. Thus, we wanted to make volunteering more accessible on both ends. We also learned about frontend, as none of us had ever built an app before. It was a long process learning react-native, but one we thoroughly enjoyed and are looking forward to taking forward in our development careers.","We imagine the future of Vuzz to be volunteering at scale. We are hoping to partner with as many volunteer organizations as possible - ones you’ve heard of, and perhaps more importantly, ones that you haven’t. We also plan to expand into the student body, and college communities across the country as a starting point. From there, the sky is the limit!","firestore, gcloud, react-native, typescript",https://github.com/foxboxxx/treehacks25/tree/7am-cut
TreeHacks 2025,Posthuman,https://devpost.com/software/posthuman,False,"Software Diagram About the projectInspiration: The growing digital footprint of individuals, coupled with the lack of estate planning, inspired us to build Posthuman. With most people unprepared for managing their digital and financial legacies after death, we saw an opportunity to create a secure, AI-powered solution that preserves memories, ensures financial continuity, and simplifies the transition for loved ones. Too often these days, uncanceled subscriptions and unfound bank accounts take a toll on already grieving family members. We hope that our solution covers use cases for everybody in daily life, and hopefully can be adopted by many. What it does: Posthuman is a comprehensive digital estate and legacy management platform that: How we built it: We leveraged: Challenges we ran into What we learnedThe importance of privacy-first AI development when handling personal data.
How to effectively use vector databases and blockchain for estate planning.
So many other new technologies and processes to build software! What's next for Posthuman",,,,,,,"docker, eigenlayer, elastic, elevenlabs, hardhat, infura, luma, mistral, musetalk, next.js, ollama, scrapybara, vercel","https://github.com/r-agni/posthuman, https://docs.google.com/presentation/d/1Pkberz8pt2w35VmKCpIlouXlYrs8ZRXCfQo6l2RSj3Q/edit?usp=sharing, https://youtu.be/cqggYszzq9o"
TreeHacks 2025,TheraPlushie,https://devpost.com/software/theraplushie,False,"TheraPlushie Agent TheraPlushie Hardware Sample TheraPlushie Conversation (Short) Have you ever, on a desperate day, resorted to using ChatGPT for therapy? (ChatGPTherapy, if you will.) However, as anyone who's used ChatGPT for therapy has noticed, there's a sort of emptiness when conversing with a textual agent, which ironically results in feelings of isolation that are counterintutive to its original purpose. Enter:TheraPlushie Though clearly not human, TheraPlushie offers the emotional support that no computer alone can. Designed to bring in the physical element to emotional conversations TheraPlushie allows users to have some to be there for them while actually being there for them. (As a plus, it also offers unlimited hugs!) TheraPlushie is a conversational agent meant to provide users a medium for emotional expression ranging from stress to excitement. Built into a cute, cuddly stuffed animal, this conversational agent provides users warmth, soothing, and companionship. NOTE: ORIGINALLY INTENDED TO BE INTERACTED WITH VIA VOCAL INPUT THROUGH A MICROPHONE, WE UNFORUNATELY RAN OUT OF TIME TO IMPLEMENT THIS. THE LOGIC IS THERE, BUT THE HARDWARE WAS DIFFICULT TO INTERFACE WITH. TheraPlushie takes in conversational user input via microphone, then it converts the input to an audio file and then to text via (ipywebrtc and OpenAI’s Whisper, respectively). The text is then transferred to an LLM (trained on Llama3.1 via Ollama) and converted to speech via ElevenLabs (using their Voice Library to craft its voice). To train the LLM as well as the vocal output, we entered in prompts and tweaked them, with testing involved to see if we got the results we wanted. For the LLM model (called theraplushie_X with X being a number between 1-7), like a therapist, we wanted it to be not only empathetic but also engaged in the user’s discussions– which we didn’t see much in early iterations, as we’ll discuss in the next section. Here, we decided to focus more on how the agent interacted with the user rather than how it conveys speech. Rather, conveying speech was something we focused on within the vocal characteristics of the model itself and results in a more engaging and fufilling experience. For hardware we used a Jetson for primary ""brain"" for our various processes, a bluetooth speaker for vocal output. 1) Language model selection. Initially, we had chosen Llama 3.3 for the LLM, however we were unable to run it on our Jetson. This led us to select Qwen, a much smaller language model that ran well, however didn’t run with the outputs we wanted. It would repeat the same or similar phrases and wouldn’t continue the conversation as empathetically as a human would. Thus, we had to findthe Goldilocks LLM, an LLM that was large enough to have a wider range of capabilities, but small enough that we would be able to run it. 2) We would encounter technical difficulties on-and-off with the Jetson and its wifi connection; this was hard on us as we were SSH’d into the Jetson when editing code. However, we took this time to develop our projects in other ways whether through documentation, acquiring materials, or researching. 3) We were definitely on a time crunch and didn't get to implement all the features that we had originally wanted. The teddy lived another day! (It was about to get electronics placed into it :) ) Angela: “I’ve never worked with text-to-speech, speech-to-text, LLMS, or voice prompting before, so I’m very proud that I was able to interface with all these technologies and come so far with our product. I also learned that Incorporating values and human interaction is actually a bit more difficult than one might think. It’s one thing for a solution to do its job; it’s another thing for a solution to do its job in a way that its human user will be satisfied with.” Miguel: “Perserverence.  A lot of perserverence. A lot of parts would throw error a lot or didn't work or met our requirements. We came in with zero plans and there were only two of us. I'm just glad we were able to come out with something impressive."" The current iteration of TheraPlushie is at its simplest core. We’ve yet to take into consideration starting factors (ex. a wakeup call or button) and what happens when the model is interrupted while it is speaking. The agent’s body and “mind” are still separate as well.  Additionally, we did want to add in a hug feature with actuators in the plushie’s arms to give hugs when prompted. How would we power within a stuffed animal? These are questions left unanswered and waiting to be developed.",,,,,,,"elevenlabs, ipywebrtc, nvidia, ollama, openai, python",https://github.com/akomsskai/theraplushie
TreeHacks 2025,"Avengers, Assemble!",https://devpost.com/software/avengers-assemble-zj1vlo,False,"Commanders need squad-level insights to make strategic decisions during important missions. We're building a unified platform to assess soldier health and enable real-time logistical planning. Minimize disruptions, cut emergency substitution costs, and keep your Avengers at full strength!",,,,,,"We will run bootstrapping experiments to simulate entire squads of Avengers, testing different team compositions under various scenarios. By generating synthetic cohorts and assessing their collective health, we can predict which squads perform best in battle. We’ll develop cohort-level embeddings by making our transformer task-aware, distilling squad readiness into a vector representation in latent space. These embeddings will allow us to generate radar charts for visualization of team strengths and vulnerabilities.","flask, ngrok, node.js, python, pytorch, react, scikit-learn, scipy, terra, typescript, v0, vercel","https://github.com/Kourosh-Salahi/Treehacks2025/tree/main, https://docs.google.com/presentation/d/1eTzZiG1dfpgGPCvN3J3ID8M4frccTPyswlQs5QDohXc/edit?usp=sharing"
TreeHacks 2025,LeXcel,https://devpost.com/software/lexcel,False,"Spreadsheets are powerful but often frustrating tools. They require users to memorize complex formulas and spend time on tedious formatting, leading to a slow and efficient process. We wanted to create a solution that enhances productivity, reduces errors, and makes spreadsheets smarter and more intuitive.",LeXcel is an AI-powered add-on that enhances traditional spreadsheets by:,"We developed LeXcel as a seamless add-on in Google Sheets that is easy to download and run for anyone: no coding experience required. Our main tech stack is javascript, google script, sheetAPI, OpenAI, and Gemini.",,,,,"cursor, googlescript, html, javascript, openai, spreadsheetapi, windsurf",https://github.com/liunicholas/LeXcel
TreeHacks 2025,Sventos,https://devpost.com/software/sventos,False,"The inspiration of this project was from my own experiences with Zoom. With sometimes having bad eyesight, I wish I had better access to zooming into certain windows and prioritizing different frames that were presented to me during virtual school.","Sventos is built on the Zoom Video SDK Next.js template, which allows 1-1 communication using Zoom's APIs. I then extended this to support multiple users, and most importantly, screensharing, making up the key elements of virtual school. Sventos gives users fine control over what they view from meetings/presentations/webinars by being able to move and resize their frames, allowing users quality of life and educators/content-creators the ability to create immersive education/other immersive experiences.","Sventos is built as a fork of the Zoom Video SDK Next.js template, using Next.js, tailwind, and Node.js for the API routes. I further used Windsurf, the agentic AI IDE, to assist me with tricky tasks and overall frontend design.","Understanding the Zoom API was difficult, but it helped me understand how much control it could give to users if used correctly. Extending the front-end to allow dragging and resizing of frames was a very time-consuming product.","Constructing the MVP, I thought I wouldn't be able to finish it at some point, but I was able to get a majority of what I wanted to on the board. It also helped me understand the lengthiness of the ideation and building process.","Software is tricky, but it doesn't always have to be complicated to make an impact. While it wasn't complicated in nature, I believe Sventos has a lot of future potential.","Supporting multi-screen sharing, I still need to explore more of the Zoom API to see if this is possible, but this may require me to write certain components from scratch. I want to hash this out to meet education use-cases and beyond, as I feel this has good potential for content-creators, especially in the streaming (see, Twitch) industry.","next.js, node.js, tailwind","https://github.com/aksharye/sventos, https://sventos.vercel.app/"
TreeHacks 2025,deep reach,https://devpost.com/software/deep-reach,False," Recruiting in tech is broken. After speaking with several sponsors and mentors, we realized that companies need candidates with hyper-specific experience—like Chroma, which looks for engineers who’ve worked at startups with under 50 employees for at least a year, or Tesla, which seeks researchers with top publications in Gaussian splatting, a niche field in 3D rendering. Yet, current hiring platforms fail to surface the right talent, relying on outdated resume filtering and generic keyword searches. The best candidates aren’t just those with the right job titles—they’re the ones who’ve actually built, researched, and contributed to cutting-edge fields. At Deep Reach, we’re building the future of AI-driven recruiting. Our multi-agent AI system automates candidate discovery, screening, and outreach—scraping platforms like LinkedIn, Google Scholar, DevPost, and GitHub to construct deep, work-based profiles. Unlike traditional job boards, our agentic RAG system iteratively refines searches to find top candidates who match a company’s exact needs. With one click, recruiters can schedule AI-powered pre-screening interviews and instantly connect with talent via automated, customized outreach. We developed Deep Reach on a highly scalable AI-native stack, leveraging:
    • Vercel V0, TurboPack, and TurboRepo for rapid development, scaling, and deployment.
    • ChromaDB + Vector Embeddings to enable deep candidate profiling and AI-powered matching.
    • Retrieval-Augmented Generation (RAG) to iteratively refine searches based on recruiter preferences.
    • Perplexity, Groq, and OpenAI to drive natural language processing, AI-driven outreach, and automated pre-screening interviews.
This combination allows us to bridge the gap between research, engineering, and real-world hiring needs—ensuring companies find top-tier candidates without relying on outdated resume filtering. One of the biggest hurdles was data fragmentation—candidate information is spread across multiple platforms (LinkedIn, DevPost, GitHub, Google Scholar, Twitter), making consolidation and insight extraction complex. We also had to refine query precision, as AI models initially struggled with niche fields like Gaussian splatting or self-supervised learning. Additionally, balancing automation and personalization was critical to ensure candidates still feel engaged and valued in the process.
Recruiting is fundamentally flawed, as traditional job boards fail to capture a candidate’s true expertise, relying on surface-level filters rather than a deep understanding of their work. AI-driven search requires continuous iteration, with RAG-based queries significantly improving results by learning from recruiter interactions and refining searches over time. Ultimately, speed and accuracy define hiring success—the faster and more precise the AI, the higher the success rate for both recruiters and job seekers. We are actively building, refining, and pushing the boundaries of AI-driven hiring. Our focus is on expanding multi-agent capabilities to scrape and analyze more platforms, integrating AI-powered candidate coaching to help talent optimize their profiles, and developing adaptive ranking models to predict hiring success based on company preferences and past hires. Additionally, we aim to enhance real-time recruiter-candidate AI interactions to further streamline the hiring pipeline. The future of recruiting isn’t just about finding candidates—it’s about deeply understanding their work and seamlessly connecting the right talent with the right opportunities.",,,,,,,"bun, chroma, chromadb, groq, ispeech-text-to-speech, next.js, ngrok, node.js, openai, pandas, perplexity, puppeteer, python, rag, scrapybara, selenium, sentence-transformer, speech-to-text, tailwind, turbopack, twilio, v0, vector-embedding, vercel, whisper","https://github.com/Francescocrivelli/recruit-ai, http://francescocrivelli.com/recruitdemo/index.html, https://youtu.be/TPq0v4tHLlw, https://youtu.be/3PMd1DM9t2A, https://youtu.be/9PO4aDIZaOE"
TreeHacks 2025,WabiSabi,https://devpost.com/software/wabisabi-iqc3m5,False,,,,,,,,"firebase, flutterflow, gemini, swift, terra","https://app.flutterflow.io/share/tree-hacks-g6j8b4, https://github.com/AshleyJoyE/WabiSabi-Swift"
TreeHacks 2025,Pitcher,https://devpost.com/software/pitcher-6v28a3,False,"We are visual learners. Purely text-based learning leaves us quickly feeling sleepy and disinterested. Textbooks provide as much utility as a rough pillow. Whether through pestering our professors, coworkers, and friends, the only way we can grasp concepts is through interactive learning. Thus, we build a system where given a codebase, documentation, textbook, or novel can produce visual elements as well as talking us through the concepts. Our system can also pause, answer questions, and modify the presentation on the fly giving endless opportunities for long fascinating tangents.","Given access to your code base, documentation materials, and a prompt. Pitcher can create a powerpoint presentation that contains relevant images/information    from documentation you gave and also custom data visualization charts. Pitcher allows you to showcasing your project in whichever angle you want. Moreover, Pitcher will present to you in its own voice the project that you included. When interrupted verbally Pitcher will stop to answer questions while simultaneously generating more slides to answer your question as well as directly citing information from the codebase/documentation. Key features:
generates slides from codebase and documents
generates and includes contextual images and data visualizations
auto-presents with automatic voiceover
Allows user to interrupt and ask questions
Generates additional slides for clarification after questions are asked.","To extract the essential information from the input sources (github, google drive) we developed a chunking system that allows us to take full advantage of the context length of LLMs. This chunking system allows us to extract essential statistics and central ideas from multiple documentation files concurrently. We also built tools to extract and analyze images within pdfs as well as summarization tools to understand the functionality of entire code bases. All this information is vectorized into a IRIS database and is stored for later RAG. The summarized input sources are then used to design and build powerpoint presentations. To automate this process we employ MARP, a tool that allows us to create powerpoint presentations with markdown files. The ideas are passed through multiple sets of AI agents. The first set finds the ideal location for text making sure to leave enough space for future images while brainstorming ideas of what visual graphs or photos are best for the situation. Before the next pass this graphics’s information is autonomously retrieved and built by another array of agents based on the framework Mermaid. The presentations are then passed through the build agents that add the images and modify the presentation to be more visually appealing. This presentations are then rendered out and displayed to the user via an iframe. The presentation as well as speaker notes generated during the last round of agentic building are then passed to a reactive Text to speech system which responds to interruptions with relevant information thanks to the RAG system of the entire project.","One of the challenges we ran into was the sheer scale of the entire project. With multiple sets of agent agents, an immense amount of computational logic needed to extract essential detail and summaries of code bases, RAG system and database management software as well as interactive interfaces needed to provide a solid user experience. This is a huge project. What helped us overcome this challenge was deciding on our project vision early with extremely productive ideation sessions. We also effectively delegated our work with each member taking up a clear piece of the project.",,,We plan to scale this project to service thousands of visual learners. We also plan to improve the capabilities of our agentic system to produce more accurate presentation information as well as give more precise verbal responses.,"google, nextjs, openai, python, sonar, typescript, vectordb",https://github.com/KevinZWong/Pitcher
TreeHacks 2025,Russel HomeBot,https://devpost.com/software/autonomous-robot-2a9c3r,False,"Growing up, many of us have witnessed firsthand how difficult life can be for disabled individuals, the elderly, and those who are chronically ill. Simple tasks that most of us take for granted—like walking to get a glass of water or taking medicine—become monumental challenges. These moments can be isolating, frustrating, and, at times, dangerous. Two of our team members have mothers who are struggling with health conditions, which made this project personal for us. We want to make a difference in the lives of people who face these challenges every day, and we believe technology can help.","Our autonomous robot is designed to bring independence back to those who need it most. It can receive voice commands from the user, locate an item, move to it, pick it up, and bring it back to the user. Our robot is also equipped with the ability to climb stairs, making it a versatile helper in various environments.","We focused on four core features to bring this idea to life: At the heart of our robot is theNVIDIA Jetson Orin Nano, and we are extremely grateful to the Nvidia team for providing us with this powerful tool and supporting us throughout the hackathon. We used anArduinoto control the motors for the wheels, which is connected to the Jetson for coordination. Other components include:","One of our biggest challenges was designing and building the circuits from scratch. We didn’t have the right connection wire for the Arduino to the Jetson, so we had to figure out how to wire it up and troubleshoot issues along the way. Each team member stepped up in different ways, learning new skills as we went. We also had to find creative solutions to integrate all the hardware, software, and communication systems into one cohesive unit.","In just 36 hours, we built a fully functional, product-grade autonomous robot from the ground up. The fact that we were able to incorporate so many advanced features into such a short timeframe is something we are truly proud of. But beyond the technical achievement, knowing that our work could make a meaningful difference in someone’s life makes it all worth it.","This project pushed all of us out of our comfort zones. As a team, we had to learn how to design and build robot parts, write code for a complex system, and put together intricate circuitry—all while staying focused on our ultimate goal of improving lives. Our experience taught us the power of teamwork and how valuable it is to keep learning. We also learned how to improvise and innovate when faced with unexpected challenges.","Looking ahead, we plan to expand the robot’s capabilities. We want to integrate more IoT devices, such as smart home systems, to further enhance its utility. Our ultimate goal is to bring this technology into production, helping people with disabilities and elderly individuals lead more independent and fulfilling lives. This project is just the beginning, and we are excited about the potential impact it can have.","arduino, cad, jetson-nano, python, ros2","https://github.com/KshitijGoyal2022, https://github.com/KshitijGoyal2022/TreeHacks-2025"
TreeHacks 2025,MUSES,https://devpost.com/software/muses-d2xfrt,False,"Our team wanted to address the Education Track Challenge presented by Zoom— in particular addressing music education. As avid lovers and musicians, all 3 of us in the team wanted to join in the process of gamifying education by using AI algorithms to aid learning an instrument or vocal singing.","Our app will first display a tailored feed of music recommendations. They may choose to like, save, or scroll past each piece of music. The app will process data from likes, saves, and retention in a cosine similarity algorithm to determine musical repertoire recommendations for the user to practice and play.","Our team used Flutter for frontend development and SQL relational database for backend development. With Flutter, we ran both an IOS and Android simulator to see the UI / UX design element changes in real time. The SQL database was used to store all the songs and musical pieces in our mobile app, while OpenAI API was used to generate vector embeddings for our recommendation engine’s cosine similarity search. Hence, our tech stack included Dart in addition to algorithms in Python for data ingestion and C++ for app development.","We didn’t know how to create a recommendation system to match users with songs they might like, for which we want into challenges while generating and storing vector embeddings. In addition to navigating vector databases, our team struggled with merging the git commits and push-pull requests as working in a team in a hackathon was a new experience for all of us.","The queue shuffler for songs on the home page of the mobile app is very enjoyable, as our AI LLM powered recommendation algorithm presents these songs to our users based on their favorite genres, previous liked and saved songs. Users have the ability to both save and like the songs recommended by our queue shuffler. The app also collects data from these interactions to fuel the recommendation algorithm.","All of us in the team learned how to code in Flutter and FlutterFlow for frontend mobile app development, which has been a very enjoyable and learning experience for us. From possessing the added flexibility to drag and drop design elements to our OS simulator to learning how to create complex screens with embedded media links, learning to code in Flutter has been the highlight of Treehacks for us. Additionally, we were able to utilize the new IDE developed by Codeium, Windsurf, whose Claude Sonnet-powered copilot has helped us brainstorm and plan complex algorithms.","We have already started creating customer surveys for fellow hackers to gather user testimonials and interest. Based on the vast majority of our users listening to music through Spotify and YouTube Music, we plan to integrate Spotify song links into our mobile app’s homepage in addition to the YouTube Music integration. In the future after the hackathon ends, we plan to iterate MUSES and pivot if necessary to reach a bigger audience and spread the joy of learning music.","flask, flutter, flutterflow, openai, python, shell, sql",https://github.com/Hecate946/Muses
TreeHacks 2025,VisionMate,https://devpost.com/software/hachi,False,"We came together as a team over our shared passion to create high-impact products and solutions to contribute to the healthcare sector. A member shared how she came across visually-impaired influencers on social media detailing the hardships they faced on a daily basis—just how difficult it was to complete simple tasks such as navigating around an obstacle. Watching her suggested videos, we echoed her empathy and decided that we wanted to build a product specifically targeted at improving ease of mobility for the visually impaired. Hopefully, starting with a smartphone app, we can make walking outside a safer, more practical task.","Walking around within a bustling surrounding is typically dangerous for a person that is visually impaired, sometimes even for those that are not. But with this innovative mobile application, users can take photos of their surroundings with their phone, which are then processed with image recognition and depth estimation algorithms to identify obstacles and provide personalized and quantified safety suggestions from LLMs for blind users to travel within their surrounding environment.","This fullstack project is built through a seamless integration of a robust backend and innovative frontend, with hidden features that enhance the user experience. For the backend, we implemented FastAPI for its speed and flexibility. After processing the image taken by the user, through OpenAI API calls and carefully engineered prompting using techniques such as Chain-of-Thought, we verbalize surrounding of the visually impaired. When an obstacle is detected, we use the Segment Anything Model (SAM) and Depth Anything V2 to estimate its distance from the user. Since no highly accurate measuring AI model exists, we combined Depth Anything V2 for depth mapping with SAM for object segmentation, creating a precise obstacle mask to improve distance estimation. On the frontend, we utilized FlutterFlow, which allowed us to send surrounding photos to the backend through Firebase. Additionally, we integrated special features, including Eleven Labs for text-to-speech, the Whisper API for speech-to-text, LangChain for advanced language processing, and depth estimation frameworks to provide an extra degree of surrounding information for those in need.","Some of the biggest challenges we ran into were our indecisiveness in mapping out the project benchmarks, individual technical challenges which prompted us to reach out to mentors and other hackers, and arguably the largest of them all, scrambling to beat the deadline through all our attempts to perfect our project.","Having two new hackers on the team, along with our decision to explore a variety of new apps and methods we had never encountered before, has been our biggest challenge. However, we’re all really proud of how well we collaborated and pushed our limits.
We’re also especially proud that our work resulted in an app that contributes to accessibility and addresses important social issues, making a meaningful impact beyond just the technical side.","Through this process, we learned the value of time and it being the most valuable resource especially in hackathon situations. This involved improving methods of time management, finding new ways to brainstorm ideas, and having a high sense of adaptability. Additionally, we learned to familiarize ourselves with novel technologies and use them in ways to solve problems which meant a lot to us.","The next couple challenges that we want to tackle with the app are fine-tuning the LLM image recognition model to evoke more precise responses that can be better quantified to provide the user with pin-point information, and implementing a translation text to speech feature to further simplify the user's surroundings in a foreign environment. There are several directions with which this project can be advanced, but we feel that the combination of the wide variety of technologies that we used exhibited the power of fusing cutting-edge resources together to create impactful developments to the field of healthcare.","dart, depth-anything2, elevenlabs, fastapi, firebase, flutterflow, langchain, ngrok, python, pytorch, segment-anything, whisperapi","https://github.com/lisere5/blindNavigation, https://app.flutterflow.io/project/blind-navigation-8fbmw1, https://app.flutterflow.io/run/o6GcCqQjsygO1bMUNXCZ, https://app.flutterflow.io/project/blind-navigation-8fbmw1, https://app.flutterflow.io/run/o6GcCqQjsygO1bMUNXCZ"
TreeHacks 2025,NeuroScribe: Brainwaves to speech and image,https://devpost.com/software/magic-words-9vs8d2,False,"The spoken word is the most intimate connection that we share with other humans; for most, speech was the first way in which we communicated ideas, creativity, and emotion. Yet, our world is home to 64 million people that suffer from childhood-induced or paralysis-induced mutism (inability to speak) — 400,000 alone suffer from amyotrophic lateral sclerosis, a neurodegenerative disease that inhibits motor function and speech. So we asked ourselves: For someone with no motor control and unable to speak, how can we not only restore their ability to communicate, but also add emotion and expression to their speech? This is why we created a system that deterministically decodes EEG brainwaves to text with perfect accuracy. While sign language is a physical alternative to speaking, it is not a replacement for emotionally rich vocal speech; moreover, sign language is infeasible for those with motor control impairments. There are some potential solutions that try to replicate vocal speech using electrical stimulus or EEGs to try and predict what mute individuals would like to say (e.g. Stephen Hawking's wheelchair); yet, these are highly expensive and cannot convey emotional or artistic expression / imagination. Artistic and emotional expression is a core element of being human. Not only do we derive text from the brainwaves, we also add an element of emotion (detected by alpha, beta, theta, gamma, and delta from EEG) which we incorporate through two mediums: 1) Generated Image: This visual representation brings the user’s imagination to life by using their text input and emotional state to guide an AI-generated image, and 2) Speech: By incorporating the user’s emotional state, we add natural inflections to a synthesized voice, creating a more authentic and expressive reading of the text input. This is ultimately where NeuroScribe comes in. We aim to transform how those with speech and motor impairments communicate by developing",,,"Our electrodes picked up an incredible amount of noise due to the fact that we weren't collecting stimulus directly from the user's head; we placed electrodes behind the user’s ears instead (user convenience in exchange for signal power), so mapping to different action states was very difficult for us! We had to create some interesting DSP algorithms that still captured the peaks of the system and we spent a lot of time calibrating and figuring out the features with the highest importance. We also had the task of prompting ElevenLabs with emotional data. Figuring out the variations in alpha/gamma/beta/theta/delta information, and then mapping that to an arousal states and different prompt definitions, was a very complicated flow due to the fact that real world emotional states are reflected through complex voice inflections and attunations. ElevenLabs had dense documentation for tweaking its audio appearance, so we spent many hours optimizing voice characterizations to associated emotions/excitement states.","Not a single one of us had experience with BCI prior to this hackathon. In fact, we spent the first 14 hours of treehacks building the BCI system and trying to make sense of what exactly the electrodes were relaying. Yet, after 36 hours, we were able to generate images and voices by almost imperceptible facial muscle contractions. 
We’re super proud of the novelty of our idea — we spent a significant amount of time just thinking about how you could create words from brainwaves, and so we’re proud of the creativity involved in ideas like using a Huffman Tree to encode binary to every character or mapping continuous-time signals to discrete action classes through jaw clenching.","We learned how you can map complex continuous-time brainwave signals into 3-4 discrete action classes that can effectively generate every word in natural language — this is incredibly powerful. Moreover, Luma AI
ElevenLabs
OpenBCI EEG Headset + Electrode Sensors
BrainFlow API + OpenBCI SDK
Flask, FastAPI, Tailwind, WebSocket",,"brainflow, elevenlabs, fastapi, luma, openbci, tailwind, websocket",https://github.com/JoelGrayson/Treehacks25/tree/main
TreeHacks 2025,annaOS,https://devpost.com/software/anna-ph5o7v,False,"What if AI wasn't just a voice or a chatbot, but a living, breathing holographic presence in your daily life? That's howAnnawas born — an AI assistant that transforms into a portal for seamless communication and productivity.",,,,,,,"electron, fastapi, node.js, openai, perplexity, python, spotify, swift, tailwind, twilio",https://github.com/ethandgoodhart/anna
TreeHacks 2025,x-rAI,https://devpost.com/software/x-rai,False,"AI is evolving at an incredible pace, but one major challenge remains—how do we ensure AI models are not just powerful but also explainable and efficient? As AI is increasingly used in high-stakes environments, from finance to healthcare, understanding why and how it makes decisions is crucial. We wanted to build a solution that optimizes AI performance while maintaining transparency and adaptability.","x-rAI is an AI inference system that uses a Mixture of Experts approach combined with adaptive learning to optimize processing at the token level. It dynamically allocates resources, ensuring efficient computation without compromising accuracy. Our system also employs an auxiliary load balancer and caches at the expert cluster level to accelerate inference speed. This results in faster, more reliable, and scalable AI decision-making. One of the most powerful applications of x-rAI is in accelerating software development cycles. Traditionally, teams spend weeks or even months defining Agile and Scrum stories, mapping out processes, and planning ceremonies. With x-rAI, we can bring this down to minutes by automatically generating Agile story templates, defining sprint goals, and optimizing backlog grooming based on past project data. Our AI system intelligently understands team dynamics, past velocity, and project goals to automate the planning phase, allowing developers to focus on execution rather than time-consuming administrative tasks.","We started by implementing a Mixture of Experts architecture, allowing different specialized models to process different types of inputs. Then, we optimized inference by caching frequently used computations at the cluster level, reducing redundant processing. The system also incorporates an adaptive learning mechanism, refining expert assignments based on real-time feedback. By leveraging efficient resource allocation and caching, we significantly improved performance while keeping computational costs in check. The Judge Agent validates the results, ensuring accuracy & explainability. To accelerate Agile workflows, we trained our model on extensive historical project management data, enabling it to generate precise and actionable Agile stories, sprint plans, and Scrum templates in real time. The system continuously learns from team feedback, making recommendations more accurate and relevant with each iteration.",,,,"We plan to expand x-rAI’s capabilities by integrating more robust explainability features, allowing users to better understand why specific expert models were chosen for different inputs. Additionally, we aim to explore real-world deployment scenarios, testing x-rAI in industries like finance and healthcare where speed, accuracy, and explainability are critical. For software development teams, we want to refine Agile automation by integrating real-time AI-powered retrospective analysis, predictive backlog grooming, and automated sprint planning, making project management seamless and hyper-efficient.","docker, elasticsearch, excel, langchain, llama, mistral, openai, python, spreadsheet, sqlite, terra, vectordb",https://github.com/rkat7/xAI-MoE
TreeHacks 2025,Bob: The Agent Builder,https://devpost.com/software/bob-the-agent-builder,False,"First, we had foundation models. Then we had AI agents built on top of these. Now we have agentic workflows composed of several agents with different specialized abilities working to solve a complex problem. Existing tools for creating agentic workflows, despite being low/no-code and having modular drag-and-drop designs, are hard to use. The more complex the problem, the more specialized components we need to manually add and connect. These tools have a high learning curve and you need to bend your task to fit the tool set available in these applications. We want to change this paradigm of agentic workflow creation.","Bob, the agent builder, takes a single text prompt as input and then intelligently breaks it down into the sub-tasks that need to be solved to complete the whole solution. The interface is familiar with blocks and connecting edges but now the entire graph is automatically generated and the blocks and edges can be any agentic workflow, as simple as code generation or LLM reasoning or as complex as computer use. A global planning agent chooses the best tools for each subtask by factoring in the demands of efficiency, nature of the subtask and input and output requirements. The entire agentic workflow is generated hierarchically with checking at each stage to ensure consistency and logical soundness. You just tell it what to do and it designs the best way to do it.","We designed generalized JSON schemas to represent tasks and links for inter-task communication. A planning agent transforms the user prompt into a task object according to this schema. Then we iteratively pick the best tool for a task and if required break it down into simpler tasks. At each stage we keep track of the inputs and outputs connecting between these tasks. The entire graph of agentic blocks is tracked with LangGraph. This way we can substitute the planning model for anything of our choice: Perplexity, OpenAI, Gemini, or even custom fine-tuned models for your domain. The agents currently support tools like computer use with Scrapybara, LLM based search and planning, and code generation and deterministic execution. The tasks at leaf nodes immediately start running and wait on the outputs from other subtasks to be ready before they take these as inputs for their own processing maximizing parallelism of tasks.",,"We are blown away by the things this model is capable of. We exceeded our expectations by putting together several new technologies including Computer Use Agents (Scrapybara), Langchain, Structured output and reasoning with LLMs, and building and managing interprocess communication in an agentic workflow. We strongly believe that automating agentic workflow creation is the future.",,There are several features we want to add to our agentic workflow:,"agents, flask, gemini, langchain, openai, perplexity, python, react, react-flow, scrapybara",https://github.com/pringlesinghal/bob-the-builder
TreeHacks 2025,Eco Field,https://devpost.com/software/eco-field,False,"Carbon net-zero policies are essential for ensuring a clean future, yet industrial adoption often lags because very few cost-benefit tools make it easy to compare carbon-efficient approaches with conventional methods. We believe thatuser-friendlyanalysis drives faster decarbonization. Enhanced Rock Weathering (ERW) remains underutilized partly due to the lack of accessible geochemical modeling platforms.","Eco Fieldis aweb-based platformthat integrates aCrunchFlow-based geochemical model. Users enter feedstock types, soil composition, and application rates to estimate carbon sequestration potential. The tool then providescost and revenue calculations, includingcarbon credit earnings, and showsbreakeven metricswithout requiring any specialized background inFortranordeep geochemistry.","We containerizedCrunchFlow(a legacyFortranmodel) usingDocker, wrapped it with aPython FastAPIlayer for input/output orchestration, and built a front end usingReactandTailwind.External APIssupply data such as temperature based on a user’s location. By consolidating and transforming these diverse inputs, we ensure seamless interaction withCrunchFlowso that simulations run smoothly.",,"Wemodernized a Fortran-based codebase, enabling it to run in acontainerized environmentand allowingnon-expertsto access geochemical simulations through a simple interface. This approach has the potential to drive realenvironmental impactby making advanced carbon sequestration methods accessible to a wider audience. The combination ofacademic rigor (CrunchFlow) and practical user experienceis a significant step forward.","We discovered that aligning onmission-oriented goalscan sustain motivation, even when tacklingunfamiliar frameworksandcryptic Fortran bugs. Working on a project withgenuine climate relevanceproved that aclear purposehelps the team push through each technical obstacle.Open communication and celebrating incremental winskept progress steady.","We plan toapproach potential pilot users—such asfarmers, soil scientists, and climate-focused organizations—to see howEco Fieldperforms under real-world conditions.More carbon capture methodswill be integrated to broaden the tool’s scope. Over time, we’ll incorporateadvanced policy-driven carbon pricinganddynamic soil datafor more comprehensive analyses.","langchain, python, react","https://github.com/miguel-merlin/Carbon-Factibility-Chatbot,https://github.com/nicohenning/treehacks25,https://github.com/Aveeyy/carbon-farm-factory"
TreeHacks 2025,Macro - Swarm of Verifiable Agents for Portfolio Management,https://devpost.com/software/macro-swarm-of-verifiable-agents-for-portfolio-management,False,"As a college student, I’ve developed a considerable interest in stocks and financial markets. Traditional institutional systems create significant barriers to entry, including KYC requirements, brokerage restrictions, and other friction points. That’s whereMacrocomes in. Macroempowers anyone to:",Macro empowers anyone to:,,,,,,"bash, coinbase-agentkit, coinbasesdk, eigenlayer, eigenlayeravs, hardhat, javascript, nextjs, node.js, python, react, rust, shadcn, solidity, typescript, v0, vercel",https://github.com/shrithan1/macro
TreeHacks 2025,Memody,https://devpost.com/software/memody,False,"The inspiration for Memody came from understanding how the human brain processes information through multiple sensory channels. Research shows that multi-modal learning - combining visual and auditory inputs - can significantly enhance memory retention and comprehension. This will be extraordinarily helpful for learners with dyslexia and ADHD. We were also inspired by how music has been used throughout history as a memorization tool, from ancient oral traditions to modern educational songs. By transforming text into music, we're creating a new way to experience written content that leverages these powerful cognitive connections.",Memody transforms the reading experience by creating a synchronized audio-visual experience:,,,,,,"ai-music-generation, chrome, javascript, llm, perplexity, windsurf",https://github.com/Axelmannen/memody
TreeHacks 2025,VShop AI,https://devpost.com/software/vshop-ai,False,"Imagine having your very ownvoice-activated personal shopperthat not only finds the perfect items for you but also lets you buy them instantly—whether you want to pay inUSDorETH. Youspeakyour shopping wish—like “I need tennis balls” or “Find me a new laptop”—and theAI assistanttranscribes your voice, interprets what you’re asking for, andautomaticallysearches with Exa.ai for the best matching products. You get a curated list of recommendations with images, prices, and descriptions. Then, with a single click, you can buy your chosen item using eithertraditional payments(USD) orcrypto(ETH). The project seamlessly handles the purchase flow, calculates how much ETH you need (pulling real-time price data from an oracle), and even interacts with yourweb3 wallet(via Wagmi hooks and RainbowKit). It’s built onNext.js 13for the modern UI, plusRainbowKitandWagmifor a clean crypto wallet experience. TheAIside uses custom endpoints to parse user intent, fetch product data, and manage the cart logic. You get a smooth, voice-driven, multi-currency shopping experience all in one place.","VShop AI is avoice-first, AI shopping agent. You talk to it, and it:",,,,,,"eleven-labs, eoracle.io-avs, exa.ai, gpt-4o, json, next.js, rainbow-kit, ryepay, typescript, wagmi",
TreeHacks 2025,Axon,https://devpost.com/software/axios,False,"In today's day and age, media sources are increasingly more polarized, as is the entire world. This got us thinking. What if you could have an all-in-one news briefing on things you ACTUALLY cared about each morning? What if you could listen, read, explore, and understand what went into the news YOU care about? What if it was free of bias, and you could dive into the details while exploring a variety of sources that tell the whole picture, not just one side?",Axon creates an all-in-one customizable newsletter and podcast that you receive each morning. It includes fact-checking features and research on topics of interest through a deep-research interface.," Frontend We created an onboarding customization system, along with a user dashboard with the previous day's newsletters/podcasts that is accessible through Supabase Auth. We use Resend to send an email with the plaintext custom newsletter each morning. This email encourages users to go to the website, where they can interact with and explore sources and topics contained in the newsletter. We use a deep research implementation and hardcoded websites to find opposing sources and data that users can interact with to get a holistic view of events they are interested in. Server We used Supabase for user accounts and settings, including personalization settings, UUIDs, etc. We used AWS S3 buckets for large files and data manipulation to coordinate among our many scripts. We use AWS services to coordinate our scripts through the queue system each day to get a custom newsletter and podcast ready for each user and to handle async requests that take more time than a regular API endpoint would handle. We used IMAP to subscribe to various newsletters across different topics and parse data into our database. We then used OpenAI models to synthesize the topics into one data/information file for each topic. We extracted keywords for later use. We used our keywords to scrape for different news websites across the political spectrum and use them in our deep research implementation. Scrapybara finds resources based on keywords extracted from the newsletters to use as a basis for fact-checking/extra information. We used a reinforcement learning method to generate and score a final user-facing synthesized newsletter, then repeat it until a satisfactory score is reached. We had Claude 3.5 Sonnet read and judge our GPT-4 Turbo's output with feedback to generate the best possible custom newsletter. We implemented a fork of Scira, an LLM tooling platform, in order to explore user-relevant topics in the newsletter and allow users to view a wide variety of sources. We use our final newsletter and the ElevenLabs API to create a podcast version of the newsletter that is included on the protected dashboard page.","Prompt engineering to perfect fact-checking methods, content generation, and optimizing usage of a virtual machine through the Scrapybara interface to navigate and scrape websites reliably. 
Implementing reinforcement learning among AI models to create interesting, natural-sounding newsletters/podcasts. Putting together a wide variety of scripts that we each worked on individually into a scalable production environment was difficult but doable through the great tools available in the AWS suite. We also had some trouble figuring out the right sources to hardcode, as well as which sources we could trust as fact as a fact-checking service. This is still a work in progress, especially with regard to scaling and caching strategies, and we look forward to continuing work on it.","We shipped a scalable, working, persistent system in 36 hours. We’re super excited about the mission, and glad we were able to work on such an interesting project.","We learned a lot about working in a team in a production environment, breaking things, merging the wrong branches, and never giving up. We also used many new tools we had never touched, including innovative services like Scrapybara.","We don’t want to stop here. There’s a data crisis warring, and we believe a solution like ours is what is needed to truly provide perspective on world events and interests accurately and efficiently. Our next steps include collecting and hardcoding pipelines for a gradient of sites and sources on current topics, and adapting even more to what users want to see each morning, perfecting our personalized experience. By providing a customizable daily dose of truth, we want to unify a splintered public.","html/css, python, sql, typescript",https://github.com/orgs/Palantuah/repositories
TreeHacks 2025,FlowScope:The Agentic Simulator for Predictive Flow Dynamics,https://devpost.com/software/flowscope-global-dynamics-visualizer,False,"In recent months, the world has witnessed several tragic aviation incidents in the US—heartbreaking events that remind us of the fragile nature of our interconnected world. Behind every route and every flight lies a story of lives, livelihoods, and human connections. These events deeply moved us, fueling a vision to build FlowScope:The Agentic Simulator for Predictive Flow Dynamics, so that the world can be modeled and predicted at unprecedented heights. Our platform seeks to bring clarity to the invisible patterns of movement that shape our world—marine and air transportation, refugee flows, and other logistical patterns—so that we can predict, prepare for, and prevent future disruptions.","Imagine a world where transportation disruptions, refugee movements, and logistical challenges are no longer unpredictable mysteries. Introducing FlowScope --- your command center for global flows, an AI agent powered platform that forecasts, simulates, and visualizes dynamic movement patterns in real-time. In this demo, we focus on marine transportation, showing how FlowScope empowers decision-makers to anticipate weather disruptions, geopolitical tensions, and operational bottlenecks. Our platform isn't limited to one domain; it's designed to adapt and scale across industries, from aviation to humanitarian logistics.","AI Agent Architecture: We use multiple AI agents powered by OpenAI's GPT models with LangChain, Perplexity Search, and real-time API calls. These agents work collaboratively to bridge the gap between static LLM task planning and the need for dynamic, real-time data. We also use existing marine traffic dataset (Automatic Identification System observation and dirways) to help with routing simulation. We use Leaflet map library for React to visualize maps. Intelligent Data Retrieval: Perplexity Search fetches relevant news based on location and keywords, providing our agents with context-aware insights. Path Planning and Prediction: We employ both LLM-based geopolitical risk assessment and traditional algorithms such as A* search to predict and simulate flows efficiently. We model ports and trade routes between them as nodes and edges in a graph dropping ports from this graph whenever critical ports or waterways are shut down like in the 2021 incident when the Suez Canal was obstructed for 6 days. Our ML model learns to assign weights to each of the edges in the graph, representing the cost of traversing that edge, based on a set of conditions: directional water ways, shallow water areas, sharp turns, vessel speeds in certain waters based on vessel class, geographic constraints, and historical route patterns. A* search is then used to traverse this graph and find the optimal route. Real-Time Updates: APIs like OpenWeatherMap feeds update our platform in real-time, enabling the AI agents to adjust predictions dynamically. We provide comprehensive data analysis across different metrics, including voyage types, fuel prices, and distances. Prompt Engineering: Crafting precise, contextual prompts allowed the agents to infer, reason, and respond accurately to new data inputs, enhancing reliability and interpretability.",,,"FlowScope is more than a transportation simulation tool; it's a step toward a future where global flows are predictable, manageable, and resilient against disruptions. Join us as we bring this vision to life!",,"fast-api, langchain, openai-api, perplexity-search, python, react",https://github.com/HowardHuang1/TreeHacks2025
TreeHacks 2025,dAIos,https://devpost.com/software/daios,False,"Landing Page Utilize agents to create whitelist criteria for your DAO based on requirements DAO page with Token Details Determining whether user is eligible for DAO using agents and custom data connectors Recently with the release ofdaos.fun, decentralized autonomous organizations have gained immense popularity with some DAOs generating over $2 billion in market cap. The TL;DR of daos.fun is that people create DAOs hosted by the site and then are able to raise money for their organization through funding rounds and whitelists. This sounds great on paper, but in practice, it has one major problem:the whitelist problem. The whitelist problem is the issue of determining which users get early access and allocation to join each DAO in order to raise initial liquidity to support the DAO token’s liquidity pool. Currently, the way in which whitelists are determined is through DAO creators manually reviewing thousands of addresses that they were flooded with on Twitter or a Google Form they sent out. This process is often frustrating for both the creator of the DAO, who is just trying to find the right people for their organization, and even more frustrating for whitelist applicants who are fighting against an army of Twitter bots all trying to take their spot. To fight against this issue, we introducedAIos, a DAO launchpad with eligibility entirely managed by autonomous agents. To create a DAO, all a user has to do is talk to our agent to determine what the necessary entry requirements should be. The agent will generate a list of requirements to join as well as give the creator the ability to mint DAO tokens and NFTs for future members of the DAO. NFT holders will be whitelist members and will be given a fixed allocation they can buy into before the token launches in order to get in early and generate initial liquidity. The DAO owner can also decide if NFT holders can get a share of the revenue generated by trading once the DAO token launches. To join a DAO, you will have to meet the eligibility criteria set by the DAO owner, determined by an agent that will search for your information online and ask you questions if it needs to confirm anything. If the agent finds you to be eligible, you will be given the DAO NFT and the opportunity to buy into the DAO token early through allocation. For example, say someone createdRevengeDAO, a DAO where the target audience is people who have lost a lot of money in the past. In order to join, the agent might ask if you have any wallets that show you lost a considerable amount of money on a project. If you qualify, you will be given the NFT and early access to the token. This project tackled a problem that all of our team members have personally experienced when using daos.fun. The whitelist process is full of bugs, spam, and major challenges for everyone involved. We truly believe that this product coulddrastically improve the status quo. When building this project, we learned just how difficult it is to build acohesiveapp that manages so many components at once. Our biggest challenges were: In the end, we were left with anMVP that we are proud ofand excited to develop further.",,,,,,,"agent, avs, blockchain, contracts, eigenlayer, etherscan, fastapi, foundry, langchain, next.js, openai, shad-cn, trpc, typescript, v0, web3","https://github.com/rohan-patra/daios, https://github.com/vardhanshorewala/avs"
TreeHacks 2025,QuickQuack: The Ultimate Exam Grading Tool,https://devpost.com/software/quick-quack,False,"Six hours, uninterrupted, grading a single exam. That's not a typo, that's our team member’s reality. As a TA for one of the largest CS classes at Stanford, she spends up to 12 hours per quarter just grading tests alone, even with a 100-person teaching team. That's roughly 4800 hours per year dedicated to grading, hours that could be directed towards impactful teaching, not tedious paperwork. This is because CS exams are handwritten. TAs cannot run student code against the provided rubric. Instead, to grade, TAs must drudgingly review every line written and carefully discuss their possible results, opening up room for human error in the process. To alleviate this burden, we developed a web app that automates the grading processes, creating a system that automatically extracts handwritten code, executes the code in a sandbox environment, and evaluates code output against a set of tests and rubrics that can be customized by the user. Based on our user testing and interviews, our tool has the potential to decrease grading time by 80%.",QuickQuack is an AI-powered grading application that extracts submitted handwritten code and evaluates them against user-inputted test cases and grading rubrics. Here’s what it does:,"Frontend: We used FlutterFlow to prototype and develop the overall architecture of our application, allowing us to create a clean and intuitive user interface. FlutterFlow’s streamlined graphical interface, combined with custom API integrations, made it possible to connect our grading system seamlessly with the backend.Backend: To process handwritten code submissions, we leveraged advanced OCR (Optical Character Recognition) technology combined with large language models (Gemini, ChatGPT) to extract handwritten text from uploaded images. This allowed us to convert handwritten student submissions into digital code that could then be executed and graded automatically. For code execution, we experimented with multiple APIs, including Piston and JDoodle, to find a reliable solution for running submitted code. After troubleshooting issues with multi-line execution in Piston, we ultimately integrated JDoodle, which successfully handled input/output operations in our FlutterFlow application. Our backend handled API requests for grading and feedback. It managed test case evaluation, correctness checking, and AI-driven analysis for efficiency and style. We structured our API calls to ensure smooth data flow between the frontend and backend, optimizing for speed and scalability.","36 hours ago, none of us ever worked with FlutterFlow. Now, we emerge with a fully functional web app, and achieving this means addressing various challenges. 
First,navigating flutterflow interface. While powerful, FlutterFlow was new to us, and the process of discovering and learning to use its plethora of features took time. We found it especially challenging to manage both frontend and backend developments. For example, Gemini code extraction often includes garbage values, and we needed to learn how to write custom string parsing functions that cleaned up this output. Additionally, the process ofprompt engineeringwas tricky. Because our code extraction and grading features make calls to LLMs, we must ensure that model outputs remain consistent and accurate over usage. This looked like many trials and errors as we perfected the optimal prompt that elicited the best results from our models. Moreover,achieving reliable code executionwithin FlutterFlow presented technical challenges. While AI models such as ChatGPT/Gemini were valuable for other aspects of our development, we found that their outputs lacked the necessary consistency for deterministic code evaluation. Similarly, Piston API presented challenges with multi-line code execution within FlutterFlow. Ultimately, we discovered JDoodle, which successfully executed inputted code within our FlutterFlow application.
Finally, we are thankful to our Flutterflow mentors (shoutout to Patricia Wei!) for guiding us through some of our implementation hurdles along the way.","Our solution is designed to be highly scalable; it can be used to streamline the grading process of thousands of CS classes around the world. QuickQuack significantly reduces the time and effort required for evaluation while maintaining accuracy and consistency. In addition, we are proud of our successful integration of essential back-end functionalities with FlutterFlow, enabling a seamless and efficient user experience. We have deepened our expertise in working with APIs, leveraging them to optimize our system’s performance and functionality. One of our key achievements in this regard was implementing a robust and reliable code execution system. After extensively testing multiple APIs, we identified the best fit for our platform, ensuring efficient and secure execution of student submissions. Through this process, we have not only built a powerful tool but also expanded our technical skill set, gaining valuable experience in software development, problem-solving, and system design. Finally, we iterated on our product through user testing and interviewing real CS TAs, continuously learning user insight, improving user experience, and ensuring that our tool addresses real user needs.","Throughout this project, we gained valuable insights into several key areas.
First and foremost, we deepened our understanding of FlutterFlow development—learning how to navigate the platform, integrate APIs, and design a user-friendly interface. We explored FlutterFlow’s capabilities, troubleshooting issues along the way and adapting our approach to work within its constraints.
Next, we strengthened our skills in API integration, experimenting with different APIs for remote code execution. Initially, we faced challenges with Piston, which struggled to handle multi-line code execution in FlutterFlow. Through trial and error, we learned how to debug API-related issues and ultimately implemented JDoodle as the most reliable solution. This process taught us how to structure API calls effectively and manage data flow between a web-based frontend and a FastAPI backend.
For automated grading, we researched different approaches to ensure a fair and efficient system. We learned how to assess code correctness, analyze efficiency, and evaluate style using structured grading criteria. Additionally, we gained experience in designing a scalable architecture that could handle high volumes of grading requests, making our system adaptable for different courses and institutions.
Beyond the technical aspects, we learned valuable teamwork and problem-solving skills. Working through technical roadblocks required strong collaboration, clear communication, and effective task delegation. We developed a better understanding of how to efficiently split responsibilities, support each other in debugging complex issues, and iterate on solutions together.
Overall, this project taught us not just the technical skills needed to build an automated grading platform, but also the importance of adaptability, persistence, and teamwork in tackling complex engineering challenges.","We want to further scale up our project to be a multifunctional Gradescope. Right now, QuickQuack can grade individual submissions, but we want it to handle large-scale processing—automatically sorting, grading, and generating reports for hundreds or even thousands of students in one go. Eventually, we want it to be able to take in pictures of many students and create separate auto-grade reports for each student. This would make it an even more valuable tool for massive courses with high enrollment numbers, like CS106A and CS 106B. Beyond this, we could get it to expand to even more classes, including other classes that require handwritten code.","flutterflow, gemini, jdoodle, json, openai, python",https://app.flutterflow.io/run/xtzmhNYztFILRF2hkz74
TreeHacks 2025,Green Growth,https://devpost.com/software/green-growth,False,"Sustainability activists advocate for a revolutionary shift from fossil fuel to clean energy, and green jobs could represent around 14% of total U.S. jobs by 2030…but is our workforce ready? Acknowledging that marginalized communities are disproportionately affected by both climate change and economic change, our project was inspired by the idea of ajust transition,which the International Labour Organization defines as the “greening the economy in a way that is as fair and inclusive as possible to everyone concerned, creating decent work opportunities and leaving no one behind.” Further research brought to light the hardships faced by many workers in the fossil fuel-dependent industries. In fact,roughly 1.7 million workersin the US are projected to lose their jobs. Furthermore,around 20% of fuel sector unemploymentconsists of fossil fuel jobs in 2023, and this number is projected to grow as the US rapidly moves towards renewable energy sources. We aimed to create a platform that opens new opportunities to job seekers and policymakers hoping to get ahead of the curve by embracing clean energy.","Green Growthpredicts the next big thing in sustainability to support a just transition. Our platform analyzes a novel dataset of clean energy sector trends to: Green Growthhelps workers prepare for sustainable careers and adapt to the evolving job market without the risk of displacement. Additionally, it provides data-driven projections that can guide policymakers in making informed investment decisions, ultimately fostering a cleaner, more sustainable world.",,,,,,"colab, flutterflow, numpy, openai, pandas, perplexity, python, scrapy, sheets, windsurf","https://github.com/night-pianist/treehacks, https://treehacks2025-rtxkyq.flutterflow.app/"
TreeHacks 2025,WatchTower,https://devpost.com/software/watchtower-fulnqt,False,"Given the recent events with the media focus on airplanes and the incidents that arise from the complex situations pilots need to deal with, our team has created WatchTower to provide pilots with the information they need to reduce incidents. A study by Boeing found nearly half of all aviation accidents were during landing while 14% were during takeoff. Our product is designed to reduce the number of accidents happening in these situations but also help during emergency situations. Traditional panels for airplanes are direct sensor information and the pilot has to interpret multiple sources of data at once for a large vehicle with precision.","Similar to Tesla's dashboard, our product provides a depth map to give pilots a stronger sense of spatial awareness. Our product also gives recommendations on actions to take next for pilots to avoid potential collisions.",Our team used a single web camera and by leveraging OpenCV we processed the video feed frame by frame. We used the MiDaS model developed by Intel in a bilinear mode for the model to determine the depth of each pixel of the screen to associated depth of the environment from the camera. We also used edge detection powered by the YOLO model to create an overlay graph that shows both the depth map and the edges of the video feed.  The overlayed map is provided to the user to assist in the pilots spatial awareness as stated before. To create a point cloud for the collision simulation we discretize the time to create time points to create a point cloud for every few frames. The point cloud is then sent using our backend API to a create an endpoint to allow for streamed data access for our collision simulation. The collision simulation takes the  point cloud of the environment and a point cloud taken manually of our plane object to simulate movement. The object is taken a point of reference which allows us to keep it still while the environment point cloud changes allowing us to detect if potential collisions in the path of the airplane point cloud.,Our initial  challenge was using stereo vision with two web cameras to create a depth map real time which led to calibration issues resulting in an incomprehensible depth map. To solve this issue we found a method to use one camera and use the MiDaS model to create a depth map. We also had an issue of converting a depth map to a point cloud which was solved by creating a point of the location of the pixel for x and y and using the depth as the coordinate for z. By taking a point every 4 pixels we were able to create a point cloud with a max limit which we set at 500 points to ensure high speed. Another challenge was creating the point mesh for the airplane object which resulted in us using open source libraries to model the airplane.,We were able to accomplish our project goal by using less hardware than we initially had planned for with the use of MiDaS and one web camera. We also created a real time low latency depth map and edge detection to allow for users to dynamically see their surroundings compressing the information into one source for users to look at.,We were learn about optimization using pytorch GPU mode to increase the speed that our visualization tools would run and also for the calculations we were doing. We also learned how to manage packages better and how to revert for virtual environments.,We hope to expand to full autonomous airplanes using an improved version of our software. We would like to use ai models to expand our recommendation engine and improve its accuracy. We would also  like to improve our frontend to create an informative and pleasing UI better than our current one.,"flask, machine-learning, matplot, midas, python, yolo",https://github.com/hpuppala26/TreeHacks2025/tree/main
TreeHacks 2025,Theia,https://devpost.com/software/theia-p0mces,False,"The LA wildfires opened our eyes to a serious challenge facing firefighters. 77% of firefighter fatalities occur from disorientation during rapid breaches of enclosed structures.. When these first responders enter smoke-filled buildings, they often can't see more than a few feet ahead - making an already dangerous job even riskier. That's what inspired us to create Theia, a technology that lets firefighters detect people through walls, even in zero-visibility conditions. By giving them this ""sixth sense,"" we're helping ensure both firefighters and the people they're trying to save make it home safely. The project name ""Theia"" was named after the Greek goddess of sight and vision.",,,,,,,"c, c#, c++, cuda, esp32, jetsen, linux, python, tensorflow, vr",https://github.com/Timothy2105/theia
TreeHacks 2025,EverCare,https://devpost.com/software/wellnest-a32yt4,False,"Having witnessed firsthand the challenges that one of our teammates' grandparents faces living alone, we became acutely aware of the difficulties involved in maintaining their health and well-being. Elders like them struggle with remembering daily health updates, deciding when to seek medical attention, and navigating modern technology. This experience deeply inspired us to create Evercare—an app designed not only to simplify health tracking and make it accessible for elders with speech-to-text capabilities but also to provide peace of mind for both seniors and their families. By offering an AI-powered 100% speech-to-text system, Evercare allows seniors to effortlessly record their health updates, which can be easily shared with doctors. This data exchange helps medical professionals assess conditions more accurately and efficiently, improving care and ensuring that both seniors and their families feel more connected and supported in managing health. Our goal is to make it easier for seniors to manage their health, reduce stress, and foster a sense of security in their daily lives.","Users can describe their symptoms to a conversational AI agent, which then processes and delivers personalized responses based on a user’s background information and previous entries. These responses help the user better understand patterns in their health and flag issues that may require a visit to a healthcare provider. The responses are converted to speech using text-to-speech software in the backend, allowing users to hear them aloud. Users can access their previous conversations with the AI agent, allowing them to review past discussions and track their symptoms over time.",,,,,,"elevenlabs, flask, javascript, openai, perplexity, python, react","https://github.com/ksantoso2/EverCare, https://www.canva.com/design/DAGfOS53Yjo/XSIQnfjRvW0U3Q195A5g_g/edit?utm_content=DAGfOS53Yjo&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton"
TreeHacks 2025,Clampsai,https://devpost.com/software/clampsai,False,"Security cameras are everywhere, but they're only useful if someone is actively watching them. We saw this problem firsthand in retail stores and public spaces where security personnel struggle to monitor multiple feeds simultaneously. This inspired us to create ClampsAI - a system that could intelligently monitor multiple cameras and alert security personnel only when necessary. This enables us to build an autonomous system that intelligently monitors multiple security feeds and takes the appropriate action—whether it's notifying emergency services with real-time details, alerting security personnel, or contacting a designated family member in relevant situations. ClampsAI aims to revolutionize response times, enhance situational awareness, and transform security monitoring.",ClampsAI is a real-time surveillance system that:,We built ClampsAI iteratively: Our tech stack:,"One of the largest challenges we faced was with the gemini multimodal live api endpoint receiving only one stream of video. Our goal was to synchronize multiple video streams and perform natural language understanding tasks over them. In order to do this, we had to reverse engineer the Gemini Multimodal Live API to stream live bits of video from each individual streaming node into Gemini 2.0 FLASH in order to replicate the experience of using Multimodal Live. Gemini 2.0 Flash's video understanding with low latency allowed us to accomplish this. Another large challenge we saw had to do with high latency. We have multiple agentic steps with a lot of data coming in continously but need low latency for our realtime phone calling feature. We fixed this by parallelizing our threat analysis and cross-camera synthesis and experimenting with our video capturing pipeline to reduce latency.",,,,"eleven-labs, fastapi, flask, gemini, google, html, javascript, twilio",https://github.com/agamg/treehacks-clampsai
TreeHacks 2025,prototyp3d,https://devpost.com/software/prototyp3d-vc96d5,False,"Prototyping has revolutionized digital design—Figma made web and mobile seamless, Canva simplified graphic creation—but 3D and VR remain stuck in the past. Existing tools are slow, overly technical, and inaccessible to most creators. We wanted to change that by building Prototyp3D, the first effortless, AI-powered 3D/VR prototyping tool that generates real, working code and an easily interactive environment solely from natural language descriptions.","Prototyp3D enables anyone—developers, designers, educators, and entrepreneurs—to bring their 3D and VR ideas to life instantly. Users simply describe their vision in plain text, and our AI Agents go through an optimized pipeline to generate functional 3D/VR code, while rendering the results step by step for the user to interact with. It’s like having a virtual engineering team that plans, builds, tests, and refines your prototype—all without requiring coding skills.",,,The scope and complexity of projects that Prototyp3d can create far outpaces anything current models can output. We are proud to have utilized AI agents to create a debugging/feedback pipeline that models the thought process of real seasoned engineers. Developed a working end-to-end pipeline where users can describe ideas and receive functional 3D/VR applications in minutes. We are proud to have created a customer oriented interface that allows users to not only see and edit the generated code but also interact with the rendered 3d environment itself.,"The amount of possibilities that AI agents unlock is incredible: the use of them to simulate human engineers unlocks the possibility of creating projects over 10 times as complex as a query to the latest models. Natural language AI for software engineering is incredibly powerful, but structuring its output into reliable, functional code takes careful design
.
Speed and quality are hard to balance: we focused a lot on outputting quality code, but that meant sacrificing run times and having to iterate through long pipelines.","Expanding VR Compatibility: Adding support for Unity, Unreal Engine, and other VR platforms.
Interactive Queries: We want to flexibility to the next level, letting users specify wanted changes by interacting with the 3D environment itself.
Collaboration Features: Enabling real-time, multiplayer prototyping so teams can build together. An even bigger future: we think our ai agent pipeline can be generalized to more types of generation. It can be used to generate projects outside of the 3d space. It can be used to write essays. It can be used to generate videos. We think our pipeline unlocks massive potential for generating quality output, and think that this technology can be generalized and used for so many different products.","anthropic, bash, css, html, javascript, openai, python, react, scrapybara, ubuntu, vm","https://github.com/miyaliu627/prototyp3d, https://bcad-129-105-61-146.ngrok-free.app/devtool"
TreeHacks 2025,KERneL,https://devpost.com/software/kernel-ls5pqa,False,"Optimizing code for GPUs is notoriously difficult despite its great potential in AI training and high-performance computing. Writing efficient CUDA kernels requires deep expertise, making GPU acceleration inaccessible to many developers. We were inspired by the idea of democratizing GPU programming—leveraging LLMs to generate optimized GPU kernels from high-level descriptions. Our goal? Lower the barrier of entry, so that anyone can harness the power of NVIDIA GPUs without needing years of CUDA experience.","KERneL is an AI-powered kernel generation tool that takes high-level code descriptions and transforms them into optimized CUDA kernels. By leveraging large language models (LLMs), KERneL automates the process of writing efficient GPU code, allowing developers to:
✅ Generate CUDA kernels from simple prompts
✅ Optimize performance with AI-assisted tuning
✅ Reduce the learning curve for GPU acceleration With KERneL, more developers can tap into GPU computing for AI, graphics, and scientific simulations effortlessly.","Backend: We utilized NVIDIA Build Cloud to integrate LLMs like Qwen 2.5 7B and DeepSeek R1 for kernel preprocessing and generation, as well as OpenAI’s state-of-the-art tools for additional kernel optimization. Frontend: A user-friendly interface built with Streamlit allows seamless interaction for generating, testing, and refining CUDA kernels. Cloud Computing: Hosted on Brev.dev Cloud with H100 GPUs for high-performance compute tasks, integrated through an ngrok tunnel for secure access.","Backend-frontend integration: Coordinating data flow between the Flask API, compute instances, and the NVIDIA Build Cloud was complex. Kernel validation: Ensuring the generated CUDA kernels met performance expectations required extensive testing and refinement. LLM prompt engineering: Guiding the language models to understand PyTorch timing details and compiler logs was time-intensive.","We fixed the challenges! Above all, we are proud that we cooperatively finished the project and helped each others out to augment our skillsets.",,Next updates will include the following features:,"c++, cuda, gpu, kernel, n-grok, nvidia, python, reasoning, streamlit",https://github.com/radi-cho/KERneL
TreeHacks 2025,Haven,https://devpost.com/software/haven-6nfwr9,False,"Anxiety is real. According to the American College Health Association, a staggering 66% of college students reported experiencing overwhelming anxiety in the past year. Even more concerning, 41.6% of college students meet the clinical criteria for an anxiety disorder. 
For us, we decided to combat this issue when we realized that there was no way to easily manage anxiety during those critical moments. As students, we juggle academic pressure, social expectations, and personal challenges—all of which can trigger anxiety. We've each had our share of panic attacks in public, moments where we felt isolated, unsure of how to stay calm. It's painful, and sometimes, help isn't within reach. We asked ourselves: What if there was a way to make these moments easier? What if we could create something that not only helped us deal with anxiety in real time, but also fit seamlessly into our busy, social lives? A solution that wouldn't be obvious to others or require pulling out a gadget or device, but one that could work discreetly and automatically in the background. Through research and conversation, we discovered that anxiety often showed up in physical signs: like a racing heartbeat, a hot flash, or maybe shallow breaths. These are things we could measure. Thanks to smart devices like the Apple Watch, we could track our biometric data—like heart rate variability, skin temperature, and motion patterns—and use this information to understand when anxiety was starting to take over. We also learned that there were a vast variety of therapies and technologies proven beneficial for anxiety, like deep pressure or vibration, that are currently underutilized. Therefore, that became our goal: create a garment that could adapt to those triggers and respond with these underutilized therapies. That’s when we combined this issue with our passion for fashion tech: what if we created clothing that can help with anxiety? This was the birth of Haven: the world’s first garment that detects and cares for anxiety.","Haven is a smart garment designed to detect and alleviate anxiety in real time. Using data from pre-your own wearables and biometric sensors embedded within the garment, it continuously monitors physiological indicators like heart rate variability, skin temperature, and breathing patterns. When it detects signs of anxiety—such as an elevated heart rate or erratic breathing—Haven automatically activates its built-in therapeutic mechanisms. These includedeep pressure stimulation, mimicking the calming effect of a weighted blanket, andvibration therapy, which provides soothing rhythmic pulses to help regulate stress responses. All of this happens seamlessly, without the need for user intervention, allowing wearers to manage their anxiety discreetly and effortlessly.","We used Python to develop and train machine learning models that analyze raw PPG data collected from long-term use of our wearable device. Our models leverage data from the Terra API, processing photoplethysmography (PPG) waveforms to classify heart rate patterns as “regular,” “irregular,” or “AFib.” Beyond classification, our models generate personalized probability scores and severity indicators for various heart conditions, aligning with the diagnostic insights provided in our metadata.csv. By tracking heart rate variability (HRV) trends over time, our system can detect dysregulated patterns indicative of potential health risks. For example, elevated HRV in a chaotic pattern could suggest an underlying anxiety disorder, prompting users to seek further evaluation or medical intervention for long-term relief. By combining wearable health monitoring, machine learning, and personalized cardiac risk assessment, our project empowers individuals with real-time insights into their heart health, supporting both preventative care and early diagnosis.","Machine Learning ComplexityWe initially experimented with deep learning models, but simpler traditional ML models proved more efficient and accurate, leading us to pivot our approach mid-development. Real-Time Wearable Data ExtractionExtracting real-time data from the Apple Watch was challenging without an Apple Developer account. We built custom Swift scripts to pull HealthKit data, but transmitting it to our Arduino-connected clothing via Firestore caused compatibility issues and slow speeds. Ultimately, we simulated real-time streaming using TerraAPI’s Garmin data. Hardware & Time ConstraintsDesigning a functional wearable typically takes months—we had hours. We improvised with scrap materials, modified existing 3D models, and streamlined hardware-software integration to build our prototype within the hackathon timeframe. Despite these hurdles, we adapted quickly and delivered a functional prototype for real-time cardiac monitoring and anxiety detection.","Once the raw PPG data is correctly classified, our ML models generate probability scores and severity indicators with high accuracy! Beyond functionality, we also focused on aesthetic design—our hardware device is not just wearable tech but a fashionable, trendy, and timeless top that seamlessly integrates health monitoring with style.",,,"3dprinting, arduino, autodesk-fusion-360, c++, firebase, javascript, machine-learning, mongodb, motors, node.js, numpy, pandas, python, pytorch, scikit-learn, sewing, swift, tensorflow, terraapi, xcode",https://github.com/fukaikeda/treehacks2025
TreeHacks 2025,QPQP: Flexible Exchange Platform,https://devpost.com/software/qpqp-flexible-exchange-platform,False,"Kintsugi, or the art of repairing pottery with gold-dusted lacquer, is a Japanese practice that portrays damaged possessions in a manner that sometimes elevates their significance beyond standard ""perfect"" ones. As a result, the value associated with them factors in the story of the objects' lives, not just their present condition. This philosophy primarily extends tochanoyu, the Japanese tea ceremony, but by applying such perspectives to their daily lives, participants can gain a greater appreciation for the flaws and minor incidents that make every experience unique. On the other end of the spectrum, the US is the symbol of rapid consumerism. We are always eager to buy new things in the spur of the moment and all too often judge their quality too much by their price tag, but when the novelty eventually fades (either due to an unforeseen accident that harms the item, increasing maturity correlated with decreasing spare time, or merely the shifting zeitgeist), these things are often discarded. Of course, recycling centers and donation campaigns exist, but much of the population isn't incentivized to participate so the vast majority of used objects very quickly wind up in landfills. And that's still a better alternative to having them float into the Great Pacific Garbage Patch, which is a giant clump of trash about the size of Texas. Synthesizing these two ideologies, I came up with the concept ofQPQP, orQuid Pro Quo Plaza, which is an exchange platform. However, it differs from other platforms in two key ways: firstly, I hope to build this into the most multipurpose trading site that ever existed, because increasing flexibility can only help facilitate better swaps, and secondly, the framing of such goods or services is not that of an inferior pool to choose from, but rather a more interesting or more intrinsically valuable set to sample from, which can be relatively easily achieved with descriptive stories of previous users. My hopes are that these two points combined can simultaneously help our communities be better connected with each other and with the environment, and that this principle can be a guiding light in some current trying times of darkness.","QPQP is (for now) a web app that attempts to simplify the user flow as much as possible. Once someone logs in, they are presented with 3 buttons: one to set up an offer, one to set a request, and one to view their history of offers and requests. The current version of the design includes just a bare minimum number of variables to fill out in each of the first two cases: a title for searching and bookkeeping, a distance metric to quantitatively gauge an individual's interest in traveling for the good or service, a category selection menu for filtering, and a description field. While most forms tend to leave this part optional, I intended for it to be required because it is the user's opportunity to tell a story about what they're offering, or if they're not feeling particularly talkative, they can opt to say something more light-hearted that can maybe brighten someone else's day. If an offer is submitted, it goes immediately into the database, and if a request is submitted, it immediately returns a select few items from the database that may be of interest to the user. However, the matching process (even at this early stage) is twofold: first, a pretrained word2vec mapping is used on the delimiter-tokenized strings in the title and description fields, and second, the results are fed into a cosine similarity matrix constructor that finally generates optimal rankings based on relevance, then distance. These two slots could realistically have used any word embedding model and any similarity metric, respectively, because the end goal is to fine-tune a pipeline that is most agreeable with the user's transaction history, which can be viewed in their profile along with the transaction statuses. Once a user finds a satisfactory request result, they can choose to directly message the provider. In the best case scenario, this would result in a mutually beneficial trade sans official payment (and the various hurdles associated with that), but even if not, the chain of benefactors can extend through both parties so that it may later be possible to set up a three-or-more-way exchange.",,,,,"In the short term, I want to develop a better system to coordinate exchanges from the backend, because the crux of the functionality is there in the matching quality. I know there are also more modern word embedding techniques and presumably similarity metrics as well, so I'll try to incorporate those into the algorithm too. Currently, the messaging hasn't been set up between users because I haven't figured out how to optimally present the trade situation, but that is another goal that will be quite an important stepping stone. In the long term, I want to evolve QPQP into a sustainable business (both operationally and ecologically) because I firmly believe in the idea that human beings are very much capable of doing good things for each other and our world. Just like inkintsugi, we have nearly come apart at the seams as a society countless times in the past, but there have been heroes large and small who have acted as the adhesive that continues to support us. Living through tough times does not make us weaker, but rather highlights our superhuman resilience when different strengths come together. If we can somehow convince each other to metaphorically place one hand on humanity's heart and one hand on that of Earth, then I have faith that technology can steer the future in a positive direction.","anthropic, cosine-similarity, deepseek, firebase, javascript, mongodb, natural-language-processing, node.js, openai, python, shadcn, tailwindcss, typescript, vercel","https://www.qpqp.site/, https://qpqp.vercel.app/, https://github.com/kl31415/QPQP"
TreeHacks 2025,NeuroParallel.ai,https://devpost.com/software/neuroparallel-ai,False,"In the fast-paced world of AI research, staying current with the latest developments is increasingly challenging. As AI researchers ourselves, we found it difficult to efficiently process the overwhelming volume of papers published daily across various conferences and journals. We needed a tool that could not only analyze research trends but also help implement the concepts we discovered. This inspired us to create NeuroParallel.ai, an intelligent research assistant that bridges the gap between theoretical understanding and practical implementation.","NeuroParallel.ai is an advanced AI research assistant that transforms how researchers interact with academic papers and implement new technologies. It: Analyzes Research Trends: Processes and summarizes recent AI research papers, identifying emerging trends and breakthrough technologies
Generates Implementation Code: Converts theoretical concepts into practical code implementations, complete with documentation and best practices
Decomposes Complex Tasks: Automatically breaks down research queries into manageable subtasks and processes them in parallel
Provides Citation-Backed Insights: Delivers comprehensive analysis with proper citations to source materials
Offers Real-Time Interaction: Streams responses character-by-character with syntax-highlighted code blocks for immediate feedback","We built NeuroParallel.ai using a modern tech stack designed for scalability and performance: Frontend:
Flask for server-side rendering
Server-Sent Events (SSE) for real-time streaming
Custom CSS for syntax highlighting and responsive design
Backend:
Python with async support for parallel processing
GPT-4 for sophisticated analysis and code generation
ChromaDB for efficient message storage and retrieval
Custom task decomposition system for breaking down complex queries
Architecture:
Agent-based system for handling different types of tasks
Controller-router pattern for efficient request handling
Streaming response system for real-time feedback",,"Intelligent Task Processing: Successfully implemented a system that can understand complex research queries and automatically break them down into parallel subtasks.
Beautiful UI/UX: Created a clean, responsive interface with real-time streaming and syntax highlighting that makes reading research summaries and viewing code a pleasure.
Efficient Code Generation: Developed a system that generates well-documented, production-ready code that follows best practices and is immediately usable.
Scalable Architecture: Built a modular, maintainable codebase that can easily be extended with new features and capabilities.","hrough building NeuroParallel.ai, we gained valuable insights and skills: Async Programming: Mastered Python's async capabilities for handling parallel tasks while maintaining response coherence. This was crucial for our task decomposition system.
Prompt Engineering: Learned the art of crafting effective prompts for GPT-4 to generate high-quality code and analysis. We discovered that clear, structured prompts with specific constraints yield the best results.
Real-time Data Streaming: Developed expertise in implementing Server-Sent Events (SSE) for smooth, character-by-character streaming while maintaining proper formatting and syntax highlighting.
System Architecture: Gained practical experience in designing modular, scalable systems. We learned how to balance between immediate functionality and future extensibility.
UI/UX Design: Discovered the importance of user feedback and interface responsiveness in creating a seamless research experience. Small details like typing indicators and smooth scrolling make a significant difference.
These learnings have not only made our project successful but have also equipped us with valuable skills for future development work.","We have exciting plans for the future: PDF Processing: Direct integration with PDF papers for immediate analysis and summarization.
Collaborative Research: Adding multi-user support for collaborative research sessions and shared insights.
Custom Research Focus: Allowing users to define specific research areas and maintain persistent knowledge bases.
Enhanced Code Generation: Adding support for more programming languages and frameworks, with automatic test generation.
Interactive Visualizations: Implementing dynamic visualizations of research trends and relationships between papers.","chromadb, css3, flask, html5, javascript, langchain, openai, perplexity, python, sonar, windsurf",
TreeHacks 2025,ModifAI,https://devpost.com/software/modifai,False,"A friend of Anagha’s, a brilliant automotive engineer, was sharing his experiences of struggling to read heat graphs due to the color gradient. As someone who is colorblind, he was often unable to interpret data solely because of its presentation. This led our team to think about all the ways the web discriminated against those of different abilities. ModifAI is built to change that.","Modif.AI is revolutionizing web accessibility by using real-time AI-driven code generation to modify websites dynamically, ensuring that users get an experience tailored to their individual needs. Instead of relying on static accessibility settings or waiting for developers to build accessibility features into websites, Modif.AI allows users to instantly adjust websites on their own terms—just by making a request. At its core, Modif.AI learns and adapts over time. By analyzing the changes users request—such as increasing font sizes, enabling high-contrast themes, or restructuring page elements—it refines its ability to predict and automate future accessibility needs. This means that as more people use the platform, Modif.AI becomes smarter, automatically applying accessibility optimizations based on past interactions. If a user frequently changes a particular website layout or color scheme, the system remembers and applies similar modifications the next time they visit a comparable site, eliminating the need for constant adjustments. Key Features of Modif.AI
-Instant Accessibility Adjustments: Users can request larger text, high-contrast themes, text-to-speech functionality, or hands-free navigation in real-time, and Modif.AI applies the modifications instantly.
-Dynamic Code Generation: Instead of relying on preset accessibility tools, Modif.AI analyzes website structures and generates custom code on the fly, ensuring that even poorly designed or inaccessible websites can be transformed.
-Hands-Free Navigation: Users with motor impairments or mobility challenges can navigate websites using voice commands, allowing them to click buttons, open links, and interact with elements without needing a mouse or keyboard.
-Personalized Learning & Automation: Modif.AI remembers user preferences and applies similar accessibility modifications to related websites, ensuring a consistent and adaptive experience.
-Continuous Improvement: As more users interact with the platform, it learns which modifications are most effective, improving accessibility solutions for all users over time. By combining real-time AI-powered code generation, predictive learning, and adaptive automation, Modif.AI is breaking down barriers and making the internet more accessible for everyone—one modification at a time.","We built Modif.AI using a combination of agentic AI, real-time code generation, and automated accessibility tools to create a seamless browsing experience for users with diverse needs. To handle dynamic website modifications, we integratedMistral’s Codestral model for rapid code generationandScrapybara to extract and analyze webpage structures.Scrapybara also allows the extension to remotely start an agent instance, enabling users to interact with otherwise inaccessible website elements by automating actions like clicking links or buttons via voice commands—a crucial feature for users with motor impairments. Our agentic logic is powered by Dain, allowing users to request accessibility changes naturally, while Eleven Labs' text-to-speech API provides high-quality auditory support. To enhance personalization, we implemented avector database on InterSystems, which enables Modif.AI to store and retrieve user preferences efficiently. This database plays a key role in recognizing similarities between websites by storing structural embeddings of visited pages and comparing them using tensor-based similarity analysis. When a user visits a new website, the system checks for similarity to previously modified sites and automatically applies the same accessibility preferences. This ensures that users don’t need to reconfigure their settings for every page, creating a smarter, adaptive browsing experience. Additionally, we leveragedCodium’s Windsurf Browserto aid in development and testing, allowing us to iterate quickly and validate our AI-generated modifications across different web environments. Windsurf’s flexibility helped us fine-tune real-time code execution and ensure that our AI-driven accessibility adjustments worked seamlessly across various sites.","The biggest challenge was to navigate Chrome’s security policies when deploying the extension. The security policies make it difficult to edit code in real time. We eventually used Electron that allowed us to bypass these security constraints. Another major challenge was to evaluate the cosine similarity between different websites to determine whether it is similar to a previously visited website. Standard metrics were unclear due to variations in the website’s HTML. This was solved using a tensor-based approach, deploying deep-learning feature extraction to determine the website similarity.","We are incredibly proud of Modif.AI’s speed and effectiveness in modifying websites in real time. With Mistral’s near-instant inference, we can generate and apply code changes almost immediately, providing a seamless, frustration-free user experience. A key accomplishment is our innovative use of AI-driven code generation, allowing Modif.AI to dynamically adapt websites instead of relying on static accessibility settings. This makes it far more flexible across different online environments. We also successfully integrated AI agent tools like Scrapybara and Dain, enabling automated interactions such as clicking links and buttons for users with motor impairments. Additionally, Eleven Labs’ text-to-speech enhances accessibility for those with visual impairments, dyslexia, or reading difficulties. By combining real-time AI modifications, automated navigation, and intelligent accessibility enhancements, Modif.AI allows for a new era of web accessibility.","We had to learn several skills along the way, such as vector databases, building AI agents, and working with Electron. Aside from learning how to build a project from scratch, our biggest learning was how to design with empathy and consider the consequences of what we build for the world.","We want to prepare for public launch, with initial beta testing. Additionally, we want to expand the range of features we have such as introducing voice-controlled browsing, auto filling forms, and integrating sign language for audio content. We also want to bring ModifAI to application and the phone so it is not just limited as a Chrome extension.","dain, electron, eleven-labs, html, intersystems-vector-database, javascript, mistral, python, scrapybara, windsurf",https://github.com/JCoonradt/Modif.AI/tree/main
TreeHacks 2025,MedMentor,https://devpost.com/software/medmentor-o7lh9r,False,"landing page login signup dashboard popup chatbot live recording settings This project is a Health Companion Assistant built for TreeHacks 🌳💡. Our goal is to empower patients during and after their consultations by providing an intelligent companion that records, transcribes, and summarizes their doctor visits. In its beta development phase, the app focuses on diabetes 🍬, while future iterations will expand support to broader chronic disease management and personalized symptom tracking. Our team wanted to work on a project that has real impact on a group of people, and we brainstormed different fields that could benefit most from AI developments. We determined that even though diabetes is a relatively common health condition, it requires great effort for individuals with diabetes to manage their health and blood sugar levels all the time. At the same time, there is not a perfect solution available on the market to meet the specific needs and features for diabetic patients. We saw an opportunity to better the daily experience of people through utilization of AI and took this opportunity to build upon this idea. Our Health Companion Assistant is designed to bridge the gap between patients and their healthcare providers by ensuring no detail is missed during consultations. By automating transcription and generating actionable insights, the app empowers patients to take control of their health and follow up effectively on doctor visits. With a context-aware chatbot that leverages transcripts and patient data as a knowledge base, users receive personalized advice tailored to their individual needs. While initially focused on diabetes in beta, our roadmap includes broader support for chronic diseases, paving the way for a healthier future. We each chose to challenge ourselves by learning technologies that we were unfamiliar with, so we each experienced a learning curve. Our development process required us to integrate each of our parts near the end of the time, and ensuring that each part worked well together was a challenge that we debugged through together.",,,,,,,"css, elevenlabs, firebase, next.js, openai, perplexity, python, react",https://github.com/nishashastry/treehacks2025
TreeHacks 2025,TreeCycle,https://devpost.com/software/treecycle-umd2yx,False,"After awkwardly staring between the trash can and recycling bin trying to figure out where our empty Roost & Roast takeout containers should go, we realized that none of us really knew the answer. Despite seeming like such a simple and straightforward binary problem, there is so much ambiguity and misinformation regarding recycling. We wanted to tackle this issue from the bottom up by building something for environmental education. After some initial research, we found two main causes of unsustainable habits. The first is a lack of environmental curricula, specifically for students in grades K-6. The second was the ""one person can't make a difference"" mindset—a sort of diffusion of responsibility that many use to justify making unsustainable decisions. Brainstorming ways to tackle these issues, TreeCycle was born. We decided that we could gamify this process of learning to discard waste and turn it into a VR application that would be a fun and engaging method to teach kids how to get rid of their waste. In addition, being able to control the environment of the virtual world allowed us to change the scenery depending on whether or not the user chose the right bin, helping teach kids that their choices do directly impact our Earth.","Our product sets up an environment for people to explore how to discard their waste through two methods: practice and clean mess. For the practice method, we created a welcoming environment that allows users to interact with the world around them and practice discarding the various articles of waste scattered across the ground, updating the dynamic world around them to reflect the effects that their choices would have on the environment. An additional feature that is extremely useful is the clean mess method that is built from a mobile application where you are able to take a picture of articles of waste that you want to clean up, and a model that we've trained identifies what waste they are, and populates the VR game with those exact waste articles so you can practice cleaning up your mess in a fun environment, while simultaneously learning how to actually clean up your mess.","Our product consists of three parts, the iOS application, the ML model, and the VR Application. Our ML model was trained using NVIDIA's Brev.dev platform where we rigorously trained data using a finetuned YOLO model to identify pieces of trash from a picture containing multiple articles of trash. The model identifies what trash it is and creates a JSON file with the objects it found. In order to get an image of the trash, the iOS application has been built to have the capability to take a picture and then send it to an API that handles the image and returns data containing what objects it found. Then, the VR application is able to access that same data and create a game to work with sorting the pieces of trash that you have taken a picture of. Additionally, the VR Application has access to common trash items and can create a randomly generated game that helps people learn to sort their trash even when they don't have a mess.","One of the main issues we ran into was that we worked using Apple's Vision Pro, and due to the fact that it was recently released, there isn't a great deal of documentation for the device, and we often had to do rigorous research to get even basic features implemented due to how complex the Vision Pro is. Another issue was being able to integrate all of the moving parts of this product since there were so many different factors that came into play, whether it was the iOS Application, the classification model, or the VR app, being able to get everything to work between each other was difficult and challenging to integrate.",,"Aside from tackling the technical challenges of integrating VR, iOS, and computer vision, this project that designing meaningful educational experiences requires extensive consideration for the intended audience. Through researching current discussions on environmental education, we learned that creating learning experiences for young students is far more than telling someone ""recycling is important."" Showing them the direct impact of their choices through immersive visualization can fundamentally change behavior. The immediate visual feedback in our VR environment—seeing the trees around you grow when you make the correct decision—helps break down the common ""one person can't make a difference"" mindset. Our most important takeaway is thatdeveloping human-centered technologies requires consideration for human psychology and cognition.",We intend to reach out to elementary school teachers for feedback on our product. We are working toward conducting testing sessions with students and gathering direct feedback on the game's engagement. We also recognize the price of VR headsets raises concerns regarding our product's accessibility in underresourced communities and intend to start conversations with educational nonprofits to ensure our product isn't furthering socioeconomic gaps.,"augmented-reality, brev, fastapi, python, reality-kit, swift, vision-pro, yolo",https://github.com/FeoluK/Treehacks-Sustainability-Project
TreeHacks 2025,AfroVision,https://devpost.com/software/afrovision,False,"This project is deeply personal for me. As a software engineer apprentice at Sony Interactive Entertainment, I often join professional meetings on Zoom and Microsoft Teams. However, I frequently leave my camera off—not out of preference, but because Zoom’s virtual backgrounds fail to represent me properly. My curly, textured hair is often distorted, cut off, or blends unnaturally, making me feel misrepresented and self-conscious on screen. This isn't just my experience; many Black professionals and individuals with curly or afro-textured hair face the same issue. Existing segmentation models are primarily trained on straight and wavy hair, leaving our textures as an afterthought. AfroVision aims to fix this by improving hair segmentation for virtual backgrounds, ensuring people with all hair types are represented accurately and equitably.","AfroVision enhances virtual background rendering by improving the segmentation of curly, coily, and afro-textured hair. We implemented DeepLabV3 with ResNet101 to segment hair while testing how different models handle diverse textures. Our system processes images, generates segmentation masks, and seamlessly blends the subject with a virtual background.","Data Collection: We sourced datasets focusing on curly, wavy, and afro-textured hair, scraping additional images to ensure diversity. Segmentation Model: Used DeepLabV3-ResNet101 for semantic segmentation. Image Processing: Preprocessed images, normalized data, and resized masks for improved accuracy. Testing & Evaluation: Applied segmentation to real-world images to assess effectiveness on different hair textures.","Dataset Limitations: Many publicly available datasets lack representation of diverse hair textures. We had to scrape and manually verify images to supplement the dataset. Google Drive Issues: Encountered technical difficulties accessing and managing data stored in Google Drive, slowing down our workflow. Processing Errors: Debugging file path errors and refining image preprocessing took significant time. Time Constraints: With a limited window, fine-tuning on afro-textured hair remains a next step beyond this hackathon.","Successfully segmented multiple curly hair textures and demonstrated virtual background blending. Created a working pipeline for future fine-tuning and improvement. Identified key areas where segmentation models fall short for Black and curly-haired users, setting a foundation for addressing this bias.",Importance of Representation in AI: Bias in training data leads to biased models. We saw firsthand how standard segmentation models struggle with diverse hair types. Fine-Tuning is Essential: Pre-trained models work to an extent but must be customized to better capture curls and coils. Efficient Debugging Matters: Managing large datasets and troubleshooting Google Drive path issues is a skill in itself.,"Integrate with Zoom SDK: Implement AfroVision into Zoom's virtual background system to enable real-time segmentation improvements. Fine-Tune on Afro Hair Data: We need to train DeepLabV3 further with more labeled images of afro-textured hair. Expand Dataset & Annotation: Manually annotate and generate higher-quality segmentation masks for kinky, coily, and tightly curled hair. Improve Real-Time Performance: Optimize processing speed for live applications like Zoom and Google Meet. Open Source Contribution: Publish the dataset and model improvements to benefit other developers tackling bias in computer vision. Fine-Tune on Afro Hair Data: We need to train DeepLabV3 further with more labeled images of afro-textured hair. Expand Dataset & Annotation: Manually annotate and generate higher-quality segmentation masks for kinky, coily, and tightly curled hair. Improve Real-Time Performance: Optimize processing speed for live applications like Zoom and Google Meet. Open Source Contribution: Publish the dataset and model improvements to benefit other developers tackling bias in computer vision.","colab, deeplabv3-resnet101, google, matplotlib, opencv, pil, python, pytorch",https://github.com/Pamelachristina/AfroVision
TreeHacks 2025,NutriVision,https://devpost.com/software/nutrivision-msxjyl,False,"      I (Archita) have tried countless ways to count my calories, but every time, I ended up giving up—it was just too much work. Manually adding up numbers everyday became overwhelming. Yes, iPhone apps exist that let you take pictures of your food, but they aren’t live, or capable of tracking your daily caloric intake–-I would have to end up writing down and adding my total calories myself by hand, and existing iPhone applications do not already do this for me. They also don’t tell you whether you’re actually working toward your food-intake goals and making progress. The bottom line problem: I want to lose weight, but I need a live assistant to watch what I eat and count my calories because I am too busy to manually add up the numbers. While iPhone applications make you take a picture of your food, and tell you the calories for a particular meal, users want a truly effortless experience. I (Archita) have always dreamed of someone to watch what I eat, and give me a notification whenever I binge eat chocolates, or when I cross my daily caloric intake. Our application provides an effortless way to track food intake in real time. Once you use it, you’ll wonder how you ever lived without it! And we pulled it off! 🚀 The report below follows the assumption that we are deploying our application in the Ray-Ban Meta Glasses—a sleek pair of glasses. In our demo, our application runs on Meta Quest 2.We assume that these glasses will be as common as a smartphone in the future.Our AI-powered VR application will automatically track what you eat, logging calories, protein, fat, potassium, and carbs in real time. If a user is binge-eating unhealthy foods, the system sends a live notification, telling them to stop and helping them work towards their goals. It’s incredible that we created a novel application with the potential to revolutionize the future of health tracking! Wow! We will most definitely use this app to track our calories when VR/AR glasses become more common. We developed an object detection model based on YOLOv11 to perform initial food detection and integrated our application with the OpenAI API for calorie counting. Since the original YOLO model was trained on the COCO dataset, which lacks class labels corresponding to different kinds of food, we scraped food images from online datasets and used the Grounding DINO model to annotate them with bounding boxes for YOLO finetuning. The model was fine tuned on a Jetson Orin Nano. The model is deployed on a web app on a Meta Quest 2 using the onnxruntime JS library and accesses the live video feed to make predictions. Specifically, the model detects when food is in the frame (future support will be added for detecting when a person is eating) and the web app automatically captures a picture to be used as input for a query to the OpenAI API (the YOLO model makes this process less expensive by reducing the number of queries). The OpenAI API call allows us to determine what food the user is eating, its weight, and the approximate nutrient content. We make a further API call to the USDA caloric food database to ensure that our nutrient predictions are more accurate. This nutrient information is sent to an SQL database and stored for future reference. If the user exceeds their daily calorie intake goal, they will receive a notification from the app. $10 Billion Dollar Market Cap estimation. 
We expect our app to be used on the Meta Ray-Ban Glasses, and aggressive AI/VR adoption in the next 5-10 years. 
As if they aren’t already becoming common, the Meta Ray-Ban Glasses are rapidly gaining traction and will soon be ubiquitous. We solve the core calorie-tracking pain point better than any other app, and this could be the next MyFitnessPal but smarter, faster, and fully automated. Our app assumes aggressive AI/VR adoption, seamless integration into daily life, and solving the core calorie-tracking pain point better than any other app. If executed well, this could be the next MyFitnessPal but smarter, faster, and fully automated. The Meta Quest has privacy issues with live streaming. We are not allowed to live stream on the Meta Quest, and we would need to submit a petition to get our use-case approved. It is possible to get our livestream purpose approved, but the approval process takes a very long time and is too long for the duration of this hackathon. So, to demonstrate a proof of concept, we quickly pivoted to using a web-cam that can stream onto MetaQuest. The Webcam connects to our local computer for the server, and streams to the MetaQuest via RTSP protocol. The webcam is mounted on top of the Meta Quest, and we behave as if the Meta Quest itself is live-streaming data. A ton! Live Streaming a peripheral hardware webcam using RTSP protocol, programming VR and AR on the Meta Quest 2, and training YOLOv11 to detect when a person is eating food! Our pipeline is built ground up and novel—we figured it out along the way with little documentation to lean on! In the future, we plan to integrate this application with Terra API as it is very easy to retrieve other health data from the user, such as daily calories burnt. We were looking forward to integrating with TerraAPI.",,,,,,,"flask, metaquest, perplexity, postgresql, vanilajs, yolo",https://github.com/enkhbold470/nutrivision
TreeHacks 2025,SoleMate,https://devpost.com/software/solemate-9jmhuf,False,"76% of adults aged 55–64 use foot orthotics, but traditional solutions are often expensive, require specialist visits, and rely on generic sizing rather than true customization. We saw an opportunity to makecustom-fit orthotics accessible to everyoneusing a device people already own—their smartphone. By leveragingiPhone LiDAR technology, we’re eliminating the need for costly appointments and one-size-fits-all solutions.SoleMate empowers users to create 3D-printed, perfectly tailored insoles with a simple scan—because comfort should be personal, not prescribed.","SoleMateis a mobile app that leveragesLiDAR technology on iPhonestoscan a user’s feet in 3Dand generate a custom-fitted insole. Using this 3D scan, we create aperfectly contoured, 3D-printable shoe inserttailored to the user’s foot shape. Users can thenorder their custom insertsor print them at home with a 3D printer.",1️⃣Capture the Foot Scan 2️⃣Generate a Digital Foot Mesh 3️⃣Invert & Shape the Insole 4️⃣Precision Refinement 5️⃣Smoothing & Final Adjustments 6️⃣Export for 3D Printing,,"📱 Successfully built aworking prototypethat scans feet using LiDAR and generatescustomizable, printable STL files.🚀 Created aseamless and intuitive UIfor easy scanning and ordering.👟 Pushed the boundaries ofcustom footwear accessibility—making orthotics available to anyone with a smartphone.",,"📱Refining the UI/UXfor a more streamlined scanning process.👨‍⚕️Partnering with podiatriststo validate and improve the fit and comfort of the insoles.🏭Exploring mass productionusing sustainable 3D-printing materials.🌎Expanding beyond shoe inserts—3D modeling for virtually anything SoleMate is just the beginning of a future wherecustom footwear is accessible to everyone, anytime, anywhere.","arkit, gcp, lidar, open3d, swift, swiftui, trimesh",https://github.com/TaylorT52/solemate
TreeHacks 2025,CARBONLE,https://devpost.com/software/carbonle,False,"A member of our team had been exposed to new sustainable farming methods during his internship last summer in Singapore. He rallied his teammates behind this newfound inspiration, and once we saw the Stanford Ecopreneurship challenge, we knew we had to make this! We wanted to gamify learning about the world's carbon emissions, to spread knowledge about the topic and encourage better mindfulness!",CARBONLE is a geographical Wordle-like guessing game where you have up to 6 guesses to identify the 'target country'. Our application leverages real-world carbon emissions data and displays them with Treemaps (like the square graphs that Stock changes are displayed on!). How It Works:,"We took many steps to complete this application. We built our frontend from scratch using create-react-app, utilizing graph libraries like Rechart and API libraries like OpenAI. We built and gathered our own datasets. We wrote scripts that utilized APIs and data wrangling methods to obtain Sector and Sub-Sector Carbon Emission data per country. The APIs we used were from Climate Trace and Climate Watch.","We had two main challenges. The first main challenge was wrangling the data into a usable format for our Rechart Treemaps due to a lack of proper API documentations and limited API endpoints to obtain data. Our second challenge was facing the limitations of the Rechart Treemap library, amongst other alternatives. Each Treemap library faced coloring and border issues when it came to nesting Treemaps within a hierarchical Treemap. Thus, we adapted and overcame these obstacles through clever API endpoint scraping concatenation and clever paired UI displays of Treemaps.","We strongly believe that the datasets that we created throughout this hackathon project are the most accurate and comprehensive carbon emission datasets; we believe we have improved the holistic-ness and usability of the Climate Trace dataset specifically. We also all learned a lot about climate and sustainability throughout this project, and we all feel better educated!","China is a HUGEEE contributor to carbon emissions! We also learned that there are some countries out there with negative carbon emissions based on their LUCF (Land use change, and forestry). We also learned that creating a gamified version of a climate game genuinely increases our desires to learn more about this topic even beyond the scope of this hackathon.","We spent hours trying to fix the colors on our Treemaps for sub-sectors. We definitely want to finally find a solution to this, so that we can display a more clear Sub-Sector UI. We would also like to add more fields so that users can filter between different types of emissions!","node.js, openai, react, rechart","https://carbonle.vercel.app/, https://github.com/bdok23/treehacks2025/"
TreeHacks 2025,CareNest,https://devpost.com/software/carenest-hkd2q3,False,"Caring for a loved one who depends on you 24/7 is both an act of love and a tremendous responsibility. Many family caregivers experience constant anxiety when they’re away from home, worrying about their dependent’s well-being. Whether it’s an aging parent, a grandparent, or a family member with special needs, the fear of not knowing what’s happening in their absence can be overwhelming. We created Carenest to provide peace of mind to caregivers. Our platform allows families to stay connected, ensuring that their loved ones are safe and cared for, even when they can’t be physically present. By leveraging smart monitoring and real-time updates, Carenest helps caregivers feel reassured, reducing stress and enabling them to manage their responsibilities more effectively.","Carenest is an AI-powered caregiving companion that provides peace of mind to families while improving the well-being of dependents. Our platform combines AI avatars, smart wearables, and intelligent video monitoring to create a safe and connected home environment. 🔹Tackling Loneliness:Dependents often struggle with isolation, which affects their emotional well-being. With our AI-powered companion, they can engage in meaningful conversations anytime, helping them feel connected and supported. 🔹Wearables to the Rescue:Health monitoring is critical for dependents who require constant care. Carenest integrates with wearable devices to track vital signs and detect anomalies, ensuring caregivers are alerted to potential health concerns in real time. 🔹Mishap Prevention:Accidents can happen when no one is watching. Our AI video agent continuously monitors for falls, unusual movements, or emergencies, instantly notifying caregivers so they can take immediate action. Carenest empowers caregivers with the tools to ensure their loved ones are safe, healthy, and never alone.","CareNest uses a powerful tech stack that ensures a seamless, responsive, and intelligent caregiving experience. Our architecture combines AI, real-time monitoring, and smooth user interactions to create a reliable platform for caregivers and their dependents. 🖥️Frontend -React: A JavaScript library for crafting dynamic and interactive web interfaces. ⚙️Backend 🗄️Database 🔗Integrations 1. AI Video Agents:Monitors and detects unusual activity using computer vision. 2. Wearable Device Integration:Captures vital signs and health metrics for real-time analysis. 3. AI Chat Companion:Provides dependents with an engaging, conversational experience to reduce loneliness. Video link for Demo of AI Assistant and Observability with Wearables: link Pipeline for wearable observability in ElasticSearch Fall Detection System Demo: Flow of Fall Detection System","Building Carenest came with several technical and logistical hurdles, which pushed us to think creatively and adapt quickly. Some of the key challenges we faced include: 🔹 Compute-Intensive Mobile App Development: We initially designed our mobile app UI using FlutterFlow, but we soon realized that the compute-heavy operations required for AI-driven monitoring and real-time analytics made it difficult to maintain smooth performance. To ensure a seamless user experience, we pivoted to a React-based web app. 🔹 TerraAPI Data Limitations: Our integration with TerraAPI was hindered by data accessibility restrictions. Fitbit’s data format was accessible for only one day of development, which limited the amount of historical health data we could analyze for anomaly detection in Elasticsearch. 🔹 Elasticsearch & S3 Permissions Roadblocks: Setting up an observability pipeline between our AWS S3 bucket and Elasticsearch required fine-tuning IAM policies. Ensuring the correct permissions for secure and efficient data ingestion took considerable effort and troubleshooting. 🔹 Customized Ingestion for Garmin Data: Wearable devices use different data formats, meaning our ingestion process had to be adapted for each device. Specifically, Garmin’s data structure required a separate parsing and processing pipeline to ensure consistency in our analytics workflow. 🔹 Limited Fitbit Data Rendering: Even after integrating Terra to send Fitbit’s health metrics to our S3 bucket, we encountered a bottleneck where only a small fraction of heartbeat monitoring data was being captured and stored—significantly less than expected. This made it challenging to generate insights using Elasticsearch.","✅ Real-Time Observability Pipeline for Healthcare: We successfully established an end-to-end observability pipeline that ingests real-time wearable data from multiple sources, processes it via Elasticsearch, and generates actionable health insights. ✅ Alert System for Caregivers: Our system provides instant notifications for critical health events, such as abnormal vital signs or unusual movements detected through video monitoring. This ensures timely intervention and enhances the safety of dependents. ✅ AI-Powered Companion for Emotional Support: By integrating ElevenLabs’ voice synthesis technology, we enabled a natural and engaging AI-driven conversational experience, reducing feelings of loneliness among dependents. Meet Nest: Your Assistant and Friend: ","Building Carenest was an insightful journey that deepened our understanding of the challenges and opportunities in AI-powered caregiving. Some of our key takeaways include: 🔹 The Complexity of Wearable Data Standardization: Every wearable device provides health metrics in different formats, making it clear that a one-size-fits-all ingestion approach isn’t feasible. We learned how to design adaptable pipelines that normalize and process data across multiple devices. 🔹 Observability is Crucial for Healthcare Applications: Real-time monitoring and anomaly detection require a robust observability pipeline. Fine-tuning Elasticsearch for healthcare data allowed us to create a more efficient and responsive alerting system. 🔹 The Balance Between AI and Human-Centric Design: While AI-powered monitoring and conversation agents enhance caregiving, they should complement—not replace—the human touch. The emotional well-being of dependents remains just as important as their physical health. 🔹 Optimizing Compute-Heavy Workflows: Our experience with FlutterFlow and mobile development showed us the importance of optimizing computational workloads. Shifting to a React-based web app significantly improved performance without sacrificing user experience. 🔹 Integration Challenges in HealthTech: Working with APIs like TerraAPI and dealing with data access limitations highlighted the importance of early-stage testing and vendor communication to avoid last-minute roadblocks. 🔹 AI’s Role in Reducing Caregiver Stress: Caregivers often experience anxiety when they’re away from their loved ones. Implementing real-time monitoring, AI alerts, and engaging conversational AI reaffirmed how technology can play a crucial role in reducing stress and providing peace of mind.","🔹 Personalized AI Assistant with Smart Alerts
Our AI companion will evolve beyond simple conversations by integrating persistent storage and sentiment analysis. If the assistant detects triggering words related to illness, distress, or loneliness, caregivers will receive real-time notifications, allowing them to step in with empathy and support when it’s needed most. 🔹 Enhanced Predictive Health Monitoring
By adding inference capabilities to the data collected from wearables, we aim to detect anomalies in vitals early and predict potential health risks before they escalate. This proactive approach will help caregivers and healthcare professionals intervene in time to prevent unforeseen emergencies. 🔹 Automated Emergency Response System
If a dependent experiences a fall or any critical mishap, Carenest will not only alert the primary caregiver but also notify the nearest emergency contacts. This ensures a faster response time and an added layer of security, providing peace of mind to families and improving the dependent’s safety.","elasticsearch, elevenlabs, flask, flutterflow, groq, kibana, mongod, python, react, s3, terraapi",https://github.com/meetp06/CareNest2025
TreeHacks 2025,Teacher.AI,https://devpost.com/software/teacher-ai-5sgld4,False,"In rural areas, limited power, connectivity, and resources often restrict access to education. Several Asian countries face a stark divide in urban-rural English literacy. English proficiency, a key to unlocking better jobs and higher earnings, remains out of reach for many due to inadequate facilities and a shortage of qualified teachers. LEP (Limited English proficiency) workers earn 25-40% less than their English proficient counterparts. We wanted to experiment with a way of providing personalized education given the constraint of poor connectivity."," Key components of our hack include the NVIDIA Jetson Nano, Logitech web-camera and AI models like: For our backend, our models are loaded on Jetson. The inputs to our program is a video live stream that is obtained using the Logitech web-camera. The video live stream is then fed through this new fine-tuned model, and it performed well in recognising our faces. The output of YOLOv8 is the class label for our face (yu xi, bryann or dexter). We have a SQLite database of students and their relevant data like reading, writing, vocabulary and speaking scores out of 100. It also contains the numerical progress for each student for each relevant chapter. When the class label is retrieved from YOLOv8, the relevant data is retrieved from the database with sqlite3. This data is passed to a distilled version of Llama3.2, an auto-regressive AI model. After, the text is passed through Llama3.2 and lesson material is curated for the student. For example, if the student is halfway through Chapter 2, the LLM might generate content to continue where the student left off. Alternatively, if the student’s vocabulary scores are very low relative to their other scores, the Llama could come up with a fun game for the student to learn new words that match their level of vocabulary. The text output of Llama3.2 is converted to speech. After it is done, the speech to text model listens for speech from the student and the process repeats. At the end of every session, the student’s progress is updated in the SQL database.","Frontend: 
We built the front-end of Teacher.AI using HTML, Tailwind CSS, and JavaScript to create a clean, interactive, and responsive interface. Tailwind CSS ensures a modern, visually appealing design, while AOS (Animate on Scroll) adds smooth animations for an engaging user experience.
The homepage features a hero section with a welcome message, while the Features page includes interactive elements like a face recognition demo using a webcam and a text-to-speech function. Users can create a profile with their name, school year, subject, and knowledge level, and ask questions in real time.
Additionally, we implemented a Jetson connection status checker in JavaScript to continuously ping the Jetson and update its online/offline status. The UI dynamically updates progress, displays system messages when the Jetson is connected, and includes a progress tracker with animated progress bars. While JavaScript plays a crucial role in handling webcam integration, speech synthesis, and real-time updates, making Teacher.AI a more immersive and intelligent assistant for students. Backend:
We created an original dataset of our selfies and preprocessed them using OpenCV to crop each picture to 640x640 pixels such that it only contained our faces. We then fine tuned the YOLOv8 model on our faces and it performed pretty well, with F1 scores of 97%, indicating a good balance between precision and recall. We set up interconnected servers and piped outputs between Jetson devices, ensuring every build was optimized for CUDA. This approach led to significant performance improvements—for example, while running whisper.cpp on a CPU takes 17 seconds for speech-to-text conversion, leveraging the CUDA capabilities of the Jetson Orin reduced that time to just 0.3 seconds. Additionally, we seamlessly integrated a 1B parameter LLM model without compromising efficiency, achieving information retrieval in approximately 2 seconds—on par with API call response times from OpenAI.
Moreover, by connecting a Logitech camera, microphone, and Google AIY Voice Kit to the Jetson, we enabled two-way communication. The GPU’s abundant VRAM ensures smooth interactions with the AI education agent, even when operating offline.","Integrating everything together at the systems level was quite challenging as there were many moving parts. For example, the video livestream caused many problems downstream due to it operating in the same thread as most of the other main code","1, Personalized to each student’s unique needs (Students in personalized learning environments outperformed their peers by 9 percentile points in reading over a year, according to a study by the Bill & Melinda Gates Foundation)",,"We want to integrate more AI models to enable students to learn other subjects like mathematics and science as well. We hope to make the hardware components of Teacher.AI more integrated, such that it is easier to carry around. A student’s grasp of English is also a lot more than just four competencies, so we hope to better represent a student’s understanding.","css, cuda, elevenlabs, google, html, javascript, jetson, jetson-nano, kokoro, llama, nvidia, openai, opencv, python, raspberry-pi, replit, sql, sqlite, tailwind, yolo",
TreeHacks 2025,BackSeat_Driver,https://devpost.com/software/backseat_driver,False,"Our inspiration came from a series of different methods to detect traffic signs, cars, and scenerios on the rode. Our main usages are BLIP, CLIP, a RoboFlow model using LISA data, and YOLO8n model for car detection models to detect scenarios, cars, and traffic signs. We also have seen the power of LLM like chatGPT, so we used this model to do so. The project name was inspired by the fact that the model would be responding to traffic related scenarios, acting like an arrogant backseat driver, (by giving advice).  Hopefully it proves helpful!","Our project uses pre-trained models to extract from videos different traffic related data, building a scenario into text in which then we feed to a chatGPT api in order to determine a related question. Videos and multiple choice questions are given to us by Tesla.","We build our project on NVIDIA VMs, for that extra compute power on the cloud and in order to run some of our models like YOLO and BLIP. We used BLIP, CLIP, a LISA based model, and YOLO8n image model in order to extract what exists and what is going on in a driving scenario. Each model extracts different things, like a description of what is going on, specific traffic signs, object detection, or object movement. We harness each model's strength, by taking a video and frame by frame (8 frames per 5 sec video approximately), and running these models on each one. Afterwards we aggregate this data per video and feed it into chatGPT's API. Additionally we give it a multiple choice question and with that data, and prompting it will make an answer.Afterwards the answer is taken an converted into a csv file, which we use to submit.","So many dependency issues, too many installations and conflicts! Training models is time consuming! We opted to use pre-trained ones to save time, but training them ourselves would have allowed for on device models paired with NVIDIA VMs, thus faster compute.",We were able to build a VLM while making use of several models! We also learned to use new tools like chatGPT and RoboFlow APIs and NVIDIA VMs!,We learned to use new tools like chatGPT and RoboFlow APIs and NVIDIA VMs!,We hope to put many of the models we call via API onto actual device plus tune them with more data.,"blip, brev, clip, cuda, jupyter, nvida, python, roboflow, yolo",https://github.com/LuisArizmendi13/TreeHacks2025
TreeHacks 2025,Truth AI Agent,https://devpost.com/software/rhetai,False,"I was inspired by the visual novel ""Ace Attorney,"" the story of a defense attorney who investigates crimes, gathers evidence, and dramatically battles prosecutors in court. I created Truth AI to bring truth back to the internet. 
With the increase in popularity of AI agents and lucrative social media monetization programs (ie X / Twitter), content has hyper tumbled into an unoriginal spew of click begging slop. Although firms are profiting from the increase in traffic, soon we will reach a point where interaction with a real human on the internet will be a rare occurrence, if you can even recognize it.
Truth AI agents takes a stab at helping elevate the quality of content by discouraging low value content while educating users on better practices.","Truth AI agents can be deployed on any social media platform. They are powered by autonomous and verifiable AI compute to deliver high quality, unbiased, untampered opinions. By using AI to analyze content, platforms can efficiently eliminate or demonetize content that doesn't meet criteria. Incorporating the AVS allows for transparency. Any user can view the prompt that was used to analyze the content. This ensures that platforms are not injecting their own biases into the AI.","It utilizes Opacity, an Eigenlayer AVS, to make off-chain OpenAI API calls. Thus, any decision the agent makes is a consensus and can be verified; it is impossible for an entity to corrupt the opinion of the agent. The agent lives on an express server with only 1 endpoint necessary to connect to a frontend. The agent can be customized with txt files that the arguments are injected into.",,,,"Truth AI agents raise the dilemma of whether AI can truly be unbiased. Since we are relying on the AI to pass judgement, it is critical that it is carried out impartially. But given that AI models are trained on datasets created by humans, who carry inherent biases, is it realistic to expect these systems to achieve a level of impartiality that transcends human judgment?","avs, eigenlayer, express.js, next.js, opacity, openai, tailwindcss, typescript",https://github.com/owenkrause/truth-ai
TreeHacks 2025,HeartHack,https://devpost.com/software/hearthack,False,"""In today’s fast-paced world, spending hours mindlessly swiping on dating apps feels like a chore. Finding love shouldn’t be exhausting—so we built a smarter solution. Everyone knows what they’re looking for, but sifting through endless profiles? That’s the real challenge. With HeartHack, you can skip the tedious search and jump straight to meaningful matches."" Actually, it just seemed like an idea with quite a big LOL potential.",,,,,,,"css, gemini, html, javascript, kotlin, maestro, react",https://github.com/avah1026/treehacks-2025
TreeHacks 2025,AeroForge 3D,https://devpost.com/software/aeroforge-3d,False,"Aerospace manufacturers still rely on outdated 2D blueprints for critical aircraft parts—especially for legacy jets like the F-16 and B-52, which were built in the 1960s and 1970s.No CAD models. No 3D visualizations. No streamlined way to modernize these critical assets. During my DoD fellowship, I watched engineersflip through binders of outdated schematics, frustrated by the time and expertise needed to convert them into 3D models for prototyping and repairs.The DoD, Lockheed Martin, and Boeing spend millionsjust to convert outdated PDFs into usable 3D models and be mission-ready. I saw an opportunity to fix one ofaerospace’s biggest bottlenecksby applying state-of-the-artgenerative AI and visual computing. What if an AI system could instantly reconstruct a 3D model from old blueprints?What if we could automate an entire workflow that takes weeks in seconds, saving millions of dollars and engineering hours? That’s what I built.AeroForge 3D.","AeroForge 3D is anAI-powered system that transforms legacy 2D blueprints into photorealistic, engineering-accurate 3D models in seconds. 🔹Defense & Aerospace-Grade Precision→ Our system processes military blueprints and ensures compliance with aerospace manufacturing standards (MIL-STD-31000). 🔹Seamless AI-Powered 2D-to-3D Conversion→ Extracts key technical details from PDFs and reconstructs them into high-fidelity, immersive 3D renderings. 🔹Rapid Prototyping & Upgrades→ Engineers can visualize multipledesign variationsinstantly, leading tofaster, more informeddecision-making. 🔹Mission-Readiness at Scale→ Instead of weeks of painstaking CAD modeling,our AI does it in seconds—allowing defense contractors tobe proactive, not reactive. This is more than just automation. It’s a new way to engineer, visualize, and modernize aerospace technology.","I combined OCR, AI-powered text processing, OpenAI’s GPT-4o, Sora, Perplexity AI’s Sonar, and Luma AI’s Dream Machine to create a seamless pipeline from blueprint to 3D model. Here’s how I approached the problem, broken down into steps. 1️⃣Extracting Data from Legacy Blueprints • Used PyMuPDF for OCR (Optical Character Recognition) to extract dimensions, materials, and annotations from technical PDFs. • Parsed structured text with Perplexity AI’s Sonar to generate engineering-grade structured JSON. 2️⃣Generating the Perfect 3D Prompt • Used OpenAI’s GPT-4o to analyze extracted blueprint data and generate a military-grade Sora prompt with MIL-STD-31000 compliance. • Made sure that the prompt had geometric accuracy, correct material definitions, and compliance verification before visualization. 3️⃣Creating a 2D Aerospace Visualization • Passed the processed blueprint data into Sora, generating a 2D visualization of the aircraft part. • Once the 2D image was technically accurate, we proceeded to 3D reconstruction. 4️⃣Bringing It to Life in 3D with Luma AI • Used Luma AI’s Python SDK and REST API to convert the Sora-generated 2D image into a photorealistic 3D rendering. • Leveraged Luma AI’s Dream Machine (Ray-2 model) to create cinematic, immersive 3D animations. • Applied keyframe-based camera motions to showcase part assembly and integration into real-world aircraft. 5️⃣Guaranteeing Engineering-Grade Usability • AI cross-references generated models against aerospace manufacturing standards to ensure compliance. • Engineers can export AI-generated models for further refinement in CAD, AR, or VR environments.","🚨Parsing Legacy Blueprints– Since aerospace PDFs vary in format, it makes OCR tricky. I fine-tuned AI parsing to extract dimensions, tolerances, and materials with precision. 🚨Ensuring MIL-STD-31000 Compliance– Unlike CAD, my AI had to validate structural integrity and tolerances. I built custom validation logic to ensure military compliance. 🚨Balancing Realism & Accuracy– Luma’s AI creates stunning renders, but I optimized prompt engineering to keep dimensions engineering-grade while maintaining cinematic quality. 🚨Optimizing for Speed– Processing high-res PDFs, images, and videos needed fast, scalable API calls to handle real-world defense workloads efficiently. 🚨Pivoting from Sora API– Since Sora lacks an API, I initially tried Playwright & direct requests but pivoted to perfecting the Sora prompt for seamless integration with Luma AI. 🚨Building This Solo– After checking into TreeHacks on Friday, I realized I had all the tools necessary to solve this aerospace and defense problem. Working solo meant extra effort, but I took full advantage of the resources and mentorship available at the hackathon to bring this vision to life.","✅Automating a Process That Normally Takes Weeks→ What used to take weeks of CAD work and engineering hours now happens in seconds. ✅Seamless AI Pipeline from 2D to 3D→ Successfully integrated OCR, structured data extraction, AI-powered prompt generation, and cinematic 3D rendering. ✅Military & Commercial Applications→ This isn’t just for the DoD. It has massive commercial potential for aerospace manufacturers, airlines, and engineering firms. Also (I just learned) for the oil and refinery industry! This problem is surprisingly common. ✅Game-Changing Visual AI for Engineering→ We’re proving that generative AI isn’t just for art—it’s revolutionizing how critical systems are designed, upgraded, and maintained.","💡AI can bridge the gap between legacy engineering and modern 3D visualization. 💡Generative AI can serve mission-critical roles beyond entertainment.Luma AI’s Dream Machine isn’t just generating cool videos—it’s reshaping aerospace workflows. 💡Aerospace & defense need automation, and they need it fast.Millions are wasted every year on manual blueprint conversion—our AI solution is an immediate cost-saver. 💡CAD isn’t enough. CAD doesn’t automate compliance verification or enhance real-time visualization—our AI does.","🚀AR & VR Integration→ Engineers can inspect, modify, and interact with AI-generated 3D models in real-time. 🚀Automated Compliance Checks→ AI-driven MIL-STD-31000 validation ensures defense-grade accuracy. 🚀Beyond Defense→ Expanding to commercial aerospace for faster prototyping and modernization. 🚀Interactive & Editable Models→ Users can tweak AI-generated designs, test integrity, and refine structures. 🚀Seamless CAD & Manufacturing Integration→ Direct CAD exports and real-time AI-assisted prototyping. 🚀Revolutionizing Aerospace→ AeroForge 3D streamlines engineering from blueprint to battlefield.","css, dreammachine, flask, github, gpt4o, gunicorn, html, javascript, json, luma, openai, perplexity, python, ray-2, sonar, sora","https://github.com/ShreyesS/AeroForge3D, http://127.0.0.1:8000/"
TreeHacks 2025,engagED,https://devpost.com/software/teacher-teacher,False,"Every day, teachers step into classrooms filled with students who have different personalities, learning styles, and challenges. Some students are eager to participate, others are easily distracted, and a few might resist authority altogether. A teacher's ability to navigate these interactions can mean the difference between an engaging classroom and a chaotic one. Yet, there are few opportunities for educators to practice classroom management in a realistic, risk-free setting. So, we asked ourselves: What if teachers could train for the classroom the way pilots train in flight simulators? Could we create an AI-powered environment where educators could interact with dynamic student personalities, receive real-time feedback, and refine their teaching techniques so that they can accel in the classroom? That’s how we builtengagED—an interactive classroom simulation that prepares educators for real-world teaching challenges. By leveraging AI-bots that simulate student behaviors and a performance analytics dashboard,engagEDhelps teachers develop strong communication skills, manage classroom dynamics, and build confidence—all in a safe, controlled environment. We believe that ifengagEDis implemented in teacher training programs and professional development workshops, it could revolutionize how educators prepare for the modern classroom—leading to more engaged students, less burnout, and stronger learning outcomes across the board.","engagEDsimulates a classroom environment where teachers interact with AI-powered student personalities, each with unique behaviors, engagement levels, and challenges. The platform allows educators to practice managing real-world classroom dynamics—whether it’s handling a disruptive student, encouraging a quiet learner to participate, or maintaining engagement during a lesson. engagEDresponds to natural teacher interactions in a virtual classroom setting, adjusting student behaviors based on teaching strategies. Just like in a real class, students may ask unexpected questions, lose focus, or react differently depending on the teacher’s approach. At the end of each session,engagEDprovides a performance dashboard, offering insights into key performance metrics and providing feedback for improvement.","engagEDwas built by a mix of hackers from various backgrounds in frontend and backend roles. We began by wireframing and designing our user workflows, and then iterated upon these designs to create a seamless experience. We used a variety of cutting-edge technologies in this platform, which are outlined below.  Our tech flow The frontend was build inReactandTypescriptusingNext.jsas our frontend framework in order to maintain a structured codebase and fast loading times. The backend server was built inPythonandFastAPI, allowing us to utilize a variety APIs such asElevenLabs,OpenAI,Groq, andLangChain. A websocket connection was also utilized between the frontend and backend in order to constantly stream audio data to the agents and LLMs, allowing for low latency. The database was created inMongoDBin order to store user sessions in a structured manner. Client logic flow Diving deeper into our client logic, there are a lot of moving parts that bringengagEDtogether! We begin with theteacheruser instructing the virtual class, and theAI studentslistening in. We then utilize OpenAI'sWhisper APIin concurrence with Groq in order to convert this instruction to a text transcript. We then feed this transcript back into the AI Agents in order to provide them context and prompt them to chime in if neccesary. Each agent was build withLangChain Agentsand Groq to be able to respond to conversation with low latency. Initially, we ran each agent in parallel, but soon realized that this would cause them to talk over one another as they responded to the teacher. Thus, we implemented a round-robin algorithm between each agent and the teacher in order to minimize conflicts. Then, we utilize ElevenLabs'text to speech APIin order to convert the agent's conversational input into speech, and played this in the virtual classroom setting. The speech from the teacher is now combined with the latest transcripts and messages from the AI Agents to create a master transcript. This is now used as the context provided to the AI Agents for them to respond to the teacher. Once the session is ended, the master transcript is sent to OpenAI to summarize and provide analytics data for teachers. This is then displayed to the teachers in the following page. The data is then stored with their unique session id in MongoDB for retrieval and visibility later. Our tech stack","Zoom API Issues: We attempted to use the Zoom API to programmatically start meetings with Zoom bots. However, after numerous attempts and insights from Zoom mentors, we realized that the Meeting Bot approach was not viable due to API limitations and planned deprecation. We ended up building our own solution for streaming web and audio to the backend and adding having agents receive this context. Low-latency Interaction: We had to optimize AI interactions to maintain near-real-time responsiveness, ensuring that the virtual classroom felt immersive and natural. We attempted to achieve low-inference at every step possible using the Groq API to speed up Speech-To-Text and Agentic LLM reasoning. We found ElevenLabs API to be quite fast for Text-to-Speech. Agent Coordination: AI student agents needed to understand each other's context and avoid talking over one another. Synchronization and maintaining conversational order proved to be a complex challenge. We used a mixture probabilistic activations for each agent, a cooldown period specific to each agent, and locks to ensure a clean virtual classroom experience. Analytics APIs: We wanted to utilize a dedicated API in order to give users analytics on their sessions, but had a hard time finding options that were either sufficiently documented or free to use for this task. As such, we decided to utilize OpenAI's capabilities in order to analyze our transcripts. Audio Formats: Both theWeb MediaRecorder APIand the audio-generation services have limited information on how it streams ""chunks"" of audio. For example, one API returned base64-encoded data that consistently started with the same few octets, but looking them up as magic numbers turned up dry. We ultimately discovered that these chunks had to be concatenated into one single audio file—only the first chunk had the necessary header data needed to make the file playable. However, this was not feasible for generating instant responses and transcripts in response to the user's microphone audio. Our initial solution for this case was to accumulate chunks, but this led to large contexts for the AI models. Our breakthrough came when we used adeprecated methodfrom the Web Audio API to collect raw audio samples, and manually formatted each chunk as a WAV file so that each chunk could be enjoyed in isolation.",There are many things we are proud of:,We learned many things:,There are many aspects ofengagEDthat we could improve:,"elevenlabs, groq, javascript, langchain, mongodb, next.js, openai, python, react, typescript, whisper",https://github.com/jasmine-dragons/engaged
TreeHacks 2025,gflow: probability data optimization,https://devpost.com/software/gflow-probability-data-optimization,False,"We tackled the Tesla Real-World Video Q&A challenge, excited to take on a real-world ML problem. Some of us had experience with AV perception systems and cognitive modeling and were excited to develop reasoning with VLMs. We approached this as both a research initiative and an engineering problem: there was a need to read literature and ideate a novel solution since reasoning with vision models remains an unsolved problem. But there was also a requirement to build robust, computationally efficient algorithms to tackle our latency and resource bounds. After much exploration, we settled on a two-step prompting pipeline that utilizes time-stamped event logging, data caching, and structured data formation to address the challenge.",,,"One of the biggest hurdles we faced was familiarizing ourselves with Vision-Language Models (VLMs) and understanding the extent of their computational capabilities. Downloading the models, running inference on our training dataset, and evaluating the reasoning performance was rather tedious.
Another major challenge was optimizing latency using the Nvidia Jetson. While we successfully set up the hardware, we faced permission errors, dependency conflicts, and import mismatches which made using the system challenging. Additionally, integrating multiple models led to compatibility issues, because they had different input-output formats and underlying architectures. Some couldn't work with certain modalities whilst others needed feature representation to be in certain dimensions (which would overhaul the rest of the architecture). Lastly, our dataset was a significant limitation. We were only given access to 50 5-second video unlabelled clips. This meant that we spent a decent amount of time manually annotating correct answers for each question. In many cases, the MCQ answers were ambiguous, and that made it more difficult to evaluate the performance of the model.","Despite these challenges, we're really proud of what we accomplished—especially since we tried to encode reasoning into vision models, which is an unsolved problem. We weren’t deeply familiar with the architecture and working with a small, messy dataset made the problem more challenging. And yet we were able to reason to a certain degree of accuracy about the environment ego was in. We also experimented with a range of techniques like prompt engineering, chain-of-thought reasoning, inference with various multi modal models, and novel approaches like using G-flow nets.","How to analyze datasets and ideate solutions
How to set up compute on NVIDIA jetson hardware 
How to perform rapid inference using different pretrained models","We would like to continue developing reasoning capabilities in VLMs. This time around, we were limited by time and knowledge of existing model architectures and pre-trained models for the autonomous use case. Going forward we would like to begin by fine tuning on our own dataset, incorporating various modalities like LiDAR, depth, GPS, and IMU data, and work with other novel methods of image and textual embeddings.","gemini, llava, openai, python, vlm",https://github.com/cslegasse/gflownet-tesla
TreeHacks 2025,AERO,https://devpost.com/software/aero-yu405v,False,"I set out to build an AI assistant that goes beyond providing information—it takes instant action. In a world overloaded with digital clutter, I wanted to create something that cuts through the noise and makes interacting with technology effortless. That’s how AERO was born—an intelligent, intuitive, and fully interactive AI assistant designed to streamline workflows for work, communication, and entertainment.",,,,,,,".exe, bootstrap, css, gemini, groq, html, javascript, json, openai, parallel, parallel-programming, python, stt, threading, tts, vision, zoom","https://aero-bho2-nafiulkhalids-projects.vercel.app/, https://github.com/TreeHacks25/AERO/tree/main"
TreeHacks 2025,Odin,https://devpost.com/software/odin-e3hd4w,False,"Collecting rich and up-to-date data is often tedious, fragmented, and difficult to scale. High-quality datasets are used everywhere from training ML models, running data analytics, test running small projects on Kaggle, etc, yet hard to retrieve and preserve. We wanted to build a tool that allows users to effortlessly generate structured real-information datasets from a single prompt.","From a single user-prompt and user-defined schema, our software will scrape the web in realtime and iteratively generate schema-defined row data. This could be anything from ""build a dataset of recent merges & acquisitions"" to ""build a dataset on popular podcasts across the United States in 2010s"".","We extracted a relevant and recent textual data by using Scrapybara's web scraping abilities on web articles, and streamed it to our groq endpoint to extract quantitative row values out of the textual data. Backend: Flask, Scrapybara for automated browsing & scraping, Groq for AI-powered entity extraction.
Frontend: React for an interactive and dynamic UI.
Database: Supabase to store the structured data for each user.
Streaming: Real-time data flow from backend to frontend for a seamless user experience using Flask Response DataStream API.","We had to do a lot of prompt engineering towards our Scrapybara instance to run efficiently across the web. 
Hosting the Flask Datastream across the backend to frontend and rendering components smoothly was also difficult. 
As we used many tools, calling API across the network came with many different types of access errors and we need to mediate.","Successfully automating data collection from a simple prompt.
Implementing Groq to extract structured insights from raw web data.
Streaming enriched data to the frontend in real time.
Building a fullstack application that is both functional and scalable.","Best practices for ethical and efficient web scraping.
Optimizing AI entity extraction for real-world applications.
Handling real-time streaming of structured data in a fullstack application.
The power of combining automation and AI for dataset generation.","Expanding support for more data sources beyond web scraping.
Enhancing entity recognition with fine-tuned models.
Improving UI/UX for a smoother user experience.
Allowing users to export structured datasets in various formats (CSV, JSON, API).
Exploring integrations with tools like Google Sheets or Notion for direct data syncing.","deepseek, flask, groq, python, react, restapi, scrapybara, supabase, typerscript",https://github.com/taeyangcode/treehacks-25/invitations
TreeHacks 2025,Scrollarly,https://devpost.com/software/scrollarly,False,"Access to quality, reputable information is a necessity, yet research as it stands today is highly restricted and devalued, as political and institutional barriers limit who can engage with what topics. 
Academic research remains largely insulated, often circulating within exclusive communities. Paywalls and outdated discovery systems hinder the dissemination of knowledge. As a senior studying Computer Science and Child policy research, who is also conducting thesis research and hopes to continue with research/academia, making access to information more equitable is extremely important to me.Scrollarlyaims to dismantle some of the barriers to finding and engaging in research by leveraging AI and agent driven methodologies and employing a clean, user-friendly interface.","Scrollarly is a continuous-scrolling research discovery platform that applies the engagement mechanics of social media to academic literature. By integrating InterSystems IRIS Vector Search, it enables natural language-driven discovery, personalized recommendations, and multi-vector semantic retrieval, which makes sure that users find the most relevant research without reliance on rigid keyword matching. This platform reimagines research consumption and makes it intuitive and accessible to a broader audience.",,,,,"More features! A explore page, comment functionalities, more AI-driven paper insights","ai, api, css, intersystems, openai, python, react, sql, typescript, vite",https://github.com/reaganrazon/scrollarly
TreeHacks 2025,TalkTuah,https://devpost.com/software/talktuah,False,"I was at the Columbia hackathon last week when I had the opportunity to seesocial Stockfish(an AI that predicts conversation flow, like a chess engine for chats) get built next to me. It blew my mind. Shoutout to@eddybuildand@cadenbuildfor their work on the chess glasses (the reason I bought the meta ray bands lmao)  and social Stockfish---it was a huge spark for me. I wanted to create something similarly badass, a glimpse into the future of AI-powered wearable social intelligence. So, I jumped into asolo projectto challenge myself at one of the biggest stages. The result?TalkTuah---a real-time conversation coach that listens, analyzes, and guides you through any interaction.",,,,,,,"blackhole, elevenlabs, groq, meta, openai, opencv, speechmatics",https://github.com/IbrahimKhanGH/TreeHacks2
TreeHacks 2025,CogniSync,https://devpost.com/software/cognisync,False,We are faced by a stark reality: 1 in 10 adults aged 45 and older experience worsening memory loss or confusion - early signs that often go unnoticed. What's most concerning is that these subtle changes can appear up to 10 years before becoming obvious symptoms. This gap between early signs and diagnosis is what inspired us to create CogniSync.,,,,"Our biggest achievement was successfully integrating complex health monitoring into a single, intuitive interface that makes data accessible and meaningful. We're also particularly proud of our selection of  cognitive games, intended to balance assessment and engagement.","The development process taught us crucial lessons about balancing real-time data processing with system performance, while maintaining accuracy. We discovered the importance of translating complex clinical metrics into user-friendly insights that anyone can understand. Most importantly, we learned how continuous sensor data, when properly analyzed, can reveal subtle patterns that indicate significant health trends.","We plan to expand our system by incorporating more sophisticated health metrics, developing additional cognitive games, and refining our threshold detection algorithms based on clinical research. We're also exploring ways to make our platform more accessible through mobile development and improved data visualization techniques.","html, imu, machine-learning, numpy, plotly, python, streamlit, tcp/ip, transformers",https://github.com/Sanskriti-Slngh/CognitiveApp
TreeHacks 2025,Odin AI,https://devpost.com/software/odin-ai-m5nw4d,False,"Check out our homepage to explore your knowledge graphs across history or embark on a new learning adventure! Here is a visualization of our knowledge graph with computed node and edge weights based off similarity scores and learned knowledge gaps. We leverage multi-modal foundation models, light-weight LSTMs, and several classic graph algorithms to transform learning journeys. Odin constructs a knowledge graph from your (possibly handwritten) notes and compares it to a knowledge graph constructed from lecture recordings. From this, we identify lecture topics that are missing from your notes, as well as interactions between main concepts and themes. Based on these personalized knowledge graphs, Odin can also generate quiz questions at the frontier of your abilities, intended to maximize the efficiency of your study process (in accordance with psychology principles such as the Zone of Proximal Development). You’re in a lecture hall, fluorescent light shining harshly on the walls, the screeching sound of the chalk and the professor’s voice filling the air. They’re talking about pointers, or maybe eigenvalues, but all you can think about is the ant crawling on your friend’s hand. Should you tell them? Or is it more amusing if they don’t know? You snap out of your haze. How long has it been? 30 seconds? 5 minutes?  Whatever it was, you completely missed that part of the lecture. For our team members, this scene hits close to home. As college students who attend large, fast-paced lectures, it’s often impossible to fully capture the content taught in lecture. What’s more, amid P-Sets, clubs, and spending time with friends, it’s hard to block in time to rewatch lecture recordings to see what we missed. And even if we do find the time, it’s possible that we will miss the content again. To solve this problem and make learning more efficient, we created Odin AI. In Norse mythology, Odin sacrificed his eye to see everything that happens in the world. In the same way, Odin AI superpowers your studies, providing visualization and analysis tools that ensure you never miss a topic again! Simply upload a photo of your handwritten notes from class and a recording from lecture, and let the magic happen. Odin AI will compare your understanding with the ground truth of the lecture recording and construct a personalized graph with sample questions to help you study. With a graph to easily visualize class concepts, you can see how different ideas relate to each other, as well as areas to keep focusing on. As you keep practicing, your graph will evolve to reflect your mastery. No notes? No recording? No worries, Odin doesn't need both. Upload what you have, and Odin AI will still generate a study plan and a concept map. Summary of our main features: This project heavily leveraged multimodal foundation models and fast-inference AI systems, as well as classical algorithms, to cohesively merge together our multiple data sources. Odin includes the use ofOptical Character Recognition(OCR) via Tesseract,Speech-to-Text Transcriptionvia Whisper,Large Language Modelprompting via OpenAI API,Markov chain traversals, and self-designedgraph algorithms. Our web application is built withReact.jswithMaterial UIstyling. We useReagraphfor graph visualization. Once a.pngof handwritten notes or a.mp3of a lecture recording is uploaded, we process the different data sources and generate transcriptions of each. We use Tesseract for OCR on handwritten notes, and Whisper for transcription on lecture recordings. We run these initial texts through a ""markdown transform"" LLM prompt which adds organization and corrects spelling mistakes in the initial transcripts. From there, we use an LLM to generate a list of key concept nodes and edges between nodes that describe their connection to each other. Now, we have two graphs: one which describes what we call our ""ground truth,"" or the lecture recording, and the other which describes ""understanding"", or the handwritten notes. To calculate the importance of each concept, or node, in our graphs, we use anode centrality algorithmwe created ourselves that uses metrics like node degree, BLEU score, and term frequency inverse document frequency (TF-IDF). From here, we’re able to calculate edge weights to measure how relevant the connections between two nodes are. Once we’ve composed these two graphs and populated them with these values, we’re ready to merge them and find the knowledge gaps! We use cosine similarity in concept embedding space to consolidate similar nodes together and update edge weights in our ""understanding"" graph, using an algorithm that computes a value based on adjacent nodes in the ""understanding"" and ""ground truth"" graphs. Assume that Node A, of weight a_0, connects to Node B, of weight b_0, with Edge E, of weight e_0, in our ""ground truth"" graph. Similarly, there is a connection of weights a_1, b_1, and e_1 in our ""understanding"" graph. As we merge to find gaps in understanding, we re-weight e_1 to be: e_1 = (a_1 * b_1 * e_1)/(a_0 * b_0) We now have consistency between our ""understanding"" and ""ground truth"" graph. For the final step, we compare edges between our graphs and reweight them into a personalized graph where an edge weight is (for some fine-tuned alpha): edge weight = ""ground truth"" weight + max(difference(""ground truth"" weight, ""understanding"" weight), 0) * alpha This formula allows us to appropriately weigh understanding with the importance of content. From here, we use these recalculated weights to visualize our personalized graph and write focused practice questions to build mastery. Our graph color-codes nodes that belong in our ""understanding"" graph as purple and ""ground truth"" as red. Node sizes and edge thickness are also scaled to correspond to importance. To generate questions, we first conduct a Markov chain traversal on the personalized graph to identify combinations of concepts that would produce the most challenging questinos, given the user understanding. We then use an LLM to generate questions from those concepts, and update our edge weights as mastery increases. This is visualized on our graph in real time. One of our main challenges arose in our multimodal transcription methods. OCR can be highly inaccurate and has a hard time deciphering a lot of handwritten text. To help smooth these discrepancies, we used an LLM to clean up our text and infer what was actually written or spoken in our files. This worked a lot better and made our model more robust towards bad handwriting and unclear audio, similar conditions one would face when uploading lecture notes and recordings. Building our graph schema was also a challenging undertaking. Rapidly adapting our structure to incorporate more features as we went along resulted in frequent merge conflicts, which took quite a bit of time to resolve. Additionally, coming up with our algorithms to calculate weightage and importance was hard to standardize and resulted in a lot of conversations where we discussed the best way to measure and visualize understanding of material. Building the graph and visualizing edge weights was difficult as well. Our graphics library (Reagraph) had inconsistent scaling on the backend, making it an arduous undertaking to change the thickness of our edges in a way that was aesthetically pleasing and representative of edge importances. We normalized our node weights, which reflected in the size of our nodes, and scaled our edge weights to demonstrate which parts of our understanding were weaker or stronger. We are delighted at how we were able to synthesize our data from multiple sources and compare them in a meaningful way. The first time we were able to see our concept graph from just a simple image was a momentous milestone for our team, and we definitely scared the couple having lunch together at Arrillaga with our excitement. From designing effective graph algorithms and building strong agentic workflows to visualizing the data and integrating our various components, our project was a feat of engineering (with 100+ GitHub commits) which we are proud of. Most of all, we're delighted that we developed a tool that we would use on a regular basis and recommend to friends. We all came into this hackathon with varying levels of experience. For some of us, this was our first time using LLMs and handling complex graph visualizations. From this project, we learned so much about web development, AI transcription, graph algorithms, custom data structures, and graphics libraries. Despite the late hours, we had a lot of fun, although the energy drinks and llamas definitely helped with that. We want to keep working on this project to build functionality for some of the code we’ve already written: diagram transcription with VLMs. Additionally, we’d like to improve our visualization capabilities, especially with regards to our edge weights. We’re excited to keep working on this and make education and learning more personalized and efficient. A few specific extension features we have in mind:",,,,,,,"material-ui, openai, react, reagraph, tesseract, typescript, whisper",https://github.com/ML72/Odin-AI
TreeHacks 2025,IntelliCare,https://devpost.com/software/intellicare-hr3q2j,False,,,,"One major challenge we faced was the lack of publicly accessible medical data, especially video datasets of patients. Given the highly sensitive nature of medical information, there are strict privacy regulations that limit access to patient videos, particularly those depicting symptoms such as facial asymmetry in stroke patients or swelling in anaphylaxis cases. Another challenge was balancing false positives and false negatives. While we wanted to ensure that every critical case was flagged, excessive false alarms could overwhelm healthcare providers and lead to alert fatigue.",,"Through this project, we learned that AI in healthcare requires explainability. Clinicians need to understand why a patient is flagged as high-risk in order to trust and act on AI-generated recommendations. Simply providing a risk score is not enough; clear, interpretable insights are essential. Additionally, we gained valuable perspectives into the real-world challenges of AI deployment in healthcare. Hospitals have strict compliance and regulatory requirements, making AI integration more complex than initially expected. Understanding these constraints is crucial for developing AI solutions that can be successfully implemented in clinical settings. Moving forward, we aim to give doctors and nurses ahead startin preventing medical crises.Because every second counts.",,"cursor, lightgbm, openai, openface, python, streamlit, vscode",https://github.com/anakhag07/intelli-care
TreeHacks 2025,TeslaQA,https://devpost.com/software/teslaqa,False,"Current Vision Language Models (VLMs), despite their ability to learn ""common sense"" from internet data and handle long-tail cases, fall short in autonomous driving applications due to two critical limitations: they can't process high frame-rate video inputs and can't meet real-time latency requirements. This gap presents significant implications for real-world applications:","TeslaQA automatically analyzes traffic videos and answers questions about driver behavior, traffic rules, and road safety. By first converting video content into descriptive text using BLIP, and then leveraging ChatGPT for question-answering, our system provides detailed explanations of traffic scenarios.",,,,,"Our goal is to make roads safer by providing better tools for understanding and analyzing traffic scenarios, ultimately contributing to the development of more reliable autonomous driving systems and better driver education. The kaggle link is listed below. And our team name is 'passionfruit.'","api, blip, jupyternotebook, openai, python, tesla, vlm","https://www.kaggle.com/competitions/tesla-real-world-video-q-a/leaderboard, https://github.com/HangLiu01/2025treehacks_tesla"
TreeHacks 2025,cafe fra.me,https://devpost.com/software/cafe-fra-me,False,"I remember sitting in a family doctor’s waiting room, frustrated as I watched the doctor’s machine reboot—only to discover it had no record of vaccines I received at another clinic. With all these so-called robust ERP systems and endless spreadsheets, how could this happen? The truth is, the entire healthcare management system is broken. There’s no shared documentation, no privacy, no transparency between physicians and patients, and a clunky lookup process that forces you to rely on specialists like nurses or assistants. It’s a system crying out for change.",,,,,,,"ethers.js, gemini, jina, openai, perplexity, polygon, python, react-native, solidity, swift, terra, viem, wagmi","https://cafefra.me, https://github.com/jeofo/cafeframe"
TreeHacks 2025,Revi,https://devpost.com/software/revi,False,"Search interface Review loading interface AI-generated review for MacBook Pro M3 Social media reviews Social media reviews cont'd While around 98% of consumers read reviews before making a purchase, the process remains time consuming. Consumers jump from site to site, reading numerous written reviews and ingesting video reviews on social media platforms. Revi is here to make that process much more informative and efficient, ensuring that consumers put their money towards quality purchases. Revi analyzes ratings from major retailers across the internet, such as Amazon, Walmart, and more niche retailers like Sephora, and provides a weighted average across tens of thousands of reviews. Further, it summarizes social media review content from YouTube and TikTok, assigns a rating to it, and provides a link to the post. Revi is a Flask app that uses Python for the backend and JS/HTML/CSS for the frontend. We use Oxylab's Web Scraper API to retrieve ratings, review counts, and images for products. For social media review pulling, we use the YouTube Data API and the EnsembleData TikTok API to search for relevant review posts on YouTube and TikTok, respectively. We then use yt-dlp to download audio from these videos as mp3, transcribe the videos with OpenAI's Whisper API, and finally feed the video metadata (such as the title and description/caption) and transcribed content into OpenAI's gpt-4-turbo to assign a star rating to the review and concisely summarize the content into a review format. Looking to the future, we envision Revi's business model to be a subscription-based platform that prompts users to pay for more than three queries a month. Technically, this means that we would have to build out user management and ensure that the application is scalable for deployment. We also see pathways to monetization through running ads or collecting user data. We aim to deploy Revi to make it accessible to all, but the heavy API usage required for the platform to analyze reviews will be costly at scale. We also plan to integrate more social media platforms into our aggregation mechanism, along with embedded video viewing and support for non-english videos. Eventually, we will provide users with tailored recommendations based on their query and viewing history, using our advanced review analysis platform to suggest products that they will likely enjoy based on the opinions of thousands of others!",,,,,,,"api, css, flask, html, javascript, openai, python, whisper",https://github.com/Bloomh/revi
TreeHacks 2025,Zeno,https://devpost.com/software/zeno,False,"Content creators often struggle with managing brand partnerships, optimizing engagement, and responding to audience interactions. Zeno automates these processes, helping creators focus on content while maximizing revenue and efficiency.",,Each component of the platform plays a distinct role:,"Developing an accurate brand recommendation system.
Implementing real-time engagement automation without appearing robotic.
Integrating various API endpoints efficiently.","Successfully automating brand collaborations for creators.
Providing actionable insights that improve content performance.
Enhancing engagement workflows with minimal manual effort.",,"Expanding brand partnership recommendations using machine learning.
Enhancing AI engagement by making responses more context-aware.
Providing multi-platform support beyond YouTube (e.g., TikTok, Instagram).","gpt, nextjs, pinecone, python",https://ai-talent-manager.vercel.app/
TreeHacks 2025,Stable-RL,https://devpost.com/software/stable-rl,False,"Reinforcement Learning (RL) has achieved significant success in decision-making tasks but struggles with real-world applications where interaction with the environment is expensive or dangerous. Offline RL attempts to address this by learning from fixed datasets, but suffers from Q-value overestimation on out-of-distribution (OOD) state-action pairs. The Conformal Q-Learning approach seeks to mitigate these issues by integrating conformal prediction into RL. This method provides distribution-free uncertainty quantification with finite-sample guarantees, helping to stabilize policy learning and prevent overestimation errors. The approach is particularly relevant for safety-critical applications where robust decision-making is necessary.",Conformal Q-Learning enhances standard RL algorithms by introducing statistical confidence intervals around Q-value estimates. It ensures that:,"The Conformal Q-Learning framework was developed by integrating conformal prediction into an actor-critic RL setup, specifically for offline RL. The methodology includes:",,,,"We are looking to expand the amount of algorithms that we can integrate this in. Currently, we had time to implement this idea with Soft-Actor Critic, but integrating this uncertainty quantification into more popular algorithms is our next goal. Additionally testing its robustness on edge-based scenarios, like autonomous driving, is a plausible next step as our motivation was centered around improving robustness in these areas","d4rl, gym, mujoco, pytorch, rl, tailwindcss, typescript, vercel","https://github.com/khushgx/cql, https://v0-stable-rl-spncmb.vercel.app/"
TreeHacks 2025,Mobius,https://devpost.com/software/mobius-e7jidz,False,"Mobile automation is still stuck in the dark ages ofcomplex scripting frameworks like Appium, making it inaccessible to non-developers and tedious even for experienced engineers. While tools like Operator and BrowserUse exist forbrowser automation, there isno equivalent for mobile devices. We set out to buildMobius, anautonomous smartphone agent frameworkthat canexecute any task on a mobile device, scale to a fleet of agents, and provide a well-documented API for developers to build upon.","Mobius is anagent-driven, fleet-ready automation platformthat enables developers to: With just a few lines of code, Mobius makes mobile automation effortless:",,,,,,"adb, android-studio, fastapi, langchain, langgraph, llama, openai, python, websockets",
TreeHacks 2025,VisionZ+,https://devpost.com/software/visionz,False,"We are mainly interested in Palmer Luckey's disruptive innovations in defense systems and Marc Andreessen's forward-thinking vision of technology, this project embraces American dynamism to empower military personnel. The aim is to leverage cutting-edge wearable tech to boost productivity and efficiency on the field.","VisionZ+ is a first-of-its-kinddata visualization and analysis platformthat lets users analyze any dataset through a simple drag-and-drop interface. With ourdataset assistant, you can simply ask questions about your data and let the AI find patterns and insights for you. The platform is powered by aMulti-Agent AI Architecturethat processes data through multiple specialized agents - frominitial analysisandorchestrationtoplanning,executionvia mini-agent swarms, andvalidation. This makes sophisticated data analysis accessible to anyone, regardless of their technical background. Whether you're a medical researcher analyzing patient data, a military strategist evaluating operations, a financial advisor tracking markets, or a small business owner understanding operations, VisionZ+ eliminates the need for a dedicated data science team - just ask questions and get insights."," VisionZ+ is powered by a sophisticatedMulti-Agent AI Architectureleveraging OpenAI's API and Terra's API, enhanced with our customchain-of-thought processingand specialized agents. At its core, we developed a hierarchical orchestration system where each agent serves a distinct purpose: The real innovation lies in ourMulti-Agent Architecture- specialized mini-agents that work in parallel, each focused on specific aspects likedata processing,visualization,pattern recognition, andstatistical analysis. These agents communicate through a custom-built coordination layer, ensuring efficient task distribution and resource optimization. Every output passes through ourValidator Agent, which performs rigorous quality assurance and cross-validation before delivering results. The frontend provides an intuitive drag-and-drop interface, while our robust backend handles the complex orchestration of these AI agents. This architecture enables us to process complex analytical tasks with high accuracy and computational efficiency through systematic decomposition and parallel processing.","One of our biggest hurdles was working with theTerra API integration. The authentication flow required setting up our ownSSH tunnelto generate and handle authorization links, which proved to be quite tricky. We spent considerable time troubleshooting the tunnel configuration and ensuring secure, reliable connections for our users' wearable data. The process taught us a lot about handlingOAuth flowsand secure data transmission in production environments. The implementation of ourMulti-Agent Architecturepresented its own set of complex challenges, particularly around processing time. We discovered that different types of data analysis required varying levels of computational resources and model capabilities. This led us to develop adynamic approachwhere we utilize different models based on the specific analysis needs - from lightweight models for quick statistical analysis to more sophisticated reasoning model for complex pattern recognition. Integrating our dynamic frontend with the backend focused on formatting and passing payload history for wearable data intoMongoDB. Managing state on wearable.js involved designing a payload structure that supports seamless, real-time data transfer. The main challenge was reliably passing the complete payload history while maintaining data consistency.",,,"We're pursuing an ambitious vision: creating the world's first trulyautonomous data scientist. Beyond just analyzing data, we're developing VisionZ+ to proactively identify opportunities, generate hypotheses, and conduct independent research - much like a human data scientist would. Our goal is to enable VisionZ+ to not justanswerquestions, but toaskthe right questions and uncover insights that users might not even know to look for. Looking ahead, we're enhancing ourMulti-Agent Architecturewith more sophisticated reasoning capabilities and expanding into industry-specific solutions. We envision VisionZ+ becoming an indispensable partner indata-driven decision makingacross all sectors - fromhealthcareandscientific researchtobusiness strategyandpublic policy. The future of data science isautonomous, and VisionZ+ is leading the way.","express.js, html, mongodb, nextjs, open-ai-api, react, tailwindcss, terra-api",https://github.com/AntoDono/visionzplus
TreeHacks 2025,Octavius,https://devpost.com/software/octavius,False,"In the chaos of the battlefield, every second counts for a combat medic. With lives on the line, they must triage injuries, administer aid, and make split-second decisions - all while carrying heavy gear and navigating unpredictable threats. But what if they had an extra set of hands?","Octavius quickly detects injuries among soldiers in the battlefield and evaluates what next steps a combat medic should take. A robotic arm can also assist in common battlefield procedures like chest trauma and wound packing or simply holding instruments for medics. We showcase both these capabilities through abstract scenarios. During evacuation, automated voice calls can also coordinate efforts among multiple combat medics. And finally, combat medics may access the medical history of all soldiers to make fast and informed decisions.","Injury detection is done through the Apple Vision Pro and transmitted through Zoom. After transmission, the Jetson Orin Nano processes the real-time video feed using Nano Owl and sends instructions to the robot arm (that we fully build ourselves)! ElevenLabs is exclusively used for voice AI support and InterSystems is used for fast retrieval of all medical records through their vector embeddings.","A major challenge we faced was obtaining the video feed for processing. The Apple Vision Pro has strict protocols around accessing this kind of data, so we had to get creative to tinker with it for this project. On a very high level, we shared the Vision Pro view to a Macbook, then shared it via Zoom to a second laptop, from which we streamed it to the Jetson Nano via OBS RTSP. This process is documented in more detail in our repo’s readme.","ROBOTIC ARM - we fully prototyped, built, and iterated on the robotic arm ourselves at Stanford PRL!","Combat medics are heroes. This project further proved to us their importance and impact on any battlefield. Secondly, Apple’s privacy protections are remarkable, especially when it comes to accessing the camera feed from the Vision Pro. We believe we were able to hack together the most optimal solution to access the camera for our use case (which is completely novel) and has under 1 second latency!","We are proud to have built Octavius. Next, we plan on bringing more AR support for combat medics which in many cases will require Apple’s Enterprise Permissions. We also plan to explore the use of fully autonomous humanoid robots to work alongside combat medics.","apple-vision-pro, elevenlabs, intersystems, jetson-nano, nano-owl, openai, python, robot-arm, zoom",https://github.com/treehacks-2025-octavius
TreeHacks 2025,Bee Well,https://devpost.com/software/bee-well-03t1a9,False,"neck pain :( Moving neck to collect flowers as a bee. Bzz... AirPods for head tracking Tilting head to move bee avatar A neck exercise game to help you move your neck! 
In the game you play as a busy-bee going from flower to flower. As programmers, we spend countless hours at our computers—and we know how neck and back pain can cramp your style.Bee Wellis our gamified solution to this common problem. At TreeHacks, we tested our product with over20hackers. We’ve transformed neck exercises into an immersive gaming experience that not only alleviates pain but also makes moving fun! Bee Wellchallenges you to guide a bee to a flower—all using head tilts! Motion-Driven Gameplay:Using Apple’sCMHeadphoneMotionManager, we track your head movements in real time. Tilt your head (yaw, roll, and pitch) to control the bee’s path. Spatial Audio Cues:Our dynamic audio engine adjusts volume and panning based on distance and direction, letting you play with your eyes closed if you dare. Adaptive Challenges:With a built-in countdown timer and collision detection, each round provides a fun, engaging challenge that encourages regular neck movement and exercise. Design Process Tech Stack:  SwiftUI & AVFoundation:For a sleek and responsive user interface and immersive sound integration. Apple Core Motion:Capturing head tilt data (yaw, roll, and pitch) in real time from your AirPods. Dynamic Audio Processing:Leveraging spatial audio to create realistic directional cues that adapt as you move. Our Approach: Real-Time Head Tracking:We fine-tuned head tracking to be both responsive and smooth, ensuring that every tilt translates into precise bee navigation. Spatial Audio Magic:By dynamically adjusting audio cues, we help you intuitively locate your target—a virtual flower—making the experience both engaging and therapeutic. Iterative Development:We worked together to overcome technical challenges, iterating rapidly and collaborating closely to ensure a fluid, enjoyable experience. Balancing Responsiveness & Smoothness:Fine-tuning head tracking and designing an audio-interactive experience that is intuitive and delightful. Crafting Natural Audio Feedback:Designing spatial audio that feels intuitive and immersive. Seamless UI Performance:Ensuring our UI remains fluid while processing continuous head movement data. Building a Narrative:Creating an engaging story around a bee that keeps players coming back for more. Controller-Free Navigation:Implemented head-tilt-based navigation—no external controllers needed. Audio allows users with low vision to also be able to play. Immersive Adaptive Audio:Developed real-time adaptive audio feedback that makes the game playable even with your eyes closed. Accessible & Engaging Experience:Designed a game that not only provides a fun distraction but also encourages healthier movement habits – we built something we ourselves will use. These are the features we would like to work on next: Daily Challenges:Dynamic, daily challenges that allow people to come back and continue to improve on their neck mobility and strength. Expanded Gameplay:Add levels with obstacles, different storylines, environmental changes, and more audio-led experiences. When we were thinking about what games can take advantage of motion control, many options came to mind, from Pong to 2048. We would like to diversify the game options users have. Multiplayer Modes:Enable collaborative challenges and competitive play for enhanced social engagement. VR/AR Integration:Explore mixed-reality experiences to further elevate immersion and interactivity, like using the gyroscope from a headset to create a digital landscape shaped by head motion. AI-Powered Analytics:Integrating AI to provide personalized feedback on neck movement trends and overall well-being.",,,,,,,"cmmotionmanager, procreate, swift, xcode","https://github.com/lindaktong/treehacks, https://docs.google.com/presentation/d/1LWnt_5TzodOZ0M6csjatNXX-jH3G9vShAxK544DNvdI/edit?usp=sharing"
TreeHacks 2025,StoryTime,https://devpost.com/software/storytime-zgml96,False,"One of the most universal experiences of childhood is storytelling—whether through books, TV shows, or cherished tales passed down by loved ones. Storytelling is at the heart of human existence, shaping young minds, imparting valuable lessons, and creating lasting memories. Beyond the wisdom these stories carry, early engagement with reading is one of the strongest predictors of future success. It's clear that storytelling plays a crucial role in a child's development—but what stories are available for them to read? Many traditional fairytales and popular children's books fail to truly reflect a child’s unique culture, identity, and personal experiences. Meanwhile, parents often struggle to create engaging, meaningful stories on the spot—ones that not only captivate their children but also instill a lifelong love of reading. Enter StoryTime—a personalized digital storytelling experience designed to bring inclusivity, creativity, and educational value to every child’s bedtime story.","StoryTime generates custom, interactive storybooks based on a child’s profile and a lesson or theme chosen by the parent. It features AI-generated stories tailored to the child’s background and interests, and narrated audio to bring the stories to life. There are AI-generated animations for every page, making the experience more immersive. Our platform is also hands-free and accessible, with the capability to simply talk to the platform and watch your story unfold. StoryTime allows children to have their own personal library with engaging, educational books at their fingertips.","Our tech stack includes: FastAPI for backend development.
OpenAI’s GPT models to generate personalized stories, and their text to speech API for narration
OpenAI’s Whisper model for speech-to-text functionality.
LumaAI to generate AI-powered video content.
React & Figma for a seamless and visually engaging frontend experience.","Some challenges we ran into was video latency. Generating videos took around 15-20 seconds per page, which posed a challenge in maintaining a smooth reading experience. To counter this, we implemented background processing, ensuring videos generate progressively while the user reads or listens to the story. This way, the next video is ready just in time. Additionally, integrating OpenAI’s Whisper model with our frontend posed technical challenges. Handling different file formats and ensuring smooth real-time transcription required multiple iterations.","We are proud of having a fully functional, end-to-end product that aligns with our original vision. There is seamless AI integration across storytelling, voice narration, and video generation. Most importantly, 
our platform makes personalized storytelling more engaging and representative of diverse cultures.","We learned about seamlessly integrating between the frontend and the backend, as well as being able to use Luma's product and prompt engineer to get as consistent of videos as possible. We also learned how to handle asynchronous generation efficiently.","We want to work on having better video consistency and ensuring character and style continuity across pages. We would also like to experiment with extended animations and video lengths for richer storytelling, and give users more opportunities to customize their story, including animation styles, narration voices, and more. We would also like to add the capability of live interruption, where children can stop the flow of the story as it is generating and change the plot, characters, or anything else, and the story will accordingly adjust.","luma, openai, python, react","https://github.com/naishagarwal/treehacks-2025-personalized-storybook, https://www.canva.com/design/DAGfRF572EU/x1Ns1a9Y5e7Tk0JMgv5Hag/view?utm_content=DAGfRF572EU&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h2c14011d0f"
TreeHacks 2025,BuckyUp,https://devpost.com/software/bucky,False,"Throughout the renowned Captain America series, Winter Soldier Bucky Barnes served as a faithful companion. Unfortunately, Bucky also suffered from serious PTSD. This project was inspired by Bucky’s story—I hoped to build something that can help prevent mental health crises like that of Bucky and almost 23% of American veterans through efficient, human-less triage (1). As of this writing, the US lacks a unified mental health screening system for deployed soldiers and the social stigma surrounding mental illness inhibits many from seeking help (2). As a premed student, I’ve seen firsthand that all decision making in healthcare is holistic. With this in mind, BuckyUp integrates numbers, sights, and sounds from the user.","Mental health triage needs to be faster, smarter, and more effective—especially in high-stakes environments like the military, where mental health is inextricably tied to performance. BuckyUp is an AI-driven solution that performs rapid psychological assessments in just five minutes. Pulling together real-time conversational AI (GPT-4o), computer vision (MediaPipe), and biometric data (Terra API), BuckyUp evaluates speech patterns during a classic SIGECAPS-based screening interview and measures blink rate and vitals to flag at-risk individuals dynamically. Why measure eye blinks and heart rate? Both have been shown to increase in clinically depressed patients (3, 4). By overlaying these insights into a visual report, we empower healthcare professionals to make informed, data-backed decisions. BuckyUp also offers a guided meditation for interested users—empowering them to prioritize mental health.","I leveraged ElevenLabs for voice generation, enabling a calming and immersive guided meditation experience. My real-time conversational AI agent was built on GPT4o, enhancing interactivity by picking up on user feedback through verbal cues. We also collected biometric data via the webcam, which was running my custom computer vision model built with MediaPipe. The tech stack featured a Next.js frontend for a seamless user experience and a Flask backend for efficient processing and integration.","One of the biggest challenges I faced was tackling this project as a solo developer and first-time hacker! However, I soon realized that being on a solo team didn’t mean I had to work alone—I could collaborate and co-work with other TreeHacks attendees. From mastering the integration of Terra’s APIs to build the perfect tool, to fueling up on enough caffeine to power through the night, the challenges were relentless. Yet, the support of my newfound friends made the journey all the more rewarding. I also found it tough to integrate the various components of the algorithm together on the web app, particularly streaming the Terra API data while also running the Computer Vision model.","Built an end-to-end app by myself for the first time. Having spoken to doctors at Stanford Med, it’s clear that AI is a real application for triage—the need is there, and it’s exciting to have developed an application which is truly practical and relevant for healthcare professionals.","Rather than building algorithms from scratch, it’s often easier to start from baseline, open-source models. User experience is a crucial element of app success. Caffeine stops working after a while.","The current app is largely a proof of concept to which a host of additional features can be applied—from syncing additional Terra API data to developing a more comprehensive interactive assessment. After conducting additional needs-assessments among healthcare experts, I hope to continue building out BuckyUp’s features and test in military users to assess the precision of the risk assessment score.","elevenlabs, flask, mediapipe, next, openai, supabase, terra",https://github.com/bnatasha24/treehacks25
TreeHacks 2025,LLMaOS,https://devpost.com/software/llmaos,False,"The average typing speed is 40 words per minute, which is considerably slower than the average speaking speed (150 words per minute). However, the keyboard is still the main way in which most people interact with their devices, and it has stayed that way for decades. There are existing voice applications like Siri and Alexa, but those products did not process other streams of information such as the display. I propose a voice-controlled operating system, with the entire stack (the assembler, the processor) enhanced by AI components. This should remind people of J.A.R.V.I.S. and the robots in WALL-E.","As an operating system, LLMaOS simplifies the way you interact with the computer. Below are some examples (non-exhaustive). (1) ""Play me the song Espresso"" (2) ""What is the score between Manchester City and Real Madrid?"" (3) ""When is the next Codeforces contest?"" LLMaOS is voice controlled, transparent (you keep a log of its ""assembly-level"" instructions and can see what it is doing), and possesses screen-processing capabilities.",There are three modular components:,"The main challenge lies in image analysis. For example, it is difficult to detect a cross symbol on a high-pixel-density screen. It is also difficult to understand the texts on the screen. Due to these, models' confidence in bounding boxes sharply decreased, which made it hard for the assembler, particularly the LEFT_CLICK x y instructions. To alleviate this problem, I threw three giant models at it, but there are still failure cases. In the end, I had to hardcode some portions for demonstration purposes. (I would also have liked to use NVIDIA's ocdrnet model, but there is a tradeoff between the image sizes it supports and the clarity of texts.) The latency and user-friendliness of the operating system are also crucial - the calls to the three API endpoints are parallelised (using Python's async). However, there is still significant latency due to image-processing the screen. The context switching between LLMaOS (especially maintaining the RealTime session) and other applications like Chrome is not very significant but still noticeable.",Made a demo-able (though slightly hackish) prototype. Come by to try it out!,(1) The power of abstraction and (2) connecting many layers of the computer stack together.,"I'll focus on improving the AI subcomponents first, by using/training better models that are specifically for this task. Then, add more features and optimisations to the LLMaOS stack.","asyncio, gpt, mistralai-pixtral, nvidia-neva, openai-realtime",https://github.com/jianzhi-1/LLMaOS
TreeHacks 2025,TalkThru,https://devpost.com/software/talkthru,False," Technical interviews, presentations, salary negotiations, sharing bad news, thesis defenses … what do all these have in common? You need to practice. You need feedback. TalkThru is the professional you need for on-demand, face-to-face practice. Technical or behavioral, emotional or cold: test your speeches, knowledge & improvisation with immediate corrections. You provide a topic to the agent. You provide the agent a role. The agent holds a conversation with you and summarizes what you should improve on at the end. It’s that easy. 5 steps: Speech-to-text: pause identification, fast responses, storing & processing audio real-time were challenging aspects of the speech integration. Lip-syncing: realistic interview preparation requires mouth movement. Whereas LumaLabs provides vastly creative features, it does not support facial expressions.Our first feature release will enable lip syncingso that users can carry realistic conversations. CUDA integration: lip-syncing requires torch access, CUDA-based GPUs & many pre-trained models. Thanks to our sponsor NVIDIA, we were able to develop this feature. Run LLMs on CUDA! End-to-end integration of LLM queries for on-demand user requests. Successful project scoping There is so much ground to cover:",,,,,,,"aniportrait, cuda, elevenlabs, flask, html, lumalabs, perplexity, python, speech-recognition",https://github.com/Andrea-MiramonSerr/tree-hacks.git
TreeHacks 2025,"Swiper, No Swiping!",https://devpost.com/software/swiper-no-swiping-qwjv28,False,"This project was inspired by the fright of leaving your expensive personal belongings at a hackathon - or airport, or train station, or anywhere for that matter. Want to leave your bag to claim a seat or desk but don't want it stolen? Simply place the SNS device in your backpack and wander off in peace, knowing that the moment anyone tries to move your bag, you will instantly be notified and the fright of the sound will scare the culprit off.","The SNS device constantly monitors for movement. If it detects that it moved, ie someone has picked up your important belonging, it will buzz loudly and uncontrollably, frightening the culprit until they leave your bag alone. Not only that, but it will send you an email notification so you can come back to check on your belongings. What's more, you can access the web server to see the history of your device's movement. If it only moved for a brief few seconds, perhaps it was merely pushed - but if the convenient graph indicates that it moved for an extended period of time, you know someone is trying to steal your stuff.",,,,,"What I had wanted to implement this hackathon, but couldn't due to broken ESP32 boards, was connectivity to Bluetooth speakers. Users would be able to record a custom scare-off recording on the web app which would upload to the device. Then, instead of buzzing, the device will play this recording to scare the thieves away. Additionally, the ability to enable and disable the device through the web app can be implemeneted.","arduino, buzzer, css, esp8266, html, http, javascript, mpu6050, php, txt","https://char787.com/technology/swiper-no-swiping/, https://github.com/Char787/swiper-no-swiping"
TreeHacks 2025,CapyGuardian,https://devpost.com/software/capy-guardian,False,"Imagine a world where care is both personalized and immediate—where every moment counts, and every individual’s unique needs are met instantly. CapyGuardian harnesses the power of agentic AI to blend wearable sensors with real-time health analysis delivering instant, tailored health insights. This approach transforms reactive care into proactive, individualized support that acts precisely when needed. Whether it's alerting a caregiver at the first sign of trouble or ensuring that a soldier in a high-stakes environment is safe, CapyGuardian is set to redefine emergency response. This dynamic system is a game-changer for older adults living independently, providing them with critical peace of mind, and for military platoons, where split-second decisions can make all the difference.",,,,,,,"c++, python",https://github.com/PatP15/TreeHacks2025
TreeHacks 2025,focusr ai,https://devpost.com/software/focusr-ai,False,"Staying engaged during a lecture isn’t always easy. A brief lapse in focus—whether due to distractions, fatigue, or simply a wandering mind—can mean missing a key concept, making it harder to follow along with the rest of the lesson. In fast-paced courses, those small moments of disengagement add up, leaving students struggling to catch up. For those with learning disabilities, this is even more so the case.
We built focusr.ai to help bridge that gap. By tracking engagement in real-time through eye movement, our tool identifies when a student zones out and ensures that missed topics get extra emphasis in the AI-generated summary. Instead of rewatching an entire lecture or sifting through lengthy PowerPoints, our users can quickly review exactly what they need to stay on track.This project isn’t about forcing focus—it’s about helping students learn more effectively, even when attention slips.",,"We built focusr.ai as a web application that tracks engagement, transcribes lectures, and generates AI-powered summaries to help students stay on top of their learning. The frontend uses React and Tailwind CSS, while the backend, built with Flask, handles audio processing, AI interactions, and engagement tracking.
To detect focus lapses, we use Gemini’s multimodal processing, analyzing eye movement patterns. OpenAI Whisper transcribes lectures, and Perplexity AI creates context-aware summaries, prioritizing missed sections. An interactive engagement chart visualizes attention levels, helping students quickly review key moments. Hosted on Vercel with cloud/local storage, focusr.ai ensures that distractions don’t become setbacks, making learning more adaptive and accessible.","A major challenge we faced was engagement tracking for long lectures. Using OpenCV and MediaPipe for eye-tracking was too slow for extended recordings. We switched to Gemini and used images rather than videos, which efficiently analyzed video input directly, eliminating the need for uploading unnecessarily large files. This drastically improved speed and accuracy, making real-time engagement tracking viable for full-length lectures.","We as a team all contributed to this project, and we are thrilled at the intuitive way in which the UI/UX has come together. We’re proud of how we streamlined the entire learning workflow—instead of just transcribing audio, our system prioritizes the moments when a student loses focus, ensuring they can efficiently catch up without rewatching everything. We originally thought about going with another project idea because we thought we wouldn't be able to learn and finish everything on time for focusr.ai, but we successfully brought it all together into a working product that makes reviewing lectures more effective.","All team members gained a much deeper understanding of the languages, libraries, and implementation techniques in their respective area of building focusr. The members: Ishan - frontend, Tyler - backend (engagement analysis), and Isabella - backend (transcript generation); all leaped into React.js, Flask, Git/Github, as well as component integration. Every member of our team worked with a language, technology, or library they hadn't previously worked with before, so we're proud to have learned while creating something we're all proud of.","We see focusr.ai expanding its impact on both students and educators. From a teacher’s perspective, implementing this system across multiple students in a class could provide valuable insights into engagement trends. If a significant percentage of students consistently lose focus at the same point in a lecture, instructors could identify and refine less engaging sections to improve overall learning outcomes. 
For students, we want to move beyond video uploads and explore live engagement tracking during lectures. This could allow real-time feedback, helping students refocus before they fall too far behind. Expanding in this direction also opens the door to making focusr.ai a Zoom app, extending its use beyond in-person lectures to online meetings, virtual classrooms, and remote work settings, making it a more versatile engagement tool.","api, flask, gemini, javascript, openai, perplexity, python, react.js, vercel","https://github.com/isabellasocci/focusr-ai, https://focusr-ai.vercel.app/"
TreeHacks 2025,Honeycomb,https://devpost.com/software/coeus-pfvn7c,False,"Garry Tan’s post about building a secure, user-focused AI app store sparked an idea that inspired us—the need for a platform that seamlessly integrates new AI capabilities without requiring developers to reinvent the wheel. We envisioned an environment where multiple language models could coexist, enabling users to experiment with prompts, connect data sources, and transform natural language into powerful workflows. We also recognize enterprises’ growing desire to streamline and automate their workflows—we decided to build a workflow automation platform that encompasses these traits, emphasizing data control, shared memory, and user-centric design that empowers both technical and non-technical users to specify tasks in natural language.","Honeycombis a unified platform for experimenting with AI models, building sophisticated workflows, and seamlessly integrating data sources:",We combined several powerful tools and technologies to bring Honeycomb to life:,,,,,"codeium, elasticsearch, google, langchain, openai, python, react, vectordb, windsurf",https://github.com/jasondu7297/honeycomb
TreeHacks 2025,EcoNomNom: Sustainability One Snap at a Time,https://devpost.com/software/economnom-di7lg6,False,"Hershey’s… or Ghirardelli? Lay’s… or Kettle? Whether you’re craving frozen peas or protein bars, at some point, grocery shopping is the ultimate paradox of choice. As we agonize over per-ounce prices, expiration dates, nutritional labels, and scrumptiousness, one key factor often goes overlooked:which option is best for our planet? Food productiondrivesover a quarter of global greenhouse gas emissions.88% of consumers want brands to help them make more eco-friendly choices, but the reality? Navigating conflicting sustainability labels feels like deciphering ancient hieroglyphs. The information consumers need is buried or inaccessible when it matters most — at the shelf. We built EcoNomNom because we, as consumers, found it near-impossible to make eco-friendly choices while grocery shopping. Existing solutions offer piecemeal recommendations — one product at a time, one barcode at a time. There are apps that display sustainability data for a single product if you scan its barcode, and there are apps that compare the footprints of different foods, but there isno existing appthat provides brand and product-specific sustainability comparisons from just a picture of the shelf. EcoNomNom makes sustainability a seamless, intuitive part of your grocery shopping experience.","EcoNomNom is your personalized AI-powered grocery shopping assistant. Simplysnap a picture of the shelf,and EcoNomNomranks products in the imagebased on farm-to-shelf sustainability metrics. For each item on the shelf, our app extracts product details and provides easy-to-understand ratings on key sustainability factors, includingcarbon footprint, packaging waste, ingredient sourcing, animal welfare, harmful chemicals, palm oil content, and more.These factors are aggregated into our own sustainability rating called theGreenScore. You can customize recommendations based on the sustainability metrics that matter most to you — whether that’s reducing emissions, supporting fair trade sourcing, or avoiding harmful additives. EcoNomNom also tracks your footprint over time, showing how much carbon emissions you’ve saved by making eco-friendly purchases.
Every dollar you spend is a vote for the future you want. With EcoNomNom, you can vote for a greener planet — without sacrificing time, money, or taste.","EcoNomNom has three primary components: We created adatabase of sustainability metricsfor various foods by aggregating several open-source datasets, such as OpenFoodFacts and the Food Sustainability Index. These datasets provided product-specific attributes like carbon footprint, packaging materials, palm oil content, and ingredient origins, which we used to calculate an aggregate “GreenScore” for each product's overall environmental impact. The processed data was stored inFirebase. We leveragedInterSystems IRISfor vector-based search, allowing fast and scalable querying across millions of products. IRIS Intelligence was critical in enablingsemantic searchby using vector embeddings of product descriptions. By querying the most related product entries, we were able to return the average eco score, carbon footprint, and other sustainability metrics for similar products. For deployment, we usedDockerand deployed ourFastAPI-based services toGoogle Cloud Run,integrating the APIs with the front-end. We built anAI-driven image processing pipelineto analyze user-uploaded images of supermarket shelves, extract product names and brands, and match them to entries in our sustainability database.OpenAI’s Visual Language Modelswere used to interpret the image and identify text labels, enabling accurate detection of product names directly from the image. The extracted product descriptions were converted into vector embeddings usingHugging Face’s Sentence Transformermodel. These embeddings served as search queries in theIRIS Vector Search Engine, which returned the top 3 most similar products from the database. By averaging their eco scores and other sustainability flags, we provided users with a reliable measure of which products were the most environmentally friendly. UsingFlutterFlow, we configured our app to display product recommendations and sustainability data for each. We also built a profile landing page to store user preferences and carbon footprint. This app can be accessed on mobile or web.","Aggregating data from multiple open-source datasets (e.g., OpenFoodFacts) was difficult due to the datasets missing several values. Cleaning and trimming these datasets required significant time. Performing vector searches across millions of product entries in the sustainability database was extremely slow at first and required optimizing the IRIS Intelligence engine.",,"We assumed finding sustainability data would be straightforward — after all, most companies advertise their eco-friendly practices. Instead, we discovered that transparency and quantification are rare, and crucial data is often missing or misleading. Creating our sustainability rating required aggregating data from multiple sources and looking beyond buzzwords.","Sustainability means different things to different companies and people. Some users prioritize carbon footprint, while others care more about ethical labor practices. We would like to take EcoNomNom’s personalization to the next level, allowing users to provide more input on how they would like to weigh different sustainability factors. We also want to account for individualized preferences such as dietary restrictions and allergies. Our ultimate goal is to create a lightweight shopping assistant that doesn’t tell you the “best” product in a generic sense, but rather, tells you which product is best for you. We hope to integrate Retrieval-Augmented Generation (RAG) into our AI pipeline to generate context-specific explanations of the environmental impact of products for each user. Convenience, affordability, and sustainability don’t have to be at odds. With EcoNomNom, they go hand in hand—one purchase, one planet-friendly choice at a time.","dart, docker, fastapi, firebase, flutterflow, intersystems, openai, python",https://github.com/Supernova-45/EcoNomNom/tree/code
TreeHacks 2025,JARVIS Health,https://devpost.com/software/jarvis-health,False,"Imagine a soldier in training, pushing their limits in a high-stakes environment, yet their personal wellbeing goes unnoticed. Over-exhaustion can lead to injuries, but traditional assessments rely on subjective judgment, leaving commanders without real-time insights to prevent risks. While wearable devices typically focus on tracking individual health, we recognized that aggregating data from multiple soldiers, especially in military contexts, is essential. That’s why we createdJARVIS Health—a solution that provides commanders with a holistic view of their soldiers' health, using multiple data points to enable safer, more effective training.","JARVIS Healthaggregates real-time heart rate and exertion data from wearable devices using theTerra API, focusing on data from multiple soldiers rather than just individuals. This data is displayed on a dashboard that provides commanders with data-driven insights into soldiers' physical states, enabling more informed decisions. By tracking these vital metrics across a group, the system helps identify early signs of over-exhaustion, optimize training, and reduce injury risk, ensuring soldiers train smarter, not harder. Powered byOpenAI’s LLM, our system also analyzes aggregated data to make actionable recommendations, such as transferring load or suggesting breaks, to ensure that each soldier maintains optimal performance and well-being.","We developedJARVIS HealthusingFlutterFlow, a no-code platform that allowed us to quickly design and deploy a user-friendly dashboard for commanders. The app connects to Firestore viaFirebasefor real-time data storage, enabling seamless synchronization of heart rate and exertion data across multiple soldiers. The Terra API aggregates wearable data, which is then stored in Firestore for easy access and analysis. We also used OpenAI’s GPT-4o-mini model, served by FastAPI, to analyze the aggregated data and generate actionable recommendations based on the wearable data. For the hackathon, we embraced an Avengers theme, with each soldier represented by an iconic character to make the app both engaging and memorable.","As first-time users of FlutterFlow, we encountered a learning curve with its unique workflow, especially in handling logic and making API calls. While FlutterFlow enabled rapid development, we found it challenging to structure complex queries due to its support for only one level of subcollections. Additionally, we worked with the Terra API to aggregate wearable data, but its limitations in flexibility required us to get creative with data retrieval and integration. We also had limited access to data from only two types of wearables, which constrained the diversity of our dataset. Incorporating OpenAI’s GPT-4o-mini to provide real-time recommendations was another challenge. Initially, we struggled to get the concise and actionable responses we needed. We found that the AI wasn't always producing the right level of specificity, so we had to experiment with different approaches to prompt engineering. After refining the prompts, we were able to ensure the responses were brief, clear, and helpful, giving commanders high-level, data-driven recommendations. These challenges provided valuable learning experiences and helped us refine our problem-solving skills as we adapted and built the app effectively.","We’re really proud of how we took on this project and experimented with a lot of ideas and tools to come up with a solid solution in such a short amount of time. Despite being new to FlutterFlow, Terra API, and Firebase, we were able to quickly learn and use them to build a functional dashboard that tracks heart rate and exertion data. We also integrated OpenAI’s GPT-4o-mini to help provide real-time recommendations based on the data, giving commanders better insights to keep their soldiers safe and performing well. It was exciting to experiment, adapt, and learn new things on the fly, and we’re really proud of what we were able to build under such tight deadlines. The whole process was a great mix of problem-solving, learning, and seeing our ideas come to life.","For all of us, it was our first time using a no-code tool like FlutterFlow, and it was a great introduction to how quickly we could bring our ideas to life without needing to write tons of frontend code. Along with that, we got hands-on experience with Terra API and Firebase, which were new to us but became key parts of our solution. We learned how to integrate these tools to gather and process data, and the importance of thinking creatively when working with limitations. Working with OpenAI’s GPT-4 was also a highlight. It opened our eyes to how AI can generate real-time, actionable insights from raw data, and we were able to incorporate that into our app to help commanders make better decisions. Overall, this project taught us how to be resourceful, adapt quickly, and blend new technologies in ways that can have a real-world impact. It was a great learning experience and a lot of fun!","Looking ahead, we see significant potential to expandJARVIS Healthbeyond military training. We plan to tailor the platform for sports teams, where coaches and medical staff can monitor athletes’ exertion and recovery, reducing the risk of injuries during intense training sessions. We also envision first responders using the system to track the physical well-being of paramedics, firefighters, and police officers, ensuring they stay at optimal performance levels during high-stress, physically demanding situations. Additionally, we aim to bringJARVIS Healthto healthcare providers focusing on elderly care, allowing for continuous monitoring of vital signs to detect early warning signs of health issues, helping to ensure the safety and comfort of seniors in care facilities or at home. By expanding into these areas, we hope to create a broader impact on health and performance across a range of critical sectors.","firebase, flutterflow, llm, openai, terraapi",https://github.com/asaju67/JARVIS_Health/
TreeHacks 2025,Myst,https://devpost.com/software/myst-p09ai2,False,"In an era of ever-increasing media consumption, individuals often fall into reinforcement bias—engaging only with content that aligns with their existing beliefs. Our project was born from the need to break this cycle and promote diverse perspectives in online media. By providing users with real-time insights into their content consumption habits, we empower them to make informed choices about what they engage with and how it shapes their worldview.","We created a Chrome Extension that automatically captures screenshots of online activity upon tab changes and significant keystrokes and analyzes the content in real time, all while minimizing storage usage by updating a rolling classification vector instead of storing every screenshot. Using Vision-Language Models (VLMs), content classification, and sentiment analysis, we generate a numerical consumption vector that categorizes a user's browsing habits on progressive scales for various categories: motivational (generally optimistic vs. pessimistic content), educational (informative vs. brain rot content), political (conservative vs. liberal content), and economic (capitalist vs. social content), and whether or not the content is misinformation. Users are then given an easy to read summary of where their content consumption leans. By continuously updating rather than storing every screenshot, our system provides real-time insights without unnecessary data overhead. These insights are displayed dynamically, allowing users to track shifts in their media consumption and even receive recommendations for broadening their exposure.","We started by assembling a basic Chrome extension, learning the required structure for opening and deploying an extension on the Chrome Web Store. Next, we developed the core functionality and pipeline: capturing screenshots of the user's screen, extracting text using the OpenAI GPT-4o API, and classifying the content using Mistral AI. Finally, we performed sentiment analysis with our fine-tuned model on TogetherAI to generate meaningful insights about user consumption habits. The last stage was connecting these two components through the use of a Flask endpoint, enabling seamless communication between the extension and the backend. To make the insights more interactive, we used Luma Labs to visualize the classification data dynamically. Instead of simply displaying raw numerical values, we transformed the running classification vector into a blob-based 3D visualization, where the shape, size, and color of the blob evolve based on the user’s browsing trends. This provides an intuitive and engaging way for users to see how their content consumption habits shift over time.","We got stuck a few times when hosting the Flask backend. A lot of time was spent trying to debug the json outputs for the Mistral API realizing that formatting was creating a large amount of errors. After fixing this, debugging was difficult since understanding niche errors in database management and quicks of Flask took time to individually discover and address. We also have a hard time synchronizing data across the users as we added capabilities to maintain a user’s data across different devices. This meant sharing the json file on a backend hosted database.","We are very proud that we were able to deliver an end-to-end extension that we had no idea how to do prior to the Hackathon start. Hosting on Flask was something we had never done, so getting an extension that fully integrates with user behavior and provide useful insights was incredibly rewarding. This extension is truly something that I would want to use in my everyday internet usage, for which, I am excited to continue building and fine-tuning to my usage patterns. The UI for this extension is gorgeous and came together with the metrics swimmingly. We are always excited to see what our unique ""Blob"" will look like!","Throughout this journey, we learned:","If we were to continue developing Myst, we envision several key improvements and expansions to enhance its impact:","css, flask, html, javascript, lumalabs, mistral-ai, openai, python",https://github.com/matthewjguck/myst
TreeHacks 2025,Morpheus Ray-Bans: Your ASL Teacher,https://devpost.com/software/morpheus-iq2g1y,False,"We believe everyone deserves to be understood. While over 500,000 people use American Sign Language (ASL) as their primary language, there's still a huge gap in real-time communication between signers and non-signers. While voice-to-text is everywhere (think Siri or Alexa), ASL translation tech hasn't kept up. We wanted to change that, not just with translation, but by creating a patient, understanding tutor that helps bridge these communication gaps.","Morpheus is like having a friendly ASL interpreter and teacher in your browser. 
It can:","Think of Morpheus as having three main parts: Clean, simple web design using HTML/CSS that works on any device
Real-time video feedback so you can see your signs
Progress tracking that doesn't feel overwhelming MediaPipe to track your hand movements
A smart AI system we trained to recognize ASL signs
A tutoring system powered by OpenAI that adapts to your learning style
(Details of the Technical Stack used can be found on the 2nd Slide) Super fast connections so everything feels instant
Careful error handling so things don't break when mistakes happen
Smart data management to keep everything running smoothly","Building Morpheus wasn't easy! We faced some tough challenges: Our largest challenge was ensuring everything worked in real-time without lag. The Meta Ray-Bans don’t even have an official developer environment, so had to do something similar to a “man in the middle attack”, with a ton of trial and error. We experimented across streaming platforms, video sharing, frame-by-frame capture. In the end, we found using OBS screen capturing of a Facebook Messenger call the fastest approach.","Looking back, we're really proud that we:","This project taught us about integrating advanced AI and computer vision with wearable technologies, something that none of us had done before. We also learned about the incredible complexity and beauty of ASL, and learned a ton of signs along the way.",We're just getting started! Here's what we're excited about:,"css, flask, html, mediapipe, numpy, openai, opencv, pillow, pygame, python, rayban-metas, scikit-learn",https://github.com/stevensegawa/asl-recognition-web.git
TreeHacks 2025,Solana Rivals,https://devpost.com/software/solana-rivals,False,"Our vision forSolana Rivalswas born at the crossroads of frontiers: the rise of autonomous AI agents, the birth of decentralized markets and web3, and our shared belief in the power of gamification. As humanity has always pushed boundaries, we wanted to harness that same pioneering spirit. We merged cutting-edge tech with the thrill of competitive gaming to create a platform where trading strategies meet legendary playable avatars. Behind the scenes, our “brain” is a sophisticated AI orchestrator that coordinates with three specialized sub-agents: These agents empower the main AI to autonomously decide when to buy or sell tokens, with a dedicated sell agent managing exits based on user-defined risk profiles. We’re witnessing the golden age of crypto—an era where innovation is surging and the decentralized future is unfolding. With blockchain technology still in its infancy, now is the perfect time to push boundaries. Our childhood passion for games likeNBA2KandMinecraft, where strategy and character design were paramount, naturally evolved into our vision for Solana Rivals. In this project, we merge the world of competitive gaming with the transformative power of blockchain technology, celebrating our adventurous past while forging a decentralized future. In just 36 hours, our team integrated multiple cutting-edge technologies: The orchestration of our autonomous trading agent swarm—which dynamically weighs market trends, whale activities, and data insights—sets our project apart from typical trading bots. We even built dynamic graphics using DaVinci Resolve and dusted off our Photoshop skills to create compelling avatars and UI assets. Building Solana Rivals wasn’t without hurdles. We faced: These challenges deepened our expertise in the Web3 crypto space and taught us how to work effectively under pressure—even with a little sleep deprivation. Solana Rivals is more than a game—it’s a sparkplug for a new era in decentralized finance. By fusing competitive gaming with AI-driven trading, our platform transforms the traditionally exclusive world of trading into an accessible, dynamic arena. Here’s how we’re making an impact and shaping the future: Solana Rivals isn’t just reshaping digital asset management—it’s redefining what’s possible in decentralized finance for generations to come. In the spirit of exploration and innovation, Solana Rivals is our bold leap into the next era of blockchain-driven competitions. We’re thrilled to demo our project, share our journey, and discuss how our agent swarm could redefine trading on decentralized networks. Join us as we challenge the status quo and unlock a future where technology, creativity, and competition converge.",,,,,,,"dexscreener, docker, express.js, google-gmail-oauth, helius, html, next.js, node.js, openai, react, shadcn/ui, solana, supabase, tailwindcss, typescript","https://github.com/aneeshvathul/thbe, https://github.com/tremanus/thfe"
TreeHacks 2025,LogFlowAI,https://devpost.com/software/creatorai,False,"During our time in the industry, we saw how reactive troubleshooting could cost valuable time and resources. Inspired by the potential of predictive analytics and real-time monitoring, we asked:What if we could forecast system failures and diagnose issues before they escalate?This vision led us to create a platform that marries deep technical prowess with strategic business insights—empowering teams to move from crisis management to proactive decision-making.",,,,,,,"anthropic-claude, brev, coveralls, cuda, django, docker, fast-api, github-actions, iris-vectorsearch-intersystems, javascript, langchain, langgraph, makefile, ngrok, numpy, nvidia, open-ai, pandas, perplexity, pytest, python, pytorch, scrapybara, sphinx, sqlite","https://github.com/DhyeyMavani2003/logflowai, https://dhyeymavani.com/logflowai/, https://coveralls.io/github/DhyeyMavani2003/logflowai"
TreeHacks 2025,Snap Chef,https://devpost.com/software/snap-chef-bfwep7,False,"With 40% of the entire US food supply ending up in the waste, helping to eliminate food waste—even at a small scale—can go a long way. We wanted to tackle the problem at a consumer level to help individual households use their leftovers more effectively. Furthermore, many people (our team included) spend parts of the day worrying about what they want to make for dinner with the ingredients in their fridge. We decided to chip away at both of these problems with a new app household cooks can use on a daily basis.","Snap chef just requires a picture of your fridge, either taken live or previously taken and uploaded. With this, it is able to use Google Gemini to parse the image and create delicious recipe ideas with the data and other parameters specified by the user.",,,"Above all, we are happy to have competed in our first hackathon! Honestly, we're extremely proud that we managed to get the whole thing working and talking to each other and learning entire frameworks on the fly, given our lack of experience in this area. We are very proud of how far we've learned and grown from this project.","Together, we all learned react-native, the Gemini GenerativeAI API, and flask for the first time! We also learned that it might not be the best idea to develop the frontend of a project before the backend.","We hope to incorporate an actual recipe API, such as allrecipes or edamam, so we don't have to rely on Gemini for recipes. Furthermore, we hope to activate a paid plan for Gemini so we can get passed the 5 minute rate limit on requests. With these, it would be possible to put the app on the google play store and app store.","expo.io, flask, gemini, git, github, google-generative-ai-api, python, react-native",https://github.com/ryan-tock/snap-chef
TreeHacks 2025,Eduverse,https://devpost.com/software/eduverse-nzuaqj,False,"Traditional learning lacks interactivity, making engagement difficult. Eduverse leverages AI to create an inclusive, accessible learning experience through speech transcription, natural language processing, and voice synthesis, pushing the boundaries of AI in education.",Eduverse enhances learning with AI-powered assistance for note-taking and interactive studying:,,,,,,"elevenlabs, groq, meta, node.js, oculus, openai, python, quest, three.js",https://github.com/pollyliu1/EDUVerse
TreeHacks 2025,Orbit Cola,https://devpost.com/software/orbit-cola,False,"As aerospace engineers, our experiences have taken us across a variety of commercial space companies. Just last summer, two of us worked directly on orbit determination and satellite operations in commercial space. This firsthand experience exposed us to critical gaps in the collision avoidance pipeline—challenges that create inefficiencies, increase costs, and demand better solutions. Enter Orbit Cola.","Orbit Cola pulls data from the United States Space Command's ""Space-Track"" database, and uses advanced physics simulations to project the satellite trajectories forward in time in order to compute collision likelihood between any two satellites. Our web-based tool visualizes satellite trajectories and depicts collision risk for satellite operators to assess how to best mitigate risk at low costs. More specifically, our pipeline gathers Two-Line Element sets from Space-Track [1] and propagates them using an orbit propagator called SGP4 [2]. It then stores this data in an SQL database hosted by Python Anywhere. From here, the pipeline uses the data to investigate potential collision risks and stores those (along with their collision probabilities) in the database as well. Orbit Cola's website contains many tools useful for satellite operators to look into potential conjunction events, future launches to watch out for, and where current satellites are in space.","The bulk of the work for this project was done in Python. The frontend utilizes HTML/CSS and Plotly Dash. These tools were used to create the interactive UI on the website where users can view projected trajectories of satellites around Earth, their ground tracks, and investigate potentially conjunction events in more detail.
The backend was built on SQL and Python. The general backend pipeline follows the process outlined here:
1) Use Space-Track's API to gather TLEs for all satellites in Low Earth Orbit
2) Propagate the TLEs using dSGP4 (a modified version of the standard SGP4 propagator designed for quicker multiprocessing)
3) Store the propagated states in the database for visualization and further analysis
4) Use custom conjunction analysis algorithms to determine which satellites are at risk of collision, and calculate the probability of collision using the industry standard Pc calculation
5) Present all this information to a user in a data-driven visual format","One of the most prevalent issues in collision warning and avoidance is quantifying uncertainty in satellite motion induced by measurement and modeling inaccuracies. Computing this uncertainty is critical for national defense as effectively tracking foreign objects in space being a highly sensitive topic. For these reasons, the US Space Command doesn't want this information, nor techniques for obtaining this information, readily available. Thus the challenge. We spent a great deal of time addressing this issue and validating the accuracy of our methods by contriving unit tests on artificial collision scenarios. We hope to continue to refine these techniques with time. Another challenge is the rate limits imposed by Space-Track when querying data. Users are only able to pull satellite positional data once every hour. On top of that, the satellite data is only updated by the Space Command every 8 hours. Talk about a bottleneck! This required both patience and resourcefulness to validate portions of the code without this data available, along with imposing extra constraints and considerations to prevent accidental over-querying data and being blocked from the Space-Track database. An additional highly technical challenge is calculating the probability of collision. While it may seem simple at first, the equations utilized to do these calculations require a firm grasp of linear algebra (including projecting a 3x3 matrix onto a 2D plane). Furthermore, the standard model is sensitive to cases with high uncertainty, so developing ways to adjust for that was a challenge in itself. Finally, the biggest challenge was trying to see what kind of features satellite operators would be interested in. Given the limited amount of time we had to develop our product, it was critical for us to look into features that would be highest impact for satellite operators. We had many discussions along the way about which features are worth devoting man-power to, and the sum total of these difficult decisions is the product you see today.","We are most proud of our ability to host this website publicly, accurately depicts the (many!) LEO satellites that crowd our near space environment, and showing the paths these objects take throughout their orbit around Earth. We are also proud that we can trace the ground-tracks of the satellites, depicting their paths over Earth's surface as it rotates about its axis. Additionally, we are very proud of our conjunction analyses, presenting potential collisions to satellite operators in an intuitive, visual format.","We learned how to host a publicly facing website, constructing both front and back ends. Additionally, we had the opportunity to investigate the theory surrounding satellite collision avoidance. Finally, discussions with potential users gave us deep insights into the satellite industry as a whole.",A major component of this business model would be—once collision detection is finalized—to use advanced off-line planning techniques to automate maneuver design for satellite operators. We also wish to provide a platform to seamlessly connect two satellite operators whose satellites are tagged as being at risk of collision. This full pipeline will offload the work of strategic orbit planning and communication via automation.,"dash, python, pythonanywhere, space-track.org, sql","https://www.orbitcola.com, https://github.com/Orbit-Cola/treehacks-2025"
TreeHacks 2025,3Duroam,https://devpost.com/software/3duroam,False,"In a world of advancing technology and shrinking attention spans, education is a field still built largely around passive learning--textbooks, lectures, and coursework that struggle to keep students engaged and passionate about their learning. Our team wanted to reimagine the future of education, a world where learning is a highly personalized and immersive experience. We wanted to allow students to fully exercise their curiosity and make learning feel like an adventure. With the recent developments in generative AI, we felt that endless possibilities were opened up in the field of education and children could benefit from a more fun and visual representation of their coursework. That's why we built 3Duroam, an imaginative educational platform adapted towards each individual user. Our goal is to transform the way students engage with knowledge, moving beyond passive memorization and allowing for active exploration of important topics. We believe that curiosity should be nurtured, not stifled, and that every student deserves an experience tailored to their unique way of learning. By reframing education as a journey of discovery, we aim to inspire the next generation to think creatively and develop a lifelong passion for learning.","Generates immersive VR experiences geared towards education. Imagine learning history as if you were living it—walking through the 1700s alongside George Washington! Picture exploring the rich culture of Korea or uncovering the hidden intricacies of farm operations. With our platform, your imagination becomes reality. By simply entering a prompt, our advanced pipeline—powered by LangChain AI agents, NVIDIA LLMs, Perplexity search, and LumaAI—conducts in-depth research to create a detailed, engaging, and accurate learning experience. Within this immersive world, we integrate ElevenLabs' conversational AI, allowing real-time interactions with a personal AI guide, so you can ask questions and deepen your understanding. A fully personalized, interactive way to explore and learn—wherever your curiosity decides to roam!","For the development process, we started by creating our AI Agent network, as described further in detail in the Rox write up section. With this network, we were able to integrate NVIDIA LLMs, Perplexity Sonar Pro, and Luma AI to transform natural language outputs to detailed AI-generated images. We decided to use these tools since they fit our use case extremely well. The VR environment was developed using Unity and the Meta Quest SDK, with careful attention paid to optimizing performance for mobile VR hardware. We implemented custom shaders and lighting systems to ensure the generated visual content could be rendered efficiently while maintaining high visual quality. The application's architecture was designed around a modular system that allows for easy integration of new educational content and features. Our conversational AI system utilizes ElevenLabs' API, which we enhanced with a custom audio processing pipeline to handle the hardware-specific challenges of the Meta Quest 2. We implemented a real-time audio conversion system in which users could ask questions to get answers—which happens to be great for learning and exploring!","Working with VR was definitely a challenge as most members of our team had never developed on the headset before. It was difficult managing UI elements and navigating some hardware-specific features, especially audio. A big part of our project is the integration with ElevenLabs's conversational AI, but in order to communicate we had to modify our audio input as the Meta Quest 2 headset naturally takes 48000 Hz audio in its mic but ElevenLabs's API was set to process 8000 Hz. We spent a lot of time trying to properly capture audio through the Oculus and convert it so a format that could be picked up and registered by ElevenLabs. We also ran into challenges in the realm of 3d generation. Our idea was very ambitious and we experimented with various different cutting edge models, spending a lot of time training/deploying them. Ultimately, we decided to keep the 3d generative AI as a future alternative as the models we were able to generate took a lot of time and weren't accurate enough to be fully viable in the app.","Our team feels proud of how much we were able to learn this weekend. We set out with a very ambitious goal and were able to come out with a very satisfactory product. Despite many challenges along the way, we were able to adapt and pivot and create something that is meaningful to us. We solved many technical difficulties and were able to persevere when it seemed like some of the things we wanted to create weren't possible. Overall, we feel like we were able to challenge ourselves while still having fun and were able to develop a pretty cool app.","This was a great learning experience on how to learn. Our entire team was mostly unfamiliar with VR software and most had never used it in the past. Throughout this hackathon we had to be efficient in planning out our workflow and learning challenging technologies on the spot. Our team had to navigate the use of novel AI agents, complex generative AI models, and foreign hardware challenges that forced us to adapt and pivot as we built out this project. We were able to grow a lot as developers, learning new skills in 2d and 3d image/asset generation, audio capture and conversion, and AI agent workflows.","Throughout our development, we extensively experimented with various NVIDIA AI models and research papers—from Edify-3D and Edify-360-HDRI to Instant Neural Graphics Primitives and Instant Splat. Our goal was to push the boundaries of generating immersive, personalized worlds. While we found some success leveraging NVIDIA computing, such as training models with Brev to generate 3D meshes, we weren’t fully able to bring these worlds to life. Moving forward, we’re excited to bridge that gap. With AI and computing capabilities advancing rapidly, we've been truly inspired by what’s possible. Our vision for 3Duroam is to develop a seamless pipeline that transforms text into interactive 3D worlds—whether for recreating historical events, simulating real-world locations like Italy or Tokyo, or simply for fun. Ultimately, we aim to be the go-to platform for education and exploration. Let's 3Duroam the universe together!","c#, cuda, elevenlabs, firebase, langchain, lumaai, metaquest, nvidia, perplexity, python, vr",https://github.com/justbustin/TreeHacks25Win
TreeHacks 2025,Operator Optimization,https://devpost.com/software/operator-optimization,False,"Fix: -Error: This happened because the partitioner isolated the linear operation in a subgraph with shape (B*L, C), while the overall model expected (B, C, L). Fix Options: Most of the time, adjusting the Python side is simpler. If I had more time, I would: While I managed to get it all running, there wasn't enough time to work on actual optimizations for the hardware.",,,,,,,"c++, executorch, python, pytorch",https://github.com/kaloca/liquid_treehacks_challenge
TreeHacks 2025,Wander,https://devpost.com/software/forgetable,False," Three years ago,Jone'sgrandmother,Kathy, was diagnosed with dementia. Since then, her symptoms have progressively worsened. She has begun wandering away from home and family gatherings more frequently, and her ability to sleep well at night has significantly declined. Witnessing these challenges inspired us to focus on the often-overlooked struggles of caregivers—typically family members—who dedicate themselves to supporting loved ones with dementia.Our mission is to empower these caregivers by providing them with the tools and resources they need to deliver compassionate, high-quality care while still maintaining their own well-being and fulfilling their personal lives.Together, we can make a difference in the lives of both caregivers and those they care for.","The Wander Bracelet is a cutting-edge cyber-physical system designed to enhance the safety and well-being of individuals with dementia while providing peace of mind to their caregivers. This innovative device offers a suite of powerful features: it tracks the wearer’s GPS location in real-time, monitors their movements, and alerts caregivers instantly if the wearer leaves a designated safe area, such as their home. Leveraging advanced AI technology, the bracelet can gently guide wearers back home by creating personalized audio messages—based on 1-minute recordings by a familiar voice—to help reorient them. The Wander Bracelet also integrates seamlessly with SMS, enabling caregivers to receive location updates, activity reports, and notifications without needing to download additional apps. For added safety, the bracelet is equipped with lights and vibration motors to alert both the wearer and those nearby in case of wandering incidents. Additionally, the SMS text agent provides caregivers with access to advanced analytics, offering valuable insights into the wearer’s patterns and behaviors. With its combination of real-time tracking, AI-driven guidance, and user-friendly communication tools, the Wander Bracelet empowers caregivers to provide compassionate care while ensuring the safety and independence of their loved ones.",,"During the development of our project, we encountered two primary challenges that tested our creativity and focus. One of the biggest hurdles we faced was the size of available technology and the time constraints of the hackathon. Working with bulky, modular components limited the form factor of our product, making it less sleek and user-friendly than we envisioned. However, throughout the design cycle, we gathered valuable feedback from TreeHacks participants—including fellow hackers, mentors, and sponsors—which helped us iterate over several wearable versions to determine the best form factor. These insights will guide future iterations to create a more compact and wearable design.  The problem we aimed to solve is complex and impacts millions of people worldwide. Narrowing down the scope to create a meaningful and impactful project within the hackathon timeframe was a significant challenge. We had to carefully prioritize which features to implement, ensuring that our solution remained focused and achievable while still addressing a critical aspect of the problem. This experience taught us the importance of balancing ambition with practicality to deliver a functional and impactful prototype. These challenges not only pushed us to innovate but also reinforced the value of user feedback and strategic planning in creating solutions that truly make a difference.","We are thrilled to showcase the groundbreaking AI integration we’ve achieved withEleven Labs, which has enabled us to implement a suite of innovative features, includingAutomatic Voice Cloning,Conversational AI, andAI Agents. These cutting-edge tools have allowed us to create a project that sits at the exciting intersection of AI and elder care, offering a truly transformative approach to supporting individuals with dementia and their caregivers. Beyond voice-based AI, our application leverages powerful machine learning algorithms toanalyze and translate sleep and activity data. By utilizing scikit-learn, we’ve developed advanced models trained on sleep pattern datasets from Stanford University, enabling us to uncover meaningful insights into dementia progression and care patterns. This complex data is then distilled into an intuitive, user-friendly interface, empowering caregivers with actionable insights to improve the quality of care. Our work represents a significant step forward in combining AI innovation with compassionate care, and we’re excited about the potential to make a lasting impact on the lives of millions.","One of our greatest takeaways was the immense value of across-functional team. Despite our diverse backgrounds and areas of expertise, each team member’s willingness to contribute and unique knowledge base enabled us to tackle the wide range of challenges that come with developing a cyber-physical product. From technical complexities to business viability and design excellence, our team’s multi-disciplinary approach was instrumental in creating a product that is not only functional but also innovative and user-centric. This collaboration allowed us to deliver a solution with impactful features and a seamless integration of hardware, software, and AI—proving that diverse perspectives are key to building something truly extraordinary.","The Wander team brings together a diverse mix of expertise inbusiness strategy,design consulting, andindustrial systems engineering. With our combined experience and shared passion for innovation, we are confident that the Wander system has the potential to extend far beyond Stanford and create a meaningful impact on aging populations worldwide.Our unique blend of skills and dedication positions us to deliver a solution that not only addresses the challenges of dementia care today but also evolves to meet the needs of tomorrow’s global aging community. ","arduino, elevenlabs, esp32, firebase, mqtt, openai, python, twilio","https://github.com/JonesMays/wander, https://youtu.be/Kpq2egWsZLo"
TreeHacks 2025,AutoLeARn: AR + AI-Based Learning for Personal Car Repairs,https://devpost.com/software/autolearn-ar-ai-based-learning-for-personal-car-repairs,False,"Introduction:Ever had a car issue that you have no idea to fix? You take it into the shop, wait for a few hours, and then get hit with an astronomical bill—often costing hundreds, if not thousands, of dollars—to fix superficial issues. The thing is, you're not alone. Thousands of people around the world face the same frustrations, not only with struggling to identify what is wrong with their cars, but also with being overcharged by auto repair shops for services that could easily be handled on their own. The Problem:A recent survey of 1,000 randomly selected U.S. adults reveals alarming insights about car maintenance habits: 46% of drivers admit they’ve paid for a repair that could have been avoided with better upkeep [1]. It’s mainly because people don’t actually know how to care for their cars (whether that be through regular oil changes, brake replacements, or other maintenance work), leading to excessive spending on repairs that, for many, might be financially out of reach. Customer Insights:See Infographic Above",,,"Due to interface issues between unpredictable LLM output and XR interaction, the feature of having a video ready for any step of the user experience had to be simplified to complete the project within the time frame. Integrating XR with multiple LLM features and video tutorials, due to the unpredictability of LLM response and current material availability, was a challenge. We were faced with limitations of the Meta Quest 3 which did not include full support for features like display image capture. We had to develop workarounds with a special screen capture API. Also, we attempted to implement an AI agent-based system but faced limitations in how we wanted the LLM to use the provided tools. Through many iterations, we realized that incorporating AI agents within our project’s scope and timeline would not be feasible. After reaching this conclusion, we simplified our AI system to a chain of LLMs and VLMs that work together to generate steps for the user to follow.",We are proud of creating a project that fuses the worlds of AR and advanced AI while innovating an impactful product. With a team of hackers with mixed experience we were able to work well together as a team and learn a lot collectively.,"With a variety of experience going in, we all learned different skills and got to test different tools for the first time. Some learned about API interfacing and testing LLMs  such as Groq, Gemini, OpenAI, Deepseek, and Perplexity. Some learned a lot about how development in XR works. We learned a lot about limitations of the technology we work with and implementing innovative workarounds like a special API for image capture on the Meta Quest 3 headset.",,"ar, c#, education, ffmpeg, flask, groq, langchain, llm, perplexity, python, quest, unity, vlm, vr, whisper, xr","https://github.com/zaid-ahmed1/AutoLeARn, https://github.com/Guuzzeji/AutoLeARn-backend"
TreeHacks 2025,Poof!,https://devpost.com/software/poof-3qshno,False,"Going on exchange should be one of the most exciting and fun experiences of a student's life. But too often, the process becomes tedious and stressful, triple-checking eligibility, verifying course equivalencies, and juggling countless open tabs. We hope to simplify this process and allow students to find all of this info in one page.","Our platform lets you search for equivalent courses and compare schools—all in one place. Simply input a course from your home school, select your desired exchange school, and instantly see relevant course matches. Plus, you can bookmark courses to compare entire schools and find the best fit for you!
After you select a few courses that interest you, Poof can help you decide on a final course load, simplifying the painstakingly difficult process of ensuring that the courses are right for you.","We decided pretty quickly that combing through dozens and dozens of pages of information was going to be pretty difficult and we decided to use Perplexity to try to simplify this process. Using Perplexity’s Sonar pro model, we were able to create a few requests that could pull relevant information such as university courses, course descriptions, and the similarity between different courses. Then using Flask, we were then able to create a few apis that could then utilize the Perplexity API calls. We also used MongoDB to store existing queries for courses, courses that the user has bookmarked, and also equivalent courses for previously queried courses. Keeping this in mind, we were able to heavily reduce the number of tokens that we used and we also were able to reduce the loading time significantly by reducing the total queries made. We then used React to build the frontend, sprinkling in features such as login, infinite comparisons, and search by course code.","One of the earliest challenges we ran into was dealing with the Perplexity queries as oftentimes Sonar Pro would hallucinate, resulting in inaccurate information being sent back from the request. We then focused on reducing the size of the queries and making sure that the new segmented queries returned accurate and useful information. Another issue that we ran into was dealing with the increasingly long times of the Perplexity queries. At one point, one of our features had to query 16 different times to Perplexity. A single query would take 16+ seconds to finish and would take far too long to offer a good experience to the user. We then decided to start caching results that our users make, ensuring a quick way to access common queries and we would then be able to quickly return information to ensure a good user experience.","We’re proud of how seamless the user experience is and the accuracy of our results. During testing, our platform consistently delivered results that matched what we would have found through manual research. By combining thoughtful UI decisions with precise and efficient results, we’ve created a product we’d confidently use ourselves.","We learned the importance of properly fleshing out all the features and flows before we started developing the project. While we were in the middle of designing and developing we realized that one of our flows wasn't properly thought out so we had to pivot and change some functions. Although we were aligned at the end and we reached an efficient user flow, it ultimately added more work. Additionally, we learned the importance of strong UI/UX design. A well-designed, approachable interface makes a huge difference—turning what could have been a stressful, information-heavy experience into one that feels intuitive and inviting for users.","From the very beginning, we aimed to build a product that would be applicable to our daily lives - and we hope to continue expanding and making it a diverse platform. We hope to continue to polish and add features that can allow our peers to use this during their own exchange process. We aim to add features like auto drafting an email so that students can easily send them to their advisors and a 3D map which will allow students to get an overall picture of all the available and most compatible schools for them.","flask, perplexity, react",https://github.com/tongliu999/TreeHacks
TreeHacks 2025,Rebase,https://devpost.com/software/rebase,False,"Having worked with large codebases before, we know how overwhelming it can be to navigate and understand them—both for humans and AI. We wanted to create a low-token representation of a codebase that preserves its structure while making it easier to explore. Our goal was to fundamentally change how developers view and interact with code by introducing a hierarchical, AI-powered interface that abstracts away complexity and allows users to drill down only when necessary. We were also inspired by tools like Windsurf, which we love, and wanted to bring that intuitive experience to AI-powered code analysis.",Rebase is an AI-powered code analytics tool that helps developers quickly understand large codebases. It features:,"We focused on designing an intuitive hierarchical interface that enables users to interact with code at different levels of abstraction. Our backend processes code efficiently, generating AI-powered summaries, complexity scores, and visualizations. We also worked on GitHub authentication and integrating real-time analytics into the dashboard.","One of the biggest challenges was handling GitHub authentication—we were stuck on it for a while. Managing dependencies across files efficiently and optimizing AI context window usage were also tricky. Balancing performance with usability was another key challenge, especially when working with large codebases.",,"Throughout this project, we learned a lot about:","This is just the beginning. Moving forward, Rebase can expand to integrate with popular AI software engineering services, provide deeper insights into code structure, and support more advanced AI-driven refactoring suggestions. Our goal is to make navigating and understanding large codebases effortless for developers everywhere.","flask, openai, postgresql, react, windsurf",https://github.com/evnkim/treehacks2025
TreeHacks 2025,EmberScout: Autonomous Rescue for when Humans can't,https://devpost.com/software/quakescout-the-ai-pathfinder-for-saving-lives-in-the-rubble,False,"Third interation of the RC car In the trenches Fifth (and final) iteration of the RC car in progress The workspace Third iteration of the RC car Fifth (and final) iteration of the RC car completed The team Computer vision algorithm Computer vision algorithm Computer vision algorithm In the face of devastating wildfires that recently engulfed parts of Los Angeles, over7,500firefighting and emergency personnel rushed to battle the inferno. The response force swelled as reinforcements arrived from across the nation - Texas alone contributed more than 130 personnel and 45 engines/ambulances, while Oregon and several other states dispatched their bravest. Even international assistance poured in from three countries. Yet despite this massive mobilization, we discovered a haunting reality: the sheer scale of such disasters forces emergency teams to make impossible choices. With the majority of firefighters necessarily focused on containing the spread of fires, fewer resources remain for rescue operations.The tragic consequence?Precious lives are lost to smoke inhalation and burns while trapped in areas that rescue teams simply cannot reach in time. This sobering realization drove us to ask: What if we could multiply the reach of our rescue teamswithoutdividing their resources? What if we could send scouts into dangerous conditions where human rescuers cannot safely go? EmberScout is our answer to this critical challenge - an autonomous RC car built specifically for deployment in disaster zones where traditional rescue methods fall short. But what sets it apart isn't just its ability to navigate treacherous terrain autonomously. We designed EmberScout to operate entirely onedge computing, a crucial feature in disaster scenarios where smoke, embers, and infrastructure damage can render cloud-based systems useless. Our prototype can continue searching for survivors (human body parts) by running advanced detection algorithms locally even when network connections fail. This edge-first approach means EmberScout doesn't need to rely on external GPU systems or cloud services that are often unavailable, guaranteeing usefulness in emergencies such as wildfires. EmberScout's hardware foundation starts with a custom 3D-printed PRL chassis housing a TreeHacks-provided Jetson Orin Nano. By integrating NanoOwl and computer vision processing algorithms, we developed autonomous control systems that enable the RC car to navigate independently and identify people trapped in rubble. This runs entirely on edge to maintain functionality in communication-compromised environments. The system is powered by DC brushless motors controlled via ESP-32, while a Next.js interface driven by v0 enables both real-time demonstration capabilities and potential remote teleoperation - crucial for keeping human operators at a safe distance during active disasters. EmberScout seeks to identify the goal of a human body part to ""identify"" a victim and send a signal to nearby humans to pinpoint the location of a victim. Hardware development at hackathons presents a unique set of challenges, and our experience at TreeHacks drove this point home. First was printing the RC car itself, being able to adjust for the modularity of the 3D print such that we didn't need to run back and forth. In the end, we realized that we needed to bootstrap much of the hardware with tape and hot glue, especially with a design that revolved around our ever-changing electronics. Power management emerged as our most significant challenge. Fitting sufficient power delivery for robust motors onto a compact RC car base required innovative thinking. Our breakthrough came through implementing a 5V voltage regulator to power the Jetson Orin Nano via GPIO pins (Thanks Sarvesh!). Our next breakthrough was scouring around for a powerful battery that could power our motor systems and carry the weight of our Jetson Orin Nano amongst other devices on the system. Ultimately, we found that the real complexity lies inthe interdependence of our software and hardware systems. Building a solution where both elements needed to work in perfect harmony required more than just technical skill - it demanded a crystal-clear vision at every iteration, without losing sight of our overall goal and the time crunch. While working on parallel development pipelines, we needed not only strong product direction but a comprehensive understanding of our complete implementation pipeline before writing a single line of code, ensuring seamless integration between software and hardware components. After this, being able to wire everything correctly and understand hardware that we had never worked with before in a software setting was another difficulty where we parsed through hundreds of documentation pages trying to understand foreign documentation on our electronics. Ultimately, we got everything to integrate together through lots of communication and help from the TreeHacks hardware team and trial-and-error. Currently, the implemented features include computer vision for object avoidance, path-finding, and manual remote control once the human is found. The EmberScout's target can be set to human body parts such as arms or hands so that the car can detect humans partially hidden under rubble. The next step is to have a response protocol when the RC car finds a person in need of rescue. Our goal is to attach a speaker/microphone model to relay conversation with the victim along with a robust flair/pinpointing system such that rescuers can find the victims. The addition of a thermal camera would also aid in finding people, leading to a more effective disaster response. In conclusion, we'd like to thank the amazing TreeHacks team for being so ready to help us out, the PRL, the amazing sponsors, and all the mentors (especially Sarvesh, who stayed up with us until 5 AM to aid our hacking and solve crucial problems with us). We hope that EmberScout can serve as a rough prototype for AI-on-edge devices for disaster recovery and we will continue to iterate on the idea due to our vision of its potential. https://tinyurl.com/rc-car-treehacks25",,,,,,,"arduino, c++, esp, jetson-nano, next.js, nvidia, openai, perplexity, python, v0, vercel, vlm",https://github.com/josiexw/treehacks25
TreeHacks 2025,ZoomPulse,https://devpost.com/software/convocriticpoopify,False,"As meeting hosts and educators, it may be difficult to discern audience engagement and get live feedback from presentations. We want to offer a quick and efficient method for hosts to gauge how well the participants might be digesting their content, such that they can easily reflect on others' experiences to better recognize students' learning styles and adapt teaching approaches to improve quality of learning for more students. As students ourselves, we feel that sometimes it could be uncomfortable providing direct feedback to our instructors. Traditional methods to the feedback cycle often require students to offer feedback at certain time intervals, such as at the middle and end of a course. It can be a struggle to verbalize our thoughts and we exponentially lose recollection of details as time passes. Despite the insights that these methods offer, they can lack accurate representation of all students at relevant time intervals. With ZoomPulse, Zoom hosts can automatically receive aggregated and immediate feedback about their meeting based on participant expressions, post end of the recording.","Once ZoomPulse is running in the background, hosts can run and record their Zoom meetings. After the recording is complete and the host has finished the meeting, ZoomPulse quickly generates the average sentiment of the participants and display it onto the website. ZoomPulse's approach to determining the sentiment involves taking snapshots of the Zoom recording, analyzing each frame for emotions, and taking aggregates of the sentiment to determine overall meeting satisfaction.","ZoomPulse is a web application built on the Flask framework, with Python running in the backend and ReactJS materializing what our users see and interact with. We implemented a webhook to receive events sent by Zoom to our application's endpoint on our server side. The webhook has been developed such that any Zoom meeting video recorded on Cloud will be retrieved after the corresponding meeting ends. We also employed the Python Deepface library to assist in facial attribute analysis in regards to participant emotions throughout the meeting recording.","Overall, our team have gained tremendous experiences with web development and with the features of the Zoom Developer portal.",,,"So much! We see lots of potential features and verticals that ZoomPulse can embark in during the near future. Exploring the use of ZoomPulse in the workplace: In addition to fostering more inclusive, accessible learning experiences for students everywhere, we anticipate ZoomPulse's ability to enhance empathy between people to be valued in general workplaces. Just as student engagement is important in effective teaching, team meetings are most productive when the attendees are engaged and participating as well. ZoomPulse can contribute to a collaborative space that brings more transparency to the team, helping team leads and managers determine areas for improvements, keep track of the team's mental well-being and note tasks for prioritizing.","flask, python, react, zoomapi",https://github.com/alanliu2009/treehack25
TreeHacks 2025,EarlyBird,https://devpost.com/software/early-bird-ujiqvh,False,"EarlyBird Problem Our Solution Agentic Pipeline Supported By Langchain Podcast Generation Page(w/ Voice) Podcast Embedding Space Visualization Early Bird is an AI-powered podcast generation system that curates personalized news content based on your unique interests, all sourced from a dynamically generated 3D embedding graph of current events. By leveraging state-of-the-art technologies like LangChain, Flask, and ElevenLabs, Early Bird ensures that every listener can enjoy a tailored, interactive news experience with the ability to ask questions and engage in real time. Early Bird redefines how we engage with educational content by curating personalized, interactive podcasts. It fosters knowledge sharing and empowers listeners to explore topics at their own pace, contributing to lifelong learning. By leveraging AI agents and vector embedding spaces, Early Bird uses Intersystems IRIS Vector Search to store and retrieve highly relevant content, making it an ideal fit for solutions that utilize advanced data retrieval and GenAI techniques. The dynamic agentic workflow powering Early Bird, from event scraping to text-to-speech transcription, showcases the potential of AI to handle complex tasks autonomously, creating efficient systems that respond to user needs in real time. Early Bird is powered by Perplexity Sonar, which plays a crucial role in gathering the most relevant and up-to-date news for podcast creation. It leverages the search and reasoning capabilities of Perplexity to ensure that the content is always current and insightful. The integration of ElevenLabs's text-to-speech technology enables Early Bird to provide a natural, human-like podcast experience. By transforming AI-generated scripts into seamless, lifelike audio, Early Bird pushes the boundaries of what's possible in AI-driven voice technologies. Early Bird makes exceptional use of LangChain to manage complex workflows involving multiple agents. LangChain serves as the backbone for agent orchestration, ensuring that each step of the process—from content scraping to podcast generation—is automated and seamlessly integrated. Mistral's AI APIs are used to power both the Expert Agent and the Host Agent that generate the podcast scripts. Their low-latency, dynamic response capabilities are central to ensuring a smooth and interactive experience for users. Early Bird uses Elastic for data storage and retrieval, ensuring fast, efficient access to podcast episodes and user preferences. The integration with Elasticsearch enhances the speed and accuracy of content recommendations.",,,,,,,"11labs, chatgpt, flask, mistral, nextjs, perplexity, shadcn",https://github.com/roshanbellary/EarlyBird
TreeHacks 2025,SimuCHATlator,https://devpost.com/software/simu-chat-lator,False,"we've all been there--staring at a text from the girl in your math class, wondering ""will this come across the right way??"". what if there was a way to practice and simulate the nuance of personal relationships? just like dr strange saw 14 million possible outcomes, what if you could freely practice any social situation over and over again until it becomes a reality?","our one-click no-code button exports your entire iMessage chatlog into our fine-tuning pipeline, turning that LLM into someone that textsexactlylike your best friend, boss, partner or romantic interest. rehearse infinite messages in a sandbox, add some context and what you hope to achieve with this individual, whether personal or professional. get your drafted messages scored in real-time, before you hit the 'send' button -- suggesting edits like “add an emoji here—they respond 73% better to the sobbing emoji in awkward situations” or “shorten this paragraph—they usually disengage after 2 sentences.” not sure how to respond to their latest text? get suggested messages powered by the same monte carlo algorithm that powered alphago.",,"parsing imsg data is messy. also generating replies took a long time, but sql/wasm caching works wonders.",,,,"carlo, fastapi, headless, monte, next.js/ts, ollama, python, sqlite, tailwind, uvicorn",https://github.com/lamboenthusiast/sim
TreeHacks 2025,VoteSmart,https://devpost.com/software/votesmart-548672,False,"In 2024, during the General Election, thousands of Santa Clara County residents (including us!) received a ballot and the Santa Clara County Voter Information Guide—a 44-page long rundown of each candidate and ballot measure. Critical to making informed voting decisions, this guide presents lengthy, densely worded explanations of ballot initiatives—many of which have significant implications for local communities. As students who are now voters, we have heard from friends and other adults that their struggles with the voter guide can lead to confusion or distress, a lack of clarity on what to vote for, and decreased enthusiasm for civic engagement and the election process. Furthermore, the phenomenon of “undervoting” can be exacerbated by this confusion. In the 2024 General Election, counties in the United States had up to 9% undervoting on various propositions, from tiny Alpine County in California to the much more sizable Harris County in Texas. In each of these cases, the outcomes of various races may have changed had there been fewer ‘undervoters’. For this project, we have decided to begin our focus right here in Santa Clara County, as we already have a young and inexperienced (and frankly, impatient) voter base yet to be tapped. We also zeroed in on one piece of voting often neglected by ‘undervoters’: ballot measures. These pieces of legislation are key sticking points for voters, and their complexity often leads to disengagement—yet they shape policies that directly impact communities. By addressing this critical gap, we aim to empower voters with the knowledge and confidence needed to navigate ballot measures effectively, recognizing that their participation in these decisions is fundamental to a truly representative democracy.","VoteSmart is an AI companion built to understand a user’s political leanings and assist in the voting process. Our mission is to increase voter awareness with a transparent and comprehensive AI that eliminates the verbose terminology often found in a Voter Handbook. By leveraging AI to cultivate a culture of deeper engagement in the democratic process, we aim to empower voters with greater autonomy while restoring trust among those disillusioned by an often needlessly complex voting system. Once their profile is saved with their political preferences, a user can immediately view their customized ballot, featuring propositions relevant to their city. Most importantly, artificial intelligence tailors ballot measure descriptions to their political leanings, offering a straightforward Yes or No voting recommendation. No more falling prey to circular language, dense jargon, or skipping votes out of frustration. VoteSmart encourages participation while giving users the autonomy to learn more about what their vote really means.","We used TreeHacks' website starter repo as a lifting-off point, then added our own functionality and style. We recognized immediately that a website would be the most accessible manner to reach our users and create an effective tool to serve them. We used Windsurf to fill gaps in our technical skills, and develop a functional base we could build off of. We also drew inspiration from the Depolarizing GPT framework, which seeks to reduce political polarization by offering a multi-perspective approach to political issues.","As first time hackathon attendees, building our own application (from scratch!) was extremely intimidating, and we had to teach ourselves many complex coding concepts as we built our site. Using Windsurf was also a learning curve, as we discovered how to tailor our prompts and find the most efficient ways to use the program, while still incorporating our own technical skills and stylistic choices.",,"We now know how to create a website from scratch, run it locally with Flask, and iteratively improve our UI to provide the best user experience. We also learned about NLTK for language processing, as well as how to use Github to streamline our code commits.","Our plan is to expand our application to counties across the country, include political candidates in addition to propositions, and offer a wider variety of languages. We currently have a version of VoteSmart that uses OpenAI to more comprehensively offer voting recommendations to users, and would like to obtain an API key to fully implement this in the future. It’s all about inclusion, education, and engagement! And until then, we hope you VoteSmart!","babel, codeium, css, depolarizinggpt, flask, html, javascript, nltk, openai, python, windsurf",https://github.com/yaycobparker/VoteSmart.git
TreeHacks 2025,SocratesAI,https://devpost.com/software/socrates-ai,False,"In today’s digital world, misinformation spreads rapidly, influencing critical areas like politics and health. With the exponential growth of online information —both true and misleading— fact-checking has never been more important. AI has revolutionized research, enabling in-depth analysis at a speed and depth impossible for humans. We believe AI should play a crucial role in ensuring discussions remain informed, reliable, and insightful. Inspired by Socrates' pursuit of truth, we built SocratesAI, an AI-powered real-time fact-checking assistant for Zoom calls.",,,,,,,"amazon-s3-bucket, amazon-web-services, chatgpt, elevenlab, fastapi, javascript, langchain, nextjs, openai, perplexity, pydantic, python, realtimemediastream, server-sent-event, tail-wind, uvicorn, websockets, zoom",https://github.com/AshwathKarunakaram/treehacks25
TreeHacks 2025,The Cat,https://devpost.com/software/the-cat-ek4py5,False,"Just a week ago, local news reported on an elderly man who wasn’t found in time by first responders, ultimately leading to his tragic passing. His story hit home—he reminded me of my grandma, someone I always build projects for. With the recent California wildfires adding to the urgency, there had to be a faster way to help in these situations. Why a Cat?","This 8 jointed cat has the dexterity of well... a cat. This means that no matter what kinds of debris, rubble, or housing, The Cat can overcome. With this ability, it runs around burning houses in search for people. After finding them, it sends a summarized report to emergency services on them including their age and medical health so first responders can act more efficiently. It also has a swinging tail to be more visible in fires, a flashlight to improve the quality of reports it creates through smoke, and a gimbal for a head to stabilize it's camera no matter how much damage it takes. It can also converse with people with low latency, giving them instructions like creating a damp rag. On top of this, it is able to use the internet to search for streets to find the nearest fire hydrants so fire fighters know exactly where to go.","Lots of blood, sweat, and tears went into making this—mainly blood though because it kept scratching me. The hardware system is built off of 9 metal gear servos, (4 legs, 1 tail), controlled by an Arduino NANO + PWM Servo Driver, powered by x2 14500 batteries ran through a step down transformer. On the side, there is also a gyroscope which helps stabilizes the head turning it into a gimbal. A majority of the materials are made from scraps like eraser heads for the feet!","Making the cat walk took up more than 12 hours, second only by the design reiterations for the cat. Building without planning proper weight, materials, or knowledge on walking robots, were some of the greatest obstacles I've ever faced at a hackathon. Getting through airport security was also pretty tough with a box full of electronics.",SOLO. This was my first solo in-person hackathon and created one of my most technically advanced projects. I am proud that the walking looks smooth on the cat and the overall functionality met my expectations.,,,"arduino, elevenlabs, erasers, glue, openai, python, sticks, yolo",https://github.com/bulletchicken/ILOVETREEHACKSSS
TreeHacks 2025,Flow~State,https://devpost.com/software/flow-state-1s5qxk,False,"Every morning, millions of people wake up to check their sleep scores, monitor their heart rates, and track their daily steps. We've become a generation obsessed with quantifying our well-being, wearing devices that capture every heartbeat and movement. Yet beneath this endless stream of metrics lies a deeper human truth: we're not just seeking numbers – we're searching for balance, energy, and a genuine sense of wellness in our increasingly demanding lives. We created FlowState because we believe technology should work in harmony with our bodies, not just monitor them. Our breakthrough came from a simple insight: true wellness isn't about collecting more data—it's about understanding the unique patterns of your body and lifestyle, then turning those insights into meaningful change. By combining advanced biometric analysis with real-world behavioral data, FlowState creates a personalized path to optimal physical and mental performance. This isn't just another wellness app. It's your intelligent companion for achieving that elusive state of flow, where stress melts away and everything just clicks. Welcome to the future of personalized wellness.",Flow~State is a personalized wellness platform that helps users improve their health and manage stress through tailored recommendations.,"We developed a comprehensive health and wellness platform by hosting a backend in Flask using Python, which generated all the endpoints our frontend accessed to retrieve data. To assess the user's heart health, we built a Neural Network-based prediction system trained on heart rate metrics from a biomedical PPG dataset, enabling it to predict the probability of a regular or irregular heartbeat. Our backend utilized Perplexity Sonar to analyze a user's biometric data obtained via TerraAPI, combined with historical heart health data from our machine learning model, to create a personalized workout plan tailored to the user's strengths and weaknesses. Additionally, we conducted user surveys with friends and family to identify stress-inducing and stress-relieving activities and supplemented this by scraping Google Calendar data and comparing it with wearable device data to build a comprehensive database of stressors and de-stressors. Using Perplexity Sonar, we analyzed this database to recommend activities and locations that could help alleviate the user's stress. We then integrated Google Gemini and the Google Calendar API to automatically schedule these workouts and stress-relief events into the user's calendar, ensuring a seamless experience. To calculate an overall stress score, we aggregated various metrics from TerraAPI, including Heart Rate Variability, Heart Rate Fluctuations, blood pressure changes, glucose levels, body temperature, and oxygen saturation, providing a holistic evaluation of the user's stress levels. For the frontend, we used SwiftUI to create a modern and intuitive interface that connected to all these backend endpoints, delivering a user-friendly experience.","Juggling between creativity, complexity, and feasibility, we spent a significant amount of time brainstorming and rethinking our idea, which heavily tested our efficient task distribution, critical thinking, and technical ability, to actualize our product in less than one day’s time. We have encountered difficulties while acquiring complete data from API services, integrating components using endpoints to enable all of our desired features, and debugging complex scripts that all make up our complex pipeline. To tackle these issues, we actively communicated with sponsors regarding the status of our project and acquired valuable feedback to improve our project direction. We also ensured that each project member is on top of their task and assists one another when they are stuck on a tricky issue.","We brought together diverse data streams—TerraAPI, PPG data, and Google Calendar—into one cohesive platform, unlocking a new level of personalization. By blending biometric analysis, machine learning, and user behavior insights, we crafted an adaptive wellness solution that evolves with each individual. We designed a seamless, user-friendly experience powered by automated scheduling and a sleek SwiftUI frontend. Through extensive user surveys and data scraping, we built a rich database of stressors and de-stressors, enabling precise, actionable recommendations. The outcome? A transformative tool that elevates physical and mental well-being, helping users unlock their full potential.","We discovered the critical role of data accuracy and validation in biometric and health-related insights. We mastered best practices for handling sensitive data, ensuring compliance with privacy regulations, and building trust. Leveraging advanced machine learning models, like Perplexity Sonar and Gemini, enabled us to deliver hyper-personalized recommendations. Integrating multiple APIs taught us the importance of seamless data flow between backend and frontend systems. We also learned that user feedback became our compass, guiding refinements to functionality and usability.  These lessons are the foundation of our growth and innovation.","Imagine Flow~State as your 24/7 wellness ally, blending advanced biometrics like sleep, nutrition, and stress to deliver personalized insights. It starts as a personal health coach but quickly grows into a community hub, where users share progress, celebrate wins, and inspire each other. With real-time stress detection and instant activity suggestions, Flow~State keeps you balanced and focused, no matter how hectic life gets.","flask, gemini, pandas, perplexity, python, pytorch, swiftui, terraapi","https://github.com/YGao2005/treehacks/, https://github.com/YGao2005/treehacks-ios"
TreeHacks 2025,Klaw,https://devpost.com/software/klaw,False,"Klaw was inspired by the need for more intuitive, hands-free ways to interact with technology, especially for individuals with mobility or speech impairments.  As technology becomes increasingly integrated into daily life, traditional input methods like keyboards and mice are sometimes insufficient for the nuanced interactions that modern applications demand. We wanted to create an accessibility tool that bridges the gap between users and their devices, allowing the former to take a more relaxed, agile approach towards making the most of their laptops.","Klaw enables seamless laptop control through hand gestures, facial expressions, and voice recognition, allowing users to navigate without a keyboard or mouse. It specifically supports features like gesture-based clicking, head-tilt volume control, and real-time speech-to-text captions, making technology more accessible and intuitive for users with mobility and speech impairments.","We used Google MediaPipe for hand and face tracking, OpenCV for image processing, SpeechRecognition for real-time subtitles, Numpy for the joint angle calculations, Pyautogui for the scrolling behavior, and Pygame to render the test interface. The project was written in Python and with the assistance of related libraries. The core of gesture tracking relies on MediaPipe Hands, which detects hand landmarks and extracts the index finger and thumb positions to enable cursor movement and gesture-based clicking. The program continuously reads frames from the webcam using OpenCV, processes them, and updates the cursor position via PyAutoGUI. Pinch detection, which is used for clicking, is implemented by calculating the Euclidean distance between the index fingertip and thumb tip (mostly by way of Numpy functions). For facial expression-based controls, MediaPipe Face Mesh extracts key facial landmarks to detect movements such as head tilting. The head tilt feature actually compares the vertical positions of the left and right ear landmarks to determine the direction of tilt, which then triggers volume up or down commands. Speech recognition runs in a separate thread to prevent lag, utilizing the SpeechRecognition library to capture microphone input and convert spoken words into real-time text subtitles. This text is then rendered using Pygame, ensuring that subtitles update dynamically on-screen while other interactions remain smooth. Also, to prevent unintended rapid gestures or speech inputs, we implemented threshold-based debouncing for clicking and volume control, as well as speech recognition cooldown timers to avoid excessive updates and oversensitive cursor activity.",,,,,"mediapipe, numpy, opencv, pygame, python, speechrecognition",https://github.com/NikhilSura04/KlawTreeHacks2025
TreeHacks 2025,terra,https://devpost.com/software/terra-3kl0mg,False,"In the past decade, online shopping has become a primary way people make purchases. While it offers convenience, it also has significant environmental consequences—including emissions from transportation, excessive plastic packaging, and a focus on convenience over sustainability. Corporations respond to demand, and consumers collectively have the power to influence industries by choosing to support ethical and sustainable brands.
 With technology rapidly evolving, it has the potential to be a powerful force for good. Empowered by AI agent, Terra was created to help consumers make informed, eco-friendly purchasing decisions effortlessly, making it easier to turn small choices into meaningful impact.",Terra is a Chrome extension that:,"We combinedweb scraping, AI-driven text generation, and sustainability data analysisto bring terra to life. Our development process involved:","Building terra posed several challenges, including:",,,"With terra, we're making sustainable shopping effortless—join us to shop greener.","chrome, css, google, html, javascript, json, openai",https://github.com/yuyangp16/terra-chrome-extension.git
TreeHacks 2025,Fhirband,https://devpost.com/software/fhirband,False,"For Angelenos, fires are far from unexpected. Small fires happen, but it was always ""under control."" Along the Eaton Canyon trails, my friends and I would take night hikes all through senior year, reflecting on old memories and speculating about our futures in college. The Palisades and Eaton fires changed everything. The Eaton fire not only set ablaze the mountains and memories I grew up on but devastated the community as a whole: friends without houses, scrambling business-owners, and, mostly unseen, neglected first responders. Growing up next to a fire station, I saw firefighters -- the first line of defense -- during these brutal fires facing dangerous, extreme conditions without immediate health insights and support. Enter the idea for Fhirband, a wearable device that integrates seamlessly with our self-built medical platform, allowing for real-time AI-driven insights and squad-level health analytics. During disasters, firefighters and emergency teams struggled with real-time health monitoring, often pushing their bodies to the limit without immediate feedback on their vitals. In conjunction with our overall mission to bridge testing and treatment for medical devices, Fhirband is a cornerstone to creating an open, interoperable platform to unify health diagnostics.","Fhirband is an AI-powered wearable designed for first responders, providing:","Fhirband was developed using: On the software side, we use our unified IoMT platform, Medibound, for diagnostics, integration, and telehealth to power Fhirband and any new medical devices. (To be clear, Medibound is not our project. Fhirband is our project and it's powered by Medibound. Details regarding Medibound are provided only to explain the platform Fhirband operates on.) By buildingFhirband’s cutting-edge wearable technologywith ourscalable AI-driven platform, we’ve created aseamless, interoperable IoMT ecosystemthat bridges the gap betweendiagnostics, device integration, and clinical decision-making.",,,,,"ai, ble, edge-computing, ehr, fhir, flutterflow, gemini, generativeai, hardware, healthcare, iomt, medtech, nfc, raspberrypi, react, sensors, software, spreadsheet, ux, wearable","https://docs.google.com/presentation/d/1vesDfNkNkMNkPM9Kkp7asq0tyIqyw-bk3XiUeVXNzL8/edit?usp=sharing, https://github.com/medibound/medibound-mobile/tree/flutterflow, https://github.com/medibound/medibound-flutterflow/tree/flutterflow"
TreeHacks 2025,Pet Talks,https://devpost.com/software/pet-talks,False,"    Instead of having a one-sided conversation, imagine having a conversation with your pet, where they speak in human words. Your Disney princess dreams can finally come true! Introducing Pet Talks: have conversations with your pet by detecting gestures, movements, and even sounds. Pet Talks can also detect unusual behavior/gestures that can quickly detect medical problems.
As bunny owners, we find ourselves talking to our bunny every day as if she were a human. Our pets are emotional and sophisticated creatures, and we are always striving to connect with our pets in any way possible. Have you ever looked at your pet and asked yourself: what is going on inside their head? What if… we can vocalize those emotions? Instead of having a one-sided conversation, imagine having a conversation with your pet, where they speak in human words. Introducing Pet Talks, a pet cam that allows you to have conversations with your pet by detecting gestures, movements, and even sounds. Pet Talks can also detect unusual behavior or gestures that can quickly detect medical problems.GI stasis is a common and potentially fatal condition affecting thousands of rabbits daily. Detecting its early signs can be challenging, as changes in their behavior and eating habits often go unnoticed when they're out of sight.
""Pet Talks – Because every hop, twitch, and nibble has a story to tell!"" We started implementing a desktop version, which uses our phone as the webcam and our laptop as the chat tool.
The webcam takes short videos or bursts of photos every time we start typing, and hit enter. Those photos are then fed through the LLM model configured, which generates a medical response or simply a live status update for the bunny. We initially attempted to use the Meta Oculus to implement a live video feed from the webcam. However, the Oculus had many issues starting up the wireless connection.
Also, at first, we asked Gemini to interact with the user as if it were a rabbit, which posed a challenge as the responses seemed too mindless and lacked reliable information. We would prompt, “Are you healthy?” and the bunny would respond, “I am just a cute fluffy bunny. Give me carrots!” This provided no valid information on her health or actions. We began tailoring Gemini to give a more accurate response through better instructions and some anti-jail break methods. However, we were also able to utilize multiple LLM solutions such as Gemini, PerplexityAI, and OpenAI’s models, hinting at a future where the user experience is fully customizable. 
The perplexity API call that we set up to be tailored is really nice since we can gather a detailed report with minimal effort. We learned that LLMs are difficult to effectively finetune, which is why we need to make use of the existing sponsor resources provided to us. We need a balance between cool technology and what is already familiar; since we didn’t want to be focusing too much on making use of technology, but rather let the passion in our idea make itself known.
The presentation is also equally as crucial in our project since that is our user’s first impression, and not all group members have experience DEMOing so we are very thrilled to be able to have fun during this technical challenge. With more time, we plan to create an iOS version in which users can have better accessibility when viewing real-time footage from a remote location. The IOS app will be paired with a 360 camera system with AI detection,  geofencing, and of course, the chat and pet translation feature, and the health check as well. The app will alert users wherever unusual activity occurs, such as when the bunny hasn't been eating for the past 3 hours, the user will be alerted and if this goes on for more than 12, the user will get a GI status alert and description, the user can choose to automatically send the message to their local vet, and book a vet appointment if severe. We also plan to do text-to-speech, and users can hear their pet speak in unique voices.",,,,,,,"openai, perplexity, python",https://github.com/joshua-linsanity/pet-talk
TreeHacks 2025,Meredith,https://devpost.com/software/meredith,False,"Medical residents juggle theimpossible: 80+ hour weeks, nonstop learning, and high-stakes decisions, all while battling fatigue. Our team was inspired by family and friends in residency, sharing stories of 28-hour shifts, only to attend rounds, study for exams, and prep for procedures with barely a moment to breathe. AI assistants like ChatGPT and Claude offer powerful tools, but sending sensitive medical data to the cloud is a non-starter in healthcare. Residents need real support without compromising HIPAA compliance or patient privacy. That’s why we builtMeredith: an AI-powered agent that connects to your Apple Watch, processing everything locally so clinical notes, schedules, and patient data never leave your computer. No cloud uploads. No privacy concerns. Just an intelligent, hands-free partner to help lighten the load of residency, right from your wrist. With Meredith, you’re not just surviving residency.You’re mastering it.","Meredith serves as an intelligent companion for medical residents, accessible through voice commands on their Apple Watch. It streamlines daily tasks by managing schedules, facilitating communication, retrieving patient information, and researching internet sources--all while maintaining strict medical privacy standards. The core functionality includes smart scheduling, automated documentation in standard medical formats (SBAR, admission notes, procedure notes, discharge summaries), instant access to up-to-date medical research, and intelligent task management that adapts to the dynamic nature of hospital work.","Unlike traditional AI assistants that rely on cloud processing, we developed Meredith using an Edge AI architecture so that language model inference occurs locally on our computers. This approach ensures complete patient data privacy and HIPAA compliance--no sensitive information ever leaves the device. The system is powered by an existing open-source fine-tuned model, TinyAgent-7B, that operates directly on the device via a llama.cpp HTTP server, integrating seamlessly with essential APIs: To develop the actual agent, our system builds a DAG (Directed Acyclic Graph) for task planning and execution. When given instructions, Meredith determines the optimal task ordering to build the function calling DAG where each node represents an API (like sending an email, or creating a calendar event) and directed edges represent dependencies between tasks (for example, needing to fetch email addresses before sending a meeting invite). This DAG-based planning ensures tasks are executed in the correct sequence while maximizing parallel execution where possible. Our agentic architecture also enables the flexible integration of new tools and APIs through in-context learning and prompt engineering. On the full stack end, we built our web app using Next.js, Flask, and Python. Our Apple Watch app was built in Xcode using Swift.","Adding custom tooling for the TinyAgent model. The model was originally fine-tuned to pick between 16 different tools, which made adding additional functionality impossible without retraining the last layer of the model. We solved this via prompt engineering and examples so that the LLM could leverage its ICL capabilities to choose the most relevant tools. Guiding the agent to execute more complex workflows was also difficult, as the TinyAgent model often would get stuck if a single API call resulted in an error.",,,"Meredith's potential extends far beyond its current capabilities. Paramedics operate in high-stress environments where every second counts and their hands are often occupied with life-saving procedures. A voice-first application would allow them to maintain situational awareness while documenting critical information and coordinating care. Imagine a paramedic arriving at a trauma scene: they could verbally log their arrival time, document initial patient vitals, and alert the receiving hospital about incoming trauma cases, all while physically attending to the patient. Meredith could transcribe their verbal notes into structured medical documentation, set up automated alerts to the emergency department, and maintain a real-time log of interventions and medications administered. Additionally, many of the potential use cases of Meredith operate in high-stakes medical scenarios, where errors could impact patient care decisions or critical diagnoses. We want Meredith to be as useful as possible, but building robust safety guardrails will be a significant blocker in its adoption by medical residents.","flask, ios, llama-3.3, next.js, perplexity-sonar, python, swift, typescript, xcode",https://github.com/xzrderek/treehacks25/
TreeHacks 2025,CHRONOS - Satellite Powered Disaster Response,https://devpost.com/software/chronos-satellite-powered-disaster-response,False,"Witnessing the recent devastating wildfires in Los Angeles, I was both alarmed and deeply concerned by the widespread impact. These disasters not only threatened the safety of those in the vicinity but also left lasting damage on homes, schools, and entire communities, disrupting lives long after the flames were extinguished. Researching further on the impact of climate events in the past year, I found that “in 2024, there were 27 individual weather and climate disasters with at least $1 billion in damages in the U.S.... These disasters caused at least 568 direct or indirect fatalities... the eighth-highest for these billion-dollar disasters over the last 45 years (1980-2024).” [1] Coming from an aerospace engineering background, I wondered why we were not using satellite imagery to quickly identify and alert first aid responders in these situations. We need not only accuracy, but timeliness for these natural disaster events. Inspired by these challenges, I set out to leverage satellite imagery and AI to provide first responders with real-time, actionable insights—hopefully helping to save lives and mitigate damage.","CHRONOS is a satellite-powered disaster response application designed to support first responders in quickly responding to natural disasters. By analyzing high-resolution satellite imagery, CHRONOS uses a classical ML method of change detection models (PCA & K-means) to rapidly identify land areas affected by wildfires, flooding, earthquakes, and other climate events. When a significant change is detected (enough pixels are found to have changed in the image), a Vision Language Model (VLM) is used to perform inference and generate a caption describing the event. This caption is then fed into Perplexity's Sonar API to provide first responders with further information about the area and affected regions, such as the type of disaster, its location, and potential impact. For instance, during the recent Los Angeles wildfires, CHRONOS could have quickly identified the affected areas and provided first responders with critical information about the fire's progression. Finally, first responders can receive instant notifications for critical events affecting emergency operations during natural disasters via the CHRONOS mobile app.",,,,,"References for Quotes:
[1]https://www.climate.gov/news-features/blogs/beyond-data/2024-active-year-us-billion-dollar-weather-and-climate-disasters","bilp, planetlabs, python, pytorch, vlm",https://github.com/gkim65/CHRONOS
TreeHacks 2025,Aegis Command,https://devpost.com/software/aegis-command,False,"In modern military operations, commanders must make split-second decisions that impact mission success and soldier safety. But with overwhelming data streams, how do we ensure that critical insights are not lost? We envisioned an AI-powered command system that doesn’t just track soldier performance—it analyzes, predicts, and acts in real time.
Our goal was to build something that could enhance combat readiness, prevent injuries, and optimize tactical strategies before risks escalate. That’s how Aegis Command was born—a mission intelligence system that moves from reactive responses to proactive decision-making.","Aegis Command is a real-time AI-powered military operations dashboard that: By combining real-time data, AI-driven insights, and automation, Aegis Command maximizes operational efficiency and soldier safety.","Aegis Command is built using a cutting-edge tech stack, integrating multiple powerful tools: We leveraged Vercel v0 to rapidly prototype and deploy a fully functional, interactive AI-driven military command center in record time.","Building an AI-powered command system in a short timeframe came with several challenges: Despite these challenges, we pushed through to create a high-impact, AI-driven military intelligence platform.","In just a short time, we built a fully operational AI-powered mission command center. Our key achievements: We’re proud to have created a system that goes beyond data tracking—one that actively protects and optimizes soldiers’ well-being in the field.","This project pushed us to the cutting edge of AI-driven mission intelligence. Above all, we learned that technology can save lives—when designed to anticipate risks and act before they happen.",Aegis Command is just the beginning. The future of AI-powered military operations includes: We’re building an AI-driven command center that redefines the future of mission intelligence.,"express.js, lucid, next.js, node.js, openai, perplexity, recharts, shadcn, tailwind, terra, typescript, vercel, websockets","https://v0-new-project-nhhecb7ns8k-ojqdea.vercel.app/, https://github.com/shaashvat01/tree"
TreeHacks 2025,ConnectHer,https://devpost.com/software/connecther-3bnhze,False,"Women often face barriers in networking, mentorship, and career growth due to limited access to supportive communities. We created ConnectHer to foster meaningful connections, mentorship, and skill-building, empowering women at every stage of their journey.","ConnectHer helps women find mentors, connect with peers, join communities, and access valuable resources for personal and professional growth. Users can match with mentors, discover like-minded peers, share experiences, and build lasting support networks.","We developed ConnectHer using React for the front end to ensure a seamless and interactive user experience. The backend is powered by MongoDB, Node.js, Express, Zoom API, and JavaScript. As well as an attempt at Redis database.","We faced challenges with database complexity, integrating the Zoom API (but we got it working—yay!), and using AI for web scraping.","We’re proud of how much we learned, especially as a team new to full-stack development. We successfully tackled key technical challenges and got most of our features working!","We gained experience with databases, storage, and full-stack development. While most of our prior experience was in frontend/UI, this project helped us expand our technical skills.","Integrate Backend with Frontend: Test the API endpoints for user interactions.
User Profiles & Data Storage: How will users input preferences and see recommendations?
Testing & Feedback: Once the matching system is live, test it with real users.","css, html, javascript, mongodb, next.js, node.js, postman, react, zoomapi","https://github.com/aayahosman/TreeHacks-Project.git, https://www.figma.com/design/ck49Gyz99y8R9G5eNm1mht/TreeHacks-App?node-id=13-1443&t=IyWq8WAGR6ZKwyyw-1, https://docs.google.com/presentation/d/1wVYmeJ2DC72Yy3ZlakBttByikF-z_Gzr5hHN0oaZ0X8/edit?usp=sharing"
TreeHacks 2025,Campus Food Truck,https://devpost.com/software/campus-food-truck,False,"Food trucks are a great way for students to enjoy affordable and diverse meals on campus, but they often struggle with visibility due to constantly changing locations. Inspired by the frustration of trying to find our favorite food trucks, we wanted to create a solution that helps both students and food truck owners. By providing a platform for real-time location updates, we aim to bridge the gap between hungry students and mobile vendors.","Our iOS app allows food truck owners to update their live location, making it easy for students to find them on an interactive map. Users can search for nearby food trucks, view their menus, and get notified when their favorite trucks are close by. This improves visibility for food truck owners while enhancing convenience for students.","We developed the app using Swift and SwiftUI for the front end, integrating MapKit for real-time location tracking. Firebase was used for backend services, including authentication and database storage. We also implemented push notifications to alert users when a food truck updates its location. The project followed an agile development process, with iterative testing and feedback loops to refine the user experience.",,,,"We plan to expand the app’s features by adding user reviews, pre-ordering capabilities, and AI-based recommendations based on past preferences. We also aim to partner with more food truck vendors and explore cross-platform development to make the app available on Android.","ios, swift",https://github.com/chenxyuhua/campus-foodtruck-finder
TreeHacks 2025,Super Smash,https://devpost.com/software/super-smash-pixel-edition,False,"We just loved playing this game and were playing it while trying to come up with ideas and decided to just build it ourselves. While going through it, we realized it would be a great fun way for people to track their progress through PT and other","Basically just a reimplementation of the game, that can be played in multiple ways depending on how comfortable you feel using your hands. We have different control layouts for disabilities, and also have reaction timing so you can tell if you're healing from arm injuries.","We used Java and Python to build this, python to help many inputs work properly, and Java for the core game. We used sprites and cut them ourselves to make gifs which worked really well, chose sounds and implemented everything so the game worked really well.","Cutting the sprites to be pixel perfect was really hard, and required finding the perfect blend of images. It was also hard to actually get the game pads to work, which we had to use perplexity to figure out the exact model because we didn't know it and had to search pretty obscurely to figure out input mapping.","We just think the game is very very fluid and actually is fun to play, which is really difficult compared to most games that are just coded up in two days, somehow we managed to nail mechanics pretty well.","We hadn't coded using Java graphics before, and also hadn't used sprites before. Also working with game controllers, and how sprites worked for games.","We tried to add VR, and think that would be so cool, to just be able to have a VR goggle to view the game through and use both controllers even more fun.",java,
TreeHacks 2025,hydRation,https://devpost.com/software/hydration-cr0f4m,False,"Our project enables users to capture a video of their fingers to extract a PPG signal, analyze hydration levels, and notify them if they need to drink more water! Dehydration is a widespread problem, with studies showing that 75% of people are chronically dehydrated (source: National Academies of Sciences, Engineering, and Medicine). Even mild dehydration can impair cognitive performance, causing issues like reduced focus, fatigue, and memory problems. In fact, research has shown that just 2% dehydration can significantly reduce cognitive function, which affects decision-making and concentration. We were inspired to address this issue through our own experiences with dehydration, especially during hackathons where long hours of focus and limited hydration led us to feel drained and unfocused. With our app, we aim to empower users to monitor their hydration levels in real-time, helping prevent these negative effects and promote overall well-being. By providing immediate feedback on hydration status, we make it easier for people to stay hydrated and avoid the health risks associated with dehydration.","Our app helps users monitor their hydration levels in real-time using just their phone’s camera. By analyzing a short video of their fingers, the app extracts aPPG signal(photoplethysmogram), which reflects changes in blood volume that are influenced by hydration status. The process is simple:   With the data collected from the video, we analyze key PPG features that correlate withhydration, and based on this, the app will notify you if you're at risk of dehydration and need to hydrate. By offering this real-time feedback, our app solves a significant problem:hydration management. Research shows that dehydration is a common issue that impacts cognitive performance, focus, and overall well-being. Our solution empowers users to maintain optimal hydration and avoid the physical and mental consequences of dehydration, whether they’re at work, during a workout, or just going about their daily lives.","We built our app using a combination of modern tools and technologies: For our core algorithm, we used theTerra Ultrahuman Ring datasetfromRegular Rhythmsto develop hydration classification based on physiological data. This dataset helped us establish ""physiological"" hydration ranges, which were then used to classify our vision dataset for hydration levels. Below are some graphs displaying our analysis on this data. We appliedrobust signal processing methodslikebandpass filtering,color channel separation, andpeak detectionto extract meaningful information from the video data.  We also collected data by recording videos of our own fingers and friends’ fingers, with the phone camera flash on, to capture clear PPG signals. To verify the accuracy of our heart rate measurements, we compared the signals with heart rate data from Apple Watch users. This approach made our system: (1) Interpretable, (2) Robust to variations, and (3) Efficient. By using these techniques, we ensured that our app could reliably monitor hydration and heart rate withjust a phone camera.",,,,"We want to expand to heart rhythm monitoring and other biomarkers.The core concept of using just aphone camerafor monitoring hydration and heart rate is not only innovative but has the potential to be expanded into a platform foraccessible health monitoring. We plan to build on this by applying the same principles to monitor other critical biomarkers, such asblood oxygen levelsandheart rhythm. Ultimately, we aim to provide an easy, non-invasive way to track cardiovascular health, potentially even predicting conditions like arrhythmia and hypertension, all with nothing more than youriPhone camera. While we’ve made great strides using theTerra datasetto establish our hydration measurement approach, we recognize the need forlarger, more diverse datato truly validate our system. Currently, we’re relying on signal data (not video) and assuming the hydration schema still holds. The next step is to conductlarge-scale data analysisto refine our hydration measurement methodology and prove its accuracy across different demographics and environmental factors. By gathering real-world data from a broad user base, we aim to enhance the reliability of our hydration scoring and expand its applicability to other health markers.","firebase, flutterflow, numpy, opencv, python, scipy, terra","https://hydration.flutterflow.app/, https://github.com/janetguo/HydRation, https://github.com/ithons/HydRation_2"
TreeHacks 2025,Omnis,https://devpost.com/software/omnis-gb4zfj,False,"As students ourselves who have taken a breadth of classes, we realize that each class uses its own organization system and platforms. Some classes upload materials in three different places with lecture videos and supplemental videos in another place, whereas other classes keep everything in one large folder. Being able to track how each class operates is difficult, and we dont want that to impede a student's learning process. We wanted to create a smarter way to study—one that goes beyond static notes and videos. Keeping track of lecture materials and truly understanding concepts can be overwhelming, so we built Omnis to act as an interactive AI tutor, making learning more intuitive, organized, and personalized.","Our tool allows you to keep everything in one place and also uses LLMs to parse this information so you can actually interact with it! Omnis allows students to upload lecture slides, notes, videos, and homework into a single system where they can chat with an AI assistant trained on their own materials. Users can ask questions, get clarifications, and explore concepts with references directly from their coursework, creating a personalized and centralized learning experience.","We used InterSystems' IRIS Vector Search to embed the documents and information so that users can search and interact with it. We used ReactJS for our frontend, we run our own server with Python Flask, and we use Firebase for Cloud Storage and Authentication. We use OpenAI's language models for reasoning and generation. We built a pipeline to extract information from PDFs, videos, and notes into text embeddings, thus allowing the AI to generate responses grounded in user-provided content. A chat interface lets students interact seamlessly with the AI for real-time assistance. We also provide references back to the original documents that the user uploaded, specific to the timestamp and sentence so that the user can see the contextual grounding.",Chunking and Context limits. Ensuring AI-generated responses remain grounded in provided materials and don’t hallucinate.,"Increasing generality of our tool with different file types and platform integrations. Implementing a robust retrieval system that provides accurate, source-backed answers. Synthesizing all sources into a knowledge base and a clean and easy-to-use AI!","Utilizing IRIS vector search
Learned about new lecture content through Omnis!","Integrating more streams for class content, then deploying for students to use!","firebase, iris, javascript, openai, python, react",https://github.com/adityatadimeti/omnis
TreeHacks 2025,Ruminate,https://devpost.com/software/ruminate-t764yw,False,"Landing Page Specify the focus of the rumination Rumination automatically highlights key phrases Ask questions about any part of the document! (LaTeX rendering for block content) Fullstack App Architecture Ever found yourself drowning in complex PDFs? Whether it's academic papers, technical documentation, or legal texts, truly understanding dense material requires significant time and expertise. While traditional AI tools simply summarize content, I wanted to create something more powerful: anintelligent reading companion that actively engages with the text alongside you. Inspired by modern AI-powered IDEs like Cursor, I builtRuminate- an AI agent that doesn't just read the text, but comprehends it, explains it, and helps you navigate it in real-time. Think of it as having a knowledgeable colleague reading with you, highlighting important points and explaining complex concepts as you go. Adaptive AI Reading Agent– Upload any PDF and specify your learning objective (e.g., ""Explain technical jargon"", ""Focus on mathematical concepts"", ""Identify key research findings""). Ruminate processes the document block-by-block, adapting its analysis to your needs. Dynamic Interactive Experience– Watch as Ruminate works through the document in real-time, highlighting key phrases and generating contextual explanations. Each highlighted section becomes an interactive element you can click to dive deeper. Context-Aware Chat Interface– Unlike traditional PDF readers where you need to copy-paste text to ask questions, Ruminate always knows what section you're reading. The chat interface provides targeted explanations and allows for natural follow-up questions about the current content. Intelligent Block Processing– Ruminate handles various content types intelligently, including LaTeX equations, tables, and figures, ensuring comprehensive understanding of technical documents. Advanced Document Processing Real-Time AI Architecture Interactive Frontend Backend Intelligence Agent Tool Use Real-Time Processing UX– Balancing immediate feedback with thorough analysis required careful architecture design. I implemented a progressive processing system that shows results as they're generated while maintaining context. Context Management– Traditional LLMs struggle with document-length context. I developed a custom conversation memory system that maintains understanding across blocks while leveraging LLM API prompt caching to optimize cost and latency. Technical Content Handling– Rendering and processing mathematical equations and technical diagrams required specialized handling. Since many research papers include mathematical notation, I implemented LaTeX support and custom rendering. Annotation Precision– Matching AI insights to exact document locations needed both precise prompt engineering and robust string matching algorithms. Additional Tool Integration Extended Output Formats Collaborative Features Voice Support",,,,,,,"asyncio, fastapi, gemini, gpt-4o, marker, mathjax, nextjs, postgresql, python",https://github.com/vivekvajipey/treehacks-2025
TreeHacks 2025,Buzzy,https://devpost.com/software/buzzy-in8s35,False,"As students ourselves, we’ve both struggled with creating study schedules that actually work, ones that fit around our commitments while helping us retain information effectively. Too often, we’ve spent more time figuring out what to study than actually studying. We realized we weren’t alone. To better understand this problem, we developed Sophia, a user persona representing the typical student facing these challenges. Sophia is a freshman studying Computer Science, balancing CS106C, lacrosse practice, and a packed schedule. Like many students, she struggles to juggle coursework, extracurriculars, and social life while trying to study efficiently. Traditional planning methods are static, time-consuming, and fail to adapt to her learning needs, leaving her overwhelmed and stuck in an ineffective cycle of cramming. Research backs up Sophia’s struggles—poor time management increases academic stress and lowers performance. Yet, there’s no widely accessible tool that seamlessly integrates into students’ lives while optimizing their learning. That’s what inspired us to build Buzzy—a personalized, AI-powered study planner that removes the stress of planning and helps students focus on what matters: learning smarter, not harder.","Buzzy is an AI-powered study planner that dynamically generates personalized, research-backed study schedules. It seamlessly integrates with Google Calendar, analyzes coursework materials (syllabus, exams, problem sets), and tailors study plans based on both general coursework and specific weak areas. By aligning study sessions with scientific techniques like spaced repetition and active recall, Buzzy helps students retain information more effectively. With adaptive scheduling and a stress-free planning experience, students can focus on what matters: actually learning.","We started with user research, developing our persona Sophia to deeply understand the challenges students face with study planning. To ensure our solution was science-backed, we reviewed cognitive psychology research, identifying active recall and spaced repetition as essential techniques for retention and long-term learning. These insights shaped our AI-powered study schedules, which dynamically adapt to individual learning needs. With a clear problem statement, we mapped out a sample onboarding user flow through whiteboarding, refining how users would interact with Buzzy from the start. We then collaboratively designed mockups in Figma and FigJam, iterating from low-fidelity wireframes to mid/high-fidelity prototypes to perfect the user experience. Moving into development, we explored Flutterflow for the frontend, leveraging its AI-driven tools like Sketch to Component to streamline the transition from our designs to a functional interface. Our focus remained on aligning the UI in Flutterflow with our Figma prototypes, ensuring a polished and intuitive experience that stayed true to our initial vision.",,,,,"figma, flutterflow",
TreeHacks 2025,FitStreak,https://devpost.com/software/name-iskuvt,False,"FitStreak is inspired by the challenge of maintaining long-term fitness habits. While many fitness apps focus solely on tracking metrics, we recognized that building and maintaining streaks can be a powerful motivator for consistent exercise and healthy habits. The gamification of fitness through streaks, combined with AI-powered insights, creates a unique approach to health and wellness.","FitStreak is a comprehensive fitness tracking and motivation platform that: Connects with wearable devices to track daily activities, sleep patterns, and body composition
Uses AI to analyze fitness data and provide personalized insights and recommendations
Implements a streak-based motivation system that encourages daily goal completion
Tracks multiple fitness metrics including distance, time, and calories burned
Provides detailed analysis of sleep patterns and body composition
Offers a clean, modern interface for goal setting and progress monitoring","FitStreak is built using a modern tech stack: Frontend: React.js with Redux for state management
Backend: Python Flask server
Integration with Terra API for fitness data collection
OpenAI integration for intelligent fitness analysis
Firebase for authentication and user management
Real-time streak tracking and goal monitoring system
Responsive design for both desktop and mobile use","Implementing accurate streak tracking that accounts for timezone differences and edge cases
Integrating multiple data sources (Garmin, sleep data, body composition) into a cohesive analysis
Creating meaningful AI-powered insights that are both accurate and actionable
Ensuring real-time synchronization between fitness data and streak updates
Building a reliable authentication system that maintains user privacy","Successfully created an AI-powered fitness analysis system that provides personalized insights
Implemented a robust streak tracking system that motivates users
Built a clean and intuitive user interface that makes fitness tracking enjoyable
Developed a scalable architecture that can handle multiple users and data sources","Integration of multiple APIs (Terra, OpenAI) in a production environment
Best practices for handling real-time fitness data
Techniques for effective state management in a complex React application
Methods for secure user authentication and data privacy
Strategies for processing and analyzing fitness metrics effectively","Integration with additional fitness devices and platforms
Agentic workout recommendations based on user progress
Social features to allow users to compete and share achievements
More detailed analytics and progress visualization tools
Mobile app development for iOS and Android
Implementation of personalized coaching features
Integration with nutrition tracking platforms","firebase, flask, flask-cors, openai, python, react, terraapi",https://github.com/leojia22/Treehacks25.git
TreeHacks 2025,Dashify,https://devpost.com/software/dashify,False,"At work, visual dashboards are essential but tedious to create. Many engineers spend significant time building and maintaining internal dashboards, manually assembling components and struggling to determine the most relevant visualizations.
Existing solutions are static, requiring users to recreate all visualizations whenever the data changes and manually fine-tune charts. 
We wanted to build a tool that simplifies this process—one that dynamically generates interactive dashboards from a Google Sheet, making data visualization effortless and adaptable.","Dashify transforms a Google Sheet into an interactive, customizable dashboard. Users provide a link to their sheet and the tool:","We started by identifying gaps in existing spreadsheet tools and brainstorming a solution to streamline data visualization. Our development process was guided by the expected user flow. Using Vercel’s v0, we created the initial app template and then integrated Google Sheets. We integrated with the OpenAI API allowing us to generate visualizations and reason through a user's data on the fly. Using Recharts, we rendered charts that update in real time as data changes.","One of our biggest challenges was managing the variability of LLM-generated output when dealing with highly diverse spreadsheet data. Enforcing a strict structured format was too rigid, while allowing open-ended responses led to excessive unpredictability. Striking the right balance required careful prompt engineering and iterative refinement. We also faced difficulties in identifying a clear gap in existing Google Sheets tools that our solution could uniquely address. Additionally, defining the project scope was challenging, as there were many potential directions we could explore.","We are proud of coming up with an idea that addresses a common problem we all experience. We successfully built an easy-to-use user interface that makes interacting with the dashboard simple. We implemented a sync functionality that ensures the dashboard is updated on click when data changes. Finally, we were able to figure out how to get the LLM to reason over data, which was a complex challenge but a rewarding accomplishment.",,,"flask, google-sheets-api, next.js, openai, recharts, shadcn, tailwind, typescript, vercel",https://github.com/halliefaust/sheet2dash
TreeHacks 2025,mechanic,https://devpost.com/software/mechanic,False,"Inspired by friends & personal work experiences. Friends would complain at their work that code was unoptimized, leading to shit piling on top of shit. Personal experience would be that startup culture prefer bootstrapping over optimization (for good reason & proof-of-concept). Why waste your time doing code review and nitpicking at all the files? Layman’s terms, we’re amechanicthat constantly maintains your car to see if there are any problems of any sort that needs attention. Technically,mechanicconstantly reviews over your commits within PR's to suggest optimizations, apply code changes, and merge suggestions with fleshed documentation. Kind of like if you had an optimization engineer on 24/7. Using it is really easy -- install it in your repo, and it'll run every time you commit within a PR. It'll also make informative comments, where you can branch the suggestions that it makes and merge with documentation if you like it (and test it...) I built it using Rust, Axum, Groq, and GraphQL. We used Perplexity, Warp, Zed, Arc and Raycast as our development tools. Executing this project was extremely informative (&frustrating) to me as an engineer — it definitely showed me the advantages and disadvantages of Rust as a web server and its' conjunction with GitHub apps. I'm proud overall to make an app that my internal will use, and my friends will try. GitHubYouTube",,,,,,,"axum, github, graphql, groq, rest, rust, warp",https://github.com/vznh/th25
TreeHacks 2025,gitRizz.ai,https://devpost.com/software/gitrizz-ai,False,"We realized that a person’s GitHub says more about them than any dating profile ever could—commit history, README vibes, even that one cursed 2 AM push. So why not let the code do the talking? With gitRizz.ai, we’re turning repositories into relationships, because nothing says “soulmate” like burnout and a mutual obsession with clean code ❤️","gitRizz.ai is a Hinge-style matchmaking platform that curates GitHub profiles to find the most optimal matches - all you have to do is login with Github! We generate vector scores for each user, combining numerical metrics (commits, contributions, PRs) with contextual analysis (commit messages, READMEs, bios, profiles) to get a well-rounded view of their personality from both their technical contributions and textual expressions of themselves. Using vector search, we calculate similarity scores between users, ensuring meaningful connections. Lastly, we used OpenAI to generate personalized feedback for each profile since a lot of these Github profiles can have a lot of information all at once. If two users mutually swipe right, they can chat and collaborate—professionally or romantically!","Backend: We used the Github API and beautiful-soup to scrape most public data from a user’s profile—including their picture, READMEs, bio, and so on. We consolidate all thequantitativecontributions of the user (number of contributions, commits, pull requests, PRs, etc.) as well as an analysis of theirtextualcontributions such as the commit messages, READMEs, and PR content. Using OpenAI API, we are able to generate an 6-dimensional vector representing their characteristics ranging from commitment to collaboration style. Finally, using a cosine-similarity matching algorithm, we are able to generate an list of potential matches to show a specific user. Frontend: Firebase + Github authentication, and from there, it was all React + JavaScript + Flask.","For future scalability, it is important to minimize expensive API calls. At first, our output was generated way too slowly as calls were made repeatedly. Instead, by caching users vectors, we are able to reduce computation significantly. In the future, we would likely be able to utilize an ML algorithm that can perhaps avoid such calls for certain users all together and instead infer potential matches more indirectly (ex: A and B are a good match and A and C are a good match, so maybe B and C are a good match.) Also, deciding what metrics to hone in on was a challenge, one that we are still working on.","We wanted the UI to highlight just how much of the work we do for you - after all, all you do is login with Github and we calculate everything from there. Thus, the interface had to be simplistic, modern, yet easy to understand and use when it came to finding matches and prospective candidates. Mapping this out and narrowing down on a design was difficult but worthwile. Also, we’re proud of simply ideating this project, which is the perfect balance of silly and serious. Beyond classes and random cool hackathon encounters, there really isn’t a systematic tool for meeting devs with a similar vibe as you!","Designing for scale from the beginning is important! Caching user data from the start would’ve made life so much easier. Also, sometimes, the best ideas come from silliness—this was inspired by Valentine’s Day!","Adding more metrics on which to match people on.
Adding more text and video prompts.
Allow users to filter by platonic or romantic, sexuality, distance, etc.
Reward and boost users who contribute to open-source repos, are rated well by peers, etc.
Building out the chat + post-match interface. We wanted to focus on profile curation + statistics, so didn't implement this portion for the hackathon.","beautiful-soup, elasticsearch, flask, github, kibana, lucide, node.js, openai, python",https://github.com/aliceli465/treehacks25
TreeHacks 2025,Kaleidoscope,https://devpost.com/software/kaleidoscope-ewg9rc,False,"I was inspired by a friend who was working on his own startup, and he would always complain to me about how difficult it was to test public interest in his project. I figured that many companies would also be in the same situation. To me, this seemed like the perfect place to try and use AI to fix this issue.","Instead of spending many weeks/months and hundreds of thousands of dollars testing different ads and messaging on real customers, use Kaleidoscope’s simulated customers instead. By comparing your existing customer tracking data (from Google Analytics, Shopify, Stripe) and your historical customer interactions (social media posts, TV ads, news articles, customer support calls, product reviews), my product learns how different segments of customers have historically responded to specific messaging and content. Given your business goals, it then creates swaths of AI agents and conducts controlled trials with them to find the most effective way for you to message your audience.
Even household name brands find themselves short on marketing research spend and make disastrous PR blunders. Kaleidoscope catches them before the wrong person notices.","Built with NextJS, Prisma as ORM, AWS RDS as database, using fine tuned OpenAI as our AI model, and traditional machine learning techniques for causal inference and AB testing.","As someone not very experienced with hackathons and also working alone on this project, I struggled a lot with getting familiar with some new technology. Connecting all the components of the project together also took a fair amount of time. I also obviously didn't have real data from marketing campaigns to fill the databases with, so creating sample entries for those for the purposes of demonstration was a little difficult.",I'm mostly proud to have been able to get a finished demo of the project that I worked on. A specific part of the project I was proud of was the recommendation section that suggested improvements to marketing campaigns.,I learned a lot about what it takes to create an actual product in a short amount of time under pressure and how to best divide up my time in situations like this. I obviously also became much more familiar with all the technologies involved.,"I want to reach out to various companies and see if they would be interested in a product like this. If so, I believe that it could be much more fleshed out with time and more people working on this project.","amazon-rds-relational-database-service, nextjs, openai, prisma",
TreeHacks 2025,LinguaVista - Language & Cultural Learning Platform,https://devpost.com/software/linguavista-language-cultural-learning-platform,False,"A user chatting with LinguaVista's edge AI agent about French culture A user is greeted by the LinguaVista home page with options to select a subject to learn about A user can customize their profile to focus on certain subjects, interests, or strengths Users can view notes about their previously logged conversations so they can keep track of their learning. Diagram of each component of the LinguaVista app Despite a digitally-centered world that is the most connected in all of human history, many students, communities, and schools across the world still lack access to the internet. And with research as well as people claiming visual learning is beneficial for those learning complex subjects and engaging with culture, limited access to information, especially lots of visual aids and resources on the internet, makes education that relies on verbal communication challenging. The benefits of remote education tools were clearly illustrated as the world faced a global pandemic, forcing schools to push resources online, and further hurting communities that lack digital connection, and as the world continues to further become digital-centric, more solutions are needed to ensure future generations are educated no matter their location on the globe. A US elementary school teacher known by the LinguaVista team expressed her desire for more opportunities in the classroom to show students visuals and experiences for them to learn about rich subjects like science, social studies, and language arts (English as well as other languages). She specifically cited the lack of opportunities (i.e. field trips) for elementary students to receive authentic experiences of the world, the people, and the ideas around them. She also mentioned how hard it is to implement solutions in the classroom due to certain families not being able to participate at home without access to the internet. According to the U.S. Department of Education's Journal of Education and Practice, frameworks such as Problem-Based Learning (PBL) and Higher-Order Thinking (HOT) have demonstrated the need and effectiveness for lessons and curriculum involving visual aids. It claims ""learners understand information better in the classroom when they see it...such as images, flowcharts, diagrams, video, simulations, graphs, cartoons, coloring books, slide shows/PowerPoint decks, posters, movies, games, and flash cards (Kohnke, Lucas)."" This research urges the use of such resources, of which it may not be possible for educational institutions to implement, either due to technical complexity, limited budgets, or other reasons. And if the research claims visual aids work in the classroom, it can be assumed for individuals wanting to learn subjects individually, information is better obtained visually for them as well.
To solve this divide between the need for global education and the many populations without connectivity, a solution requires other forms of technology to bridge the resource gap. With the rise and improvement of generative artificial intelligence performance and accuracy, the idea of placing models on bare metal or on physical devices enables the ability to create a source of knowledge offline from network connectivity, including image generation and two-way conversation between a person and an AI model. Our team was inspired to create LinguaVista to maximize the adoption of digital, visual, and experiential learning tools across the world via the breakthrough technology of Edge AI. LinguaVista ensures no internet connection is required to learn more about our complex world, including ways to enjoy powerful conversations about subject matter. LinguaVista connects people with experiences and conversation through an all-in-one Edge AI application. Users select a subject, search for a concept they are curious about—such as a new language, culture, geography, or scientific idea—and connect with one of three options: a real person fluent in the language, an expert available online, or a generative text-to-speech and image AI model hosted on their device, which operates completely offline and independently of external services. Users can have conversations with either type of interaction, either to practice language fluency or learn more about their subject and then conclude them with AI summaries, generated visuals matching the conversation, and saved conversation notes to their profile. 🟣 Edge AI 🟣 LinguaVista boasts the ability to utilize AI all on the local application–no internet necessary–thanks to Edge AI. Our stack includes a Whisper model that transcribes spoken input into text. This text is then processed by a LLaMa3 model, which generates accurate and contextually relevant responses. To enhance user experience, a Coqui model converts these responses back into speech, enabling a fluid and natural conversation. Additionally, Stable Diffusion 3 generates AI-powered visuals that complement discussions, reinforcing learning through visual representation. By combining speech, text, and image generation, LinguaVista ensures a highly engaging and effective AI-driven educational platform, all while maintaining the capability to function offline and independently of external services. 🔴  Zoom 🔴 LinguaVista integrates Zoom's SDK to facilitate seamless video and audio communication, enhancing its AI-driven conversational learning experience. After a user initiates a query, a chat interface powered by Zoom enables real-time, two-way interaction with the AI. The platform leverages Zoom’s RTMS server to process incoming audio and video packets, which is utilized by the AI stack to generate feedback to the user providing an interactive and engaging user experience. ⚫ Education ⚫ LinguaVista’s ability to operate as a standalone desktop application through Electron and React make it an exceptionally accessible and inclusive educational tool, aligning with its mission to provide knowledge and learning opportunities to individuals without reliable internet access. ensures that users can engage in interactive conversations, speech-based learning, and visual-enhanced education without depending on external or cloud-based services. This independence from the internet makes LinguaVista an ideal solution for rural communities, remote learners, and individuals in low-connectivity regions, empowering users with high-quality, AI-driven education anytime, anywhere. We want to grow our user base and eventually support the ability for real people to chat with other real people! Another effective way to learn is by experimenting, such as practicing a language with a fluent speaker, asking experts questions, and having real conversations with great people. And what more effective way to learn than from someone in the place in question or with knowledge on the subject. Our mission for LinguaVista is to connect the world in ways not initially thought could be possible by the tech world, and with the opportunities in communication technology and Edge AI, we can leverage these incredible computing feats to educate our world for decades to come. Kohnke, Lucas. ""Edmodo: A Tool for Language Teaching and Learning.""TESL-EJ, vol. 20, no. 3, Nov. 2016, pp. 1-8. ERIC,https://files.eric.ed.gov/fulltext/EJ1112894.pdf.",,,,,,,"coquitts, electron, express.js, insanely-fast-whisper, llama3, ollama, python, react, stablediffusion3, typescript, zoomrtms",https://github.com/hcshires/LinguaVista
TreeHacks 2025,Matcha Vibe,https://devpost.com/software/matchavibe,False,"Sen no Rikyu, the founder of the Japanese tea ceremony, explained its philosophy with this phrase:一期一会 (ichi-go ichi-e): one time – one meeting. Life is aboutseizing every moment.Business pitches, lectures, and personal conversations are all fleeting—so make them count. The Problem:Remote meetings lack presence and depth—listeners get distracted, engagement is low, and tracking audience feedback is difficult. If you cannot perceive your audience's status, your meetings are left sub-par. Inspired by the Japanese tea ceremony philosophy, we set out to enhance real-time engagement in virtual meetings: The Solution:AI-driven avatars and immersive environments that make online meetings not just as good as in-person, but even better. Matcha and meditation apps likeOtsuka ekkomibrought premium meditative experiences into the digital realm. We asked ourselves: How can we take this further and enable meaningful, premium interactions to be held online? We focused on the educational and healthcare benefits of AI companionship and metrics, and hence redefined virtual meetings to be more dynamic, engaging, and memorable. Thus, we came up with ourreal-time meeting analytics toolsand responsiveinteractive experiencewe call Matcha Vibe. We create meetings that match your vibe. Everyone does Zoom meetings. We turn Zoom meetings intopersonalized experiences tailored to real-time consumer feedback. Why This Matters",,,,,,,"docker, elevenlabs, gemini, groq, javascript, luma, lumaai, node.js, websockets, whisper, zoom, zoom-api",https://github.com/treehacks2025/mockRTMSserver
TreeHacks 2025,Anim-Education,https://devpost.com/software/immersive-ed,False,"As a student with ADHD, one of our teammates often finds it challenging to focus during lectures, making it difficult to absorb and retain information effectively. Traditional learning environments are not always designed to accommodate neurodivergent students, which can lead to frustration and disengagement. Recognizing this struggle, we wanted to create a tool that empowers students like us to not only pay attention but also stay actively engaged with the material in a way that suits our learning styles. Our goal is to enhance comprehension, retention, and overall academic success by providing an aid that transforms lectures into interactive and accessible learning experiences.","We developed an application that enhances lecture comprehension by generating real-time animations that visually reinforce the professor’s words. Unlike static slides, which can sometimes feel disengaging or overwhelming, our dynamic visuals provide an interactive and intuitive way to grasp complex concepts. By bridging the gap between auditory learning and visual understanding, our tool helps students stay focused, absorb information more effectively, and retain key concepts with greater clarity.","We utilized Perplexity Deep Research to strategize the development of generative animations and streamline our coding process. This involved conducting in-depth research on animation techniques, machine learning models, and optimization strategies to ensure smooth and efficient rendering. By leveraging OpenAI models and Groq, we were able to generate highly fluid and realistic animations with seamless transitions between frames, reducing visual artifacts and stuttering. Using natural language processing via the Groq API, the system analyzes incoming text in real-time, extracting relevant animation-worthy topics. When a concept is detected, the API dynamically generates Manim code to illustrate it. The animation pipeline operates asynchronously, leveraging a task queue to render animations efficiently while caching previously generated videos to optimize performance. We implemented multithreading, allowing different components of the animation pipeline—such as data structure animation rendering, and AI-driven code generation—to run concurrently. This significantly improved processing efficiency, reducing latency and ensuring that animations could run in real-time without noticeable delays. Additionally, we optimized buffer management techniques to minimize lag, enabling near-instantaneous transitions between animation states. Through this approach, we achieved high-quality, dynamic animations that responded quickly to real-time inputs while maintaining computational efficiency. Built with scalability in mind, it supports multiple concurrent WebSocket connections, allowing interactive applications to integrate it for real-time visualization. A caching mechanism also reduces redundant processing by reusing animations for previously encountered concepts. On the Unity side, we focused on setting up a robust real-time motion synchronization (RTMS) system to ensure responsiveness and accuracy in animation rendering. We also successfully created our Unity environment, carefully configuring assets, physics, and rendering settings to support high-performance generative animations. We worked on connecting Zoom RTMS to the Meta Quest and converting the live transcript from Zoom meetings to real time captions that scroll to avoid overflow.","We initially tried to make this integrate into a VR environment using the Zoom SDK but were unable to put it together. Meta Quest could not integrate mp3 audio files, which were essential for the video processing, making creating an integrated VR environment highly difficult.",Created a self-contained generative animation agent that is ultra-efficient and performs in realtime in VR using Groq and OpenAI.,"Integrating VR into our application presented significant challenges, from optimizing performance to ensuring seamless real-time interactions. However, through this process, we gained a deeper understanding of the complexities involved in VR development. Most of us had zero experience with Unity prior to this hackathon, so tackling the steep learning curve was challenging but deeply rewarding. Additionally, we discovered the immense potential of generative animations in education—these dynamic visuals have the power to revolutionize learning by making complex concepts more intuitive and engaging. This experience reinforced our belief that interactive and adaptive technologies are the future of education.","Our next step is to fully integrate Immersive-Ed into VR, creating an even more engaging and interactive learning experience. This will involve researching the best VR platforms, refining our real-time animation system for immersive environments, and overcoming the technical challenges of seamless integration. We aim to enhance accessibility and adaptability, ensuring that students can benefit from a truly immersive educational tool that caters to diverse learning styles.","c#, docker, fastapi, groq, manim, openai, python, unity","https://github.com/msteele3/treehacksdockerdeploy, https://github.com/Benler123/TreeHacks"
TreeHacks 2025,Verifact,https://devpost.com/software/verifact,False,"In an era where misinformation spreads rapidly across social media and news platforms, we recognized the critical need for accessible fact-checking tools. The challenge of manually verifying information inspired us to create Verifact – a tool that empowers users to quickly verify claims and make informed decisions about the information they consume.","Verifact is an AI-powered claim verification service that automatically identifies and verifies factual statements in real-time. Users can input any text, and our system:","Our development centered on creating a powerful, two-stage AI verification system: The core of Verifact lies in its AI integration: The system implements smart caching and request batching to optimize API usage while maintaining real-time performance. Our verification pipeline processes claims in parallel when possible, aggregates results, and provides confidence scores based on the quality and consistency of the verification data.",Building Verifact came with several significant challenges:,,This project taught us valuable lessons about:,"We have exciting plans for Verifact's future: Our goal is to make Verifact an essential tool in the fight against misinformation, helping users make more informed decisions about the information they encounter online.","next.js-15, openai, perplexity, radix-ui, react-query, tailwind-css, typescript",https://verifact-ai.vercel.app/
TreeHacks 2025,Wellness Dorm,https://devpost.com/software/wellness-dorm,False,"We were inspired by our lives as college students, and struggling with our physical health and sleep schedules. We also wanted to add a friend-feature to health, thinking that making it competitive would be a good incentive for high-achieving college-students to take care of themselves.","The app incorporates a chatbot that answers questions and a UI built using Vercel, with our edits. We attempted to use Dain's Butterfly to build a wellness tool, but struggled to get it to work, though we worked somewhat with the code. We also hope to add a larger database using ChromaDB and integrate data from an app like Terra. Hopefully, we'll get it working in the future!",,,,,,"chroma, javascript, open-ai, vercel",https://github.com/mihir-putcha/treehacks2025
TreeHacks 2025,Tower-bot: Towers of Hanoi Solver,https://devpost.com/software/tower-bot-towers-of-hanoi-solver,False,"We were inspired by the challenge of robotic manipulation in assembly lines and industrial automation. The Tower of Hanoi problem serves as a great testbed for grasping, motion planning, and perception, all of which are crucial in real-world robotics applications. We wanted to see if we could get a robotic arm to autonomously solve this problem using perception and inverse kinematics.","Towerbot attempts to solve the Tower of Hanoi problem by perceiving the environment with RGB-D cameras, identifying the disks, and executing planned motions to move the disks between rods. The system uses an antipodal grasping strategy to pick up the disks and a motion planner to ensure smooth, collision-free movements.","We used RGB-D cameras for perception and segmented the scene using color masks to detect the disks. The robot’s grasping was implemented using an antipodal grasping method, and the movement was planned using inverse kinematics with Drake. The Towers of Hanoi solver guided the sequence of moves, and we fine-tuned the trajectory to prevent collisions.","One of the biggest challenges was accuracy in placing the rings. The robot sometimes misaligned the disks when dropping them, making it difficult to regrasp them later. Additionally, the inverse kinematics solver occasionally failed to find solutions, requiring us to implement intermediate poses to improve reliability.","We successfully integrated perception, grasping, and motion planning into a functional system that can autonomously pick up and move disks. The system works consistently for the first few moves and demonstrates key robotic manipulation concepts in a structured problem-solving scenario.","We learned a lot about robotic perception, grasping techniques, and trajectory planning. Debugging inverse kinematics failures and dealing with inaccuracies in physical object placement were key takeaways. We also gained experience in optimizing motion planning to reduce collisions and improve grasping stability.","To improve Towerbot, we need to refine the placement strategy, possibly by using a segmentation model to detect the pole position and adjust before placing the ring. Testing more complex grasping strategies and implementing better trajectory smoothing could also improve performance. We’d also love to expand this system to handle more complex real-world assembly tasks!","drake, meshcat, python",https://github.com/ishitag925/towerbot.git
TreeHacks 2025,Lentil,https://devpost.com/software/lentil,False,"explained :) generating... with RAG! hard to parse compiler message data pipeline architecture/technologies Terminal output messages often vary significantly in clarity and usefulness. Many developers know the frustration of compiling a program only to be overwhelmed by hundreds of lines of cryptic or irrelevant messages. Lentil is a tool that enhances command-line output by filtering unnecessary noise and emphasising the most relevant information. It goes a step further by suggesting potential fixes based on the context of your project files. Lentil empowers developers to navigate complex compiler outputs more efficiently, reducing time spent on debugging. Just typelentilbefore your usual compilation command! Instead of sifting through a sea of cryptic messages, view it through Lentil. Lentil captures both stdout and stderr, then uses retrieval-augmented generation (RAG) techniques to contextualise the output. By analysing your project files recursively, it converts compiler messages into a standardised intermediate representation (IR), which is then presented in a clear and structured format on the webapp. The core is built with OCaml and is responsible for: Lentil also includes a web server written in Python, serving three main functions: To start the web server: Explore the code and contribute to the project onGitHub.",,,,,,,"chromadb, llm, ocaml, openai, python, react, typescript",https://github.com/plamorg/lentil
TreeHacks 2025,calvin,https://devpost.com/software/calvin-3jpoqh,False,"the inspiration for this project actually came from my friend, calvin. he thought of this idea back back a long time ago (65 million years ago). my motivation for this project came from wanting to learn how to use the canvas apis and some of the sponsors' libraries / frameworks / services, so i thought that it would be an interesting project to do while drinking pocari sweat and eating the free food. ","to preface, canvas is a learning management system (lms) that is used by many schools including stanford, and you can imagine this application as an add-on to the lms. calvin is your personal chatbot when it comes to canvas courses and assignments. select a school subject then click on an assignment. you can, then, ask calvin about your assignment. also, you can even ask the chatbot about all your classes. you don't have to specify which assignment. you can even whiteboard out your query if you feel like it.",,,,,,"chroma, groq, instructure-canvas, langchain, langgraph, python, typescript",https://github.com/flotoria/treehacks25
TreeHacks 2025,Echo,https://devpost.com/software/echo-q58c7p,False,"Over half of all readers listen to the audiobooks. Yet, only 9% of books are available as audiobooks - and that's doesn't count other written content, like articles, research papers, or blog posts.","Echo takes any epub or PDF, cleans up any artifacts like footers/headers/page numbers, and turns it into an high quality audiobook within minutes - so you can listen to the latest research paper on your commute. Beyond this, you can TALK to Echo audiobooks. If you have a question about some confusing content, want to dive deeper into a concept, or even just define a word, you can ask Echo, which will respond with context from what you've been listening to :)",,,,,,"elevenlabs, gemini, mistral, next.js, openai, react, realtimeapi, tailwind",https://github.com/685Degrees/echo-reader
TreeHacks 2025,XrayVision,https://devpost.com/software/xrayvision-dgblcq,False,"Hack the world a better place! - This motto drives our vision to make medical imaging more accessible, trustworthy, and reassuring for everyone, regardless of their background.
Around the world, many patients - especially in less wealthy areas - face long waits for expert analysis, leading to anxiety and uncertainty. By using multiple AI models, we provide an instant, diverse opinions, increasing trust and transparency in AI-assisted healthcare. 
We aim to support the UN goals for health (Goal 3) and equality (Goal 10) by ensuring that people from all demographics, have better access to medical insights. We aim to empower patients with clarity, multiple perspectives, and a greater sense of safety in their healthcare journey.","XrayVision is a web app that empowers patients by providing instant diagnostic insights from multiple AI models. By leveraging an ecosystem of diverse AI models, we ensure greater trust, reduced uncertainty, and more reliable results compared to a single-model approach. Our platform not only detects diseases and pathologies but also offers AI-driven explanations, generates comprehensive reports, and provides personalized recommendations for next steps.  It’s designed to be a supportive tool for gaining feedback or a second opinion.","Our team developed XrayVision by combining cutting-edge AI technologies with robust software engineering principles to create a seamless and reliable platform. The front-end and back-end are built in Python using Streamlit, ensuring a user-friendly and efficient interface. We integrated multiple state-of-the-art foundation models (FM), trained on billions of text and image pairs—including models from Mistral-AI and Perplexity—to enhance diagnostic accuracy and trust.  To power our AI models, we leverage the NVIDIA GeForce RTX 3090 (24GB), enabling high-performance inference for real-time disease detection and explanation.","Balancing high performance and real-time processing capabilities without compromising user experience was complex. We are new to front-end development, so creating a front-end which shows all features in a nice way took more time than expected.Combining models that have different environment requirements",,,"While XrayVision is currently focusing on analysis of chest x-rays, the next step will go one step further from 2D to 3D, we want to include CT and MRI disease analysis.","api, app, chexagent, chexpert, edge-ai, foundational-models, llms, medimageinsight, mimic, mistral, perplexity, python, vlm",https://github.com/loufay/XrayVision
TreeHacks 2025,Sign-Speak: No voice unheard,https://devpost.com/software/sign-sight-no-voice-unheard,False,"The four of us had several shared experiences with the deaf community during grade school and were alarmed by the barriers they face in everyday conversation. Deaf people are often relegated to communicating via ASL, which very few outside of their close friends and family understand. In many situations, this confines their conversations to text, rather than the natural back-and-forth dialogue we all appreciate. What’s particularly concerning is that most commercially available ASL translation devices aren't practical for everyday conversation. Most are worn by readers rather than signers, suggesting that the average person has a means of translating ASL to text when interacting with a signer. Others are expensive and bulky ASL detection gloves that interfere with the daily activities of signers. We wondered if we could do better. We asked ourselves how we could leverage recent advancements in computer vision,hand landmark detection, deep learning, andedge computingto develop a wearable device for the deaf that translates ASL signs into spoken audio in real-time, breaking down the barriers to communication that currently exist between the deaf community and the rest of the world.","Sign-Speak is a pair of smartglasses that allow deaf users to translate their ASL signs into spoken audio in real-time. Most existing solutions focus on translation from the reader's perspective, and to our knowledge there isn't anyone working on translating ASL from the signer's perspective. Evidence for this is the lack of labelled data for back-of-hand ASL signs, which is a challenge that we overcame early in the project (see ""Challenges we ran into""). Built with MediaPipe's hand-tracking neural network and hosted on NVIDIA's Jetson Nano, the glasses extract precise hand landmarks, process them through a custom neural network trained on our custom ASL dataset focused only on backside hand movements, and deliver instant voice translations via an attached speaker. Our CV and speech-to-text models are hosted completelyon device, making Sign-Speak a true Edge AI product pushing the frontiers of accessibility tech. Our goal is to empower the deaf community to communicate freely and create a world where every conversation transcends the barriers of sound.","There weren't any available datasets for back-of-hand ASL signs, so the first step in training our model was to create one. To do so, we developed somescriptsthat took and labelled nearly 6,000 pictures of our own hands while we were signing the 26 letters of the ASL alphabet (along with some useful stopwords). We then used Google's MediaPipe models to translate these pictures into 21 coordinates representing the joints in the model's hands (""landmarks""). Our last step was to apply random scaling, rotation, and Gaussian noise to the extracted landmarks and train a feed-forward neural network that classifies ASL signs from inputted hand landmark data. The brains of our operation rely on the small but powerful NVIDIA Jetson Orin Nano upon which our computer vision model runs, allowing our users to use SignSight wherever they want. We packaged our image classification model into a .onnx file and uploaded it onto our Jetson to run inference. The Jetson itself is connected to a webcam that captures input, passes it through the model to detect what letters the user is signing in succession, and then usesPiperfor on-device text-to-speech translation, sending the audio to a miniature Bluetooth speaker connected to the frames. We 3D-printed our glasses based on custom CAD designs to allow for the mounting of a webcam in the temple area, and fashioned an elastic band around the back of the frames with a Bluetooth speaker to enable the sound output of the TTS translation. Users are meant to carry the Jetson Nano (which is housed in a custom 3D-printed case) in their pocket, allowing them to use Sign-Speak's technology anywhere on Earth.","No existing model or dataset currently focuses on first-person American Sign Language (ASL).Most available ASL datasets and computer vision models concentrate solely on capturing signs from the front, as seen by an observer, rather than from the perspective of the signer. Initially, we considered utilizing synthetic data from vision models. However, we discovered that these models had not been trained on sufficient footage of back-of-hand ASL signs, which hindered the generation of high-quality synthetic data for our needs. As a result, we faced the challenge of developing our dataset and model from scratch. This involved taking thousands of photographs of ASL signs from a back-of-hand perspective. To do this, we implemented MediaLabs' Hand Landmark Detection model, which was effective at estimating finger landmarks, even when joints were obscured from the camera's view. We used this model to convert our images into hand landmarks and used these as inputs to our model. Model overfitting and confusion among similar letters.One significant challenge in developing a robust dataset for back-of-hand ASL detection lies in the complexity of the task. Many signs appear remarkably similar from this perspective, leading to confusion and decreased confidence in our model for certain words. To address this, we utilized hand landmarks instead of relying solely on raw image pixels. This approach allowed MediaPipe to effectively infer the positions of obscured fingers, enhancing our model’s accuracy. Additionally, we faced issues with overfitting due to the custom dataset we created. To mitigate this, we carefully managed the number of training epochs and introduced random noise to improve our model's generalization capabilities.",,"While creating SignSight, we learned about developing deep learning models on constrained resources and the tradeoffs associated with developing on small computers. While developing a seamless user experience, we realized that effective assistive technology must blend into daily life without adding complexity. It took thoughtful design choices to decide how a Jetson Nano, speaker, and camera would fit into the form factor of wearable glasses.",,"3d, cad, cuda, jetson-nano, mediapipe, numpy, opencv, python, tensorflow","https://github.com/vkodithala/sign-speak, https://huggingface.co/mehulrao2/POV-ASL-Classification, https://huggingface.co/datasets/mehulrao2/POV-ASL"
TreeHacks 2025,PrescribatronJelqinator,https://devpost.com/software/prescribatronjelqinator,False,"""Can you provide your family history of chronic heart disease?""""I just answered that on the sign-in form.""""Do you recall your Epstein-Barr virus titers from your last five blood tests?""You're the doctor—I just get my blood drawn."" Healthcare should befluid and intelligent, not a repetitive Q&A session.PrescribatronJelqinatorgives doctors instant, structured access to a patient'sentire medical history, allowing them to askreal questionsinstead of rehashing paperwork.","PrescribatronJelqinatoris asecure, AI-powered medical assistantthat allows doctors to:✅Instantly retrieve patient datafrom encrypted hospital records.✅Ask context-aware questions(e.g., ""Does this patient have a history of anemia?"").✅Run locallyfor privacy while maintainingHIPAA compliance.✅Summarize clinical notesandcross-reference medical historyin real time. Instead ofsifting through records manually, a doctor can simplyaskand get anaccurate, contextual responsebased on the patient'srealmedical history.",,,,,"🚀Cloud Hosting– Deploying tosecure, hospital-grade cloud infrastructure.🏥EPIC Hospital Access– Integrating directly withEPIC medical record systems.📝Clinical Note Understanding– Allowingdirect interaction with physician notesfor deeper insights.🎙Doctor-Patient Voice Transcription– Enablingreal-time note-takingbased on conversations. Note: we cannot include any gallery images for HIPAA compliance, but come check out our live-demo booth at the expo!!",json,https://github.com/davidmaemoto/prescribatronjelqinator
TreeHacks 2025,TrivAI,https://devpost.com/software/trivai-vle72m,False,"TrivAI was born from the desire to revolutionize the traditional trivia game experience. My girlfriend and I like trivia and we noticed that conventional trivia games often lack personalization, asking questions about topics we don't care about. By leveraging AI technology, TrivAI seizes an opportunity to create dynamic, engaging, and customizable trivia experiences that adapt to players' interests, helping us learn more about our favorite topics!",TrivAI is an interactive multiplayer trivia game that uses AI to generate unique questions based on player-chosen topics. Key features include:,We built TrivAI using a modern tech stack:,,,,,"nextjs, openai, pusher, typescript",https://github.com/kavelrao/TrivAI
TreeHacks 2025,ClassPulse,https://devpost.com/software/classpulse,False,"Education is evolving, but professors often struggle to gauge real-time student understanding during lectures or identify key learning gaps from homework. Inspired by the idea of using AI to bridge this gap, we set out to build a solution that provides professors with deeper, actionable insights—making classrooms more interactive and helping students succeed.",ClassPulse is an AI-powered assistant for professors that helps them better understand their students through two key approaches:,,,,,,"gemini, next, openai, python",https://github.com/speedstorm1/treehacks25
TreeHacks 2025,nomnom!,https://devpost.com/software/snacksage,False,"landing page ex 1: similar snacks generated after uploading snack image (treehacks chips from our live demo!) ex 2: similar snacks generated after uploading snack image ex 2: snack descriptions While traveling, we often discover amazing snacks that we wish we could find back home. However, finding similar snacks in different countries can be challenging.nomnom!was born to bridge this gap—helping snack lovers find their next favorite treat, no matter where they are in the world. 📸 Take a picture of a snack they enjoy.
🤖 Use AI to identify and suggestsimilar snacksavailable in other regions.
🌍 Filter results by country to find local equivalents.
📚 Explore aglobal snack databasewith images, flavors, and recommendations.",,,,,,,"firebase, flutterflow, gemini",https://github.com/haleynpatel/nomnom/tree/main
TreeHacks 2025,Eigensurance: AI x Web3,https://devpost.com/software/lalala-vqp6xg,False," what if? zkLTS is the key to providing trustless LLM inference since it can't be done on chain Smart contracts! To make a profit, we invest collected premiums. Profits are given back to restakers and operators! try at https://eigensurance.replit.app/     Coming from Southern California, I'm very aware of the immense risks posed by wildfires, like the ones in LA last month. In fact, there was a brush fire just a few miles from my house. Unfortunately, increasing wildfire risks means thatmost home insurers are leaving the state,leaving millions without insurance. And, the remaining insurers have increased prices significantly. I've never done any crypto/web3 stuff before but wanted to explore for the hackathon. After reading about decentralized projects likeFilecoinandHelium, I wondered:could we do the same for insurance? Instead of one profit-oriented corporation, services could berendered transparently, quickly, and fairlyon the blockchain, assisted by AI agents. I realized that insurance companies basically have three jobs: Having humans do this is very expensive. ~27% of annual premiums collected ($560B) is spent on operating expenseslike salaries, sales commissions, management fees, etc. I realized that all three of these components could bereplaced by AI and web3technology, driving down costs exponentially—meaning that insurance can become cheaper, faster, and more fair.
All this is enabled by the** Eigenlayer AVS system**, which enables us to get zkTLS verified off-chain LLM responses through Opacity and Cloudflare. Eigenlayer provides a way for independent operators to provide services like LLM inference, and they are incentivized to be truthful because they get slashed (lose their stake) if they are caught lying. Also, by being on the blockchain, we can ensure transparent pricing and payout conditions via smart contracts. While I wasn't able to finish the entire thing due to the complexity of the project and working solo, I was able to develop various working + open sourced key components including an AI agent and various smart contracts. **To invest collected premiums to make profit to reward restakers/operators and pay policyholders: First of all, it was only possible thanks to a lot of explanations and help from the Eigenlayer andOpacityteams since this is my first time building a web3 project. To build things, I relied on the sample repos, Eigenlayer Hello World AVS and the Eigenlayer AI quickstart.
The main languages were Solidity, TypeScript, and some HTML/CSS for the frontends. I also usedRemixwhich is kind of like Replit for web3, which helped me test my smart contracts.
I also used Google Colab to code and test the stock trading agent and then Replit to deploy the live demo. Learning the intricacies of smart contracts, on‑chain/off‑chain interactions, and integrating AVS
Getting set up with a MetaMask wallet
Remembering how much gwei, wei, and eth are Building working smart contracts
Learning about web3 and Eigenlayer How restaking, zkTLS, smart contracts, and AVS work I would like to work on deploying this live!",,,,,,,"eigenlayer, eth, evm, openai, perplexity, python, solidity, typescript, vercel","https://eigensurance.vercel.app/, https://eigensurance.replit.app/, https://www.youtube.com/watch?v=pKcpF_wBnoM, https://github.com/andrewgcodes/eigen, https://github.com/andrewgcodes/eigensurance_agent, https://github.com/andrewgcodes/ai-trading-agent"
TreeHacks 2025,GHGuessr,https://devpost.com/software/ghgguessr,False,"GHGuessr is a single-player game with the mission of building climate awareness, especially in the realm of greenhouse gas (GHG) emissions and other air pollutants. Players gain points when they answer correctly. There is a “hint” button that provides additional facts about the country, so that players can have some more information about the country they are trying to guess. Few people realize the magnitude of the amount of emissions that are released worldwide, as well as how many types of GHGs there are. This game was designed with the idea of educating others into becoming better aware of the state of our planet. Our idea was inspired by one of Stanford Ecopreneurship’s Sustainability Challenges, specifically Challenge #4, “Tradle, but for emissions”. Tradle is a game that educates players about the types of products that countries export, by providing some data on exported products and asking the players to guess which country the information comes from.  Our game has a similar concept, where we provide data on GHG emissions and players will guess which country the data comes from. In addition, the title of our game is inspired by the game GeoGuessr, which is a global geography game; this game has a similar aspect with the idea of guessing countries from around the world. Through using Perplexity to understand the intersection between sustainability challenges and user needs, we saw that the significant knowledge gaps in understanding national greenhouse gas emissions hinder effective climate change mitigation efforts. We assume that data-driven learning, such as in the form of this game, is effective. We aim for GHGGuessr to help with the public better understand the magnitude of climate issues, and that the gamification of facts will incentivize people to engage with emissions data more. We first used reviewed common climate-related data visualizations and tools to understand our users' needs. Afterward, we created a mock front-page using Vercel's v0. Subsequently, our team met with Jonathan Lipman, leader of Strategic Initiatives at Speed and Scale. We discussed our idea for the project, and he provided ideas on what data to utilize from the API from the data source Climate Trace [Link:https://climatetrace.org/]. From these conversations, we were able to gather ideas on how to best gamify our ideas. We then tested the following hypotheses:",,,,,,,"api, css, html, javascript, react","http://treehags.vercel.app, https://github.com/amanda-jenkins/TreeHAGS/tree/main/src"
TreeHacks 2025,PrepTracker,https://devpost.com/software/preptracker,False,"As I approach my graduation this May, I realized I needed to brush up on my skills for upcoming tech interviews. Many computer science students face the same struggle when preparing for exams or interviews—staying motivated, setting clear goals, and tracking progress over time. This led to the creation of PrepTracker, a tool designed to help students manage and track their study goals in a personalized, AI-driven way.","PrepTracker allows students to set daily, weekly, and one-time study goals, track progress via a heatmap dashboard, and receive AI-generated motivational reminders via email. The tool helps students stay on top of their goals, providing a visual representation of their achievements and keeping them engaged throughout their study process.",,"One of the key challenges I faced was storing and managing student progress over an extended period, from start to end date. Keeping track of dynamic data across multiple goals and dates proved complex. Additionally, date management and operations were a major hurdle, as the project heavily relied on precise date calculations and dynamic updates. I had to ensure that progress was properly tracked across different goal types, which required careful handling of dates, progress mapping, and synchronization across various components of the app.","I'm proud of creating a functional AI-driven system that not only tracks but also actively motivates students to reach their study goals. The heatmap dashboard is a standout feature, giving students a visual representation of their progress. Additionally, I was able to work with dynamic dates and their management successfully.",I learned the importance of user-centered design in developing motivational tools. Handling real-time data and building AI-powered reminders that feel personalized was a major takeaway. This was also my first time working with cron-jobs. I had to write a cron-job that ran every dat at 12am and sent users their goals for the day.,"I plan to expand PrepTracker’s functionality by adding support for more complex goal types, improving AI-generated recommendations, and integrating with third-party calendars and task managers to automate goal creation.","express.js, firebase, next.js, node.js, react.js, tailwind","https://prep-tracker.vercel.app/, https://github.com/alaminShaheen/PrepTracker"
TreeHacks 2025,EasyRead,https://devpost.com/software/easyread-5axzc3,False,EasyRead was a feature we always dreamed of - an effortless way to simplify complex words while reading. We thought it would be amazing to have this as a web app or a Chrome extension to help users read smoothly without interruptions.,"EasyRead makes reading easier by allowing users to double-click on complex words to see a simpler version. If they want to revert, a single click brings back the original word. Additionally, it builds a personal dictionary of all clicked words, helping users track and learn new vocabulary effortlessly.","This was our first hackathon and our first experience with web development. We built EasyRead using: Since we were new to web development, we spent a lot of time researching and experimenting with different implementations.","Building a website from scratch was much harder than we expected. Even basic functionalities took time.
HTML & CSS were trickier than they seemed. Integrating OpenAPI required some trial and error to set properly and upgrade Requests Per Minute Rate, but it turned out to be one of the easier parts.","We successfully built a functional prototype of EasyRead.
We learned full-stack development (React, Node.js, APIs).
We participated in our first hackathon and completed the project!
Despite challenges, we created a working tool that can genuinely help people read better.",,We’re excited about the potential of ReadEase! Some next steps include: We’re excited to keep building!,"css, dictionaryapi, figma, html, node.js, openai, react, wireframing",https://github.com/ym256-stanford/tree-hacks-2025.git
TreeHacks 2025,xxx,https://devpost.com/software/xxx-lrki7u,False,,,,,,,,"elevenlabs, groq, javascript, langchain, lumalabs, mistral, openai, perplexity, rust, typescript",https://github.com/zcsabbagh/zu-lm
TreeHacks 2025,Slumber Synthesizer,https://devpost.com/software/slumber-synthesizer,False,"Homepage - Slumber Synthesizer Community Dreams Statistics (And Dark mode!) Dream Analytics Slumber Synthesizer combines personal dream journaling with AI-powered analysis and community features, creating an engaging platform for recording and exploring dreams while maintaining user privacy when desired. It adds a fun way to share your silly dreams with friends worldwide while cataloging all of your dreams and analytics privately. The inspiration for our project,Slumber Synthesizer, came from a shared fascination for understanding dreams and how they affect sleep. Dreams often hold untapped insights into our subconscious, yet they are fleeting and difficult to analyze. We wanted to create a platform where users could not only record their dreams but also explore patterns, themes, and meanings in a secure and engaging environment. Throughout this journey, we gained valuable insights into both technical and non-technical aspects of building a web app. Technical Insights: Non-Technical Insights: Each tool was chosen with consideration for the developer experience, performance, maintainability, and scalability. This stack provides a modern, robust foundation for building high-quality web applications while maintaining excellent developer experience and end-user performance. Building Slumber Synthesizer was not without its challenges: Designing AI models capable of accurately analyzing subjective dream content required extensive research and iterative fine-tuning. Ensuring that user data remained secure and private was a top priority, which involved choosing the correct tools and implementing robust encryption protocols. Creating a platform that felt both personal and community-driven required careful balancing of features to cater to individual users while fostering meaningful interactions within the community. As with any hackathon project, time was limited, so we had to prioritize features while maintaining a high-quality user experience. Despite these challenges, we are proud of what we accomplished in such a short time frame. Slumber Synthesizer is not just a tool for journaling dreams but a gateway to self-discovery and connection with others who share an interest in exploring the subconscious mind. During our coding and development process, we usedPerplexityas a valuable tool to streamline our workflow. Perplexity provided detailed explanations, troubleshooting tips, and best practices, which significantly improved our efficiency. It helped us in various areas: Slumber Synthesizer isn’t just a dream journaling app—it’s a platform built for real-world impact. We’re addressing a significant personal wellness and mental health opportunity by enabling users to capture, analyze, and learn from their dreams. It also adds a fun way to share your silly dreams with friends around the world. Our product offers a personal wellness tool for self-reflection and incorporates a social media element—allowing users to share their dreams and interpretations within a vibrant community. There are over 4.7 billion social media users globally. If we capture just 0.1% of this vast audience, that’s roughly 4.7 million potential users. In addition, the digital mental wellness and self-improvement market is estimated at around $3–$4 billion today, with projections to reach $10 billion or more in the next 5–10 years. Assuming a conservative premium conversion rate of 5–10% among engaged users, capturing 0.1% of social media users could yield 235,000 to 470,000 premium subscribers. With an average revenue per user (ARPU) of $20 per year for premium features, this translates to an annual revenue potential of roughly $4.7M to $9.4M While our current implementation includes essential social sharing features, this is just the base. We plan to expand this aspect further by introducing personalized feeds, interactive community challenges, and integrations with popular social platforms, ensuring users remain engaged and inspired. We are committed to the long-term vision of continuous innovation and user-centric development, we’re poised to grow into a product that people truly want and trust. SecuritySecurity has been at the forefront of our design. We’ve implemented robust authentication protocols, encrypted data storage with Supabase, and strict privacy controls to ensure that every dream remains confidential. Our commitment to security isn’t an afterthought—it’s an integral part of the user experience, reflecting our dedication to building a trusted platform where users can safely explore their subconscious. Intuitive UXOur design philosophy is centered on clarity, accessibility, and engagement. By harnessing modern frameworks like React, Shadcn/UI, Tailwind CSS, and Framer Motion, we crafted an interface that is both visually appealing and remarkably intuitive. Every element—from the fluid animations to the responsive layouts—was designed to guide users effortlessly, ensuring that even the most complex features are easily accessible. We also have light mode AND dark mode!",,,,,,,"dall-e, mistral, openai, perplexity, react, supabase, typescript, vite","https://github.com/MiPlayer123/slumber-synthesizer, https://slumber-synthesizer.lovable.app/"
TreeHacks 2025,Nirmaan,https://devpost.com/software/nirmaan,False,"More than 60 million construction workers in India face challenges such as wage theft, delayed payments, and limited access to government benefits due to the absence of formal employment records and a lack of awareness about their labor rights. The construction sector remains highly unorganized, allowing middlemen to manipulate wages and siphon off funds intended for worker welfare. After analyzingrecent incidents, existing policies, and government welfare initiatives such as theBuilding and Other Construction Workers (BOCW) Actand theOccupational Safety and Health Administration (OSHA) Act, we recognized the urgent need for an end-to-end framework that ensures labor welfare is accessible while eliminating worker exploitation and unlawful employment practices in the industry. Ensuring financial security and fair treatment for construction workers requires a system that is both transparent and intelligent. Our project,Nirmaan, leverages Blockchain to create tamper-proof employment records, automate payments, and establish decentralized worker identities, eliminating wage manipulation and unauthorized intermediaries. At the same time, AI powers data-driven insights, worker profiling, and policy education enables an efficient allocation of benefits and ensures compliance with labor regulations.",,,,,,,"next.js, openai, postgresql, solidity, tailwindcss, typescript",https://github.com/samarthchandrawat/nirmaan_v1
TreeHacks 2025,excelerator,https://devpost.com/software/excelerator,False,"Spreadsheets are everywhere, but adding machine learning to them usually means coding, extra software, and clunky data transfers.Exceleratorchanges that by integrating ML directly into Excel. Whether you're predicting wildfire risks for insurance, tracking disease outbreaks, optimizing disaster response, or even managing energy and water resources, Excelerator lets you train models, clean data, and generate predictions—all without leaving your spreadsheet.",,,,,,,"actions, add-ins, amazon-web-services, api, autosklearn, azure, blob, colab, css, docker, excel, fastapi, functions, github, google, graph, h2o.ai, html, javascript, keras, lambda, microsoft, numpy, office, optuna, pandas, postgresql, python, s3, scikit-learn, sqlite, storage, tensorflow, typescript, vercel",
TreeHacks 2025,HackTCHA,https://devpost.com/software/hacktcha,False,"Support for text-based CAPTCHA Support for multi-select CAPTCHA Options for obfuscation of CAPTCHA Imagine you’re looking for a new pair of headphones. You read through reviews that claim the sound quality is amazing, only to discover later that these reviews were automated—created by bots trained to mimic human language, opinions, and emotions. This problem is only growing. In recent years, AI has become eerily good at solving CAPTCHAs—the very thing designed to separate humans from bots. Traditional CAPTCHA testing is outdated. Companies rarely know which CAPTCHAs remain effective against evolving AI threats, nor do they have a real-time leaderboard showing which CAPTCHA types are safest. Instead of waiting for hackers to exploit vulnerabilities, we’re building a live testbed where AI itself uncovers weaknesses—so we can design the next generation of human-friendly, bot-resistant security. HackTCHA tests how different AI models—like GPT, Google Gemini, Mistral, and Groq—perform against various CAPTCHA types (including text and multi-select image CAPTCHAs). Here’s how it works: 1.User Inputs 2.AI Testing 3.Obfuscation Tools 4.Version History & Database",,,,,,,"flask, gemini, imgur, javascript, jsx, lucide, numpy, python, react, sqlite, tailwind, vite","https://treehacks2025-one.vercel.app/, https://github.com/dubiels/treehacks2025"
TreeHacks 2025,EigenInsure,https://devpost.com/software/eigensurance,False,"Slide #1 Slide #2 Slide #3 Slide #4 Slide #5 Slide #6 Slide #7 Slide #8 EigenInsure is a decentralized insurance protocol powered by EigenLayer. Insurance companies should protect people, not exploit them. Yet: This isunacceptable.Insurance should befair, transparent, and accessible to all. EigenInsure is a protocol that:✅Issues insurance instantlyusing AI & on-chain data.✅Processes claims fairly & transparentlywith EigenLayer’s AVS.✅Eliminates bias & human greed, ensuring claims are approved based onverifiable evidence, not profit motives. ✅ Successfully built atrustless, concensus-based insurance systemusing EigenLayer AVS and deployed it to the testnet.✅ Developed anAI-powered claims assessmentthat eliminatesbias.✅ Created a **seamless UX, allowing users to buy insurance **within seconds.✅ Managed to **get feedback and aproval* from a recognized french life insurer (SPVIE). 📌Target the housing insurance marketbecause of its non-private and easily verifiable data.📌Write explicit legal contractsfor the operators to rely on.📌Insure our insurance pool. No moreunfairly denied claims.No moregreedy insurers.Justfair, transparent, and verifiable protection. 🚀Powered by EigenLayer. Secured by unstoppable AI.",,,,,,,"avs, eigenlayer, express.js, liquidai, mistral, nextjs, openai, perplexity, solidity, typescript",https://github.com/eigeninsure
VTHacks 12,DispatchIQ,https://devpost.com/software/dispatchiq,True,"In emergency situations, dispatchers are often inundated with a high volume of calls, many of which are waiting in a FIFO (First-In, First-Out) queue to connect with a dispatcher. Compounding this challenge is the fact that approximately 20% of the U.S. population speaks a language other than English as their first language, which can further complicate communication and delay response times. DispatchIQ is designed to assist dispatchers by automating the triage process for these queued calls. DispatchIQ ensures a seamless and effective solution for managing emergency call prioritization and enhances dispatcher efficiency by utilizing: Our ""severity score"" was inspired by Common Vulnerabilities and Exposures (CVE) ratings.",,,,,,,"azure, azure-ai-speech-service, flask, gemini, google-kubernetes-engine, javascript, mongodb-atlas, python, react, speech-to-text, xgboost",https://github.com/ddaln/vthacks_2024
VTHacks 12,ProtectEd,https://devpost.com/software/protected-4q2day,True,The alarming rise in gun violence within educational institutions across the U.S. over the past two decades motivated us to develop an emergency response system that is both highly effective and cost-efficient.,"Our system provides a comprehensive emergency response solution for gun violence in schools, leveraging only the mobile phones of students and staff, along with the existing CCTV infrastructure. It delivers accurate emergency alerts and evacuation routes through three key components. The app prompts students to input their weekly schedules, allowing us to track which room each student is in at any given time. Additionally, the room locations of the CCTV cameras are stored in our database, ensuring all devices are precisely mapped. Our lightweight audio classification model uses the phone’s microphone to detect gunshots, while our object detection model analyzes CCTV footage. These systems work together to pinpoint the shooter's location based on where the mobile devices or CCTV cameras are positioned. We use MapKit along with a custom floor plan of the building to identify rooms where potential threats are detected. Everyone in the school receives immediate alerts with precise location data. Additionally, Push notifications are sent to students and emergency responders, enhancing overall safety.",,,,,,"coreml, fastapi, mapkit, python, roboflow, sqlite, swift, swiftui",https://github.com/Rushil695/ProtectEd
VTHacks 12,Telescope: Spotting AI generated Content from a Mile Away,https://devpost.com/software/telescope-4kx9u6,True,"After reading up on thelatest in AI generated content research, we wanted to both reproduce from scratch, improve on and make easily available the latest advancements in detection for AI generated content. Specifically, we were inspired by the advancements of two recent AI research papers:Binoculars(https://arxiv.org/pdf/2401.12070) for detecting LLM generated content such as the everpresent ChatGPT andAeroblade(https://arxiv.org/pdf/2401.17879) for detecting latent diffusion image generation models such asStable Diffusion. When reading these papers, we realized they had problems we could solve. To our knowledge, no one else has a platform to detectbothAI generated content in text and images to a reasonable level of quality, and we wanted to take the illustrious title of being first to the punch!","Our project acts as a one-stop shop for all your AI generated content detection needs! We offer3modes of usage: Detecting AI generated plain text, images, and a specially tuned version to detect AI generated code. Our method is able todetect inherent tracesand signatures that are left behind after image and text generation, which don't change between different image and text generation methods. Whether you are using ChatGPT, Gemini, Claude, or any other generative model, our state of the art technique can spot them even though it hasn't been specifically trained to detect them! In fact our technique will remain robust and future-proof to new and more powerful generative models since they all have the same signature that we can track. More specifically, for text data our model uses a tool called cross perplexity to calculate how ""surprising"" a human is to an AI and how ""surprising"" an AI is to a separate AI. Generally a human is statistically more ""surprising"" than an AI which in our case means has a higher ""perplexity"", and this is the signature left behind by almost all AI generated text, which we aim to detect. Conversely, there is no such measure for AI generated images, so we have to use a tool called avariational autoencoderwhich are baked into many of the most powerful image generation models today. They project images into a language (or latent space) that the model can more easily understand and many AIs simply generate the image through this AI language. However, it turns out that many AI images can be much more easily ""put in terms"" that the AI can understand than an image that a human creates. Depending on how well the variational autoencoder works on an image, we can use that to help us detect whether an image is likely AI generated.","On the backend, we primarily used a combination ofpython,flask,pytorch, and thehugging face transformerslibrary to make performant and clean code that reproduces cutting edge research in the field from scratch and builds off of that research.","Since we implemented the algorithms from scratch and made our own custom improvements, finding the correct thresholds/ hyperparameters for the exact score that consitituted AI generated content was challenging and highly experimental. In addition, we had to make sure that the interface was simple and easy to use for users unfamiliar with complicated and difficult concepts such as ROC curves to tune our score thresholds for our user's particular application.","During these few36hours, we managed toreproduce 2 academic papersand even make improvements to them usingnovel techniquesto raise accuracy. In addition we made an extremely functional and user-friendly frontend that can easily be used by people who don't understand how our detection algorithm works and how to finetune for ROC, true positive rate, and false positive rate.","Building these state of the art detection algorithms from scratch and writing improvements to them strengthened our understanding of how they worked at a fundamental level. In addition, we gained more experience with frontend web development, UI, and UX, attempting to optimize the user experience when using our website even for people who don't fully understand the technology at play.","Although still working well on a vast variety of images, currently our AI generated image detector only works optimally with a specific image size, so in the future, we could figure out some way let the variational autoencoders better handle variable sized image data. In addition, we would like to scale up this technology to be generally available. Currently, we weren't able to run this on the cloud, since we were afraid of the high costs of AI cloud computing and already had access to a high end consumer graphics card to run inference and testing on locally. In the future, we could scale this technology to be used en-mass using various techniques such as load balancing on cloud gpu clusters if given proper funding.","css, flask, html, hugging-face, javascript, python, pytorch","http://joshfcooper.com/telescope, https://github.com/JoshFCooper415/telescope"
VTHacks 12,VisionLink,https://devpost.com/software/visionlink,True,Our inspiration was a full coincidence when we passed by the Blind workshop last semester. Seeing their challenges made us realize how important it is to create a tool that can help them navigate their surroundings with more freedom and confidence.,"VisionLink helps visually impaired people by using a camera on glasses. The camera detects objects and obstacles and gives feedback to the user via audio and haptic vibrations. It also uses Google API to help users to their walking destinations, by guiding them with directions and what is in front of them.","The first step we took was getting vision to work and putting the live feed through an AI object detection model called TensorFlow Lite Task Vision, which we implemented using a VisionCamera Frame Processor plugin. The next step was locating the phone and getting directions, which we implemented with the GoogleMaps API to locate places by name and trace the path to get from our starting position to our destination, all while giving directions along the way on which turns to take. We then implemented the haptic feedback using Expo Haptics to alert the user of obstacles that appear in front of them using the confidence interval from the object detection model, which we found consistently increased as the distance from the specified object decreased. This haptic feedback happens at different intervals, depending on the proximity, with more intense feedback as the user gets closer. The final step was making the whole app able to be controlled by speech and performing text-to-speech to notify the user of important tasks such as taking a left turn in 10 meters. The text-to-speech was implemented using Expo Speech.","A major challenge we ran into was when our hardware camera component got fried due to excessive overheating. We had to change the entire implementation of how the AI can view and process live footage. We decided to use what we all had access to, our phones. Although it is not a direct physical implementation of the project, the idea and functionality of the project remain the same.","We are very proud of bouncing back from a major setback and having a working prototype. Even though we had to change our plans along the way, we stayed focused on our goal. We tried to make VisionLink simple and very accessible to use for visually impaired people.",This Hackathon gave us the opportunity to learn that adaptability is key. We also learned how important it is to understand the needs of visually impaired people.,"In the future, we plan on improving the object detection and navigation. We primarily want to get the design and comfortability(hopefully without frying our camera). Our ultimate goal is to turn VisionLink into a startup that helps create affordable products that help visually impaired people navigate different places free of anxiety.","android, expo.io, google-maps, javascript, machine-learning, react-native",https://github.com/wi/vthacks-12
VTHacks 12,VTCourseNav,https://devpost.com/software/vtcoursenav,True,VTCourseNav was inspired by the need for a clearer and more organized way for Virginia Tech students to navigate their academic journey. We wanted to create a tool that simplifies the complex process of planning course prerequisites and helps students achieve their graduation goals with confidence.,,,,,,,"beautiful-soup, css, html, javascript, json, python, selenium","http://vtcoursenav.study, http://github.com/Corrupt-Potato/vthacks-coursenav, https://github.com/Corrupt-Potato/vthacks-coursenav-backend"
VTHacks 12,VT Memegen,https://devpost.com/software/vt-memegen,True,"Even though we have access to services like VT Instagram meme pages or YikYak, we felt there was no streamlined way of exclusively consuming memes that were specific to fellow Hokies. Like Google's personal internal meme platform Memegen, VT Memegen is only accessible to members of the Virginia Tech community meaning all posts are substantially more relatable and therefore more entertaining.","After an authentication process verifying your '@vt.edu' email, users are brought to the homepage where they can see all posts. Sorting by tags such as 'Sports,' 'Dorms,' 'Night Life,' etc. yields corresponding posts. Although posting on other platforms may require a third-party technology for media creation, this is where VT Memegen stands out. Pick from several popular meme templates where you can intuitively input text into pre-allocated spaces allowing for optimal meme production. Finally, interact with other user's posts by liking, disliking, or commenting to further express your thoughts.","We built VT Memegen using various Frontend and Backend Technologies. Our Tech Stack: Firebase, React (TypeScript), Express, NodeJs, TailwindCSS, OpenAI API Our UI was developed using React (TypeScript) with Tailwind CSS for easier styling to allow our site to be easy to use and interactive. Our backend technologies consisted of Express and NodeJs which we used for handling our backend requests and Firebase for authentication, users, posts, and image storage. Finally, we integrated OpenAI’s API into our application to provide a generative AI assistant to summarize our user-generated content (memes) on the site.","Although we initially tried using PropelAuth, after many attempts, we switched to Firebase as it provided a database and authentication service killing 2 birds with 1 stone. Integrating OpenAI's GPT 4 API such that the AI would summarize each post was also a tough challenge due to passing in the image data, but it was a problem we overcame.",We are most proud of how much we learned while having a good laugh at our A1 memes.,"Throughout our development of Memegen, we gained insights about the many different components involved in full-stack applications as well as how to apply our varying levels of experiences into one collaborative project. This project allowed us to explore integrating generative AI assistants, implementing Third-Party Authentication, utilizing Database platforms, and deepening our knowledge in Front-end development.",We hope to expand beyond VT to provide a lighthearted and accessible platform for communities outside of ours.,"express.js, firebase, node.js, openai, react, tailwind, typescript","https://github.com/s-nagoti/VT-Memegen, https://main--hokiememehub.netlify.app/"
VTHacks 12,SecureAI,https://devpost.com/software/secureai-3xu8p4,True,We are a group of students deeply interested in the AI safety space. We came across GCG whilst reading a groundbreaking paper and wanted to utilize in some way within our project. We took a lot of inspiration from LLM chains as well.,We explore string attacks on LLMs and highlight its ease and dangers whilst doing so. We implement a novel approach to combat these attacks using LLM chains.,We built the front-end with React and utilized Flask for back-end logic and hosting. We rented compute from vast.ai to run the GCG algorithm.,The biggest challenge we ran into was coming up with the idea of LLM chaining. We were stuck for quite a while simply in the brainstorming phase while thinking of applications we could do as well as searching for potential mitigation solutions.,"We are especially proud of our front end and our integration with cloud compute (GPU). The first took careful planning and extreme dedication. We are especially happy with how it turned out, as well as extremely happy with our front end engineers. Additionally, we were also happy with being able to learn about how to utilize online GPU's to run algorithms.","Many of us were able to learn valuable skills from each other. We were able to learn React from our front end engineer. Some were experienced in Flask, so we were able to glean some information. Our ML engineer was also able to SSH into cloud compute as well!",,"css, flask, html, hugging-face, javascript, python, react, vast.ai",https://github.com/eshaqam23/secureai
VTHacks 12,FishBook,https://devpost.com/software/fishbook-z916jq,True,"Our group has a passion for keeping fish tanks and aquariums. In high school, Layla, worked at a local fish store. During that time, she learned a lot about the proper way to stock freshwater tanks and saw firsthand how common misconceptions can lead people to make mistakes, which will ultimately end in stressed or possibly deceased fish. This hobby can be difficult to get into because there are so many rules and factors to consider when setting up and maintaining a healthy tank. Unfortunately, misinformation is everywhere, often driven by large pet stores and corporations that profit when fish die and need to be replaced frequently. The current software solutions designed to help new and experienced aquarium owners are lacking. Many of them have poor user interfaces, making them difficult to navigate and use. For our hackathon project, my team and I wanted to take on this problem. We developed an application that simplifies the process of managing aquarium stocking by improving the user experience and addressing the underlying complexities with a smarter algorithm. Our goal is to provide a tool that helps aquarium enthusiasts, from beginners to experts, avoid the pitfalls of misinformation and enjoy a healthier, more successful aquarium hobby.","FishBook is a comprehensive web application designed for aquarium enthusiasts, providing an intuitive platform to ensure compatible fish species for their tanks. It helps users make informed decisions through a fish compatibility checker, personalized equipment recommendations, and a detailed fish directory. With a built-in chatbot, users can get instant answers to their fish-related questions at any time, making fishkeeping easier for beginners and experts alike.","We developed FishBook using a modern tech stack, including React for the frontend and Spring Boot for the backend. The database is managed with MySQL, and the app is deployed using AWS and Microsoft Azure. LangChain and OpenAI API were integrated to power the chatbot, providing intelligent, data-driven responses to user queries. We also used Lombok to streamline the development process and improve backend code readability.","One of the biggest challenges was ensuring the accuracy of the fish compatibility algorithm. Balancing the numerous factors such as tank size, water conditions, and temperament required extensive testing. We also faced challenges in integrating the AI-powered chatbot smoothly with the fish database to ensure it provided accurate, helpful responses.",,,"Due to time constraints and the number of features we aimed to develop, we had to simplify our stocking algorithm. Managing all the possible tank and stocking combinations is a complex computational problem, and there is currently no perfect or near-perfect solution. With more time and resources, we would like to further research and develop a more robust algorithm, possibly exploring the use of machine learning to improve accuracy and efficiency.","amazon-web-services, lang-chain, mysql, nginx, react, spring","https://fishbook.wiki/, https://github.com/rbouhal/FishBook"
VTHacks 12,MapAbility,https://devpost.com/software/mapability-16m47i,True,For too long society has judged those are different from the group whether it be differently abled or different sexuality. Our app allows underrepresented people to flip the script on society and pass judgement on those buildings that are not considerate of their needs and requirements.,"All a user needs to do is to search for a building they have visited before, and give it a review based on certain accessibility and friendliness metrics. Users can see the reviews for various buildings posted by other people so that they are equipped with the knowledge of which buildings are accommodating for their needs and which ones are less considerate. The user can also plan out journeys and make day planners where each building is selected so that it best fits with their desired plan as well as their specific needs and preferences.","We used MongoDB for our backend database and docker to integrate containerization and tie everything together. Containerization allowed us to easily code in one place and then quickly deploy to AWS to host the backend infrastructure of our application. For our front end we used NextJS, ShadCN, TailwindCSS, and the Google Maps API. We integrated generative AI into our project by implementing function calling technology, which allowed us to structure user inputs and accessibility data. This structured information was then fed into OpenAI's Large Language Model (LLM) to generate highly personalized and accessible activity plans, enhancing the overall user experience.","One of our biggest challenges was figuring out how to create and manage our own APIs using FastAPI. Something that we ended up spending a lot of time trying to debug was when an API call with a specific endpoint never ended up getting called because it would default to an API with a general endpoint in a very similar link structure.
Something else that we struggled with was how to aggregate various reviews into a single rating for a building. This step was crucial as it was a part of the MVP but it was an also a prerequisite for the day planner feature. What appeared to be a trivial task of just needing to take an average became actually tricky when it came to implementing it.","We are most proud of our explore functionality MVP and our overall UI/UX. Being an accessibility app, we are proud that our app itself caters to various accommodational features - dyslexia friendly text, text resizing optionality, contrast mode and text-to-speech and speech-to-text.
Our filtering system goes beyond just the basic categories the buildings belong to. Our system allows users to filter by 8 different accessibility needs so that a wide array of needs can be addressed.
Each building has a high quality of user generated reviews while also backed by AI based summaries and net aggregation giving users a seamless, dependable source of information about the buildings around them.","Our hackathon journey was a crash course in both technical skills and teamwork. We dove into AWS, FastAPI, React, and Next.js, tackling challenges like API endpoint conflicts and data aggregation. These experiences sharpened our problem-solving abilities and taught us the value of effective communication within our team.
Beyond the code, we learned crucial lessons in project management and personal growth. We mastered the art of prioritizing tasks to deliver an MVP under tight deadlines. Along the way, we discovered our capacity for resilience and adaptability, fueled by ample coffee and a flexible approach to sleep schedules. These technical and soft skills will undoubtedly prove invaluable in our future endeavors.","Going forward we need to encourage more and more people to keep using our app an reviewing buildings. Since a major aspect of our app is dependent on user input, this does pose a network externality barrier initially. However, with enough people using our app, we can start to drive change and ensure that new buildings to come as well as existing buildings start to accommodate the needs of underrepresented groups so that they do not feel excluded and or feel like outsiders.","amazon-web-services, docker, javascript, mapsapi, mongodb, nextjs, python","https://github.com/DHRUVvkdv/mapability-backend, https://github.com/modusami/vthacks-2024-frontend"
VTHacks 12,Forge,https://devpost.com/software/forge,True,"The inspiration for Forge emerged from observing the hurdles that we faced when entering the world of 3D modeling. Traditional tools like Blender are powerful but come with a steep learning curve, often requiring hours of training and practice. We wanted to eliminate these barriers and make 3D design accessible to everyone, regardless of their technical background. By leveraging AI and voice technology, we envisioned a platform where creativity isn't limited by technical skill, allowing anyone to bring their ideas to life simply by speaking.",,,,,,,"blender, openai, python, socket, threading, whisper",https://github.com/noahpro99/vthacks-2024
VTHacks 12,Subleasing VT,https://devpost.com/software/subleasing-vt,True,"Finding a dedicated subleasing platform can be challenging, often pushing people to rely on social media. Our platform fills this gap by offering a streamlined way to search for subleases that meet specific needs.","Our platform allows users to create, manage, and search sublease listings. With AI technology, users can input their preferences, and the system tailors results to best match their requirements and interests.","We developed the platform using a Swift frontend paired with a Flask backend. We integrated AI-driven search algorithms and implemented secure user authentication to ensure a smooth, reliable experience. Collaboration tools like Git and Figma helped streamline our design and development process.",One of our biggest hurdles was integrating the backend with the frontend. Each of us had to tackle new technologies and implement features we hadn’t worked with before. This pushed us out of our comfort zones but ultimately strengthened our skill sets.,"We’re proud of building a fully functional MVP within the hackathon’s time constraints. Our team successfully leveraged diverse technologies and collaborated effectively, combining our individual strengths to deliver a seamless product.","Throughout the process, we gained valuable experience integrating Swift with Flask and developing AI-powered features. We also learned to navigate unfamiliar technologies, prioritize tasks efficiently, and collaborate effectively under time pressure.","Next, we plan to enhance the AI functionality by implementing web scraping to gather more accurate and diverse data from a variety of apartment and housing listings, further improving search precision.","azure, flask, json, python, sqlalchemy, swift","https://github.com/steadman1/VTHacks2024, https://github.com/steadman1/ApartmentsVTFrontend"
VTHacks 12,IllumiLens AI,https://devpost.com/software/visual-variables,True,"    The inspiration behindIllumiLens AIcame from personal struggles with traditional forms of learning, such as lectures and rigid homework assignments. Many students turn to alternative methods to truly understand complex concepts. We were deeply inspired by 3Blue1Brown’s captivating visualization videos, which make math, computer science, and physics feel intuitive. However, these videos are limited to certain examples and topics — only what’s posted. This sparked an idea:What if we could create a personalized, interactive, and inclusive way of teaching that caters toeveryone’sunique learning needs?Especially for students who have been left behind by traditional education methods. And thus,IllumiLens AIwas born! In traditional educational settings, many students are left behind, not because they lack intelligence, but because theirlearning styleorindividual needsare not met. This issue is especially prevalent among students fromneurodivergent communities(e.g., those with ADHD, dyslexia, or autism) or individuals with different learning preferences. Traditional lecture-based learning does not cater to these students, leaving them at a disadvantage. Moreover, education is often inaccessible for those who prefer or requirevisual learning methodsbut lack access to tailored resources. IllumiLens AItackles this inequity by offering a personalized andinclusive learning experience, making complex concepts accessible toalllearners, regardless of their background, abilities, or preferred learning style. IllumiLens AIprovides a personalized,inclusivelearning experience for those who struggle with conventional methods of education. We believe learning should be accessible toall, no matter their learning style or background. 🌍 Students — particularly those withneurodivergence(like ADHD, dyslexia, or autism) or those who simply don’t resonate with lectures and textbooks — deserve a different way of understanding complex topics.IllumiLens AIteaches from avisual and intuitiveangle, helping users gain deeper insights through dynamic animations and explanations tailored just for them. Here’s how it works: We used the powerfulManimlibrary to generate visuals via Python code. To make the website most accessible, we built the frontend withTypescriptwhile using Python to streamline the backend. Leveraging theAzure AI API, we dynamically created animations tailored to specific user inputs, allowing for immense user customization. Python scripts are run on the server and videos are converted to binary to be stored in the cloud. To ensure students could save and revisit their learning materials, we implementedMongoDB Atlasfor storing their visualizations and videos, maintaining a leaderboard of animations, and customizing their AI prompts. Signing up and logging in is handled seamlessly withPropelAuth, creating a smooth, user-friendly experience, and is integrated with the database and videos to give each user a profile of past creations. Setting up Manim was a major challenge — the number of dependencies required to produce high-quality visuals was no small feat. Crafting the right code for these visuals using AI prompts was another hurdle, as the precision needed to match user requests was tricky to navigate. We also faced deployment issues and hit a roadblock with the Azure API, which stopped processing requests at 5 AM! But after some coffee and deep troubleshooting, we powered through. ☕💪 We’re incredibly proud of building a product thatwewould love as students and that we believe will make a meaningful impact for others. Our platform aims to help students who have been left behind by traditional systems not because of intelligence, but because their unique learning needs weren’t catered to. The systems of customization are built specifically around research-proven methods to enhance learning for those with ADHD, focusing on figures,step-by-step reasoning, and the ability to replace complex theory (if you want) with sensible analogies. On top of that, we’re proud of designing asmooth UIthat ensures accessibility, and tackling technical challenges (especially at the crack of dawn!) to deliver a truly functional and beautiful product. From dynamic AI-driven code generation to new tools like the Manim library,MongoDB, andPropelAuth, we’ve learned a lot. We also gained invaluable experience in bridging frontend and backend tools to create a seamless user experience. One of the biggest takeaways was the importance ofpersonalization in learning— the idea that each student can understand complex topics if taught in a way that makes sense to them. We’re excited to introducecontextual memoryinto the platform! Imagine an even more interactive experience where users can ask follow-up questions on specific parts of a visual explanation they don’t understand. 🧠 By continuously adapting to the learner’s needs,IllumiLens AIwill become a personalized tutor that evolves with the user, making learning trulyinclusivefor everyone — no matter their learning style, background, or ability. 🌟",,,,,,,"api, azure-ai, manim, next.js, python, typescript",https://github.com/ryanz222/VTHacks12
VTHacks 12,Health PantryCam,https://devpost.com/software/health-pantrycam,True,"The idea emerged from a common frustration—standing in front of the fridge, staring at random ingredients with no idea what to make with class in an hour. After spending too much time searching for recipes that didn’t fit it was clear there had to be a better way. What if you could simply take a picture of what’s in your kitchen and instantly get meal suggestions? No more wasted time—just quick, easy healthy recipes based on what you already have that keep to your dietary restrictions and goals.","Snap a picture of your kitchen ingredients, and our Azure AI and ML models do the rest—identify what’s in the photo, consider your dietary restrictions and lifestyle choices, and suggest meals using the spices and pantry staples you already have. The goal is to provide healthy recipes to students so they can focus on what matters.","We built the front end using React, JavaScript, and TailwindCSS. To develop and train the machine learning image recognition model, we used Python, TensorFlow, and Roboflow. For the recipe suggestion feature, we leveraged Azure OpenAI for natural language processing, Bing AI for image processing, and Azure App Services for hosting.",,,,"If the users know they have certain health conditions we want to be able to predict their diet. We would also like to add a dynamic element to our ingredients page. As the user adds an ingredient, we want our app to display soft pop-up recipe suggestions. Lastly, we would like to train our image recognition model with more data and a larger selection of food products.","azure-app-service, azureopenai, bingai, github, html, javascript, next.js, python, react, roboflow, tailwindcss-azure-open-ai-for-natural-language-processing, tensorflow",https://github.com/nicfigu/nicfigu.github.io
VTHacks 12,First Flight,https://devpost.com/software/person-finder,True,"I was inspired to make this project after I learned about the difficult yet crucial job of first responders during natural disasters. First responders must locate victims regardless of treacherous and dangerous terrain, often in bad weather. Moreover, speed is crucial - minutes delaying search and rescue efforts can impact the survival of victims. I decided to build a project that can assist first responders and reduce the time it takes to find victims.","First Flight uses machine learning to locate people in aerial imagery from autonomous drones. Drones can be pre-programmed to follow a certain flight path and fitted with cameras to record and stream video footage. The footage from the drone doesn’t have to be super high-res, in fact, the lower the resolution, the faster the inference time for detecting people! For this project, I used previously-recorded footage because I didn’t have access to a drone (I actually tried to build one and use a GoPro, but it couldn’t fly – maybe the best hack that didn’t work?!). After streaming video footage, I used the YoLoV8 object detection algorithm to draw bounding boxes around people, animals, cars, boats, and more in real time. The YoLoV8 model uses convolutional neural networks to locate and detect over 30 classes of objects in a video stream up to 120FPS – that’s less than 50 milliseconds per inference! After identifying people in the image, the GPS data from the drone is used to pinpoint the location of the victims. This GPS data – in the form of latitude and longitude coordinates – are used to drop pins on a google maps view, which helps first responders locate victims.","First Flight is built using the Flask framework. I chose Flask because the machine learning code for object detection uses Python, and Flask provided a simple yet effective way to integrate everything into a web app. The core of the project revolves around computer vision via OpenCV, which I used to capture and process the video feed. The video stream is then processed by TensorFlow to run the YoLoV8 model, which was trained on the COCO2018 dataset—a collection of 1.2 million labeled images across various categories. The web application serves as the front-end, allowing first responders to view both the real-time video feed and the Google Maps view, where pins are dropped based on the GPS coordinates tied to detected people. Google Maps integration plays a crucial role in pinpointing the exact location of potential victims, making it easier for rescue teams to respond quickly.","One of the main challenges was dealing with aerial imagery. My first attempts at capturing aerial imagery involved building a plane with an onboard camera. I tore up some cardboard boxes and built a make-shift plane, but the cardboard was inconsistent, weak, and too heavy. My next attempt used a cheap foam glider I purchased for 10 dollars. Although the glider flew perfectly, the GoPro I had lying around was too heavy for it to fly. Crunched on time, I had to rely on pre-recorded aerial footage. Another big hurdle was learning Flask from scratch. I had no prior experience with the framework, and integrating it with the machine learning code while also serving video streams in real time was definitely a challenge. Optimizing the object detection model for real-time performance was also tricky, especially since I was running everything on a local machine, which has computational limits.","I’m really proud of successfully training the YoLoV8 model on such a large dataset and getting it to work for real-time object detection. It’s crucial for this application that the model is fast and accurate, and I’m happy with the results. Another big accomplishment was building the Flask app from scratch, even though I had never used Flask before. I managed to get the whole system up and running, from capturing video to displaying detection results on a web app. Finally, integrating Google Maps with the object detection results was a key part of this project, and I’m proud that it worked seamlessly. It really enhances the functionality of the app and makes it easier for first responders to visualize locations.","This project taught me a lot about using Flask for web development, especially how to integrate Python-based machine learning models into a live web app. I also learned how to train and deploy object detection algorithms like YoLoV8 using TensorFlow, and gained experience working with large datasets. Additionally, I now have a better understanding of how to process and display real-time video streams, which was a crucial part of this project’s success. Finally, I got a bit of experience scoping out problems and completing tasks even with inadequate materials and resources.","The next step is to integrate an actual drone into the system so I can use real-time, live footage for detection. This will allow me to test the system in real-world conditions, rather than relying on pre-recorded videos. I also want to explore further optimization of the model to improve its performance on lower-power devices like onboard processors or edge devices. This would make it easier to run the entire system directly on the drone itself, rather than relying on external computing power. In the future, I’d also like to add features like automatic flight path generation based on high-risk areas and integrate thermal cameras for detection in low-visibility conditions or during nighttime operations. Finally, I want to test the system in an actual natural disaster or public safety emergency.","flask, javascript, opencv, python, tensorflow, yolov8",https://github.com/aneil04/FirstFlight
VTHacks 12,REFLƎCT ⱯI,https://devpost.com/software/refl-ct-i,True,"We were inspired by the feeling of doomscrolling every morning, that loss of productivity that slows you down every day. But now, with our team's latest invention: REFLƎCT ⱯI, you'll never have to worry about that again! Start your day with the most important information to get you started off right.","Oursmart mirrorhelps youself reflectevery morning by greeting you with your tasks for the day from yourGoogle or Outlook calendar. By showing users their schedule first thing in the morning, we help remind users of important tasks and responsibilities. Know what your tasks are for the day? Simply swipe your hand in front of the mirror...and boom! Magic! The AI powered mirror will show you the weather and your agenda for the week.",We usedOpenCVwith theGoogle media pipelineto detect hand gestures. We then correlated the gestures with the distance your hand traveled and its velocity to determine swiping while minimizing false positives. The User Interface is written inangular.jsand uses aWebSocketto link to the OpenCV application to move through the pages. We use aFlask APIto get personalized weather/calendar information.,"We had some difficulties moving from hand detection to motion detection. In order to overcome this challenge, we usedadvanced mathematicsto store the previous hand data to infer the gesture at a given time. Additionally, it was difficult to get the model running on the computeconstrainedRaspberry PI, so we run inference on a laptop and communicate all gesture information to the PI over theWebSocket. When linking to ourFlask APIwe found that dates and times in calendars frequently add additional information such as the timezone and Null parameters to the time of an event that isn't needed to parse the time and date. To solve this, we had to extract only the text within the time strings that fit a proper date format.","Custom trained machine learning model for gesture recognition. 
    Because we did frequent integration tests between components, our project came together smoothly and with relative ease. We spent minimal time onintegrationand were then able to enhance theUI/UXof our mirror.
TheUI/UXof our product is very natural. People who interacted with our project very quickly understood how the gestures worked and how to scroll between our various frames.","Over the course of the last 36 hours, we learned how to take popsicle sticks, some hot glue, aRaspberry Pi, and a dream, and make a functional product! With many team members new toangular.js, we had to learn on the fly to debug ourUser Interfaceand link it with ourFlask APIand theMachine Learningmodel handling the gesture recognition. We learned how to use handmark landmarks from media pipe to detect hand gestures.","We want to integrate presence detection to wake up the mirror when you walk up to it and minimize power consumption. If we were to continue with this project, we would switch from aRaspberry Pito something capable of running theMachine Learningmodel locally, such as aJetson Nano.","3d-printing, ai, angular.js, blood, cnn, computervision, flask, github, google-media-pipeline, keras, onnx, opencv, raspberry-pi, sweat, tears, tensforflow, woodworking",https://github.com/MollyShe/SmartMirror
VTHacks 12,Linguify,https://devpost.com/software/linguify-katunw,True,"Linguify - Welcome Page Central Issue Dark Mode Thumbnail Light Mode Thumbnail Why Linguify? Tech Stack Architecture ""Learning a new language is becoming a member of the club—the community of speakers of that language.""—Frank Smith Traditional language learning apps are repetitive and fail to prepare users for real-life conversations. According to a Rosetta Stone survey,73% of language learners feel anxious about using their new skills in everyday situations. Studies also show that users of popular apps like Duolingo havesignificantly lower listening proficiencyand lack sufficient speaking practice. This gap leads to boredom, decreased motivation, and high dropout rates. Linguifytransforms your environment into an immersive language learning experience: Contextual Learning with Image Recognition:Snap a photo of your surroundings, and Linguify generates relevant vocabulary and phrases, turning everyday moments into learning opportunities. AI-Powered Conversations:Engage in realistic dialogues with an AI that adapts to your proficiency level and past interactions, enhancing speaking and listening skills. Immediate Feedback:Receive instant pronunciation and comprehension feedback to improve accuracy and confidence. Cultural Integration:Learn cultural nuances and social norms to connect more deeply with the language community. Bridging the Gap to Real Conversations:By simulating real-world scenarios, Linguify reduces anxiety and prepares you for authentic interactions. Personalized Learning Experience:Adaptive AI focuses on your strengths and addresses weaknesses, keeping you engaged. Enhanced Motivation:Gamified elements like streaks, levels, and leaderboards make learning fun and encourage consistency. Comprehensive Skill Development:Linguify integrates speaking, listening, reading, and writing for holistic language mastery. High Demand:With over500 million userson language platforms and80 million active monthly learners, there's a significant market seeking more effective tools. User Dissatisfaction with Current Apps:While95.5%find apps like Duolingo user-friendly, only78.8%are satisfied, indicating a need for better solutions. Linguify offers a unique, immersive approach that traditional apps lack. We provide practical, real-world practice that builds confidence and fluency, making language learning engaging and effective. Transform your language learning journey with Linguify—where the world becomes your classroom.",,,,,,,"aceternityui, fastapi, firebase, next.js, openai, python, retell, shadecn, tailwindcss, typescript","https://linguify.study/, https://github.com/IdkwhatImD0ing/Linguify"
VTHacks 12,NestMates,https://devpost.com/software/nestmate,True,"Finding the perfect place to live is challenging enough, but the difficulty amplifies when searching for compatible roommates. Many of us have experienced the discomfort of living with people whose lifestyles and interests don't align with our own. This inspired us to create NestMates—a platform designed to simplify the process of finding both apartments and like-minded roommates by matching users based on their personal profiles.","NestMates connects users with potential roommates and available apartments that best fit their preferences, lifestyles, and budget.","We developed NestMates using Next.js to create a sleek and responsive web app. MongoDB was our choice for the database, allowing us to efficiently store and manage user profiles and apartment listings. To ensure secure user authentication and data protection, we integrated PropelAuth into our application. Our team collaborated using GitHub for version control and used Copilot for rapid development.","One challenge was integrating PropelAuth with our Next.js application. The documentation wasn’t very thorough so it took some time to get that running. PropelAuth was also a bit tricky to get working in the prod deployment, where we had to integrate it with our GoDaddy domain. We were all also fairly new to Next.js, so we ran into different issues while integrating client components with the server, and also with some mongodb serialization errors.","We're proud of creating a functional prototype of NestMates within a limited time frame. We successfully integrated Next.js, MongoDB, and PropelAuth into a cohesive application. We developed a working matching algorithm that provides users with personalized roommate and apartment suggestions. Overall, getting through the technical challenges and bringing our idea to life was incredibly rewarding.","We gained substantial knowledge of Next.js and its capabilities for building dynamic applications. Implementing PropelAuth enhanced our skills in user authentication and security best practices. Working with MongoDB improved our understanding of database management and data modeling. We also learned about developing algorithms for matching users based on multiple criteria. Beyond technical skills, we recognized the importance of teamwork, effective communication, and time management in a fast-paced environment.","Improve matching algorithm
Add real estate listings for new locations (more robust real estate scraping/api)
Refine UI/UX elements of website
Add a real estate listings map
Add more profile preferences","copilot, css, defang, git, html, javascript, mongodb, next.js, propelauth, typescript","http://nestmate.us, https://github.com/salvadorbu/NestMate"
VTHacks 12,Site Sensei,https://devpost.com/software/site-sensei,True,"As backend developers, both of us often find frontend design time-consuming and tedious. We wanted a tool that could help us quickly prototype web app ideas without spending too much time writing simple code. Site Sensei was born from our personal need to streamline the process, and we realized that many others who focus on backend development could benefit from this tool as well.","Site Sensei leverages generative AI to instantly create fully functional and interactive web pages using HTML and JavaScript. Users provide a prompt, and Site Sensei generates a complete web page based on the input. To ensure consistent and high-quality results, an additional AI enhances the initial prompt before generating the final product. Once a page is generated, users can further adjust and refine it by re-prompting the AI with new instructions. This iterative process allows users to tweak the design or content without manual coding. Each version of the page is saved, enabling users to revert to previous revisions or experiment with different prompts. When users are satisfied with the final design, they can download the HTML file to use in their own projects, giving them full control over the generated content. Site Sensei also features an Explore page, where users can share their creations and browse other community-generated pages for inspiration. Whether for quick prototypes or fully functional web pages, Site Sensei offers a seamless, AI-driven experience for anyone looking to bring their ideas to life.","We built Site Sensei using Next.js for the framework, OpenRouter for AI model access, and Supabase for our SQL database. Authentication is handled through Auth0. Claude 3.5 powers the AI-generated web content, while our platform allows real-time user interaction with the generated pages.",One of the main challenges we encountered was properly displaying generated content on the page and filtering out problematic code like external links and JavaScript alerts that could break the site.,"We successfully created sleek and functional web pages, especially when detailed prompts were given. One highlight was a personal resume page I made using the platform, with seamless animations and a clean design.","We learned that accounting for AI's randomness can be challenging. It's difficult to predict what the AI will output, and compensating for that in code, particularly with more unpredictable elements, was a learning curve.","Next, we aim to launch a live site where users can either buy credits or use their own API keys for generating web pages. As more users contribute content, we plan to refine the system further, potentially incorporating retrieval-augmented generation to enhance the quality of the results.","ai, auth0, javascript, nextjs, openai, openrouter, supabase","http://www.site-sensei.design, https://github.com/Neeeser/VTHacksSiteSensei"
VTHacks 12,SeeBoard,https://devpost.com/software/cvoard,True,We wanted to do something with computer vision and this is what we came up with!,"The program tracks your hand movements through your webcam, which is then used to allow users to control an on-screen keyboard or a virtual mouse using hand gestures. You can collapse your hand to left click and rotate your hand to switch modes.",We built this using OpenCV and a lot of other Python libraries like PyQt and keyboard.,"Fine-tuning the hand tracking was very difficult. It was also very difficult to get OpenCV, PyQt, and the other Python libraries we're using to cooperate.","Our main motivation for this project was doing a project with computer vision, as none of us had ever made one before.","We learned how to leverage OpenCV for computer vision tasks, as well as how to use mediapipe for image recognition tasks. Additionally, we used PyQT for our GUI, which was most of our team members' first times using the library.",Potentially adding an autocomplete feature using AI since typing with this software is rather slow,"opencv, pyqt, python",https://github.com/Arleee1/SeeBoard
VTHacks 12,AmiGo,https://devpost.com/software/amigo-tqnrvc,True,"We faced commuting challenges between our Falls Church campus and the hackathon venue, and noticed many fellow students struggled with efficient carpooling. This inspired us to create a solution that helps students connect and carpool easily.","Our app allows students to find and carpool with others traveling from one location to another, streamlining the process and improving travel efficiency.","We developed a mobile application using React Native, integrated facial recognition with Azure Custom Vision, implemented a recommendation system with K-Means Clustering in Python, and deployed it on Defang. The backend is powered by Express and the database is managed with Microsoft SQL.","The complex design with multiple layers and tight time constraints were our biggest challenges, making the development process intense.","We’re proud of our unique GT meter, which scores how well drivers and passengers might enjoy each other's company. Completing the project with fully integrated backend, frontend, database, and ML models in just 24 hours is also a major achievement.","We learned effective team collaboration, dividing tasks and then integrating them seamlessly, which was crucial for our project's success.","We plan to enhance the app by adding features for improved security, better passenger tracking, and scaling it for broader use.","azure, css, customvision, defang, docker, express.js, flask, javascript, mssql, python, react-native, sql, typescript","https://github.com/sanjeevp26/Frontend---VTHacks, https://github.com/ram-prashanth/backend-vthacks, https://github.com/ananyaprakashvt/match-service, https://portal.azure.com/?microsoft_azure_marketplace_ItemHideKey=microsoft_azure_cognitiveservices_customvision#@VirginiaTech.onmicrosoft.com/resource/subscriptions/8db643e9-e927-49ff-a623-750f038d4308/resourceGroups/rg-vthacks/overview"
VTHacks 12,InstaPC,https://devpost.com/software/instapc,True,"In today's fast-paced digital world, the need forflexible computing environmentshas never been greater. We realized that developers, IT professionals, and even casual users often need access todifferent operating systemsfor various tasks. However, setting up and maintaining multiple physical machines or dealing with complex virtualization software can be time-consuming and resource-intensive. This inspired us to createInstaPC, a solution that brings the power ofmultiple operating systemsright to your browser, instantly and effortlessly.","InstaPC revolutionizes the way we interact withvirtual machines. Our platform allows users toprovision and run multiple virtual PCs instantly, directly in their browser. Whether you needWindows,macOS, orLinux, InstaPC has you covered. Users canseamlessly switch between operating systemswith ease, all from a single, intuitive interface. To enhance the user experience, we've integrated anAI chatbotthat provides guidance on VM configuration for specific purposes, making it easier than ever to set up the perfect environment for your needs.","We leveraged a powerful stack of modern technologies to bring InstaPC to life. For the frontend, we usedNext.jsas our React framework, styled withTailwindCSSand enhanced withshadcncomponents to create a sleek and responsive user interface. The backend is powered byExpress, which manages the complex orchestration of virtual machines. We utilizedDockerfor containerization,QEMUfor hardware virtualization, andVNCfor remote desktop access.Nginxserves as our reverse proxy, ensuring smooth communication between components. The AI chatbot is powered by theMeta family of LLaMA models, providing intelligent assistance to our users.","The biggest challenge we faced was getting the virtual machines to display seamlessly in the browser. IntegratingVNCand setting up the necessaryproxiesto ensure a smooth, low-latency experience required significant troubleshooting and optimization. We also had to overcome hurdles in managingresource allocationfor multiple VMs running simultaneously and ensuring that the system remained responsive under various load conditions. Integrating theAI chatbotand making it context-aware of VM configurations was another complex task that required careful implementation.","We're incredibly proud of creating a solution that makesvirtualization accessible to everyone, right from their web browser. Theseamless integration of multiple operating systemsin a single interface is a significant achievement. We're also thrilled with theperformance and responsivenesswe've achieved, allowing users to switch between VMs almost instantly. The integration of anAI-powered chatbotfor configuration guidance adds a layer of user-friendliness that sets InstaPC apart. Finally, we're proud of overcoming the technical challenges ofbrowser-based VM display and management, pushing the boundaries of what's possible in web applications.","This project was a tremendous learning experience for our team. We deepened our understanding ofvirtualization technologies,container orchestration, and the intricacies oflow-latency browser-based remote desktop solutions. Working withVNCandproxiestaught us valuable lessons about network optimization and security. IntegratingAI modelsinto a practical application expanded our knowledge of machine learning deployment. Perhaps most importantly, we learned the value of persistence and teamwork in overcoming complex technical challenges.","The future of InstaPC is bright, and we have several exciting enhancements planned:","docker, next.js, qemu, rest, tailwind, vnc",https://github.com/MovementGH/InstaPC
VTHacks 12,Hungry Hokies,https://devpost.com/software/hungry-hokies,False,"On average, freshmen gain 7.5 pounds during their first year of college. Maintaining a healthy diet is crucial, often playing a more significant role in overall well-being than exercise. However, with busy schedules filled with classes and extracurricular activities, students frequently struggle to find time to focus on their nutrition. As a result, diet can become an afterthought, making it challenging for students to maintain healthy eating habits amidst their hectic routines.","HungryHokies is a convenient and efficient web app designed to help students manage their nutrition. By simply snapping a picture of their food, students can instantly access detailed information about its calorie content and macronutrients. The app also automatically logs the meal, allowing users to easily track their daily intake and stay aligned with their personalized calorie goals. This process empowers students to make informed dietary choices, promoting healthier eating habits without the hassle of manual tracking.","We built this app by combining our react front-end with a FastAPI back end. Our react front end uses tailwind for responsive page styling and directly interfaces with the APIs provided with our backend. Our backend then connects to GPT vision for our food recognition engine and database to provide users with important metrics relating to their meals and personal health.  This works by using GPT vision to detect the foods present in an image, then we query the NutritionIX database to provide the information for each item. This information is then logged for the user using a MongoDB database. We also allow users to log information such as their weight and height data so they can track their personal nutrition goals. This allows us to use formulas such as the Mifflin-St Jeor Equation to provide recommendations and assist users in their journey.","We struggled the most with creating our own custom AI model. While we were able to train our own models, we quickly realized that this would not accurately account for many foods because the datasets we used mostly included western foods. It did not sit with us that it wouldn’t be able to pick up other cuisines, especially with the amount of diverse food in Blacksburg. Instead, we chose to use GPT vision for image recognition for its much more accurate and diverse detection abilities than we would be able to employ on our own in such a limited time.","We utilized ReactJS, Tailwind, and MongoDB technologies to provide an aesthetically pleasing, responsive, and uniquely Hokie web application so that every user feels compelled to use our product. We’re proud that we are helping Hokies make smarter choices about their food consumption in an easy and effective manner.",We learned from testing that this app can help people who have a hard time reading the small labels on foods. Which means this may help people with poor eyesight.,"The next big thing for HungryHokies is to connect to apple health. This will allow us to provide much more data to the user. Specifically, how diet and exercise can help maintain or reach a person’s goal.","amazon-web-services, javascript, mongodb, node.js, python, react, tailwlind",https://github.com/AHumanMaybe/hungryHokie/tree/prod
VTHacks 12,SafePath,https://devpost.com/software/safepath-a6n59i,False,"There are a lot of prediction-based models that are used to forecast when natural disasters occur, but we still hear about so many tragic deaths and destruction caused every year by hurricanes, tornadoes, and typhoons. We wanted to create an app that not only predicts how a disaster will unfold but also assists people caught in that area so that they can get assistance on finding safety.","SafePath is a web application that provides real-time alerts and pathing to users who are in close proximity to disasters. It also predicts where the disaster will go and potential damage, and determines new paths to safety in the event of interference.  By providing the user with routes and notifications about disasters, we hope that many lives can be saved.","We used React as our framework to create the front end of the application since we could have more flexibility with adding animations and making the UI user-friendly. There was a large focus on minimalism and avoiding cluttering up the map, because, in the event of an emergency, the user wants fast and easily digestible information. For the backend portion, we used Flask and Python to create our pathfinding algorithm, and Keras + Python to create our DNN(Deep Neural Network). By training the DNN through API information from openweathermap and the Google Maps API, we were able to predict how a disaster would move on a map, and how a path could be built around it.","A major challenge we ran into was getting good accuracy with our DNN model. There are a lot of factors that goes into determining where and when a disaster will occur and what direction it will unfold in, and we only had two APIs that could give us enough information to train our model off of. However, a lot of the time there are environmental factors and destruction that cannot be accounted for with existing technology, so finding reliable data is something that would require more exploring and possibly hardware based solutions. Another challenge we ran into was creating an algorithm that was efficient enough to pathfind off the predictions and update the user. Since it is a survival-based app, we cannot take up too much time, so it was important we were efficient and minimized overall recalculations of potential safe routes to take.","We are proud of managed to finish a demoable version of SafePath, which can navigate across a map and show path calculations to safe zones based on a disaster(a tornado) occurring in the area. Working with neural networks and API integration was something new for our team, so it was very fun learning about how it worked and successfully integrating it into our final product.","We learned a lot about AI and ML model training, as well as how various frameworks such as tensorflow and keras work, and how their usages can vary based on the type of model and data you want to train/utilize. We also learned a lot about Frontend development and were able to explore a lot of fun and unique libraries that improved the overall user experience.","Going forward, we want to create a more reliable pathfinding algorithm that is more efficient and can provide faster updates to the user. We also want to flesh out our disaster detection system and try to incorporate predictions and pathfinding for multiple types of disasters. Furthermore, the current scope of how far we can predict with our model is something that can be improved upon, as we seek to be able to look over entire states and regions of countries in the future.","flask, google-maps, javascript, keras, openweathermap, python, react",https://github.com/sriyamac/SafePath.git
VTHacks 12,ConcertPal,https://devpost.com/software/concertpal,False,"Inspired by the challenge of identifying emerging artists and predicting their performance, this project was designed to help concert venues make data-driven decisions. By integrating machine learning with real-world data from Ticketmaster and Spotify, I aimed to create a tool that offers valuable insights for both venues and concert goers. Throughout this project, I gained hands-on experience with: This project lays the groundwork for an innovative tool with significant potential to transform the way concert venues and music enthusiasts interact with performance data.",,,,,,,"ajax, amazon-dynamodb, amazon-web-services, apis, azure, flask, javascript/html/css, leaflet.js, machine-learning, python, spotify, ticketmaster",https://github.com/dhu2022-dev/VTHacks2024
VTHacks 12,WhatTheFilm!,https://devpost.com/software/whatthefilm,False,"I was inspired to create WhatTheFilm! after constantly seeing clips from movies and TV shows in reels but never being able to figure out their titles! It’s a common frustration for many, so I decided to build a solution that could identify movies and shows from images instantly.","The result was WhatTheFilm!, a fully functional web application capable of detecting movie scenes, identifying the associated movie or show, and providing additional context such as cast and streaming platforms. Despite the challenges I faced, the project was a success and gave me invaluable experience in Custom MLops, Custom ML workflows, object detection, and application deployment through streamlit.","I built a custom object detection model from scratch using YOLOv8 and COCO (You Only Look Once) architecture. I manually downloaded and labeled a dataset of 1,300 images (Yes, you read that right) from various movie and TV show scenes. Labeling the data was a meticulous task that involved identifying key objects and features to help the model accurately detect and classify scenes. This project marked my first exposure to MLOps, learning how machine learning workflows and pipelines operate. It was a challenging yet rewarding experience as I encountered numerous new tools such as Roboflow for dataset management, YOLO for model training, and Streamlit for deploying a user-friendly interface. Managing the end-to-end lifecycle, from data preparation and training to deployment, gave me valuable insight into MLOps practices. After running object detection, I used an open-source large language model (OpenAI) to transform the model’s JSON output into human-readable information. This involved querying details such as the name of the movie or show, key actors, and the streaming platforms where the content could be found. The LLM parsed and interpreted the results, making the app’s output accessible to users. To make the application interactive, I implemented Streamlit as the front-end framework, allowing users to upload images or click photos through their webcam. The interface was simple yet effective, designed for ease of use. I also integrated PropelAuth for handling user authentication.",,,Working on this project gave me firsthand experience with the machine learning pipeline—starting from data preparation to model deployment. We became familiar with new tools like Roboflow for managing datasets and monitoring model performance. Learning how to orchestrate the various stages of MLOps and manage dependencies between components was a critical part of the learning process.,,"coco, css, docker, google-cloud-run, html, openai, opencv-python, propelauth, python, roboflow, streamlit, websockets, yolov8","http://whatthefilm.streamlit.app, http://whatthefilm.us, https://github.com/arnavkhurma/whatthefilm, https://youtu.be/mjn2DcnPNaI, https://auth.whatthefilm.us/en/login, https://drive.google.com/drive/folders/1lc2KZQvA4npQeE-TlU2S7gwskfoVa3PP?usp=sharing"
VTHacks 12,Synchronize,https://devpost.com/software/synchronize,False,"Discover Page, Patient List Patient Account - Left, Physician Account - Right Notification page, Patient list Landing Pages Landing Page, Sign In page QR Code scanner and QR code to identify patients Inspiration This project was inspired by Ekomobong's father's clinic, EnoObong Memorial Medical Services in Nigeria. Observing the challenges faced by both patients and healthcare professionals, especially in rural areas, motivated us to develop a solution that would streamline operations, making healthcare more efficient and accessible. Synchronize is our way of contributing to the clinic’s mission of providing better healthcare to the local community. Synchronize is a comprehensive platform that enhances the patient-physician experience by managing: Health Records:Secure and organized storage of patient medical histories.Appointments:Simplified scheduling for both patients and doctors.Notifications:Real-time alerts using websockets for appointment updates and important health reminders.COVID-19 Resources:A global map showcasing vaccine distribution using the OpenDisease API.Prescription Recommendations:A custom-built system that suggests medications based on patient symptoms, age, and sex.AI Chatbot:A health diagnosis tool powered by OpenAI to assist patients with general medical inquiries. We developed the project using a combination of modern web technologies: Frontend:React.js for an interactive and responsive user interface.Backend:Node.js with Express for managing server-side logic.Database:AWS with Prisma ORM for scalable storage and retrieval of patient and physician data.Websockets:Implemented for real-time notifications to ensure that both patients and physicians receive updates immediately.COVID-19 Data:OpenDisease API integration for displaying global vaccine distribution on an interactive map.Prescription Algorithm:Built from scratch, using a weighted scoring system to provide medication recommendations based on symptoms, accessibility, and patient age all in Javascript Notifications Using Websockets:Implementing live notifications for real-time updates was difficult, especially ensuring messages reached users when they were offline. We designed a fallback mechanism for offline users while refining the notification logic for age groups and appointment urgency.OpenAPI Key Access:Strange as it seems, acquiring the OpenDisease API key posed unexpected hurdles. After multiple attempts and debugging, we finally integrated it successfully for our COVID-19 section.Prescription Algorithm Optimization:Balancing the complexity of the prescription system while maintaining efficiency was challenging. We implemented a 3-level caching system to reduce unnecessary recalculations and improved overall performance. AWS Database:Successfully migrating the database to AWS, ensuring better scalability and accessibility for the app.Real-Time Notifications:Getting the websockets system to function seamlessly for live notifications was a major milestone.Deployment of the Website:Synchronize is fully deployed and operational, accessible from anywhere.COVID-19 Map:Visualizing global vaccine rollout on an interactive map added a critical public health component.AI Chatbot:Integrating OpenAI’s chatbot for health diagnosis provided an innovative way for patients to get initial medical insights. Building Synchronize taught us: Resilience in Problem-Solving:Especially when tackling live notifications and dealing with API integration challenges.The Importance of Scalability:Deploying the database on AWS was key to ensuring the app could handle an increasing number of users.Data Security:Protecting sensitive medical information through secure authentication and encryption was crucial for patient trust and compliance. Refining the Prescription Algorithm:Replacing the current custom algorithm with more medically proven resources to ensure even more accurate recommendations.Improving User Authentication:Strengthening the security of patient and physician login systems.Enhanced UI:Adding features like dark mode to improve the user interface and overall user experience.Integrating More Medical Resources & Expanding physician use:Tapping into more validated databases to make the application even more robust for a greater scale of medical professionals.",,,,,,,"amazon-web-services, css3, express.js, html5, javascript, node.js, open-ai-api, open-disease-api, openstreetmap, prisma, python, react, websockets","https://github.com/Synchronize-VT/synchronize-vt, https://synchronize-vt-1.onrender.com"
VTHacks 12,SNET - Secure Internet,https://devpost.com/software/snet-secure-internet,False,"Our focus was that user would just download the extension, and we will take care of the rest.    SNET LOOKUP TOOL Since we searched for anyone whose email is abc@gmail.com, we got a lot of unusable data .  With phishing attacks and online predators becoming a growing concern, especially for those less tech-savvy, we knew we needed a solution that’s powerful yet easy to use.SNET (Safe Internet)was born from the idea that everyone deserves to feel safe online, especially families. From blocking phishing attempts to giving parents peace of mind about their kids’ online interactions, SNET aims to be a one-stop solution for safer browsing. SNET is made up of two key tools: This Chrome extension works directly with Gmail. Every time you open an email, our tool analyzes it using AI (LLM) and adds a badge next to the subject line. You’ll know instantly whether the email isSafe,Cautious, orDangerous, and we’ll even tell you why. If that’s not enough, it also offers a second layer of protection. Accidentally click on a phishing link? Don’t worry—our extension scans over400,000 phishing URLs in less than 5 millisecondsand blocks dangerous sites before they can do any harm. This is for the parents who worry about who their children are talking to online. Our lookup tool helps you search byemail,phone number, ornameto find out if the person interacting with your child is who they claim to be. We scrape public data sources, likeThatsthem.com, to provide details. The goal? Keep kids safe by giving parents the tools to verify who’s behind the screen. One of our biggest hurdles was finding a way to search through 400k URLs instantly. We explored several options, like hash maps, but after researching space and time efficiency, we settled on using aBloom filterfor fast, lightweight lookups. During deployment, our backend kept crashing because loading the Bloom filter was taking too long (around 15-25 seconds). Vercel's max timeout was shorter than that! After hours of trial and error, we found a workaround by adjusting Vercel’s timeout limit, finally allowing smooth deployment. Initially, we planned to use red as the background color when a page was blocked. However, we realized that strong colors like red might create unnecessary alarm for users who are not as familiar with tech. To make it more approachable, we switched to a simple black-and-white theme that feels calm, clear, and easy to read. One of our goals was to keep user interaction minimal, especially since our tool is for children, parents, and elderly users. More options often mean more confusion. So we stripped back the interface to only what’s necessary, reducing clicks and decision points. In 2022, there were88,262 complaints of fraudresulting in$3.1 billionin losses from people aged 60 and older. Research indicates that individuals over 65 often face challenges in distinguishing legitimate emails from phishing attempts. Phishing is a prevalent type of internet and email fraud that particularly targets older adults. By designing our tool with a user-friendly interface, we aim to reduce confusion and help protect this vulnerable group from falling victim to such scams. Using aBloom filterto handle massive datasets was a huge technical win for us. Integrating a sophisticatedLLM modelfor email scanning was another leap forward in tackling phishing attacks. Every part of the system now works in harmony to ensure swift and accurate protection. We have big plans for SNET:",,,,,,,"ai, bloom-filter, css3, html5, javascript, llm, python, react, vercel",https://github.com/gdg4dev/SNET/
VTHacks 12,NestQuest,https://devpost.com/software/nestquest-ql4kra,False,The inspiration for NestQuest came from the challenges many students and job seekers face when navigating through numerous job offers and opportunities. We wanted to create a platform that not only stores this information efficiently but also provides a seamless and user-friendly experience for managing job-related data.,"NestQuest is a web-based application that allows users to manage and compare their job offers by storing details such as company names, salaries, and job titles in a structured database. The application offers easy access to all relevant information in one place, helping users keep track of their opportunities in an organized way. It compares locations and costs, taking into account federal income tax rates, local income tax rates, and average cost of living from everyday expenses (rent, food, etc.).",We built NestQuest using a tech stack that includes:,"One of the initial challenges we ran into was finding data. A lot of the API's only dealt with sales tax, and there was nothing standardized for income tax. Hence, we spent a lot of time initially gathering data to use for the project. Another challenge we ran into was implementing a database to store users, their information, and their jobs. We tried MongoDB as a database, but instead went with Firebase as it was better suited for our purpose.","Building off of above, one accomplishment we're proud of was being able to use Firebase as our database to store user information. This essentially allows for organization and saving of users on the application. Even though we ended up not going with the MongoDB, this was our team's first time using it and we learned a lot about it on the way, so we're proud that we got to gain experience in database management! Another accomplishment that we're proud of is being able to deploy our website using Vercel, and it being demo-able.","This project taught us a lot about managing front-end and back-end communication, especially with Firebase and Next.js. We gained experience in debugging UI/UX issues and learned how to design scalable and secure authentication systems with Clerk. The project also reinforced the importance of optimizing server-side rendering for performance.","Next, we plan to build a page that shows a listing of houses from homes.com, catering to user preferences for sorting and filtering. This feature will allow users to customize their search experience based on specific criteria, improving the usability and functionality of the platform.","ai, authentication, azure, clerk, css, firebase, javascript, json, llama, next.js, python, react, tailwind, typescript","https://www.nestquest.wiki/, https://github.com/maceoCK/nextquestfrontend"
VTHacks 12,Monocle,https://devpost.com/software/monocle-gflcbq,False,"The idea for Monocle was sparked by the need for a more intelligent and insightful investment tool. We studied the Co-star Group branches(Homes.com/Apartments.com/Land.com) and saw the usefulness of this idea. Existing traditional platforms felt outdated and didn’t keep pace with the evolving market complexities. There are many stock investment portals, but not specifically for real-estate investments. We wanted to build something that didn’t just track data but truly empowered investors with actionable insights.","Monocle is designed to be a game-changer in real estate investment. It can makes the following differences:-
Interactive Portfolio Management: a sleek, interactive dashboard where you can effortlessly monitor your portfolio’s performance, track property values, rental income, and spot trends. Personalized alerts keep you updated on significant changes, ensuring you never miss a beat. It allows you to see and corelate Quality of Life data with asset performance and it can help you gauge the potential of any real estate entity. It also takes NLP query and figures out the requirement of user for further analysis. Advanced Financial Calculations: From managing debt to calculating ROI, Monocle provides robust tools to simulate different scenarios and understand their financial impacts. It’s like having a personal financial advisor right at your fingertips.",,,,"We learned how to effectively combine various technologies to create a cohesive platform. Handling and analyzing large datasets taught us the importance of data accuracy and processing efficiency. Balancing complex functionalities with user-friendly design was a key learning experience. We learned propelAuth, Mongodb and d3JS technologies. We also learned about using NLP techniques to extract search queries.","We plan to incorporate some more of our ideas like:- 
Market Trends & Predictions: Monocle will bring together data from various sources, analyzes it with Azure AI, and generates insightful reports using RAG. Whether you're curious about current trends or future market conditions, we’ve got you covered with clear, actionable insights.","azure, mongodb, next.js, node.js, propelauth, rentcastapi, typescript",https://github.com/ParshwaS/VTHacks12
VTHacks 12,house.inc,https://devpost.com/software/house-inc,False,"We were inspired by how realtors and housing agents help families find homes by working with their specific needs. A good housing agent understands a home-owner's requirements and makes sure they're comfortable with the space, the neighborhood and the distance before suggesting properties to look at. When renting, it can be exhausting pouring through websites by yourself, especially under special circumstances like when you have pets, need wheelchair accessibility or wish to live with a roommate. Using AI can help make the process more human and enable people to find better housing faster.","Our website is a one-stop-shop for all housing rentals, primarily targeted at college towns. Users can find housing using AI to filter listings, sign up to meet roommates and create listings if they have roommates moving out.","For the backend, we used Java Spring Boot, which gave us a solid base for developing reliable and scalable services. On the frontend, we picked React because its component-based structure and high performance helped us create dynamic and responsive user interfaces. We used a mySQL database for data storage and retrieval because of its strong querying capabilities and support for complex transactions.","Throughout the development process, we faced several challenges. One of the major hurdles was enabling our AI bot to access updated data, as the data would constantly change based on user interactions. Another challenge we ran into was training our bot and making sure it was in sync with our database and the user's preferences.",Supporting a variety of features that someone looking for housing would expect to use.,Designing a connected system with multiple types of users can be challenging. Careful system design is essential to ensure smooth development in the long term.,"Implement support for more housing criteria like accessibility standards, parking, etc. Allow users to favorite listings and come back to them on their accounts.","java, javascript, mysql, node.js, react, spring",https://github.com/czarthak/apt-ai
VTHacks 12,Modern Inventory Management System (MIMS),https://devpost.com/software/modern-inventory-management-system-mims,False,Home Page Properlauth MongoDB CONTACT US Page Dashboard About US page,,,,,,,"azure, javascript, mongodb, nextjs, propelauth",https://github.com/Sushantsth21/VTHacks
VTHacks 12,PlasticBot,https://devpost.com/software/plasticbot,False,Inspired by the need tackle ocean plastic pollution and protect marine life.,PlasticBot uses AI and machine learning to automatically detect plastic in underwater images.,I trained a YOLOv5 model using Jupyter Notebook and data from Roboflow to detect plastic waste in underwater images.,Training the model was difficult because working with Jupyter Notebook and configuring the YOLOv5 code was challenging.,Successfully trained an AI model to detect plastic with high accuracy and created a web page to display its functions.,I learned how to train an ML model using data from Roboflow and gained experience with HTML and CSS.,I plan to expand to real-time monitoring and to collaborate with environmental organizations to deploy PlasticBot to help clean up the oceans.,"bootstrap, css, html, jupyter, roboflow, yolov5",https://github.com/tronnolaquonias/PlasticBot
VTHacks 12,StrumAlign,https://devpost.com/software/strumalign,False,Many beginner guitar players struggle with proper technique and current guitar apps only rely on audio signals to provide training while YouTube videos don't provide real-time feedback which is why we wanted to create an app that uses computer vision to provide real-time video-based feedback for users.,The application uses computer vision to map out the user's fingers on a guitar's fretboard and performs curvature analysis on their fingers and determine if their fingers are curved enough and that their wrist is properly straight.,"We used openCV and MediaPipe for hand tracking and then utilized vector math to calculate the angle of each finger based on 3 joints located within the finger. Finally, we utilized React to create a frontend to integrate our computer vision finger tracking.",Some challenges we ran into were integrating the frontend with the computer vision models and displaying them on a website with a nice user interface,We're proud of the fact that we were able to get a working project with so many complicated features to integrate properly into a nice fleshed-out product.,"We learned a lot about our tech stack like React, openCV, and integrating backend and frontend plus also using authentication with Firebase.",We wish to make this more widely available by scaling up to cloud platforms and adding more functionalities such as chord detection and musical tempo training as well.,"expo.io, firebase, flask, javascript, mediapipe, numpy, opencv, python, react",https://github.com/arpan2004/vthacks12
VTHacks 12,ClearView AI,https://devpost.com/software/clearview-ai,False,"Having viewed Homes.com a few times before the Hackathon, our team was very interested in how we could add to an already stellar application that makes searching for a home a breeze.","ClearView AI allows users to upload interior photos of houses and removes unwanted background elements from each picture, allowing future homeowners to feel that the design of the space is up to their interpretation.","Our team was able to build our ClearView AI application through the use of Python, TypeScript, Flask, and the Detr-Resnet-50 AI Model, acquired from Hugging Face.","We were originally very dedicated to creating a serverless architecture for our service via the use of AWS S3 Buckets and Amazon Lambda, however, due to limited knowledge of the service and time constraints, our team had to pivot to a localhost application.",Our ability to brainstorm on short notice! There were multiple times during our ideation and development stages of the hackathon where we had to pivot and were able to do such in a manner that resulted in a rewarding experience.,"It is always best to start working with unfamiliar technology first, rather than keeping it near the end, as such can allow you to determine a better overall plan timewise.",We are eagerly looking forward to the next Virginia Tech-sponsored hackathon and hope to participate in more MLH events before then!,"flask, huggingface, python, typescript",https://github.com/Mohammed532/VTHacks2024
VTHacks 12,PlatePal,https://devpost.com/software/platepal-s71iqu,False,"PlatePal was inspired by the need for a personalized and efficient meal-planning tool that caters to diverse dietary needs, restrictions, and preferences. We wanted to simplify healthy eating for everyone.","PlatePal helps users find recipes based on their dietary preferences, calorie goals, and meal preferences. It offers customized meal suggestions for breakfast, lunch, and dinner, while allowing users to save their favorite recipes for easy access later.","We built PlatePal using Flask as the backend framework, integrating the Spoonacular API to fetch recipe data. MongoDB was used to store user preferences and liked recipes, and the front end was created with HTML, CSS, and JavaScript for a responsive user experience.",One of the main challenges was implementing session management to ensure that liked recipes are saved across multiple searches without overlapping data from different users. Integrating multiple APIs while managing data flow was also complex.,"We successfully implemented a system that personalizes meal recommendations based on calorie intake and dietary restrictions, providing a seamless and intuitive user experience. We're proud of the app's scalability and flexibility in catering to diverse user needs.","We learned how to efficiently handle user sessions and data persistence in MongoDB, while gaining deeper insights into API integration and managing dynamic web applications.","Next, we plan to implement restaurant recommendations based on user location and dietary preferences, enhance user personalization, and introduce a meal scheduling feature for better meal planning.","css, flask, heroku, html, mongodb, openai, python","https://github.com/nathkim/PlatePal, https://platepal-cad89b326c92.herokuapp.com/"
VTHacks 12,HearMail,https://devpost.com/software/hearmail,False,"Our inspiration comes from a personal story of one our group members. His mentor and professor has suffered with vision loss despite his condition he has used all the tools around him to still read emails, however, we thought of an idea to make an even more efficient and effective tool that people who suffer with vision loss can utilize.",It provides accommodation for what might seem like a simple task such as checking emails for those who suffer from vision loss.,"We used React, typescript, html, css, OpenAI api, and other tools to make a robust and reliable web application.",We found it challenging to populate the email client with data from an email account especially for a google email account that required a lot of research since google’s new recent policies made it impossible for old methodologies to work. Another Challenge we ran into is that one of our team members had no experience in web dev and had to rely on the rest of the group in the development process.,"We’re proud to say that our app is reliable, can be used by anyone who uses email, which is everyone :)",We learned a lot of new development skills and tools. We strengthened our ability to work under pressure.,"Our next milestone is get more users to use our app, add new features, and scale out our technology.","css, html5, javascript, openai, react",https://github.com/raphaelsanchez0/hear-mail
VTHacks 12,SchedEz,https://devpost.com/software/schedez,False,"The inspiration forSchedEzcame from a problems faced by college students, people with meeting needs in general: unproductive meetings. During my time in college, I noticed that 90% of meetings often ended without concrete decisions, leading to endless rescheduling and inefficient time management. This inefficiency can be particularly frustrating when students already struggle with managing their academic, social, and personal commitments. To solve this problem, I wanted to create an AI-powered assistant that could streamline the meeting process by real time updates of its documentation of the meeting for voice recognition, jot down key ideas during discussions, and automate scheduling, so professional groups can focus on productivity rather than administrative tasks.",,,,,,,"css, dart, flutter, javascript, python, vscode, xcode",https://github.com/KPhoebeZhang/secretary-ai
VTHacks 12,EagleEye Ai,https://devpost.com/software/meelo,False,"There was a lot of inspiration for this project. One of them was addressing theft crimes which have been increasing in the recent years, so to take initiative, our team worked towards helping lowering these crimes. Additionally, assisting the elderly in their own homes and act as first responders. Looking back in Another inspiration came from Canada’s recent wildfire, the starting fire was caught on CCTV. With our model integrated we can detect fires early on, preventing any catastrophic damages.","Our project integrates an advanced object detection AI model with CCTV cameras, turning it into a proactive safety solution. This AI model constantly analyzes the video feed, identifying and labeling objects within its view. The system is designed to send alerts to authorities or emergency contacts.",,"In the process of training our AI model, we came across many errors that we had to go back to look for. One of them were training time took long more than 6 hours. Also figuring out how to track people were one of the challenges we ran to.",We were able to do a very well object detection.,We learned the do’s and don’ts of how to properly train a model. We learned how to properly classify label images.,"Our next plan is working towards integrating this system into police body cams, fact checking them and sending alerts to the higher officials.","cv2, cvzone, mediapipe, numpy, opencv, python, tesorflow, ultralytics, yolov10",
VTHacks 12,BetterYou,https://devpost.com/software/betteryou-30iw6y,False,"At the heart of well-being lies the balance of nourishing food, invigorating exercise, and restful sleep. With this guiding principle, we embarked on creatingBetterYou—an AI-driven solution designed to personalize your journey to a healthier, more balanced life. Through the power of artificial intelligence,BetterYouoffers tailored meal and exercise plans, crafted to align with your unique needs, ensuring a sound body and a clear mind.",,,"Lack of tokens to host our project in a GPU instance for snappy feeling experiences.
We axed many  good features due to lack of time and energy in this short time.","** Our Project works**
One of the redeeming factors after travelling all the from Stonybrook to New York is that we have implemented what we envisioned at the start.",Apes - Together -Strong!,The Sky! We have a lot to dig into this space. Form Community,"atlas, axios, azure, css, defang, docker, flask, javascript, llama, llm, mongodb, ollama, propelauth, pymongo, python, react","https://github.com/kallagoutham/vthacks12/tree/master, https://github.com/grpnpraveen/BetterYou-bknd-vth24"
VTHacks 12,EyeScreen,https://devpost.com/software/eyescreen-bgrpsq,False,"The recent AI revolution has been dominated by chatbots and language models, but we saw an opportunity to harness the power of computer vision to create something new. We asked ourselves: what if technology could be controlled with just a glance? Imagine being able to scroll through an article on your laptop while eating lunch, or navigating your tablet with ease, hands-free. We realized that existing solutions for people with disabilities were often outdated, expensive, or required specialized hardware and software. This sparked our idea to apply recent advancements in computer vision to create EyeScreen, an innovative solution that makes technology more accessible and intuitive for everyone.","Imagine a world where technology responds to your every glance. With EyeScreen, you can control your computer, navigate your digital life, and interact with the world around you with just a look. Our revolutionary technology harnesses the power of face-tracking to unlock a new era of intuitive, hands-free control. Using EyeScreen, you can:",We leveraged a combination of cutting-edge technologies to bring EyeScreen to life. Our tech stack includes:,"A constant challenge during the development of EyeScreen was prioritizing input commands due to the limited number of distinct facial features we could track. With only a finite range of facial expressions and movements available, we needed to determine which commands would be most intuitive and useful while avoiding confusion and overlap. Balancing a responsive control system with the constraints of facial recognition technology required us to carefully evaluate which facial movements could be reliably detected and effectively used. We explored various methods to make the most of each facial feature, ensuring that the controls remained intuitive and user-friendly.","We’re incredibly proud of what we’ve achieved with EyeScreen. Our innovative system seamlessly transforms facial movements into intuitive, hands-free control, breaking new ground in accessibility and user experience. By expertly integrating cutting-edge technologies like OpenCV2, MediaPipe, and PyAutoGUI, we’ve crafted a real-time, high-performance solution that’s both responsive and intuitive. EyeScreen not only sets a new standard for interacting with technology but also opens doors for more inclusive and adaptive computing.","During the development of EyeScreen, we gained crucial insights into mapping and interpreting facial movements, including eye, eyebrow, mouth, and blinking actions. We discovered that accurately tracking these subtle movements required a delicate balance between precision and real-time performance. Mapping eye and eyebrow movements taught us how to translate these gestures into intuitive controls, such as scrolling and navigation, while ensuring the system remained responsive and reliable. We learned to refine our algorithms to distinguish between various expressions and gestures, enhancing the system’s ability to interpret user intentions accurately.","Development is an ongoing process, but the technology is now fully ready for everyday use. It performs all the tasks a standard keyboard and mouse can handle, and offers additional functionalities that extend beyond basic tools. With intuitive controls and comprehensive features already integrated, the technology is designed to be an affordable solution accessible to anyone with a computer. The next step is to focus on distribution, ensuring that it reaches those who can benefit from it most.","mediapipe, opencv2, pyautogui, python",https://github.com/MarcoHolden/VTHackathon
VTHacks 12,Scam't,https://devpost.com/software/scam-t,False,"My inspiration comes from: 1) The recent launch of Character’s real-time conversation AI feature. It made me think that if AI can handle conversations and respond like humans in real time, maybe we could apply this technology to fight scams.
2) VTHacks 11, where the winning project was a fun, interactive app that tested users' lifespan. It inspired me to create a more interactive software.
3) A dictionary I use provides an interesting quote each day, like ""You can do things that no one else can."" or ""I was encouraged by my teachers to explore my curiosity.""","There are many apps out there that try to block scam calls and emails, but these measures are not always enough. Many people, especially the elderly, still fall victim to scams. This software targets areas where current defenses are weak. It provides real-time analysis of conversations during phone calls, with trained AI determining the likelihood of a scam. If a scam is suspected, the software suggests trusted solutions and sources of help (like .gov or .org sites, or local police). To protect user data, all conversation data can be deleted at any time, and we never share any information externally. Another key feature is Deepface detection, which analyzes video calls (like Facetime) in real time to check if the person might be using Deepface for scams. Lastly, each morning, the app sends users a short message related to scam prevention, helping them learn something new when they check their phones.","I used Figma to create the layout and demo. Since I didn’t have enough time during the Hackathon to fully develop the mobile app or learn React, I used Figma’s design-to-code feature to help complete the front-end.","I hadn’t originally planned to develop a program, and because I know only a few programming languages, I couldn’t bring all my ideas to life in such a short time. I also faced challenges setting up new coding environments for Kotlin and Python’s Streamlit.","I’m proud of how eager and sincere I was in wanting to complete a personal project. I’ve struggled to find direction in CS, and I wasn’t sure what I was passionate about. This project is an important first step for me. What we learned
1) For the next Hackathon, I need to prepare earlier. Don’t assume you won’t come up with something and lower your guard. You never know when a great idea will pop into your head that you really want to finish.
2) I want to learn React so that next time, I can focus on the front-end (which I enjoy more) and collaborate with classmates who are good at back-end development.",,"I want to perfect the features I’ve mentioned and explore adding more useful public safety features, like a “local emergency news alert” but for scams. I think that could be something really helpful for users.","figma, figma-design-convert-to-code, intellij-idea, kotlin, vscode",https://github.com/ChungChan02/vthacks12
VTHacks 12,CHUT,https://devpost.com/software/chut,False,"UrbanClap is an Indian business model, where customers can hire/book services like haircuts, cleaning, massage etc. We thought of implementing this idea among college students here at VT.","The main aim of CHUT. is for students to contact other students directly for any support, and get their service, which is affordable. Furthermore, students can register as service providers, providing any of the main 3 options on the website (carpooling, tech support, tutoring) as a way to help other students and earn money.","Front-end: Ishaan and Aditya
Back-end: Debargho and Sankalp
Ishaan was tasked with the website as he experience with react and Aditya worked on the widgets of the website. Debargho was tasked with integrating the Atlas Cloud Database to the Springboot to the front-end and aiding Sankalp in the development of the Springboot. Sankalp was tasked with making the API and linking it to the Springboot","Custom-making an API with multiple features
Integrating Springboot to Atlas Database
Working with React packages","The UI and overall design of the website
We created our own API for this project.","We learned new skills like MongoDB, Springboot, and linking API to Springboot.",We plan on expanding services to students. We have 3 services at the moment. We will be adding more facilities to the website. We also plan on taking the idea beyond the university. We aim to make this service available across the country making it easier to connect people to professionals.,"css, html5, java, javascript, mongodb, react, springboot",https://github.com/Sankalp-dasari/VtHacks
VTHacks 12,IntelliQuest,https://devpost.com/software/intelliquest-0or46l,False,"Our Inspiration was to create an easy way to play Table Top Role Playing Games, so you don't need an group or someone to run the game for you. Getting a group and someone to run a game is hard so we though AI could help fill the gaps to make a fun experience.","IntelliQuest acts as a ""Game Master"" for a Tale Top RPG , where it generates a story and allows the user to do whatever they want within the bounds they provide. They also run the game fully! So they are using your stat points to determine potential outcomes.",This was build with a react frontend & and a express backend with node.js. Additionally IntelliQuest is powered by GPT 3.5 turbo to generate all the responses and prompt the user for different information.,"Some major challenges we ran into was dealing with the backend server, it wasn't something any of us has done yet. Luckily we were able to get this experience to grow our skills.","I am honestly most happy that we completed a project. Last semester my teammate and I competed in HackViolet, and we were unable to get a working project with just a semi-functional frontend to show. So the accomplishment of having a fairly well fleshed out project is pretty exciting.",,"Potential next steps for IntelliQuest would be to add combat, as it is a common feature in Table Top RPG's, as well as adding some kind of account system to save user data & group sessions.","express.js, gpt, javascript, node.js, react",https://github.com/kenzie-leckenby/VTHax2024
VTHacks 12,Peony,https://devpost.com/software/peony,False,"The inspiration behind Peony comes from our passion for tackling the stubborn racial disparities in pay that still plague many industries. Despite progress toward workplace equity, salary gaps and unequal distribution of bonuses and equity persist. Inspired by platforms like Glassdoor, we wanted to take things further by focusing on these specific issues. We saw an opportunity to create an exciting, data-driven tool that shines a light on these inequities, helping organizations bridge the gap and foster fair, inclusive environments for everyone.","Peony is a comprehensive tool designed to analyze and address racial pay inequities within organizations. It compares median and mean salaries across different racial groups for similar roles and job levels, ensuring transparency in salary bands and equitable distribution of bonuses and stock options. Ultimately, Peony attempts to helps bring attention to pay gaps and take actionable steps toward closing them.","We built Peony using React and Next.js for a smooth, user-friendly interface, and Python for backend data processing and analysis. TypeScript helped us ensure type safety and catch errors early in development. The platform includes clear charts and graphs to show salary differences across racial groups, along with features to evaluate salary bands for transparency and fairness.","One of the biggest challenges we faced was data availability. Obtaining reliable and complete datasets, especially regarding bonuses and equity distributions, was difficult since many companies do not provide transparent salary information. We also encountered issues with missing data, which required us to use reduced datasets, limiting our analysis in some cases. In addition, interpreting extreme cases of pay disparities through mean analysis sometimes skewed results, requiring careful balancing between median and mean calculations to ensure accurate conclusions. We also struggled with proper time management and prioritising the right next steps.","We’re proud of our first attempt at this hackathon with Peony. While we didn’t accomplish everything we aimed for, we successfully developed a model that balances median and mean salary analysis, and created clear, accessible visualizations. Peony attempts highlights pay gaps and provides actionable insights to help organizations promote pay equity.","Throughout this project, we gained valuable insights into both front-end and back-end development. We learned TypeScript and basic front-end skills, which were crucial for creating a user-friendly interface with React and Next.js. We also explored the intricacies of integrating front-end and back-end components, ensuring that our data processing in Python effectively fed into the visualizations and features we built. This experience deepened our understanding of pay equity, statistical modeling, and the importance of data transparency and ethical handling of sensitive compensation information.","Moving forward, we aim to expand Peony's functionality by integrating more comprehensive datasets and refining our analysis models. One of our goals is to include additional demographic variables, such as location, to provide a more holistic view of pay equity. We also plan to develop more advanced predictive analytics to help organizations forecast the long-term impact of their compensation policies. Additionally, we hope to collaborate with companies and policymakers to promote the use of Peony as a standard tool for ensuring pay equity across industries. Our ultimate vision is for Peony to become a critical resource in the fight for workplace fairness and inclusion.","github, next, python, react, typescript, visual-studio",https://github.com/JouBarzdukas/Peony
VTHacks 12,NutriChef,https://devpost.com/software/nutr,False,"Our inspiration came from moving off-campus and taking control of our own meal preparation. As we began cooking more frequently, we realized the importance of understanding the nutrients in our food to maintain a healthy lifestyle. This insight motivated us to create NutriChef, a tool that would help us, and others, analyze daily nutrient intake and avoid unhealthy eating habits, ensuring we stay in peak health.","NutriChef is designed to analyze recipes on any website that the user visits from the browser, providing users with a personalized health score based on their specific health conditions, characteristics, and dietary preferences. In addition to evaluation, NutriChef can use AI to suggest healthier modifications to recipes, making them more suitable for the user’s health profile.","We made a chrome extension so whenever the user goes to any page that has a recipe in it, NutriChef can process it. We used the chrome API's from the frontend and kept a script running in the background of the browser. The frontend was built using Svelte, HTML, CSS, and JavaScript to ensure an intuitive and responsive user interface that displays the health score and improvement suggestions. For the backend, we used Node.js/Express.js to handle heavy processing tasks such as integrating Azure OpenAI to generate recipe modifications based on individual health needs and preferences.","One major challenge was finding an API that could suggest recipe modifications based on specific health parameters. Since none met our needs, we developed a custom algorithm that considers various health conditions and dietary requirements to generate suggestions. This required extensive research and implementation, which limited the time we had for adding other features.
We also faced persistent issues with processing our nutritional API key, which frequently broke and caused delays.","Despite the challenges, we are proud of sticking to our schedule and delivering the project on time. One of our key accomplishments was successfully integrating Azure AI, a tool that was new to us, into our solution. Additionally, we are excited to have used Svelte for the frontend, enabling reactive updates that enhanced the user experience compared to vanilla JavaScript. Completing our MVP with multiple features required long hours and efficient time management, and we’re proud to have pushed ourselves to meet these goals.","We learned how to efficiently integrate multiple APIs and realized the critical importance of planning in the development process. Creating our custom algorithm taught us valuable lessons in algorithm design, while working with Azure’s sequential text generation expanded our understanding of natural language processing. These skills helped us generate personalized recipe suggestions tailored to users’ health profiles.","Daily meal planning: We plan to add memory retention for meals, allowing NutriChef to track what users have eaten throughout the day and suggest subsequent meals based on nutrient intake. This feature will also enable users to schedule meals on a calendar which can help it be more organized and better personalized recommendations. Taste rating: We intend to introduce a taste rating feature that allows users to rate their meals. This data will be stored and used to fine-tune NutriChef’s recommendations, improving both the health and taste of future recipes based on personal preferences. Chatbot integration: A future goal is to implement a chatbot, where users can directly input recipes and ingredients to receive real-time health and taste scores, making the interaction more user-friendly.","azure, css, express.js, html, javascript, node.js, nutritionix, svelte",https://github.com/arohan0929/NutriChef
VTHacks 12,PRAP - Physical recognition activated prosthesis,https://devpost.com/software/prap-physical-recognition-activated-prosthesis,False,"We wanted to provide assistance to the disabled community through tech. More specifically, amputees have limited technologies which can assist them. Based on our previous work and experience, we know that modern prosthetics can often help but have much room for improvement. Our alternate solution provides a significantly major improvement to current actuation methods used in modern prosthetics.","Similar to most modern day arm prosthetics, the device mainly consists of a robotic hand. However, our project specifically also has a component which detects objects in real-time. This detection allows for the user to choose certain hand-held objects, and when detected, the hand will automatically grip its fingers around said object. This removes the need for manual actuation (in for form of buttons or even EMG readings), as the system will automatically close on its own. Furthermore we have developed a mobile app which detects the grip strength of each actuation over time and collects data on each trial. Statistical analysis is then run on this data for optimal grip strength needed based on each individual user's habits.","The main hardware prototype was developed by the segmentation and assembly of local craft wood. The fingers are connected via strings, and the grip actuation is initiated by 5 servo motors pulling each finger (allocated using Arduino). The object recognition system was developed in python by utilizing YOLOv8, a deep learning model designed for real time object detection, classification, and segmentation. It detects in real-time using a webcam, the surrounding view of the prosthetic, specifically looking for common handheld objects. Upon detection of said handheld objects, a communication will be sent to that e Arduino via the serial port, that the servo motors should be actuated.
The Mobil app was devised using android studio, which combines java or kotlin coding with statistical analysis report using R. Grip strength is measured using a common pressure sensor, where voltage readings are observed across a common voltage divider.","It was difficult to develop a fully cohesive device which consisted of both complex software, and delicate hardware. We kept in mind the primary audience for this device (the prosthetics community) so we took extra care with the materials involved. We had originally planned for a 3D printed device, however after some complications, we had to get creative with what we could obtain, and use the materials available to us.",Developing the physical device by hand during a short time period is a great accomplishment which we are proud of. Also the cohesiveness and smoothness of the software-hardware connection is something that we are very pleased with.,We learned that utilizing our individual strengths to work together leads collaborative success despite certain challenges.,To develop a 3D printed model for this device and improve its mechanics. Improve speed and accuracy of abject detection. We would also like to offer this this device (and future models) to amputees for their feedback and perspectives.,"android-studio, arduino, java, python, r",https://github.com/AmrBinmubarak/VTHACKS12
VTHacks 12,Food Decisive,https://devpost.com/software/food-decisive,False,"As college students with about 50+ food options to choose from every day and many more to make, we feel indecisive about what to eat. As we know many others ranging from high school students to even adults who face the same issues, we chose to make this project as a solution to our needs.","By making you solve a survey, we run your answers through Llama, an AI model to generate the best recipe that fits your needs.",We built the front end using Svelte Web Framework and our back end is based on NodeJS,Initializing PropelAuth with our web server and tunneling two different computers to host our website publicly were our two greatest difficulties,"Finished coding everything with 0 use of AI tools such as ChatGPT
Able to finish an entire AI-based project while being all Freshmen and the majority new to Hackathons",We learned how to integrate AI into JavaScript,We plan to improve our UX/UI interphase and we plan to implement a forum to share and ask others for recipes. We also plan to add customizable user pages to showcase the generated or personally created recipes with others.,"ai, css, html, llama, node.js, svelte","https://fooddecisive.co/, https://github.com/GamerBoss101/VTHacks-12"
VTHacks 12,Aperta,https://devpost.com/software/aperta-op6l8c,False,"This project idea came from wondering how textbook publishers get away with exploiting both its supplier and consumer at the same time. Looking at the alternatives, open educational resource, it seems that OER simply aren't on par with traditional publishers. Considering the similarity between Open Source and OER, it felt like OER is not really open source, only open use. We want to build a tool to allow OER to receive contributions similar to how opensource allows anyone to report issues and merge fixes or improvements.","We built a platform focused on bring git version control to the field of Open Education Resource. So far, the platform allows users to upload pdf files and make changes onto the resource they posted.","We built it with vanilla js, python flask server, and propel authentication",The frontend implementation was quite time consuming and we ran into many weird styling issues as we were implementing the design.,,We learned to use ui libraries more especially in time crunch and be prepared to deal with CORS if you want to write the server,,"css, daisyui, flask, html, javascript, python, tailwindcss",https://github.com/AnonymousMorris/Textbook_source_control.git
VTHacks 12,IntelOps,https://devpost.com/software/intelops-6rslyd,False,,"IntelOps is a platform that leverages a combination of optimization with machine learning and generative AI to assist analysts, commanders, officers, and other members of the armed forces and defense industrial base (DIB) in planning, executing, and improving military operations.",IntelOps consists of three primary components:,"Over the course of VTHacks-12, we had to overcome a few challenges: The primary challenge we faced this weekend had to do with computational limitations. Our backgrounds are primarily in machine learning, but we did not have access to the amount of compute that we would need to do a lot of the things we originally wanted to. For example, we spent a few hours trying to train a GPT model on more specialized documents relating to military strategy and history, such as Sun Tzu'sThe Art of War, scholarly articles, and historical fiction, but eventually we realized we simply were not going to be able to train a decent model on our Macbooks. It was at this point we decided to implement OpenAI's API and use ChatGPT for our model, which brings other challenges (taking user API keys as input, for example) and may sacrifice accuracy of a fine tuned or specialized model. You can achieve a lot in 36 hours (especially with enough redbull), but you can only achieveso much. Like most of the other teams, we were limited by the short time frame. I don't think this needs much explaining.","Throughout this weekend, we set timelines, goals, and deadlines for ourselves, and we were able to follow those deadlines, complete our work on time, and stick to doing what we were good at. We used our understanding of each other to create a plan and stuck to the plan. In Fall 2022, we went to North Carolina to compete in the Reinvent-the-wheel hackaton. We did not do well at this competition. We were, by most standards, ill-prepared and unexperienced. This fall, we felt confident in our abilities; in mere hours we were able to acccomplish tasks that took us days just a few years earlier. Looking back on how far we have come as individuals is an incredibe feat.","This project required working with and structuring complex datatypes, including 3 to 4 dimensional arrays and dictionaries. We learned how to traverse these data structures and how to handle them efficiently on both the client and server side. This weekend, we learned about imlpementing all aspects of a project at both a technical and non-technical level, including about trade-offs in speed and efficiency for other features. This might be our most impressive feat all weekend.","There was a lot we weren't able to accomplish this weekend that we would have liked to. Among these are: Train and fine tune a more specialized GPT modelto give better recommendations, analysis, and overall more specific responses. The model could be trained on a corpus of documents relating to military strategy or historical military operations. Unfortunately we were unable to accomplish this due to computational efficiency and power constraints. Generate exportable formats for reports, such as PDFs, HTML, or CSVs for raw data. The GPT's output combined with other ML techniques could generate a comprehensive mission plan with very little user interaction. Create interactive visualizationsof the inner workings behind the predictions and recommendations. For the sake of simplicity, ease of use, and ease of understanding, a lot of the behind-the-scenes work of IntelOps is abstracted from the user's view. It would be cool to show some of these analyses, such as calculating the best path for a given set of vehicles and terrain.","api, flask, javascript, leaflet.js, linode, openstreetmap, openstreetweather, python, react","https://github.com/jjheals/vthax, https://intel-ops.us"
VTHacks 12,deQlassified,https://devpost.com/software/deqlassified,False,"As a queer person living in the United States, especially someone new to being queer, navigating the world around you can be difficult when taking your safety into account. The world can be confusing and cruel, and it can be hard to find places where you feel safe. We aim to make this process easier by presenting easily digestible information about queer safety in the USA, doing the heavy lifting so that others can see what they need at a glance.","The main interface is an interactive map of the US, wherein users can click on a state to see information about how queer-friendly that state is, including information about the safest cities and universities, how favorable the law towards queer people is, and what the rate of hate crimes against LGBTQ+ people in that state is.","deQlassified is built around the interactable map of the USA- this was made by importing images into the software Figma and arranging them in the correct layout, which took a long time, and then exporting the Figma project into an HTML file with the map being represented by a large SVG file, with groups for each state. The data displayed to the user is obtained from sources such as the FBI, for the hate crime statistics specifically, and this data is stored in CSV files which were programmatically analyzed and converted into JSON files that can be read by the code and displayed to the user when they click on a state.","We used Pug as a template engine initially, but it didn't play well with the SVG's used for the interactive map, causing us to have to drop it, and a lot of the backend along with it. Another challenging thing was getting the Figma project to export to HTML code properly, and many of these issues came from the SVG's once again- in the end, some SVG files had to be manually edited to appear correctly.","Getting the map to look nice, with the borders not being a mess, was quite a challenge, and it was very satisfying to see it come together.","We learned a lot of the limitations associated with using SVG's and also some clever workarounds to those limitations. We also learned a lot about data analysis using python, and how to interface with CSV files.","There are a lot of avenues to expand upon from here- a smaller scale version of deQlassified could be used for cities and towns to identify safe restaurants, bars, salons, and lots more. There's also the route of more data displayed, because there is always more to analyze. Crowd-sourced data is also a possibility, since the best data comes from smaller-scale observers. A large survey or a census will not capture the individual experience better than an individual with experience.","css3, express.js, fetch, html5, javascript, json, node.js, python",https://github.com/jackdevillier/vthacks12
VTHacks 12,HokiePokie,https://devpost.com/software/hokiepokie,False,"Our inspiration came from the frustration of planning schedules for the upcoming semester. One of the major friction points was having to switch between many websites to check for class availability, average GPA, professor GPA, and sites like RateMyProfessor. So, we set out to solve this problem by consolidating all this information into a single, user-friendly website.","Our platform allows students to build their class schedules for the upcoming semester and provides historical grade distributions for courses. It also displays professor-specific GPA data, helping students make informed decisions about which courses and instructors to choose.","We built this platform using the MERN stack (MongoDB, Express, React, Node.js). The university’s historical course data is provided in CSV format, which we processed and stored in MongoDB. On the backend, Node.js and Express manage the API requests to fetch course and professor data from the database. The frontend, built with React, allows students to search for courses, view grade distributions, and build their schedules intuitively.","One of the major challenges was efficiently organizing and storing the data. We had to design the MongoDB collections in such a way that reduced data redundancy while maintaining high query performance. Aggregating grade distributions across multiple sections of the same course and building a clean interface for users to view and compare this data was another challenge. Additionally, ensuring that the site remained fast and responsive while handling large amounts of historical data was critical.","We are proud of creating a solution that simplifies the scheduling process for students and provides all relevant academic data in one place. Another accomplishment was designing a backend structure that optimizes storage efficiency by aggregating course data across multiple terms and sections, while still allowing detailed queries by professor or course.","We learned a great deal about database design, especially how to structure collections for large datasets with minimal redundancy. The importance of building scalable, efficient APIs was another key takeaway. We also gained experience in frontend design, particularly in creating intuitive interfaces that present complex data in a user-friendly way.","Next, we plan to enhance the platform by adding real-time updates for class availability and expanding the data sources to include more universities. We also aim to integrate user feedback to improve the user experience further, as well as optimize route planning for multi-app delivery, making it easier for food delivery drivers to work across platforms efficiently.","bootstrap, css, express.js, html, javascript, mongodb, node.js, react.js, typescript",https://github.com/rishvidh/hokiepokie
VTHacks 12,CSmasher,https://devpost.com/software/csmasher,False,CSmasher was created to help computer science students get out and meet other people in the field through a convenient and helpful app. Our team was inspired by apps such as Tinder that make meeting new people simple and easy.,"CSmasher uses a traditional dating-app style design to show you other CS students with high ranking leetcode profiles, and high typing speeds.","We built CSmasher using an ExpressJS backend, React frontend, and MongoDB Atlas database for storing user information.","Our team ran into many challenges throughout the course of developing our app, mainly getting a foundation for our app set up. We attempted to use several different languages and services for our backend, ranging from Rust to Java, before deciding that JavaScript would be one of the easiest to implement.",We're proud that our app has reached a state very close in functionality to our goal in such an extremely limited period of time.,"We learned that good planning is incredibly important, as well as the value of version control software when doing collaborative coding.",,"express.js, godaddy, mongodb, react",https://github.com/franklindt/-CS-masher/
VTHacks 12,Street Smart,https://devpost.com/software/street-smart-bdalnv,False,"The inspiration for Street-Smart ultimately arose from a team discussion we had, where we really talked about the difficulty of traveling through familiar or unfamiliar areas that are associated with crime. We wanted to create something that helps improve the current situation of such travel respectively.","The web application that we designed uses a machine learning model based on a Kaggle dataset to group real world crime statistics based on location to determine what areas of the U.S. have higher or lower crime densities. Based on that information, users can reroute themselves to travel in safer areas in order to prioritize their individual safety accordingly.","We used Python and Flask for the backend technologies. We used JavaScript, HTML/CSS, and Three.js for the frontend technologies. For proof of concept, we used a Kaggle dataset focusing on crime in the Los Angeles area. The other big piece of data that we referenced is Google Maps API, as that provided the locational data and helped us track the different paths from point A to point B when we factored in the geographical safety of a certain place.",One of the biggest hurdles was integrating real-time crime data with geographical information from the Google Maps API. Ensuring that the routing dynamically accounted for safety while providing an efficient path was a complex task.,"We are proud of the final user interface that we came up with as well as some of the features implemented as a part of the site, such as the 3D globe, interactive elements, and supportive chatbot to name a few. Additionally, integrating the backend (Python/Flask) with the frontend (JavaScript, HTML/CSS, and Three.js) posed some synchronization issues, especially when rendering the 3D globe and interactive elements. Coordinating these components to work seamlessly together involved troubleshooting several bugs related to data flow and UI responsiveness.","We learned that it's important to keep working as many times as possible looking at a different perspective can help us find a better way to solve it or approach it altogether. For instance, we interacted quite often with the sponsors in order to understand the technologies in more detail to figure out any issues.","In the future, we plan to expand Street-Smart by incorporating additional data sources beyond the Kaggle dataset, such as real-time crime reports from local law enforcement agencies and community-based safety platforms. This would allow users to receive up-to-date information and further enhance the accuracy of the safety routing. Additionally, we envision adding a crowdsourcing feature, where users can report incidents and safety concerns in real-time, which would further enrich the data and help others avoid potentially dangerous areas. Enhancing the chatbot’s capabilities to provide more proactive safety alerts and advice is also on the roadmap.","flask, google-maps, html/css, javascript, json, machine-learning, php, python, smarty, three.js","https://street-smart.us/, https://github.com/reetkaler/NewStreetSmart"
VTHacks 12,HackNet,https://devpost.com/software/hacknet-gz3yx8,False,"HackNet was inspired by our own experiences at hackathons—scrambling to find teammates in disorganized Discord channels or forming last-minute teams with mismatched skills. We realized there had to be a better way to connect hackers with complementary skills and goals. HackNet's mission is to provide a platform where participants can easily find theirdream team, leading to more productive, innovative, and enjoyable hackathon experiences.",,,,,,,"ai-applied-text-extract, css, generativeai, next.js, node.js, postgresql, python","https://www.hacknet.us/, https://github.com/Vishak-V/HackNet, https://github.com/nhanaa/hacknet-frontend"
VTHacks 12,Cultural Odyssey,https://devpost.com/software/cultural-odyssey,False,"Our inspiration came from our varied skills and expertise. Our team ranges from seniors to freshmen, and toeven the playing field, we decided to do something none of us had any experience with. We built the game Cultural Odyssey from the ground up, learning on the go.","Cultural Odyssey is an educational game that offers players insight into different cultures worldwide. Players explore traditions, languages, and histories through a light-hearted, gamified experience. Each world introduces a different culture, encouraging players to learn about that specific country's rich culture in a fun and engaging way, in hopes of helping players understand and appreciate global traditions.","We built this app using the Godot Game Engine. This was a new experience for all of us, so we got to experiment with how we wanted to build, which only added to the experience.","Some challenges we encountered included learning a new language, GDScript, and a lack of documentation and best practice guidelines, as Godot is still gaining traction. Additionally, it was the first time for some of us working with Git, so we spent some time troubleshooting merge conflicts on top of trying to keep the Godot game engine from freaking out when we brought in changes from Git.","As a group, we are extremely proud of how much we have learned this weekend. Our least experienced members now have a strong understanding and solid practical experience with many real-world software development skills. We worked efficiently as a team, despite having never met before.","We are honored to have been given an opportunity to learn and practice our skills. We learned how to use Git effectively, how to problem-solve successfully, and how to create games on Godot. We additionally learned how to work as a team and interact with our peers over the course of the weekend.","Our next steps are to take the levels that we have already created and improve them by adding more complex puzzles, as well as doing more research on the cultural histories of these different countries. We also plan to add other countries in order to bring more cultures into our Cultural Odyssey.","gdscript, git, github, godot",https://github.com/danieljagga/VTHacks12-Cultural-Odyssey
VTHacks 12,SkillSprint,https://devpost.com/software/skillsprint,False,"In the past year with all the tech layoffs happening, many students have been struggling to find jobs in the tech industry. 
With lower hiring quotas, interviews become more difficult to attain and interview cycles also tend to be more rigorous and demanding.
We personally know many affected by this, and wanted to come up with a tool that could help students and graduates be better equipped. 
As a result, we were inspired to build an application that provides in-depth information on the tech stack expected for the position and provide tools to prepare. This way, candidates get a better understanding of what to expect on the job, and also be better equipped to tackle those technical interviews!","The user will paste the link of a job description into the input, and it will return several cards separated by categories that give an overview of the tech stack necessary to qualify and succeed in the role. Each technology will also include study materials.","The backend of our application was built usingFlaskas our backend framework,Beautiful Soupfor web scraping, andOpenAI APIfor categorizing. Once the link was received, the web scraper will scrape the contents of the job posting and produce a job description in the HTML format expected. From there, we connect to GPT API to pass in the HTML-formatted description, prompting it to break the skills, qualifications, and/or responsibilities sections into their respective categories. 
Using some fun tricks inPython, we convert this response into the proper JSON format we want, and send this to our frontend! We built our frontend usingNext.jsandTailwind CSS. We have Next.js handle the input field and fetch the api response, then process it. We used Tailwind to make give our application a modern, and hopefully beautiful,UI/UX.","The biggest challenges we ran into all related to connecting the Flask backend with the Next.js frontend. Once the GPT API was producing the required response and we figured out the JSON conversion, sending it to the frontend proved to be quite challenging. We kept getting issues with CORS, GET requests being handled incorrectly, response body not actually storing any content, and some other HTTP errors. These were difficult to solve as this was our group's first time actually using Flask, and we had to learn a lot about what CORS does and how Flask handles API calls (especially with GPT API). This took several hours, but eventually we were able to solve it by configuring CORS the right way, and debugging the web scraper call as well as the API fetch handling in the frontend.","We were very proud of the backend that we built. This was our first time using Flask and GPT API together, and that was a difficult yet very rewarding experience! We learned a lot about these technologies and how powerful they are. We were also happy with the UI we were able to built. It was difficult to get the data into the correct JSON format we wanted, and then process it through our frontend to build the powerful cards we wanted to build. Additionally, our group had a lot of first time hackers! So it was a great learning experience for everyone!","We learned a lot about how Flask operates as a Python based backend framework. We also learned more about how to leverage GPT API's to really build a powerful application. With a backend heavy project, we got a much better understanding of how things truly work behind the scenes, as well as understanding how these technologies differ from the standard Node + JS we are all used to.","The next steps for SkillSprint will be to actually convert it into a chrome extension! We realize that this tool would be easier for people to use if they could simply click a button and have it produce the necessary details they need. Another step is to create user profiles, where they can track the applications they have submitted, see which skills have the most overlap in applications, and personalize the study content they want to focus on. We want to continue to build this platform to really make it as helpful to people as possible, to really make an impact. All super cool things we are excited to continue working on!","beautiful-soup, flask, javascript, next.js, openai, python, tailwind",https://github.com/Shardul2003/SkillSprint
VTHacks 12,ambuLLMce,https://devpost.com/software/firstaid-yjxoer,False,"A dispatcher is an important part of the first responders care process. A good dispatcher can save someones life, and a bad dispatcher can destroy the efficiency of the system. The loss of efficiency has life or death implications in EMS, and the improvement of the system can give competent advice and guidance to help people through some of the worst days of their lives. There is rigorous training, with strict protocols that drastically improve the dispatchers performance, and ideally every dispatcher should have this training. The certification system is called EMD: Emergency Medical Dispatching. The only drawback of the EMD certification and training system, is the fact that there needs to be training. Counties often don't have the ability to hire trained dispatchers, and so their EMS services suffer. ambuLLMce is our solution to this.","this application is designed to be a support tool for 911 call operators that provides automated assistance during calls. User can either upload an audio file or record a voice message. The system then produces the most relevant codes to the situation, and displays it to the user.","We built this project in 4 concurrent subsystems:Front-endThe Front-End was written in Javascript, using React. Started using the base create-react-app call to make the foundations, and tested and implemented each individual React Component. Originally tested the code with mock backend systems, and then connected to the back-end via HTTP POST and GET requests. The front-end server was deployed using Netlify. After everything was set up, we made the components more interactive, with background animations, smooth transitions, and different file pages using the React Router system. Back-endThe Back-End was written in FastAPI, where it receives audio data posted by the front end service, and then calls the in house audio transcriber to create text from the real time audio data. After the text version of the call is acquired its sent to the LLM that was RAG trained on EMD protocols, and it generates a set of directions that are sent back to the front-end. Back end was deployed to the cloud using defang. AIThe AI system was designed as a modular in house API for the back-end to call. First, we developed the RAG system using Azure OpenAI and llama-index. Once the RAG system worked, it was refined using prompt structuring to ensure consistent output. We then integrated Azure Speech to read audio data and convert it to text so it could be sent to the RAG pipeline. Project structure and DeploymentWe used Defang to deploy our back-end and netlify to deploy the front-end","We had no back-end specialists, and thus dealing with any back-end was not easy and had to be learned on-site. There were many small oddities and type errors that we are lucky to have made it through. 
At first, rate limiting was posing a serious problem when attempting to use OpenAI API directly. This was solved by migrating to Azure OpenAI , which ended up being much faster as well.","This project has come together almost exactly how we imagined it. We believe that this could be highly beneficial in its intended use case, and we are very happy to have the entire system connected and working as expected","We learned we need a back-end dev next time. Also that Azure is very helpful. We also learned a lot about developing with LLMs and using the cloud,","asynchronous microphone implementation, LLM fine-tuning, more data for RAG, vector DB.","azure, fastapi, javascript, llama-index, netlify, python, react",https://github.com/dtr34/ambuLLMce
VTHacks 12,Danger Dash,https://devpost.com/software/danger-dash,False,"Danger Dash homepage, made to be kid friendly Earthquake Example pt.1 - Question Earthquake Example pt.2 - First Response Earthquake Example pt.3 - Second Response Earthquake Example pt.4 - Feedback Danger Dashwas inspired by theyounger siblings, cousins, and childrenin our lives who often find themselves unknowingly close to danger. In a world whereeveryday risksare ever-present, we wanted to create a tool that not only teaches safety but does so in a way that’s fun and educational. By creating this game, we hope to provide children—and even adults—with theskills to navigate emergenciesand the awareness they need to stay safe. Danger Dashis aninteractive gamedesigned to simulate emergency scenarios using GenAI. Primarily aimed at kids but also engaging for adults, the game creates unique emergency situations where players must makecritical decisions.Every choice they make triggers different outcomes, allowing them to experience the impact of their actions. Through thesedynamic scenarios, players learn essential life-saving techniques and strategies in asafe, engaging environment, contributing to better public safety awareness. We builtDanger Dashwith aGenAI-powered backend, allowing us to generate new and varied emergency experiences every time. The backend leverages ourexpertise in Pythonto handle the AI logic and game mechanics, ensuring that players face realistic and challenging scenarios. By designing a system that dynamically adjusts, we’ve ensured that the game remainsfresh, engaging,and educational with each playthrough. One of our biggest challenges was working with GenAI’shallucination problem—where the AI would produce inaccurate or irrelevant scenarios. Ensuring that the emergency situations followed a strict format for game rendering was difficult. However, throughcareful prompt engineeringand lessons from past projects, we were able to refine the AI’s output to consistently generateaccurate and usable scenarios. We’re incredibly proud of howDanger Dashturned out. Despite thetechnical hurdles, we managed to create astable, educational, and engaginggame that uses AI in innovative ways. We're particularly proud of theconsistent AI-generated scenariosthat make the game feel fresh every time, and the positive feedback we’ve received on its potential to teach safety in aninteractive and funway. Throughout the development ofDanger Dash, we learned a lot aboutAI integrationin game development, particularly how to manage and stabilize GenAI outputs. We also gained deeper insights intouser experience design—especially how to make learning fun for younger audiences. Finally, we reinforced our skills inprompt engineeringand balancingeducationwithengagementin gameplay. The next step forDanger Dashis to expand gameplay by adding apoints systemandin-game gearthat players can collect and use during emergencies. This will encourage players to beresourcefuland think strategically in high-pressure situations. We also plan to introducenew emergency scenariosand possibly even amultiplayer mode, where players can collaborate to tacklecomplex situationstogether, making the experience even more immersive and educational.",,,,,,,"css, flask, github, html5, javascript, open-ai, python",https://github.com/jjoniel/dangerdash
VTHacks 12,USE IT OR LOSE IT!,https://devpost.com/software/use-it-or-lose-it,False,"TL;DR We had three week old pizza leftovers in our fridge.Shortly after moving into our dorm, Justin and I had extra leftovers from eating out the first couple days. We soon forgot those leftovers in the fridge after discovering that the food on campus was actually pretty good. Three weeks go by and the pizza is still in our fridge.",The user inputs a food item that they have and the program returns when the food would go bad by and a list of three recipes that the user can make using the item.,We usedAzure Openaigenerate the dates of expiration for the food. Our backend was made usingPythonand our front end was made usinghtmlandcss. We usedFlaskto link the two.,Implementing the AI into our code and figuring out how to use Flask were some of the hardest parts of our project. Our group members had little experience using Flask and two of our members hadbarely touched coding at all. It was definitely a steep learning curve.,We actually finished a project in our first hackathon with very little coding experience. Our team (specifically Justin) displayed so much perseverance to overcome the learning curve and the challenges we ran in to. It was also Justin and I's first project so that was cool too.,We learned how to make a website and implement Openai into our project. We learned how hackathons worked and how much hard work goes into them. We learned how to use flask and how to ask for help. We seriously could not have finished this project without the help of the many mentors we asked along the way.,"We want to improve our program to accept and interpret images of receipts to determine what items the user purchased and then make an itemized list ordered by expiration date. We also want to create a user login so user information can be stored and saved for future use. _Also we're gonna throw out that pizza, our program told us it was supposed to last three days, not three weeks. _","azure, css, html, javascript, openai, python",https://github.com/tscar43/VTHAX12/tree/main
VTHacks 12,Competitive Gains,https://devpost.com/software/competitive-gains,False,My inspiration for the project comes from my love for the gym. I want to make the gamify going to the gym to encourage more people to focus on their physical health.,"This application takes in user inputs on certain gym exercises like bench, squat, and barbell rows. the user enters the weight and how many reps they did it for. the program then gives the user a rank based on their exercise, and remembers it. This allows the user to track their progress in a visual representation through a line graph over time, and encourages the user to continue to rank up to reach the next level",I built it in VS code using react. The code is written in java script and uses node.js,A challenge I ran into was just getting it started. I didn't know how to start developing this app and I've never used react before so I had to do some research on different approaches I could take to finish the project. another challenge I ran into front end developing. I have only done a couple of full stack projects before and refamiliarizing myself with front end developing was quite challenging.,"I am proud of how the app looks in its final state. I didn't even expect to build a project joining the hackathon, so seeing a project I built from scratch in just under 48 hours is really cool and I am proud of myself for getting it done.",I am definitely better and more confident at full stack developing now than I was a few days ago so that is cool. I also learned a lot about JavaScript which I wasn't the most familiar with until the hackathon,"I want to create a mobile implementation of the app next. I've never developed a mobile app before so I created a web app due to time constraints. However, I would like to implement a mobile version of this and also add some sort of networking system so you can friend other people and share your ranks and best lifts.","chart.js, css, html, javascript, node.js, react, vscode",https://github.com/dberg21/ProjectHack.git
VTHacks 12,Produce Pals,https://devpost.com/software/produce-pals,False,"As students, we were inspired to help students and other low-income individuals have access to healthy foods at an affordable price. The inspiration also came from experience working on a farm and existing companies like Imperfect Foods which also work on a model of reducing food waste, but do not actually provide the food at a price or model that makes it accessible on a low budgetit is a subscription model that costs on average $60 per week.","Produce Pals provides a platform for farmers and grocers to sell produce that would have otherwise gone to waste at a lower price. There will be two user interfaces: one for the sellers, and one for the buyers. The seller inputs how much of different foods they have and their relative quality and condition. The food will then be sold at a flat discounted rate based on quantity and the average market price of that item as found through MongoDB. The buyer will have the opportunity to buy and reserve these bags through the website. Ideally, we would also work with schools to create hubs where farmers can bring their produce as part of this program. Grocery stores would also be able to participate, but the intended model is that consumers would pick up the groceries directly from the store as they tend to be more local.","We built the first version of the wireframe on Figma, but after encountering expensive paywalls to convert the wireframe to source code we were significantly delayed. The back end was developed in React.",Converting the front end to the source code to be able to connect it to the back end code was our most difficult challenge. The front end we developed was too advanced for us to be able to convert to source code with our capabilities in the time frameie. it doesn't work yet,We are very proud of the design of our website and the backend code that goes along with it.,,Our next step is to continue to convert the wireframe to source code to be able to connect the backend code.Our next next stepwould be to reach out to a campus about setting up a produce hub for the farmers and reaching out to vendors most likely starting at farmers markets about setting up a small test market.,"figma, leaflet.js, mongodb, react, vite","https://www.figma.com/proto/DE6C7O9Z7cbg6i3HRRYMbF/oogabooga?node-id=1-526&node-type=frame&t=AgYbV3Tz5Bb36E2M-1&scaling=min-zoom&content-scaling=fixed&page-id=0%3A1, https://www.figma.com/proto/DE6C7O9Z7cbg6i3HRRYMbF/oogabooga?node-id=10-1147&node-type=symbol&t=EDqdZPURZTP8UiQU-1&scaling=min-zoom&content-scaling=fixed&page-id=3%3A350, https://github.com/SionMarak/VThacks12, https://www.figma.com/proto/DE6C7O9Z7cbg6i3HRRYMbF/oogabooga?node-id=3-349&t=wwSYeRTS8ARriD1e-1"
VTHacks 12,Sidequest,https://devpost.com/software/sidequest-ut0xaz,False,"We were inspired by a recent road trip we took to the Appalachian Region. On our way to the destination, we passed many cool spots like breweries, lazy rivers, antique shops but did not make stops because we thought there would be more on the way and did not prefer to turn back. We realized we could have stopped at those places had we known about them earlier. Sidequest provides a list of stops you can make in your trip based on the ""vibes"" you select.","Users can add the origin and destination of their trip and along with information like the people they are travelling with, a customizable ""vibe"" for the trip, preferred duration for the trip, and the budget. We then generate a list of stops based on the vibe and the constraints (time and budget) of the road trip.",,,,,"Add interactive map features, save places (bucket lists), and more","css, google-maps, openai, openstreetmap, python, react","https://github.com/manonmission88/SideQuest, https://github.com/Sameer164/SideQuests"
VTHacks 12,Mind Fuel,https://devpost.com/software/mind-fuel,False,"I often ask a lot of my friends to send me interesting articles, videos, or content that they come across. I do this as a way of accessing knowledge that I don't have. Having smart people send me content is a good way of learning about the world. But instead of sharing these blogs, videos, and articles over iMessage or email, why not build an app for it instead? An app where the point is not to create content, but to curate it. In a world full of meaningless and even harmful or misleading information, it requires effort and diligence to separate what is useful and what is distracting.","Users can sign up with email or their Google or Github accounts to create a public profile. On their profile, users can share links they come across and categorize them by tags and collections. For having a fluid user experience, the app fetches the metadata of the URL that is pasted and autofills the fields. Once uploaded, the curated content is then public for other people to see.",,,The UI looks sick. I'm happy about that :),,"This is a product I want to build to solve my own problems. I'll validate some of the assumptions I hold, and polish up this current MVP before deploying it and getting users. I have a bunch of ideas and features I wanna try out.","nextjs, postgresql, prisma, react, shadcn, supabase, tailwind, typescript",https://github.com/EbenZergaw/vt-hacks-12-mind-fuel
VTHacks 12,HomeView,https://devpost.com/software/homeview-c4s5dt,False,"As a person who likes traveling and has been to multiple different countries, one of the biggest issues in traveling has been the housing. Home images on listings often seem bigger or brighter in pictures, but the reality is often disappointing. Our inspiration comes from real life experience, which is why we wanted to make an website that allows travelers, renters, and homebuyers the ability to explore properties in a more accurate and immersive way, ensuring that what they see is truly what they get.","Homeviewis a platform that allows users to explore real estate properties through immersive 3D models. By offering accurate, detailed models,HomeViewensures that buyers, renters, and travelers can view homes as if they were physically there. This helps users make more informed decisions, eliminating the discrepancies between online images and the actual property.","The app consists of only two parts. 
The first part consists of the front end written in Nextjs. It uses MangoDB Atlas and Edgestore for storage and Github0 Auth for authentication. It is intened to be the primay form of connecting to our app. The backend is written primarily in python, but uses a lot of code transpiled from c++. This includes tools such as opencv and pandas for image manipulation and data management, but also SFM libraries such as colmap. ",We ran into challenges such as :,"Since it's our first time interacting with motion to structure, we are proud of what we have created.","We learned about motion to structure, C++, and transpiling code to python.","Finalize Platform Development, Partnership Development, Launch Marketing and Sales Campaign, and Fundraising/Investment","auth, c++, cuda, github, mongodb, mysql, node.js, opencv, python","https://www.invadev.net/, https://github.com/Zn10plays/SFMpipe"
VTHacks 12,Simplifile,https://devpost.com/software/mealbot,False,"We created this application to help the elderly and the people who cannot understand english well. We wanted to make it easier for them to know what their length financial documents were talking about, without them getting too confused trying to read it.","The plan was to take in financial documents, like tax docs, as pdfs. Our web application would utilize Azure services like OpenAI to help summarize the documents for people, as well as provide text to speech or translations in case they were unable to read it.","For frontend, we wanted to use React for a cool design. Our backend was implemented with python. We used many Microsoft Azure tools, like Speechify Text to Speech, Blob Storage, Translator, and AI Document Intelligence. To link the frontend and backend together, we attempted to use the framework Flask.","We hit the free limit on OpenAI models so we couldn't implement the summerization features like we wanted. Another challenge was simply using React, Flask, and Microsoft Azure tools. This is our first time using these tools together, so we struggled trying to implement and linking them together.","A win for us was getting a file into Blob Storage for the first time. We didn't know anything about React or Azure, so this was really exciting for us.","We got to learn how to use React, Flask, and Microsoft Azure tools.",We have thought about a chatbot implementation within the web so users can ask it questions about the document or other actions.,"azure, css, javascript, python, react",https://github.com/sivalee-intachit/Simplifile
VTHacks 12,SafeNav,https://devpost.com/software/safenav-it9zyh,False,"SafeNav was inspired by the need to address a critical gap in navigation for people living in underdeveloped or high-risk areas. While most navigation apps prioritize speed, we recognized that safety is equally important, especially in regions where road conditions and accident rates make travel more dangerous. Communities in these areas often lack access to tools that can help them avoid hazardous routes. Our mission with SafeNav is to create a more inclusive navigation experience, offering everyone the ability to travel safely, no matter where they live or where they’re going.","SafeNav provides users with two route options: the quickest route and the safest route. Using machine learning algorithms (ex. random forrest) trained on crash data from over 200,000 incidents in Virginia, SafeNav assigns a safety rating to each crash location and calculates optimized routes that factor in both speed and safety. The app then presents these routes on an interactive map, allowing users to choose the one that best fits their needs. Whether prioritizing safety or speed, SafeNav empowers users to make informed travel decisions tailored to their comfort and security.","Training the model: To begin with the Virginia crash data was thoroughly cleaned, as we removed over half of the features along with 800,000 rows (leaving just 2023-2024). We then took a 1000 random arbitrary data points and evaluated our own safety metric rating for the road from 1-100 for every point to use as a baseline for training the algorithm. We then trained and tested the algorithm using Random Forrest which is notorious for its handling of both classification and regression problems, and applying the results to the rest of our data points. Integrating Google Maps: Next we constructed a script using flask (to integrate with the javascript frontend interface) that took the safety values along with the many different routes and calculated a safety score for each route, which we used along with the length in order to create our final recommendation. Implementing Maps: We used SSL to secure encryption between the flask server and our own hosted javascript server, then integrated the program into our map on our front end system to run every-time an origin and destination is entered. Website Design: 
Our website was design based off of HTML, CSS and JavaScript. Our team wanted to have a modern looking website with a focus on making the website easy to navigate for users. To do so, we made the format of the page simple in its looks with unique animations throughout.",,"We're most proud of the idea behind SafeNav. Not only do we hope that SafeNav can directly help saves lives, but we also believe that the thought behind SafeNav can inspire others to build similar apps. The fact that we were able to plan and execute such a project in just 2 days shows how a group of 4 people's determination can go so far.","Throughout the creation of SafeNav we learned about the history of car crashes and what their impacts are. On the technical side, we learned more about the usages of APIs, AI, and machine learning models. Most importantly, we learned about the importance of collaboration and dividing work to successfully complete this project.","In the future, we see the possibility for SafeNav to become more accurate and widespread. That involves further improvement to our ML model through better cleaning and more robust training with more applications. Additionally, we want to grow SafeNav nationwide, not just in Virginia. The larger SafeNav gets, the more lives that can be saved.","conda, css, flask, google-maps, html, javascript, python, random-forest, sklearn",https://github.com/landonDuba/VT-Hackathon
VTHacks 12,HokieSWAP,https://devpost.com/software/hokieswap,False,"During the add/drop week at Virginia Tech, we observed a recurring challenge among students. Many sought to switch into class times that better fit their schedules but were hindered by full classes. The existing method involved posting on social media platforms like Snapchat, hoping to find someone willing to trade class sections. This informal system was inefficient and risky. For instance, if one student dropped a class expecting another to take their spot, but the other student didn't follow through, the first student could lose their class entirely. Recognizing the need for a safer and more efficient solution, we developed HokieSWAP—a platform that serves as both an escrow and automation service for secure class swapping.",,,,,,,"cloudflare, docker, flask, mongodb, postman, react, sanic, typescript, vite","https://hokieswap.com, https://github.com/ikoide/vthacks2024"
VTHacks 12,LaunchPoint,https://devpost.com/software/launchpoint,False,"The idea forLaunchPointwas born out of a conversation with my mom's friend, who was considering opening a restaurant in Blacksburg. She was uncertain and frequently asked me to get feedback from students and professors on how a Korean vs American-style restaurant would do in the area, and if it was even worth investing in a business there at all. While I wanted to help, I found it difficult to provide her with the reassurance she needed using just my own observations. That's when I realized there was a bigger need for a tool—something that could provide aspiring business owners like her with reliable data and insights into the best locations for a variety of establishments. By leveraging APIs and concrete data, LaunchPoint was created to empower entrepreneurs with the information they need to make confident, informed decisions.","LaunchPoint helps business owners determine the best location to open their business by analyzing key factors such as competition, popular establishments nearby, city demographics, and socio-economic conditions. The platform generates a score out of 100, indicating whether a specific city is optimal for a particular business type. Additionally, it provides detailed explanations for each factor, including the number of competing businesses, income distribution, and population demographics, to ensure business owners understand why a location might be a good fit.","For the backend, we used JavaScript and Node.js, integrating the OpenAI API to analyze and generate insights from collected data. We also used the Google Places API to gather real-time data on competing businesses in a given area. On the front end, we utilized React, HTML, and CSS to create a smooth and responsive user interface, while also using the Google Places API to display nearby competitors visually on the map.","One of the major challenges we faced was gathering accurate, city-specific data for factors like population, income, and competition. Many APIs provide county-level data, which wasn't as precise as we wanted for our project. Additionally, working with multiple APIs and integrating them efficiently was a learning curve, especially when accounting for real-time data from Google Places. Handling edge cases, such as when no alternative city was found within the set radius, also required refining the prompt to make sure the output remained relevant and useful.",We’re especially proud of how our front-end turned out and how we managed to integrate all the features we had planned from the start of the brainstorming process.,"Through this project, we learned a lot about working with external APIs and integrating AI into real-world applications. We gained a deeper understanding of how to manage multiple data sources, optimize prompts for OpenAI's models, and structure backend systems to handle complex calculations. Additionally, we learned about the challenges and nuances involved in providing actionable insights for small business owners, especially when it comes to competition and market analysis.","In the future, we plan on adding additional factors to give even more insight by implementing data from more APIs. We also plan to improve the platform's mapping capabilities to show available properties directly on the platform.","css, html, javascript, node.js, openai, react",https://github.com/AneeshSunkarapalli/LaunchPoint
VTHacks 12,Locus,https://devpost.com/software/test-fx4cs5,False,"We are all college students from different universities with one particular shared experience: we struggle to find housing. We found that finding affordable housing that fit each of our unique lifestyles was a problem, so we came up with Locus, a web app for affordable, convenient living.","Locus is a platform for renters to find affordable housing that is specifically tailored to their needs and preferences. Oftentimes, we end up having to shop around to find the perfect apartment to rent; however, with Locus, this process is streamlined and simplified.","We used a variety of technologies to help our vision come true. For the backend, we used the Django framework and a PostgreSQL database. For the front end, we used React. For deployment and management, we used AWS Elastic Beanstalk.","A major challenge we ran into was harnessing everyone's skillset to the fullest so that each member of the team contributed in a meaningful way, but we were able to learn quickly how to work together effectively while leveraging each of our strengths. Another challenge we faced was planning out how to build the application. We found that some of the data that we needed was hard to come by, and we had to come up with creative ideas to see it. The final challenge we ran into was the time crunch we faced towards the end of the hacking period. We realized that managing time is crucial and should be made a priority from the start.","One accomplishment we are proud of was our teamwork and software development process. We set up CI/CD pipelines early which allowed us to test and deploy our code in an efficient manner. We also were able to cultivate synergy between our team members which allowed for very productive coding sessions. Another accomplishment we are proud of is our software design. We tried designing the frontend to be easy to use, efficient, and visually appealing, ensuring a smooth experience for users while helping them find properties to rent quickly. Finally, we are proud of the fact that we were able to learn a lot while having fun!","Each of our team members comes from different backgrounds and joining together to compete allowed us to share many of our skills. We were all able to get our hands on some React frontend development, Django-based backend development with AWS, and CI/CD engineering. We also learned how to work together efficiently as a team, which is a lesson we will carry forward into the future.","In the future, we plan to make our application more efficient and precise. We aim to gather data from a larger variety of sources such as Zillow or Homes.com. Additionally, we also aim to add a social feature, where users can comment on properties to offer their opinions and advice to prospective renters.","amazon-web-services, azure, django, elastic-beanstalk, machine-learning, microsoft, postgresql, postman, python, react, workflow","http://locuslistings.com, https://github.com/aaladaruncc/vthacks_frontend, https://github.com/aaladaruncc/vthacks_backend"
VTHacks 12,HouseMatchGPT,https://devpost.com/software/housematchgpt,False,HouseMatchGPT was inspired by the need to streamline and enhance property searches using advanced AI and natural language. We aimed to simplify finding the perfect home by integrating data-driven insights with natural language processing.,"HouseMatchGPT analyzes property data by incorporating features like school ratings and park proximity. It uses spaCy for intelligent keyword extraction and the OpenAI API to generate a refined table of the top 5 property options with detailed descriptions, making property searches smarter and more precise.","We utilized the Boston housing dataset, applied feature engineering to add metrics like school ratings and distance to parks, and standardized the data with MinMaxScaler. Keywords were extracted using spaCy's en_core_web_sm model, and the OpenAI API was used to match and generate detailed property options. UI is created using streamlit.","We faced challenges in integrating diverse data sources and ensuring the accuracy of our feature engineering. Additionally, refining keyword extraction to effectively match user queries and generate relevant property recommendations required extensive fine-tuning.","We successfully combined multiple AI technologies to enhance property search capabilities, providing a user-friendly interface with AI-generated descriptions. Our system effectively refines search results based on detailed and dynamic criteria.",We learned the importance of effective feature engineering and dynamic keyword extraction in improving search results. Integrating different AI technologies can significantly enhance the relevance and precision of recommendations.,We plan to expand HouseMatchGPT by incorporating additional features and data sources to further refine property recommendations. Future updates will include more advanced NLP techniques and broader geographic coverage to enhance user experience and accuracy.,"autogen, llm, natural-language-processing, openai, pandas, python, rag, spacy, streamlit",https://github.com/shekharmnnit/HouseMatchGPT
VTHacks 12,Amplytics,https://devpost.com/software/amplytics,False,"We were inspired by the growing challenge of managing rising utility costs in homes all around the world. As utility expenses like electricity, water, and gas continue to increase, understanding these trends becomes crucial for effective budgeting and energy management. Our goal is to help individuals navigate these changes by providing clear, data-driven insights into future electricity prices, ultimately fostering more informed and proactive decisions.","It uses a linear regression model using past data of electricity costs in big metropolitan cities, and from the user input of city and future year, it takes that in and outputs the predicted electricity cost per kilowatt hour for that location and year.","We used Python for the backend of our project, using the BeautifulSoup4 library to parse data and create our own csv file, then pandas, numpy and scikit-learn to create a linear regression model based off our data. Then, for handling requests in the backend, we used the Flask framework for Python. On the front end, we used JavaScript and React to design and create the web page, and the Axios Library to handle GET and POST requests.","One challenge we had was figuring out how exactly to sort our data, and make it easily able to feed into our linear regression model. Another challenge we ran into was just with making Flask and React work together, as it was our first time ever using a Python backend framework with a JavaScript frontend.","We are proud of how we were able to solve our errors and debug our code. We were often faced with many problems we had never seen before, and our logical thinking skills were put to the test. But we were able to problem solve and come out strong, completing this project which made our whole group feel proud of ourselves.","We learned how to parse data into any format we want, so that it can be used for any sort of purpose. We also got experience working with and testing machine learning models, and how to implement them into a web app.","The next goals for Amplytics is to be able to have a search bar mechanism, where users can input any city in the United States and the entire country, and have it find the predicted electricity cost, as well as convert the currency for other countries. Additionally, we want to be able to predict utility costs as well, as we had the dataset for it but not enough time to integrate it into the application.","beautiful-soup, css3, flask, html5, javascript, numpy, pandas, python, react, scikit-learn",https://github.com/yadelg/VTHacksProject
VTHacks 12,HomeRec,https://devpost.com/software/homerec-ai,False,The inspiration behind HomeRec was to create a equal opportunity platform where users of any background can utilze our technologies in researching viable investment opportunities,"With HomeRec, users can make informed decisions on the homes that they can invest in. Specifically, we utilize a cosine similarity test to identify the most profitable and affordable option for home buyers based on their query results. We also attempted to implement a dashboard where users can see their pinned homes that they wish to buy.",We built it by separating into group of frontend and backend developers to accelerate development.,"There were no readily available APIs or databases for housing, so we decided to make our own database, which is composed of 50 houses in the Blacksburg, Christiansburg, and Roanoke areas and supplemental information (area, bedrooms, bathrooms, price, etc.), which was used for our search feature. We also took the 10 year price history of each house using Zillow's online Zestimates and used it to train and test our machine learning model to predict real estate pricing.","Implemented working OAuth, usable card design for each home, search feature, made two custom datasets (one for search feature other for time series model), Utilized ARIMA model to predict future home prices","We learned a lot of valuable information involving the full development process as well as learning what worked for us to be efficient and make a product. We learned and explored new technologies, such as mongoDB, propelAuth, and forming an ARIMA model.","With time, a housing API could be used to further populate our search function and further train our ML model. The search function could also benefit with a filtering aspect, allowing users to narrow down houses based on budget and other preferences.","arima, css, fastapi, html, machine-learning, mongodb, propelauth, python, react, tailwindcss, typescript, vite",https://github.com/RBiju12/HomeRecr.ai
VTHacks 12,creditJAM,https://devpost.com/software/creditjam,False,,"creditJAM is a financial tool that allows users to input their financial background, spending patterns, and  needs to get a personalized credit card selection to help build their credit portfolio. We created our own algorithm utilizing a compatibility score (with NO generative AI) and used a database containing the majority of all U.S. credit cards excluding local banks and credit unions. For those who are new to credit, creditJAM also has an interactive course to help people take their first step into the world of credit, and leverage credit cards as a tool for financial freedom. creditJAM is very friendly towards people who do not have much experience in credit or do not have any credit history by recommending cards with no annual fees, lower interest rates, or secured cards for building credit, depending on their needs.","creditJAM was built with Next.js, TypeScript, JavaScript, and Tailwind CSS!","The most challenging part of this project was trying to design and implement our own compatibility algorithm (we called it the JAM algorithm!) We had to take into account how every person's unique attributes affected our recommendations, while also trying to cater to their preferences such as hotels, airlines, and their current financial situation. Finding the balance between practicality, accuracy, and diversity was the key obstacle we overcame to create a personalized selection for the user. Also, we ran into some issues with GitHub, as well as having some non-technical team members so development was done with Live Share which was kind of a pain in the butt.",We are proud of not only creating a polished and aesthetically pleasing UI but can also confidently say that the recommendations our tool makes are genuinely good credit card recommendations!,This was our first time using Next.js (not too bad coming from React). We also learned how to design and implement our own algorithm. We intentionally avoided the usage of ChatGPT or AI API in our recommendation algorithm so we could better control how recommendations were made.,We would like to implement more features such as: A feature where you can connect your bank account so that we can analyze spending patterns better. Location services so that cards from local credit unions can also be recommended. This project was very fun and rewarding!,"javascript, json, next.js, react, tailwind, typescript",https://github.com/aykk/creditjam
VTHacks 12,Schedule Eatz,https://devpost.com/software/schedule-eatz,False,"As college students, we bear a lot more respon",Recommends the best times to go and pick up food based on a student's schedule. We wanted to ensure that each student gets at least 3 meals a day. There is also a calorie-tracking feature to see if the user is eating healthy or not.,We used a Google Maps API through APIfy to locate all the dining halls and when they were the busiest. We then modified and cleaned the data with the Pandas and Numpy libraries to export a dataset to Maven.,"The biggest challenge we ran into was properly extracting data for the dining halls. At first, we wanted to scrape data from GrubHub, but we weren't able to access the campus version of the app. From there, we used a Google Maps API called APIfy to get information about when the restaurants were busiest.",,,,"css, html, java, javascript, maven, python",https://github.com/Shakyy-Hands/FoodPlan
VTHacks 12,⚽️ RoboLeague,https://devpost.com/software/roboleague,False,"Inspired by Rocket League, RoboLeague lets you play a real-life game of Rocket League using just your iPhone.","To play a game of RoboLeague, place two bots on the game board with a ball, and phones to each robot via Bluetooth. With phones, each player controls a robot and attempts to score in their goal on the opposite ends of the field.","We built RoboLeague using Figma/Photoshop for UI design and Xcode with Swift for the robot controller app. Bluetooth (BLE) connects the app to the robots. The robots are powered by 12V DC motors controlled via L289N motor drivers and an ESP32 microcontroller, programmed using the Arduino IDE. The robot chassis was designed 100% in SOLIDWORKS and printed using a Bambu 3D Printer with Orca Slicer.","We encountered a few issues with the electrical system, and the ESP32s not working properly. We also had an issue getting Bluetooth to work since at first we were using the wrong protocol, before switching to BLE.",,,,"arduino, bambau, ble, esp32, figma, photoshop, solidworks, swift, xcode",https://github.com/MarcoGonzalezHauger/RoboLeague
VTHacks 12,hokie housing,https://devpost.com/software/hh2-kvpbws,False,"After the first year on campus, a lot of our fellow students who aren't guaranteed housing on campus have had to spend lots of time looking for housing near campus, spending hours researching and touring different apartment complexes just to find out things like there aren't in-unit washers and dryers or that utilities aren't included in the rent price.","Hokie Housing helps Virginia Tech students find the perfect housing for them by asking a series of questions, then showing them apartments that match all of their criteria.",We used React to built the front-end and Django to build the back-end.,"Communication, expectations, and connecting the backend to the frontend, especially with first-time hackathon team members.",Connecting the frontend with the backend and integrating multiple API's. Realizing our weaknesses and working together to overcome them.,"Many of us weren't familiar with web development, meaning we had to learn React for the frontend and Django for the backend. We also learned the importance of clearly defining project goals during the planning process, as we suffered from a lot of miscommunication at first.","We can improve our rating algorithm by letting users weigh what they value more in their housing, include on-campus housing, and eventually expand to more schools. Additionally, using more extensive real estate data and adding more options for user preferences would significantly enhance the experience.","django, google-maps, javascript, python, react, sqlite",https://github.com/im6rn/vthacks12
VTHacks 12,HeadlinerAI,https://devpost.com/software/headlinerai,False,"I've personally always found news articles hard to interact with. As they have hard to understand language, are too long, and contain excess information. So I decided that a PWA webapp, would be a great solution to inspire the younger generation to stay in touch with trustworthy news sources.","Our PWA webapp delivers a daily news feed that are related to users interests. During the signup process users select 5 topics (can be very specific or broad user inputted strings), these are then used to fetch news articles relating to these topics. We use OpenAI's GPT4-turbo to summarize the news articles into a small and ""gamified"" texts. We then display 10 news articles with summaries on the homepage and use web push notifications as to notify users of these updates.","We built the application using Next.JS, AWS, Shadcn/UI, tailwindcss, javascript, and NewsAPI, OpenAI API. The main tech stack is Next.JS frontend and backend paired with a AWS Aurora Postgresql DB. Authentication through nextauth, google oauth. PWA and web push notis.","There was a lot of issues with the Postgresql database and integrating PWA. The frontend was decently troublesome as we developed for mobile, using regular web orientated desktop toolings.","We're proud of the amazing looking frontend, most of our members spent the majority of their time working on the frontend as to make it look the best it could. The backend setup is another amazing accomplishment but the SST stack greatly assisted in making it easier to accomplish.","Don't start late on a hackathon, we didn't start working on our project or even thinking of ideas till 11am this morning. So we had pulled an all-nighter to the due date as to get this much progress done.",Planning on just furthering development and refining AI performance. Maybe turning it into a SaaS startup.,"amazon-web-services, javascript, newsapi, nextjs, openai, postgresql, tailwindcss","https://headlinerai.net/, https://github.com/HeadlinerAI/website"
VTHacks 12,VoiceNest,https://devpost.com/software/homelifeai,False,"Our original idea was a similar idea with image comparison instead, as we thought it would be useful to many people, but a lot of challenges arose in that. We decided to branch off a bit and come up with something a tad bit different.","This website takes speech from the user and based on what was selected, it would find real life listings of similar houses for the user.","The frontend is done in React, JavaScript, CSS, and HTML, and the backend is done in Python. To connect the backend with the frontend, we used the Flask API. We used three AI models that worked together, with one to analyze the user request, another to read scraped data, and the last to generate a response to the user from the collected data. We also utilized the Azure OpenAI Studio and Apify to scrape Zillow information.","Since we had never done it before, we had some issues implementing the text-to-speech feature. Engineering the AIs to talk/respond to each other also proved difficult because we are utilizing three models. Scraping the web and parsing the data was also something new to us, however, the biggest challenge that we faced was connecting the frontend and backend together.","Because we implemented a lot of new ideas for this project, creating a workable demo is definitely something that we are proud of. Collaborating with multiple AI services to create a solution is also something we are delighted to implement. And although it seems small, animating the little things like the microphone button and the words on the screen make our website look that much more appealing.","We learned a lot about react and how things worked to make the front end look nicer. Getting AI models to interact with each other was also interesting. This was the first time any of us had utilized scraping, which was exciting.","We hope to improve VoiceNest by expanding types of user input (drawings, text) and expanding locations so that the website will work in a greater area.","azure, css, flask, html, javascript, openai, python, react",https://github.com/locxn/VTHack12
VTHacks 12,SmartSearch,https://devpost.com/software/smartsearch-vzmc6j,False,"The idea forSmartSearchcame from the need to simplify the property search process by making it more intuitive and user-friendly. Traditional filters for location, price, and amenities were often too rigid and time-consuming, leading to frustration. We wanted to create a solution that would allow users to express their search preferences in natural language. Additionally, by incorporating a similarity-based suggestion engine, users can discover properties even if their criteria are loosely defined, making the search experience both comprehensive and flexible.","SmartSearch revolutionizes property searches by allowing users to input search criteria in natural language. Whether typing or speaking their requirements, users can describe what they're looking for, and SmartSearch converts these inputs into precise SQL queries to retrieve matching listings. Beyond direct matching, the new similarity-based feature provides suggestions based on how closely properties match the intent or features described in the query, allowing for more exploratory and flexible searches.","We started by studying how users interact with existing property search engines and the limitations of filter-based systems. Using Azure AI’s NLP services, we trained models on real estate data to understand property-related language, capturing specific search parameters such as ""3-bedroom condo near downtown"" or ""garden apartment under $1500."" To complement this, we developed a similarity-based search engine that identifies properties related to user input based on key features, even when there’s no exact match. The pipeline integrates natural language processing with SQL query generation, ensuring both precision and flexibility in retrieving results.","The biggest challenge was creating a natural language model that could handle diverse query inputs while maintaining accuracy. Users often phrase their requirements differently, use incomplete phrases, or introduce regional variations, so our model needed to be robust and flexible. Balancing accuracy and performance in both direct queries and similarity-based suggestions was also difficult. Another challenge was optimizing the similarity-based search to ensure it provides relevant results, even when the user input is ambiguous or loosely defined.","We’re proud of how quickly we adapted to Azure AI and natural language processing. Despite limited prior experience, we built a working prototype early on that could process both direct property searches and similarity-based suggestions. The addition of the similarity feature marked a major leap in enhancing user experience, giving users more discovery-based search options. Our team's strong collaboration was essential, allowing us to effectively combine natural language processing, SQL query generation, and innovative search approaches in a seamless manner.","Through the development of SmartSearch, we gained extensive experience in integrating NLP and machine learning into real-world applications. We learned to work with Azure AI's NLP services and how to build machine learning models that can interpret diverse user inputs. The hackathon taught us the value of iterative development, rapid prototyping, and flexibility in adapting to new technologies. The project also reinforced the importance of user-centered design, particularly when balancing technical complexity with the need for simplicity and accessibility in user interfaces. Additionally, we explored image recognition for future feature enhancements, expanding our knowledge in combining text and image-based searches.","Our future plans include integrating image-based search, allowing users to upload photos of properties or design elements they like, and using image recognition to match listings. The similarity-based feature will also be expanded to include visual comparisons, recommending properties based on aesthetic and functional resemblances. We also aim to further improve the AI’s ability to handle more abstract or vague queries like ""affordable cozy home,"" where it would interpret terms like ""affordable"" and ""cozy"" based on user preferences and regional data. Finally, we plan to implement voice search, making SmartSearch even more accessible and user-friendly.","azure, flask, javascript, mysql, openai, python, react, sql",https://github.com/abhinav-TB/smart_search
VTHacks 12,HomeScout AI,https://devpost.com/software/homescout-ai,False,"We wanted to solve the real estate problem of providinghighly personalized home recommendationsthat also prioritize privacy and data security. By leveraging the best in AI and blockchain technology, we set out to create a seamless user experience for potential homebuyers, helping them find homes tailored to their exact preferences while ensuring their personal data remains secure and private.","HomeScout AIis a next-gen real estate platform that usesAI-driven personalized recommendationsto match users with the best homes available, based on their preferences. The platform integrates withCoStar's Homes.comto pull real-time property listings and leveragesAzure OpenAIfor intelligent, customized suggestions. At the core of the system isMongoDB, which stores all user preferences and house profiles, acting as the operational hub for data transfer between AI, blockchain, and other APIs. Privacy is ensured by using theMidnight Blockchain, which keeps sensitive user data confidential through zero-knowledge proofs.",,,,,,"axios, azureai, beautiful-soup, compact, css3, express.js, indexdb, javascript, langchain, midnight, mongodb, node.js, puppeteer, python, react, smartcontracts",https://github.com/BisweshD/RealEstateExtension
VTHacks 12,Splittle,https://devpost.com/software/splittle,False,"As three roommates, we have been struggling to split bills in a way that feel fair, as we don't eat very many of the same things, or at least as much as each other. So we thought, what if we had an app that could help us out! That's where Splittle came in","Splittle allows users to create groups of friends, family, etc. In these groups, you can create bills, and add items to the bills. This lets you create records within your group of the bills  you pay for. Then, Splittle allows you dynamically split it (or should allow you to) according to who got what, creating a fair experience","We created a Django project, and worked with VSCode to create the html/CSS for it. We started with the login, utilizing Django built in databases to authenticate users. Then, we started building the logic around the bills, creating some models to store bill data in relation to the users and groups who needed to see them.","The most difficult challenge by far was dealing with our unfamiliarity with the framework. It was all of our first times working with DJango, and it was obvious. We ended up completely rebasing the project after a full day, as it got convoluted.",We are proud of getting something workable using a completely new to us framework! Being forced to learn this was a valuable experience,"We learned a lot about how the backend of a web app is supposed to function. Previously, we all had little to none practical experience with Django or database, so even if this is a bust, it was a valuable learning experience","Some future plans for Splittle are a mobile port, better splitting, and we are even thinking of trying to get AI to read receipts to make splitting with Splittle even easier","css, django, html5, python, sqlite",https://github.com/tanukiJami/Splittle
VTHacks 12,HomeRun,https://devpost.com/software/realchain,False,HomeRun was created to make real estate investments more accessible and affordable by introducing fractional ownership. We aimed to simplify the investment process and support affordable housing.,"HomeRun enables users to invest in real estate through fractional ownership, making it easier to participate in crowdfunding and support affordable housing projects.","We used React and JavaScript for the front end, Flask for the back end, and PostgreSQL with Supabase for database management, creating a scalable and efficient platform.",Understanding real estate crowdfunding and managing the development process presented significant challenges. We had to learn and adapt extensively.,"We successfully completed the development of HomeRun, overcoming technical and conceptual challenges to deliver a functional platform.","The project enhanced our teamwork, error-handling skills, and problem-solving abilities, with valuable insights from ChatGPT.",We plan to explore blockchain technology to enhance security and protect transactions on our platform.,"auth, flask, postgresql, propelauth, python, react, supabase, tfidf",https://github.com/darshannere/VTHacksRealEstate
VTHacks 12,HokieGPT,https://devpost.com/software/hokiegpt,False,"Our journey began with a vision to create a PDF-based AI assistant specifically designed for VT students. As we progressed, it became evident that each student has unique learning styles and preferences. This insight inspired us to expand our mission: to develop an AI that delivers personalized learning experiences, maximizing both motivation and efficiency by catering to individual needs.","HokieGPT analyzes the content of uploaded PDFs and intelligently responds to user questions. More than just an AI chatbot, it tailors its responses and resources based on each user's learning preferences, generating custom ""DEI Markdown""—a feature that personalizes content to support diverse learning styles and promote inclusivity.","Our development process involved careful planning and extensive research into cutting-edge technologies. We explored multiple approaches, collaborated on implementation strategies, and seamlessly integrated features to ensure the AI's effectiveness in delivering personalized learning experiences.","Integrating the frontend and backend proved to be our biggest hurdle. Ensuring smooth communication between the two was critical to creating a cohesive user experience, and this required both time and problem-solving.","We’re proud to have successfully built a functional PDF-based chat AI that not only works but also exceeds our initial expectations. The DEI Markdown feature is particularly noteworthy, as it allows for truly personalized content creation that aligns with diverse learning needs.","We gained invaluable lessons on the importance of efficient time management, especially when working under the pressure of a hackathon. Strategic planning and prioritization were key to our success.","We have exciting features in the pipeline, including a Markdown display function, text-to-speech capability, interactive quizzes, and more. Our vision is to continually evolve HokieGPT into a robust, all-in-one learning assistant for every student.","clerk, mongodb, next.js, openai, python, tailwind",https://github.com/Kurookku/HokieGPT.git
VTHacks 12,HomeAwayU,https://devpost.com/software/college-livin,False,"As freshman we don't know how to navigate the off campus housing market, so we decided to help make the scene easier to navigate.",This helps users find a place to live via ai combing through the user desired filters.,"Using full stack properties mixed with openai, and copilot to make an environment for students to find their desired off campus living","Understanding how to implement new technologies, and languages we didn't fully understand","We were able to integrate a bit of Openai, and mongodb with our frontend.","We learned how to manage data with MongoDB, how to use OpenAI and integrate it in code, how to prompt copilot to help generate code, and how to work with different api's.","We will probably try and finish what we thought we would be able to on our own time, and try, and integrate new things into the program.","css, express.js, html, javascript, json, mongodb, node.js, python, react",https://github.com/Sherlockabc731101/VTHACKS-Project
VTHacks 12,BikerPro,https://devpost.com/software/bikerpro,False,"As an off-campus commuter, I have observed several instances where cyclists are unintentionally hit by cars. A common pattern I've noticed is that cyclists sometimes veer too close to vehicle lanes from their designated bike lanes. After speaking with friends who bike to campus, it seems that many cyclists are often unaware of cars approaching from behind. By the time they hear a vehicle, it is often too late, especially if they have drifted too close to traffic and the driver does not react or slow down in time.","We developed a clip-on camera that can be easily attached to the rear of a bicycle to monitor approaching vehicles. When a vehicle enters a predefined range, the system alerts the cyclist with an audible warning, prompting them to steer clear of the vehicle lane if they have drifted too close. This safety feature helps cyclists stay aware of their surroundings and avoid potential hazards from approaching traffic.","This project leverages the YOLOv5 object detection AI to identify vehicles approaching a cyclist. The AI detects cars or trucks and assigns a confidence score to each detection, reflecting how certain it is that the object identified is a vehicle. Based on this information, the code generates a boolean value, which is then transmitted to the Arduino. By integrating the Arduino Uno with our Python-based AI, we ensure that when the AI detects a vehicle with high confidence and returns a ""true"" boolean value, the Arduino activates an alert through a speaker, notifying the cyclist of the approaching car. The breadboard circuit utilizes two 1k ohm resistors and several jumper wires, connecting the speaker to the power output from the designated Arduino pin, as specified in the code. After powering the speaker, we initially used a photoresistor, connected to the analog pins, to regulate the voltage output from the Arduino. However, the photoresistor was later replaced by a camera-based AI detection system, which more effectively modulates the output by providing real-time feedback based on environmental conditions, improving accuracy over the manual photoresistor setup.","Initially, our plan was to implement color detection and monitor pixel changes in the background to identify approaching vehicles, which would then trigger the signal to the Arduino, alerting the cyclist. However, we quickly realized that many factors, such as pedestrians, traffic lights, and scooters passing by, could interfere with pixel changes, leading to false positives. We also intended to play a custom audio file we recorded, instructing the cyclist to move to the right. Unfortunately, due to the Arduino’s limited memory capacity, we were unable to store the audio file, as it exceeded the available byte allocation. This issue could have been easily resolved by integrating an SD card, which would expand the Arduino's storage capabilities, but we were constrained by the materials available at the time.","One of our key accomplishments is successfully detecting when the camera identifies a car or truck exceeding our predefined threshold, triggering an alert for cyclists about approaching vehicles. While this may seem straightforward, the camera is capable of recognizing a wide range of objects, such as cups, cell phones, pedestrians, and even various breeds of animals. The ability to precisely filter and isolate vehicle detection, ensuring it produces an accurate true boolean value, was a significant technical achievement.","Throughout this project, we gained invaluable experience in the integration of AI, hardware, and real-time problem-solving. One of the primary lessons was the importance of precision in object detection. While our initial goal was to simply detect vehicles, we soon realized that a high level of specificity was crucial. The AI's ability to differentiate between a car and various other objects such as pedestrians or smaller items like cell phones was essential in preventing false positives. This taught us how to fine-tune detection thresholds and balance accuracy with real-time responsiveness in order to produce reliable results. Additionally, this project reinforced the importance of flexibility and iteration in the design process. We initially explored using photoresistors and pixel-based detection to identify approaching vehicles, but after recognizing the potential for numerous environmental variables to interfere with accuracy, we adapted our approach. By shifting to a camera-based AI system, we achieved greater precision and reliability. This adaptability in problem-solving and the ability to pivot when faced with technical challenges were key takeaways from this experience.","The next steps for our product would involve refining both the hardware and software components to enhance performance and user experience. Upgrading the system’s memory and processing capabilities, possibly by integrating an SD card or using a more advanced microcontroller, would allow for the storage of custom audio alerts and more complex functionalities. Additionally, improving the accuracy of the object detection algorithm by fine-tuning the AI model or incorporating more sophisticated technology will help reduce false positives and deliver more reliable alerts to cyclists.","arduino, c++, python, yolo",https://github.com/NathanDocherty/BikerPro
VTHacks 12,CultureTap,https://devpost.com/software/culturetap,False,"The inspiration for CultureTap stemmed from a real-life scenario where a South Korean student, despite being warmly welcomed by his Indian roommates, faced difficulties integrating into the group due to cultural and language differences. This challenge made us realize how common it is for people to struggle with cultural adaptation, even when there's a genuine effort to connect. CultureTap was born out of the need to bridge these cultural and linguistic gaps quickly and meaningfully. We envisioned a solution that would help individuals from diverse backgrounds connect and communicate more effortlessly through fun, interactive, and educational tools, fostering more natural relationships and breaking down language and cultural barriers.",CultureTap is an iOS app that facilitates cultural exchange and helps people connect across linguistic and cultural divides. Users can:,"We built CultureTap using Swift for the iOS platform, focusing on an intuitive user interface with SwiftUI and leveraging Apple’s ecosystem for seamless device interactions. Key technologies and components include:",,,Developing CultureTap provided key insights into the nuances of cross-cultural communication. We learned that:,,"amazon-web-services, figma, langchain, mongodb, python, swift",https://github.com/hyungminkimdev/vthacks12-culturetap/tree/develop
VTHacks 12,HokieEATS,https://devpost.com/software/tomato-kxae9u,False,"Virginia Tech has a student body that very often wants food from different cuisines but has no efficient means to decide on the best restaurant choices. We envisioned a system whereby any student can quickly find a perfect meal based on his or her preferences and diet requirements. This led to the development of HokieEATS, a conversational knowledge-based chatbot with a rather friendly user interface-something very easy for students to find great places to eat around campus.","HokieEATS is a bot that suggests restaurants based on data input by the user. Food locations have been scraped from web, the restaurant information, such as the type of cuisine, calories, price range, diet, allergic options, and many others, was gathered and inserted into the MongoDB database. When users utilize the chatbot, MongoDB queries are created concerning the user preferences, semantic searches are done to present the results of the best-matching restaurant option. Results are displayed in an appealing, conversational format.","The core of HokieEATS involves the usage of theMicroservices Architectureto tear down the system into independent services. Each of these is focused on one particular function. This kind of modularity not only made development more manageable, but it also made scaling and deployment of each component independently much easier. Following is how we did the building of various aspects of the project: By choosing Azure Functions for the backend, we made sure it would scale with demand. This serverless design means the back-end can handle more requests than ever before, without any manual intervention or managing of the infrastructure. The microservices architecture is modularly designed, meaning that each part of the service, such as the web scraper, the chatbot logic, and the database querying, can be deployed and scaled autonomously where needed. This flexibility will allow us to introduce new features or improvements without causing any down time or interruptions in the service. Major was the issue of user privacy. Interactions between frontend and backend were all over HTTPS. Sensitive data such as user preference was stored in a secure database, and we made sure none of the PII was being collected or stored. All of the backend services' authentications have been done through Azure's Identity Management to ensure that only authorized services have access to sensitive resources - such as the MongoDB instance, or even the LangChain pipeline. We carefully selected technologies and architectures that might become the backbone to support our goals; thus, HokieEATS was developed as a robust, scalable platform that should provide exact real-time recommendations about restaurants matched for the needs of each user.","Data Processing
Prompt Engineering
LangChain with ChatHistory","We are proud to develop a functional food recommendation system that will enhance the dining experience of the students at Virginia Tech. By integrating multiple technologies such as LangChain, MongoDB, FAISS, and Azure Functions, we have been able to build a scalable solution for accurate real-time results.","In this project, we have learned how to scrape and manage large datasets more efficiently, generate dynamic MongoDB queries using LLMs, and implement seamless connections between the chatbot frontend and backend services. Equally valuable was the experience of knowing how to troubleshoot challenges in cloud deployment on Azure.","We will improve HokieEATS by building on the dataset of restaurants and cuisines that surround Virginia Tech. We also plan to implement more conversational improvements into the chatbot, which would make it more personal and intuitive for the users. In the future, we can build this into a system displaying restaurants in real-time with user reviews to further filter recommendations.","azure, azureapps, azurefunctions(serverless), cosmodb, landchain, llm, mongodb, openai, pymongo, python, rag, streamlit","https://hokieeats.study/, https://github.com/NikhilGaddam/Tomato-Web.git"
VTHacks 12,Swipe Overflow,https://devpost.com/software/swipe-overflow,False,"Prior to VTHacks, our team noticed the vast number of students who were searching for a team to work with via the ""looking-for-teammates"" channel of the VTHacks Discord server. We realized it was tedious for people to look through a long chain of paragraph-length messages, seeking out others they could see themselves working well with, so we used this as the basis for our project idea. We also noticed that three of the four members of our team had to travel to get here, and one didn't have a place to stay overnight, which prompted us to want to solve this issue as well. At last, we came up with the idea that would attend to all of these problems: Swipe Overflow.","While also being a play off the popular developer community forum and data structure term, Swipe Overflow is the Tinder for hackathons. Using a built-in algorithm, we match users with teammates attending the same hackathon who have either similar or complimentary skillsets, based on user preference. If students are in need of a ride or place to stay, our product accounts for this and matches them with people who are willing to provide support.","This project integrates MongoDB Atlas, Flask, and Python for the backend, with HTML, CSS, and Javascript for the frontend. We initially used Figma to plan the layout of our website, then brought those designs to life with HTML and CSS. Simultaneously, we worked on the backend and later connected both pieces together for the final product.","Due to time constraints, we were unable to implement our idea at its full complexity, so there are certain nice-to-have features that we plan to add in the future.","We are proud of the amount of work that we were able to complete within the allotted time, especially since half of the team had never participated in a hackathon.","From a technical angle, our team strengthened skills in full-stack web development and gained familiarity with API integration and AI-usage. Outside of this, we learned how to be more proficient in brainstorming and how to work with new people.","Swipe Overflow is at its very first stages, and there are an endless number of improvements that the future calls for. Getting the backend running properly is the main priority. We want to add a chat feature, where users are able to talk to each other through the website, without having to exit and communicate via email, text, Discord, etc. We also believe it would be useful to have a storage system for sharing team resources and ideas. Enhancing the matching algorithm is in the cards as well, and we would want to add the ability for users to provide feedback to facilitate even more improvements.","css, flask, html5, javascript, mongodb, python",https://github.com/AbisekNepal/VTHacks
VTHacks 12,RoomUp,https://devpost.com/software/roomup,False,"Our inspiration was the grueling roommate and apartment search that we had to go through. We combined Tinder's ""swipe"" method which is a responsive, user-friendly way to search through people along with a responsive apartment searching tool.",Finds someone looking for a roommate based on their wants and needs. For people with an apartment already it'll find roommates looking for one. It does this by having both the roommate and apartment side by side with small details about either.,"Used MongoDB, Node.js, React, and Express when making Roomup. We used MongoDB to store the user information like names, emails, wants, needs, etc. Node.js helped connect that database to the front end. To make the front end look better React and CSS Tailwind were key for it. Lastly, express was used to move requests from point to point.","Our backend ran into some issues with cloud setup forcing us to switch from Defang to Microsoft Azure. Also for a couple of hours, our database was not properly made and kept timing out setting us back from a more complete project.",The site is very aesthetically pleasing. Also once the database began to work the group as a whole was able to begin getting into the work.,"Proper planning is needed when developing a site. Use products that are easy, but strong to use like Microsoft Azure. Use a very short amount of time wisely as well!",The product as a whole is genuinely interesting to us as a group. We wish to keep this going if time allows us and make an even better site that helps people with their roommate and apartment selection.,"azure, css3, express.js, figma, google, html5, javascript, mongodb, node.js, react","https://github.com/HSingh106/RoomUp, https://www.figma.com/design/vTFOXpYoQeNjJSqcgecIQy/Roomup-Rough-Draft?node-id=0-1&t=WN893Pz9nEfFbBUG-1"
VTHacks 12,Platepost,https://devpost.com/software/platepost,False,"We love healthy cooking and experimenting with new recipes every day. When we whip up something delicious, we’re eager to share it and get others’ opinions. It’s a fun way to break the monotony of our usual meals and see what others think. This app was born out of our desire to share our kitchen creations with a wider audience and connect with fellow food lovers who enjoy trying new dishes as much as we do.","Our app allows you to upload a picture of your dish along with a brief description. It analyzes your description and provides detailed information about your meal, including nutritional facts and a health score based on our custom formula. You can view dishes from other users in your feed, check their health scores, and revisit your own creations. Plus, it keeps track of your daily calorie intake, helping you stay on top of your nutrition goals.","We set out to create a tool that could instantly provide valuable nutritional insights about the food we consume. Throughout the development process, we deepened our understanding of Flask web applications, AI model integration, image processing, and RESTful API design. We built this tool using Flask for the backend, OpenAI's GPT-4 for food detection and nutritional analysis, and implemented a robust error-handling system. Our journey wasn't without challenges; we grappled with image processing consistency, AI model limitations, nutritional data accuracy, and performance optimization.",,,,,"azure, flutter, llm, mongodb",https://github.com/Platepost
VTHacks 12,CookCal,https://devpost.com/software/cookcal,False,Freestyle cooking shouldn't stop nutrition/calorie tracking.,"Take unstructured strings of recipes, structure and store the ingredients list through Azure ChatGPT 4oMini, repeat with strings of recipe edits and update the ingredients list/calorie count. Calculates calorie count through USDA FoodData Central API.","Git, Azure, Python, Flask, Javascript","Other time commitments, first time using Flask. Technical issues with Azure subscriptions that we tried to get around by using another platform, but ultimately worked out within Azure first.","Learning Azure, Flask, first time getting JSON output from an LLM.",Picking up new tools takes more time than actually implementing them.,"Allowing edits affect recipe properly, printing results to front-end instead of console.","azure, flask, git, javascript, python",https://github.com/baeww/CookCal
VTHacks 12,RideHub,https://devpost.com/software/ridehub,False,"As a group of college students, there are often many scenarios where individuals do not have access to their car, creating a necessity for rideshare applications such as Uber and Lyft. However, deciding between the two apps can get costly if one is unaware of which ride service provides the most balanced and cost-efficient choice.","Our mobile app RideHub provides users with the compared prices organized from lowest to highest depending on their ride preference, distance, and destination by inputting their starting location and destination. This feature eliminates the need to check each service individually.","We built this project with the Javascript framework, React Native, and used Google's API's to display our map, locations, and distances.",Some of the challenges we ran into were running our application in our IOS Simulator and developing our AI algorithm to mimic the prices of Uber and Lyft to distance.,One of the accomplishments that we are proud of is being able to finish this project because most of our team was not familiar with React Native and this was most the team's first hackathons as well.,We learned how to use APIs within React Native and create a visually pleasing UI using Figma.,"Since we were given only 35 hours, we only implemented Uber and Lyft; however, if we were to continue this project, we would love to add bus transportation such as Virginia Tech's BT system.","directions-api, distance-matrix-api, figma, git, google-maps, javascript, maps-platform-api, react-native, tailwind-css",https://github.com/johntrinhvu/RideHub.git
VTHacks 12,Don't Drive Off!,https://devpost.com/software/don-t-drive-off,False,"Well... we kept getting distracted while trying to learn about new AI technologies. And after weeks of procrastinating at our screens, we came together at VTHacks to build something about it. Don't Drive Off! is our attempt at building a competitive game to help users focus on their computer screens, instead of on any pesky distractions!","First and foremost, DDO! is a real-time vision detection app. It tracks users' face and eye motions by sending webcam footage to a backend running Google AI Edge's MediaPipe Solutions. Our backend keeps track of where users are looking, giving them ambient points for focusing on their screen and taking away points for getting distracted. Our frontend rewards focus with Power-Ups to crush the competition. From ""Gotta Pee!"" that lets you take a break, to ""Attention Grabber"" that lets you steal your unfocused friends' points - we reward users for staying Locked In!","Our backend is a Python Flask app running on Nginx with a Sqlite db to keep track of things. We use React for our frontend, Tailwind to quickly style it up, and some custom assets to give it its artistic flair. We leveraged OpenAI's ChatGPT and Microsoft Co-Pilot in order to quickly iterate on our designs and get a working site up and running in no time. We chose a lot of the technologies we used in order to ensure we could get something working by the end of the weekend - Flask and React are easy to use and well-documented. We really wanted to focus on AI technologies - we honed our asset-generation skillsets, we tested how well AI can help us code, and we made the AI solve all the tricky vision detection algorithm stuff for us.","Even though these libraries provide a lot of power, getting them all working (and working together!) is never easy in a weekend. Our foremost challenge was getting the app built, and our team's FrontEnd skills were definitely a source of struggle. None of us know enough CSS... Device-specific calibration for vision detection is tricky, so our app works best on laptops right now. We'd love for the app to be able to handle any device in a way that's smart - without so much human-training on angles/lighting. This is a challenge with any developing AI model and its implementation in real-world situations.","We got a full-stack app developed in a weekend, which is always a challenge in and of itself. We're especially proud of getting a real-world AI solutions platform running on our backend server, and we tried to make that ""professional accomplishment"" as fun and useful as possible for a 2-day project. Each team member focused on a few key skills they wanted to improve on. Kaushik is the best artist of the group and he focused on bringing those skillsets to a real-time web-app - he knows a lot more React because of it. Shorya is the best with AI tools, but he hadn't gotten one running for multiple users on his web server until now. Aarav has the most full-stack experience, but relies on simple designs because of his art skills. I think we're all proud of the project that we made, and we're happy that we pushed each other to build something cool together. None of us could've made this project alone.","The pace of the hackathon really helped us prioritize a minimum viable product and get into gear quickly. Lowkey our code was... not always good - also a worthwhile lesson - but we learned the power of some steadfast teamwork and dedication over 24 hours. We also found some viable ways to cut corners, in order to get a product out the door. While we always want to strive for perfection, we're always working on deadlines and it's good to learn ways to simplify our workflows (even if the quality of the work isn't quite the same - sometimes you need something QUICK!)","Sleep? Coffee? Probably not much Venture Capital... smh. those guys don't GET IT. this is a bazillion dollar idea!!!! In the future, we'd like to see this type of functionality tested in real-time scenarios where a lack of focus can costreallives. Although this is a goofy proof-of-concept video-game, we truly think that the technologies we've used and the skillsets we've improved can help build AI tech that puts users in control of their focus and their technology tooling.","flask, google-ai-edge-mediapipes, nginx, python, react, sqlite, tailwind-css","https://ddo.shoryamalani.com/, https://github.com/shoryamalani/VTHacks12"
VTHacks 12,IncidentAI,https://devpost.com/software/incidentai,False,"Our journey began with a simple question: How are emergency situations handled on our campus? We regularly receive VT alerts during emergencies, but we wondered about the processes behind these notifications. What if we could enhance reporting with images? How are different departments assigned tasks? How do they determine if they have sufficient resources within the county, and how are these resources allocated?
These questions led us to look deeper. We explored the world of emergency management, learning about multiple levels of response and even studying FEMA protocols. Through this research, we identified gaps in current systems and saw an opportunity to make a difference.
This led us to create IncidentAI. It's an app that helps report emergencies and manage resources. We wanted to build a system that could:",IncidentAI is an innovative application that revolutionizes emergency response management:,We developed IncidentAI using a robust tech stack:,,,,,"express.js, googlegenerativeai, javascript, mongodbatlas, node.js, react, restfulapi",https://github.com/GayathriPendyala/IncidentAI
VTHacks 12,QUETZL,https://devpost.com/software/quetzl,False,We were inspired by popular social media apps.,Quetzl is an app which prompts the user to do good deeds and anonymously share with others.,We build this app using Flutter and Firebase.,Communicating between Firebase and Flutter,Finishing the project,We learned how to use Firebase and Flutter,Possible redesign and more data communicated with each image.,"dart, firebase, flutter",https://github.com/notorious-columbus/vthax12app
VTHacks 12,safeNest,https://devpost.com/software/safenest-42g1yv,False,"Finding suitable housing as a student is often a complex and stressful process. We recognized that students face challenges such as limited budgets, safety concerns, and the need for inclusive living environments. Inspired by the desire to simplify this experience, we created SafeNest—a platform dedicated to helping students find affordable, safe, and inclusive housing near their campuses.","SafeNest provides students with a streamlined platform to search for housing options that meet their specific needs. It offers property listings with essential details like price, safety score, and inclusivity features. The platform also includes a filtering system that allows users to narrow down properties based on maximum price and selected inclusivity features. Detailed information about each property is available, including descriptions, location, and safety scores to inform users about neighborhood security.",,,"Successfully building a full-stack application that meets the specific needs of students seeking housing.
Implementing a user-friendly interface with filtering options to simplify the housing search.
Deploying the application on Heroku and Netlify, making it publicly accessible and maintaining secure environment variables.","User Needs: Understanding the specific requirements of students, including affordability, safety, and inclusivity.
Full-Stack Development: Building a cohesive application using React.js for the front end and Node.js with Express.js for the back end.
Database Management: Implementing MongoDB for efficient data storage and retrieval.
API Integration: Learning how to consume and provide APIs effectively between the front end and back end.
UI/UX Design: Enhancing user experience through responsive design and intuitive interfaces using Bootstrap.
Deployment Processes: Gaining hands-on experience in deploying applications on platforms like Heroku and Netlify.",,"javascript, react","https://bespoke-squirrel-b19dd7.netlify.app/, https://github.com/jasser-darguech1/safenest-frontend, https://github.com/jasser-darguech1/safenest-backend"
VTHacks 12,Homiez,https://devpost.com/software/homiez,False,"In the past, we all have had bad experiences with roommates that simply didn't match.",A roommate matching systems that picks the best roommate for your preferences. New registers are quizzed for their personality/preferences and we match who is the right fit depending on those results.,We built it with React frontend and Flask/MongoDB backend.,Learning a bunch of new technology we weren't used to. Especially styling everything nicely with CSS.,"Learned a bunch of new softwares and technologies, and all gained invaluable experience working together.","Don't try to create the most intricate hack. Start really small and build from there, step by step.","Definetly want to keep on improving the app because there is so much to be implemented like a better matching system, more in-depth questions, AI/ML algorithms to recommend better, etc.","css, flask, html5, javascript, mongodb, python, react, tailwind, vite",https://git.cs.vt.edu/yasirh/vthacks/-/tree/frontend/backend?ref_type=heads
VTHacks 12,Campus Flock,https://devpost.com/software/campus-flock,False,"CampusFlock was inspired by the need to simplify how students discover and engage with campus events. We aimed to create a central hub where students could find events tailored to their interests, utilizing AI to enhance personalization.","CampusFlock is a web-based event management platform that recommends campus events to students based on their interests. It features a user-friendly interface for discovering events, managing profiles, and searching for specific activities, all designed to streamline the campus experience.","We built CampusFlock using Vue.js for the frontend to ensure a responsive and interactive user experience. For the frontend design, we utilized Vuetify for UI components and Vue Router for seamless navigation between pages.",One of the primary challenges we faced was the integration of MongoDB Atlas. We had planned to use MongoDB for robust data storage and retrieval but were unable to implement it within our project timeline. This required us to adjust our approach and find alternative solutions for data management temporarily.,"We are proud of creating a functional platform that effectively addresses the needs of students for discovering and engaging with campus events. Overcoming the technical challenges, particularly in implementing personalized event recommendations without the full database solution in place, has been highly rewarding.","The development process was a significant learning curve, where we gained deeper insights into Vue.js, state management, and the complexities of integrating AI with a web interface. The challenge of adapting to unexpected changes in project scope tested our problem-solving and adaptability skills.","Going forward, the first step will be to fully integrate MongoDB Atlas to enhance data handling and scalability. We also plan to refine our AI recommendation engine and introduce verified user accounts for organizers. Expanding to additional universities and incorporating feedback will be crucial as we continue to evolve and improve CampusFlock.","css, html, javascript, json, material, node.js, router, typescript, vite, vue, vuetify",https://github.com/sammyfischer/CampusFlock
VTHacks 12,Waitless,https://devpost.com/software/waitless-yvt7zh,False,,,,,,,,"blender, css, django, html, javascript, mui, postgresql, python, react, scikit-learn, tailwind, tensorflow, three.js, vite",http://waitlessus.us/
VTHacks 12,AlertAI,https://devpost.com/software/alertai,False,"911 professionals are increasingly overstressed and understaffed, with many lacking the necessary training, managerial support, and technology to perform their jobs effectively, particularly during mental health emergencies and natural disasters. A high turnover rate has further intensified the shortage of call-takers across the country. AlertAI was developed to tackle these issues by providing real-time AI assistance, streamlining call analysis, and reducing the cognitive load on responders. By improving response times and enhancing decision-making, AlertAI helps 911 professionals focus on their critical work, ensuring they can serve their communities more effectively. The idea for AlertAI was driven by the need to improve emergency response times in life-critical situations. By leveraging AI, we saw an opportunity to assist dispatchers in quickly identifying essential information from emergency calls, enabling faster, more informed decision-making and response.","AlertAI is an AI-powered tool designed to assist 911 responders by transcribing emergency calls in real-time and extracting critical information such as location, caller details, and urgency. Using Twilio for live call handling, Azure Speech-to-Text for transcription, and GPT-4 for intelligent data extraction, AlertAI provides dispatchers with a clear, interactive dashboard. The extracted data is then stored in a MongoDB Atlas database, giving responders quick access to vital information and helping them prioritize and make faster decisions during emergencies.","AlertAI was built using a combination of powerful tools and technologies:
-Twilio for real-time call transcription.
-Azure Speech-to-Text to convert call audio into text.
-OpenAI’s GPT-4 for extracting key entities and important details from the transcriptions.
-MongoDB Atlas to store the extracted data in a structured and retrievable format.
-Gradio to create a live, interactive dashboard where 911 responders can monitor and manage incoming calls and critical data in real-time.",We encountered several challenges during development:,"We are proud of building a fully functional system that integrates multiple services to provide real-time support to 911 responders. We successfully developed a user-friendly dashboard that streamlines the call analysis process and reduces the cognitive load on dispatchers. The ability to provide accurate, real-time transcriptions and intelligent data extraction is an accomplishment that could make a meaningful impact on emergency response times and effectiveness.","This project taught us the value of combining AI with real-world applications. We gained experience in integrating multiple APIs and services, optimizing real-time performance, and building responsive front-end solutions for critical tasks. Additionally, we learned how AI can assist in emergency scenarios by easing the burden on human operators, improving their ability to respond quickly and effectively.",,"azure, gpt, mongodb, openai, python, twilio",https://github.com/JayanthNarla/emergency_response
VTHacks 12,Intoxicaution VR,https://devpost.com/software/drunkvr,False,"We were inspired to make Intoxicaution VR because we know how much alcohol affects college students. We've all known someone who's made risky choices after drinking, and we wanted to address this issue in a meaningful way. In fact, researchers show that on average 1,519 college students die per year due to alcohol-related incidents. College students are going to drink but learning how to drink by trial and error in real life can be extremely dangerous. For our project, we wanted to create a safe, controlled environment where students could learn about the consequences of poor drinking choices and how to make safe drinking decisions.","Intoxicaution VR is an interactive VR game where players are placed into a party situation and are forced to make decisions throughout the night that will determine how their night ends. Throughout the game, players will have to make decisions such as whether or not to drink, whether or not to take a drink from a stranger, whether or not to drive home, and more. Their endings can be anything from getting home safely, to a car accident, and more. Throughout the game, the player's blood alcohol content (bac) will be tracked using a progress bar. The more intoxicated the player becomes the greater the ""drunk"" effect we have created becomes. During the effect, players will experience increased wobbliness and distorted sound. If a player's bac becomes too high they will experience a blackout. Throughout the game, there will also be helpful notes and tips to add additional insight into the best drinking practices.","We build this project from the ground up, mostly using Unity. We also used Blender to make most of the 3D models and the environment for the project, Figma to make the bac progress bar, Mixamo to make the animations, and some free sound effects and sound file converters to put the sound effects in the game.","We faced several challenges during the creation of this project. Figuring out how to get the drunk effect in VR proved challenging. Planning out the different branches for the choices and events was also fairly difficult. Also, inevitably Github merge conflicts and slow computers also gave us some significant problems.","The increasing drunk effect and the overall environment turned out really well. Also, we were able to include all the storylines and decisions we wanted to make in the beginning and they all work well so we are very proud of that. We are also proud of the voice acting and dancing animations we had a lot of fun writing and recording the voiceovers and finding the dance animations for the party.","We learned a lot about Git Bash, including how to navigate version control and resolve merge conflicts effectively. We also gained a deeper understanding of creating Unity effects by implementing the camera wobble and sound distortion to simulate the drunk experience. In addition, we explored the process of designing and developing UI elements like the BAC bar, ensuring it dynamically responded to in-game actions. We also dove into game design principles, from modeling assets in Blender to creating branching narratives, and learned how to incorporate sound design to enhance immersion and feedback. Overall, this project pushed us to expand our technical skills and improve our ability to collaborate effectively as a team.","If we were to continue this project we would add more choices, such as more variety of drinks with varying alcohol contents to choose from and more dangerous situations you might encounter at a party. We would also add an option for the user to put in some of their physical features such as gender or weight and use that information to make the effects of the alcohol more tailored to the individual player and allow for a more realistic experience to better educate the user on their drinking limits.","blender, c#, figma, metaquest, mixamo, unity",https://github.com/MattGallagher76/DrunkVR
VTHacks 12,NuHomes.co(m),https://devpost.com/software/nuhomes-co-m,False,"It's hard to predict the future, but it's most within reach when looking at trends from the past. New housing construction is a fantastic indicator of growth within a city or area, and our team wanted to create an intuitive visualization of new housing construction across the United States.","Our application visualizes, relatively and absolutely, an area's amount of new construction since January of 2022. Our application also offers LLM chatbot assistance to quickly and easily learn more about the area a user is viewing.","The app uses SvelteKit as a web framework, in this case serving as both our frontend and backend. This is supported by various tools, like Vite, allowing us to quickly create API routes and efficiently transfer our data, as well as plenty of flexibility in how we organized and constructed our components. The app is hosted on Vercel, and we used Git for version control. We used MongoDB Atlas for our database, and used Perplexity as the LLM behind our chatbot. Data was obtained through Zillow's research portal, available here:https://www.zillow.com/research/data/and cleaned utilizing python/pandas.","With the majority of our group being first-time hackathon participants, we weren't sure what to expect. We initially had some trouble deciding on a project, and throughout our project we had to be very careful to avoid scope creep. As expected, JavaScript rarely gave us helpful error messages, so time spent tracking down exactly where we made a mistake was costly. JavaScript's asynchronous workflow also proved to be syntactically difficult at times.","One of our group members completed a feature from data collection to cleaning, then creating an API call to retrieve it, and finally displaying and formatting on the front end. He had never even touched JavaScript before this event and being able to learn so much so quickly was extremely impressive. The two of us also greatly enjoyed coaching and pair programming with him throughout this experience and we found it highly rewarding.","Our group learned so much during our time working together. We all come from diverse backgrounds and have different experience levels. We learned much more about many tools and technologies such as map libraries, SvelteKit, MongoDB, API Calls, Pandas, JavaScript, Perplexity, and overall full-stack development. But most importantly we improved as members of a development team.","There are some more features we'd like to see, mainly an improvement in the quality and amount of data we show for each location on our site. We would also like to be able to do an asynchronous monthly pull of housing data so we can update the pool passively. We plan to add these features in the near future!","git, github, html, javascript, leaflet.js, mongodb, pandas, perplexity, python, svelte, tailwindcss","https://www.nuhomes.co, https://github.com/SaidHoja/VTHacks24"
VTHacks 12,gEYEd me,https://devpost.com/software/geyed-us,False,"We were inspired to use advanced techniques and modern machine learning to increase accessibility for visually impaired individuals. By digitizing the purpose of a seeing-eye dog, we aimed to reduce the financial burden one may have on an individual. By training and using our own computer vision model, we hoped to make the process of navigating our world more feasible and seamless for those underrepresented in our society.","Our app, gEYEd me, continuously observes your surroundings using your device's camera to detect unpredictable obstacles and will warn you if you are en route to collide with one. Additionally, you can prompt the app by saying ""Hey, guide me"" to ask a question about your surroundings. For example, if you ask where the nearest door is, our app will read out loud how to avoid obstacles and reach your destination. Other examples are as simple as ""Am I in front of room 1030?"" or ""What am I holding?""","Our main application is made using React and Javascript. Roboflow hosts our computer vision ML model. We trained the ML model based on objects around campus that we deemed could become obstructions, such as tables and chairs. React's Speech Recognition library is used to process user prompts. API calls to OpenAI are used to alert and respond to user-prompted queries. Our web app is deployed using Defang and is hosted on an AWS ECS. The domain was registered through GoDaddy.","Where do we start... As this was our first time with a few of these technologies, there was quite a steep learning curve. Our first challenge was using the Speech Recognition library to identify a custom prompt by continuously monitoring user speech. A point of emphasis was to eliminate all touch gestures by allowing controls through speech. The main challenge was capturing the ensuing statement without restarting voice recognition. The next challenge we faced was in regards to getting the model trained and hosted. We originally had too much data and the model would take 9 hours to train, so we cut out 10,000 images. Now, each time we had to train the model it would take around 2 and a half hours. After the model was trained and deployed, accessing it via the Roboflow API was a huge challenge as the documentation is not up to date nor are any of their examples using React. As such, a lot of the object detection development required heavy testing and reiteration, slowing down our ability to write clean and concise code. This means that there are still some bugs with detecting that we were not able to fully drill down. We also ran out of AWS credits while hosting through Defang. So while we were able to deploy multiple times this weekend, near the end, AWS stopped our deployment and we weren't able to restart it. Even though we had a really difficult time trying to get every system working in harmony, we are still extremely proud of the outcome.","We are proud of phrase detection, the automatic timeout for prompt completion, increasing performance for mobile Android devices, training and hosting our own ML computer vision model, using real-time camera footage to alert for obstacles, OpenAI integration for object pathfinding, international web deployment, and giving these features in a highly accessible format.","We learned to containerize and deploy React applications with Docker, process audio and video streams through a web framework, utilize a Kanban methodology, train and deploy an ML model, and wrap it all into a single application.",We plan on expanding accessibility to iOS devices.,"amazon-web-services, computer-vision, defang, godaddy, javascript, machine-learning, object-detection, openai, prokbun, react, react-webcam, roboflow, speech-recognition","https://geyedme.co/, https://github.com/Esheka/geyed"
VTHacks 12,PeerPlates,https://devpost.com/software/peerplates,False,"We’ve all been there—cooking at home to save money, and there are a lot of us who'll definitely agree with me on ""Oh I'm done with McDonald's or Chipotle""
That’s how the idea for PeerPlates was born: connecting people who crave wholesome, home-cooked meals with fellow students who love to cook. Why not match the two?","PeerPlates is an AI-powered app that pairs home chefs with students looking for nutritious, home-cooked meals. Users can search for meals based on their preferences, and the AI recommends the perfect dishes, making meal-sharing fun and effortless.","Users can describe their cravings in plain English, like ""I want something spicy but high in protein,"" and PeerPlates’ AI, powered by Azure OpenAI, translates this into meal recommendations. We also use Azure Computer Vision to analyze images and extract nutritional data from user uploads. The backend, built with Python's FastAPI, orchestrates these services and manages image storage through Azure Blob. On the frontend, we built a user-friendly interface using React.","One major challenge we an implementation flaw which we realized only during a dry run. With only 5-7 documents, the database was already bogged down due to storing base64-encoded images, which made requests balloon to over 5MB. We solved this by switching to direct image uploads to Azure Blob Storage and returning only URLs in the response. The toughest part, though, was orchestrating multiple services and stitching everything together.","Our biggest win? Designing an intuitive UI despite our team’s systems engineering background. Leveraging AI tools like Copilot and ChatGPT, we managed to create a seamless user experience that we’re really proud of.","We gained a deep understanding of AI and how to integrate various Azure services, along with the challenge and excitement of designing a full-fledged product from scratch.",,"ai, azure, computervision, fastapi, javascript, llms, mongodb, python, react","https://peerplatesai.us/, https://github.com/ba11b0y/peerplates"
VTHacks 12,Calm Compass,https://devpost.com/software/calm-compass,False,"    Inspiration: 
CalmCompass was inspired by the need for a digital tool that supports mental well-being in a calm and aesthetically pleasing environment. Recognizing the challenges many face in managing stress and maintaining mental health, the goal was to create an intuitive app that fosters a sense of tranquility and self-reflection. What it does:
CalmCompass is a web application designed to support mental well-being through a serene and aesthetically pleasing interface. The app provides a calm environment with soothing colors like purple and lilac, helping users feel relaxed and centered. Users begin by logging in, which ensures their data and personalized features are secure. Once logged in, they can access the customizable affirmations feature, allowing them to create and style their own affirmation notes, promoting positivity and self-reflection. The app initially displays a clean and distraction-free login page, revealing additional features only after successful authentication to maintain a minimalist design. Overall, CalmCompass aims to offer a user-friendly space for reflection and self-care, combining functionality with a visually calming experience. How we built it:
The project was built using a combination of HTML, CSS, and JavaScript to ensure a responsive and interactive experience. Key features included:
Login Page: A clean, focused interface that displays only after successful user authentication.
Affirmations Feature: Allows users to add and style their own affirmation notes, contributing to their personal growth.
Aesthetic Design: The app employs calming colors like purple and lilac to create a serene visual experience. Challenges we ran into:
One of the main challenges was ensuring that the app maintained a balance between functionality and aesthetic appeal. Integrating a user-friendly interface while preserving a calm and minimalist design required careful attention to detail. Additionally, implementing secure authentication and managing user data presented technical challenges that were addressed through thorough testing and validation. Accomplishments that we're proud of:
We are proud of several key accomplishments in the development of CalmCompass. First, we created an elegant and intuitive design that combines a calming aesthetic with user-friendly functionality, utilizing soothing colors like purple and lilac to enhance the overall experience. The implementation of a customizable affirmations feature stands out, allowing users to create and style their own affirmation notes, which enriches their self-care journey. Additionally, we developed a secure login system that ensures user data is protected, while maintaining a minimalist approach by displaying the menu bar only after successful login. This design choice helps users focus on their mental well-being without unnecessary distractions. Overall, the effective user experience and thoughtful design reflect our commitment to creating a supportive and engaging tool for mental health. What we learned:
Throughout the development of CalmCompass, I gained valuable experience in creating user-centric web applications. I learned how to implement features such as user authentication, customizable affirmation notes, and a minimalist design approach. Additionally, I deepened my understanding of web development practices, including the use of CSS for aesthetic enhancements and the implementation of user-friendly interfaces. Community and Support: Developing community features such as forums or support groups where users can connect, share experiences, and offer mutual encouragement.
Mobile Application: Creating a mobile version of CalmCompass to make the app more accessible and convenient for users on the go.
Enhanced Features: We aim to introduce additional features such as mood tracking, guided meditations, and mindfulness exercises to provide users with a more comprehensive mental health support system.
Personalized Insights: Implementing data analytics to offer personalized insights and suggestions based on user interactions and preferences, helping users better understand and manage their mental well-being.",,,,,,,"css, html, javascript, react",https://github.com/hansapradhan/CalmCompass
VTHacks 12,ResponseFlow,https://devpost.com/software/dff,False,"The inspiration behind ResponseFlow came from the need for better resource allocation in cities. We noticed that emergency calls often face delays in prioritization due to human error, miscommunication, or lack of resources. We wanted to solve a problem to help ease being a first responder.","ResponseFlow is an AI-powered 911 operator system that ranks incoming emergency calls based on the type and severity of the crime. The system uses Generative AI to analyze the details of the emergency in real-time, then prioritizes the most critical incidents for immediate attention, ensuring that life-threatening situations are addressed first. It streamlines the dispatch process, improves response time, and minimizes errors in crises.","We built ResponseFlow with the Vapi pick-up inbound calls and answers with an AI Operator. We used webhooks from Zapier to create a pipeline that makes a JSON Report with the help of GPT-4. We implemented real-time processing with an integrated priority-ranking algorithm that dynamically assigns resources to the most urgent cases. On the front end, we used Next.js to create a user-friendly dashboard for dispatchers to monitor and interact with live emergency reports.","One of the biggest challenges was designing an AI model that could accurately interpret a wide variety of emergency call scenarios while maintaining high accuracy. There were also challenges with balancing real-time processing speed and ensuring the system could scale to handle large call volumes without lag. Additionally, integrating the AI model with existing emergency dispatch systems presented compatibility issues that required troubleshooting.",We are proud of the accuracy of the AI model in correctly classifying emergency cases by priority. A major achievement is implementing a real-time system that can handle multiple inputs simultaneously without significant hallucination.,"We learned a lot about the intricacies of emergency response systems and the importance of rapid, reliable decision-making. On the technical side, we gained deeper insights into real-time processing, scaling AI models for high-traffic applications, and integrating LLMs with existing dispatch protocols.","Moving forward, we plan to refine the model to handle more complex emergency scenarios and expand its multilingual capabilities to serve diverse populations. We also want to incorporate machine learning algorithms that can predict potential outcomes of emergencies based on historical data, which could help preemptively allocate resources before a situation escalates.","css, gpt-4, html, next.js, python, typescript, vapi, zapier",https://github.com/sumtzehern/ResponseFlow
VTHacks 12,VeggieSwap,https://devpost.com/software/veggieswap-f2z0m4,False,Were inspired to help reduce food waste.,A platform that connects those with the interest of bartering produce.,We used React.js frontend and Django backend with a SQLite database and JWT authentication.,"Not being familiar with ReactJS and trying to use NextJS for the framework despite having no exposure to it, and eventually switching to Django, which delayed our progress. Mostly the connection between Django backend and React frontend is also very new to us.",Successfully implementing a React JS frontend! Being able to get data from database to client. Working on the full stack !,"Proper planning not just for frontend, but backend is really important. Choices with small impact in the beginning can cause a lot of trouble down the line, like how models are designed.",Lots of things (vegetables). Also getting a working implementation of the POST method.,"django, react.js, sqlite",https://github.com/blairezh/veggieswap/tree/main
VTHacks 12,CrossDash,https://devpost.com/software/50-gkenc1,False,We were inspired by the need to improve response times and provide crucial assistance during emergencies. We wanted to create a tool that bridges the gap between users and emergency services using modern technology.,"The project contacts 911 using speech recognition to relay emergency details hands-free. It also provides AI-driven first aid tips, a chatbox for support, and real-time ambulance ETA updates.","We used Flask for the backend, React for the frontend, and integrated many APIs such as Google Maps, Open Ai, and more to build our features.",Styling the frontend was the most time consuming part of the project.,That we were able to implement numerous of APIs which work together seamlessly,"We learned the importance of seamless integration between different technologies to create a real-time, responsive system.",Our team is inspired and ready to compete again next year.,"css, flask, github, google-maps, html, javascript, open-ai, python, react",https://github.com/prabadanribeiro/vthacks
VTHacks 12,Dinologue,https://devpost.com/software/dinologue-apemgf,False,"VTHacks has a prehistoric theme, so we decided that there was no cooler topic to focus on thandinosaurs!","Dinologue offers insightful information on three time periods, a dinosaur chatbot named Atticus that can vocally answer your most curious dinosaur questions, and artsy 3D graphics to ensure an immersive experience.","We build our application on the Next.js framework, using Tailwind CSS and daisyUI for the frontend, and leveraged OpenAI NLP and ElevenLabs text-to-speech models on the backend. Additionally, we built an interactive 3D model via Blender.","Configuring the models to our niche needs took a lot of effort, as well as displaying the 3D model to the user. For many of us, it was our first time utilizing the technologies mentioned above, so there was a substantial learning curve overall.","We are proud of how well we were able to divide the project into individual deliverables. A lot of communication and teamwork was needed to make sure everyone's code was combined seamlessly, and that iterative process is what made this hackathon experience both fulfilling and enjoyable.",We gained an understanding of full-stack web development fundamentals and explored various effective tools that complemented that process.,"There are many exciting ideas for Dinologue! We believe we can build an even more immersive experience by including user prompts directly within the static web pages. That is, instead of rendering all of the information of a time period at once, we would divide the sections and allow for user interaction directly. Additionally, we were thinking about implementing a timeline so that users can view how far along they are, and include progress information to allow users to recall what topics they have covered. Most importantly, we believe this template for learning can be applied to various topics, which is our main takeaway from this project!","blender, daisyui, elevenlabs, next.js, openai, tailwindcss","https://github.com/jjdsilva0/VTHacks24, https://www.dinologue.study"
VTHacks 12,HokieHomes,https://devpost.com/software/hokiehomes,False,"HokieHomes was inspired by the need for Virginia Tech students, especially freshmen, to have a reliable resource for honest dorm reviews, helping them make better housing decisions based on real experiences.","HokieHomes provides a platform where students can explore dorms, read unbiased reviews, and rate key factors such as cleanliness, Wi-Fi, and room size, helping them choose the perfect on-campus accommodation.","We built HokieHomes using React, Next.js, and Tailwind CSS for the front end, while the back end is powered by Node.js and MongoDB. We also integrated authentication( PropelAuth) and user data management to ensure secure interactions.","One of the key challenges was ensuring the seamless integration between the back-end database and the front-end user experience. We also faced difficulties with authentication, which we amended using PropelAuth.","We’re proud of building a platform that offers a real impact for students, helping them make informed dorm decisions. Our smooth and modern UI, along with robust back-end functionality, is another accomplishment.","We gained valuable experience in full-stack development, particularly in building scalable web apps. We also learned how to address user needs and build features that add genuine value to a community.","We plan to enhance HokieHomes with more advanced search features, add user personalization, and implement a recommendation system for dorms. We also aim to expand the platform for off-campus housing in the future using real estate data.","express.js, javascript, mongodb, node.js, propelauth, react.js, tailwind-css, typescript","https://hokiehomes.co/, https://github.com/turboslapper/hokiehomes"
VTHacks 12,Realtor 101,https://devpost.com/software/realtor-101,False,Members of our team had deep family connections to real estate.,"Simulates a client-realtor interaction, where the AI plays someone searching for a home.","In a react.js framework, using node.js for package management and node.js and express for backend. We also used OpenAI's API for gpt 4.0.","Integrating the AI into our project, as we are all very new to working with AI; for example, one struggle we initially had was getting GPT 4.0 to act as a client instead of a realtor. We faced merged conflicts and data loss.",We succeeded in creating an AI that acts as client for our user. That all visuals in our website are original.,"How to successfully use GitHub, how to implement and design a react app, understanding full stack development, using RESTful API, and understanding of Artificial Intelligence.","We would use an actual database of houses, such as homes.com, to provide a more realistic enviroment for the user. We would also implement a button for users to submit a house directly from our database to the AI client, instead of having to manually type the description of the house.","asprite, css, express.js, figma, javascript, node.js, openai, react, restfulapi",https://github.com/AidenRacelis/HackVT12-Project-
VTHacks 12,Stripe Play,https://devpost.com/software/google-calendar-punishment,False,I want to learn about how to charge credit cards.,It tries to charge my credit card.,Using the stripe api.,"Some of the information on the stripe api is outdated, and they don't like you inputting card information directly into the api. Rather Stripe is built to take this information from a browser, and for security reasons they don't let you pass this information as a parameter in the python code without explicit permission from the Stripe Company.","I'm proud of messing around with the strip api, while hanging out with my friends at Virginia Tech and exploring campus.","I learned that the Stripe API might be more difficult to deal with, and that there are major security concerns that have to be dealt with whenever making an app that deals with other people's money.",I am gonna make some future applications that allow me to charge other people in a secure way.,python,
VTHacks 12,Aviate.ai,https://devpost.com/software/aviate-ai,False,"The North American bird population has declined by 2.9 billion since 1970, a decrease of nearly 30%. This staggering figure is the result of habitat loss due to urbanization. We believe it's crucial for the public to understand the impact we have on bird populations and to inspire people to take action to protect them.","Our application allows users to identify what birds they are hearing, share their findings, and load that data into a larger database for researchers to track historical bird populations. When a user begins a hike, they hit the ""Get Started"" button on our homepage. The app then uses machine learning audio recognition to identify bird calls with greater than 85% accuracy. It then shows that data to the user throughout their hike while sending that information to a database, displaying the information for researchers to track bird populations and biodiversity in the area. Users can also view what birds they have seen in the past and share their sightings with friends.","We spent sufficient time preparing our tech stack for this project. To build the front-end we used React and Materials UI, for the back-end we used Flask and MongoDB. We utilized Flask to handle API requests and manage interactions between the client and server. Flask acts as the central hub for processing data and executing logic. For data storage and management, we chose MongoDB,  to efficiently record various forms of data including information about hikes, bird sightings, and unstructured data like bird sounds. The machine learning component, responsible for identifying bird calls, leverages thebirdnetliblibrary. By integratingbirdnetlibwith our front-end, we offer a seamless experience for users and contribute valuable data to support ongoing research and conservation efforts","There were many challenges that we faced during these 36 hours. Early into the project, we encountered an issue with retrieving audio recordings and encoding them into a base64 string to transport to Flask. However, there were many issues with decoding and analyzing it with thebirdnetliblibrary. Once we resolved the encoding and decoding issues, we encountered complications with interval recording and updating MongoDB dynamically during a hike. Ensuring that the database reflected real-time data proved to be a complex task. Additionally, we ran into an issue with the API endpoints from Flask. We were getting insufficient responses from the API requests and no data was being returned. Despite these hurdles, we preserved and worked together to address each issue and deal with them appropriately.","We are proud that we were able to create our product in such a short amount of time, especially since we had never used MongoDB beforehand. We feel that the UI is professional and we are proud of how scalable the backend is, leaving room for future improvements.","We learned how to delegate tasks and work efficiently as a team. A challenging part of the project was mapping out all the moving parts, and we are glad that we were able to develop a plan to complete our vision within such a short period. We also learned a lot about utilizing Flask as an API and integrating MongoDB, which we were previously unfamiliar with. Overall, we felt that the main takeaway was the importance of strong organizational skills when working on a complex project.","We plan to implement more visualization tools for researchers to view historical trends and easily determine regional biodiversity. Another step would be to gamify the app, adding features like friends, tracking other users, and a competitive aspect such as a points system for rare birds.","amazon-dynamodb, birdnet-lib, flask, javascript, materials-ui, mongodb, propelauth, python, react, roboflow, tensorflow, unsplash",https://github.com/acai16/vthack2024
VTHacks 12,Goku Homes,https://devpost.com/software/goku-homes,False,"We wanted to create a tool to facilitate home searching, helping estimate potential homes for users and raise awareness about the increasing prices within the housing market.",Goku Homes helps those in search for a home with the ability to forecast housing prices in certain areas based on the inputted location and salary withgenerative AI.,"Using theStreamlit ** library in combination with **OpenAI API, numpy, plotly, and general python libraries, we were able to create Goku Homes.","We had difficulty implementing theOpenAI API, generating pictures with the AI, learning ** Streamlit** library, creating trendlines for the graphs, and cleaning the data.","We were able to create interactive graphs withlinearandquadratic regressionandplotly, implementOpenAI API, and learn theStreamlitdocumentation","We learned how to clean and read in data, useplotly, useOpenAI API, some css,styling, and theStreamlitlibrary","Implement and train our own AI, get access to a database API, move from Streamlit to React for more customization, and create more sliders for users to input data","css, numpy, openai, pandas, plotly, python, streamlit","https://github.com/newtonma7/vthax12---GOKU-HOMES, https://gokuhomes.streamlit.app/"
VTHacks 12,911 Emergency AI assistant (EmAA),https://devpost.com/software/911-emergency-ai-assistant-emaa,False,We were inspired by the need for more efficient and effective emergency response systems. We wanted to leverage AI and natural language processing to create a tool that could quickly and accurately assess emergency situations and provide critical information to responders.,"Our project, 911 Emergency AI Assistant (EmAA), is a web-based application that uses speech recognition and AI analysis to process emergency calls and provide responders with vital information. It can:
Transcribe spoken language in real-time
Identify key details such as location, incident type, and severity
Provide suggested responses and protocols for responders
Offer language translation for non-English speakers","We built EmAA using a combination of technologies, including:
React for the frontend UI
Node.js and Express for the backend API
Google Cloud Speech-to-Text for speech recognition
Natural language processing libraries for AI analysis","Integrating speech recognition with AI analysis
Ensuring accuracy and reliability in emergency situations
Handling multi-language support
Designing an intuitive and user-friendly interface
Accomplishments that we're proud of
Successfully integrating speech recognition and AI analysis
Achieving high accuracy rates in testing
Creating a user-friendly and intuitive interface
Implementing multi-language support",,"The importance of testing and iteration in AI development
The challenges of integrating multiple technologies
The value of user-centered design in emergency response systems","Further testing and refinement
Integration with existing emergency response systems
Expansion to support additional languages and incident types
Exploration of potential applications in other fields, such as healthcare or customer service","gpt2-large, llm, natural-language-processing, python, react","https://code.vt.edu/aishwaryak/vthack12-a/-/tree/main, https://docs.google.com/presentation/d/1EB0Iss2YT-1SqL5pSpLLXqMOooJ30G1S/edit#slide=id.p1"
VTHacks 12,UniHub,https://devpost.com/software/unihub-lun42r,False,"As an incoming student, settling into college can seem overwhelming, so we created a hub where any student, regardless of year, can find whatever they need. We want students of all backgrounds to have equal access to all resources and adventures their university offers!","UniHub is dedicated to connecting students with essential resources such as housing, transportation, and self-care. We aim to provide a one-stop platform that simplifies student life, making it easier for you to focus on your education and personal growth. Join our community today and discover all that UniHub has to offer.","To start, we downloaded a general template with a basic layout. From there, we edited the front-end aspects to create the user interface we wanted and made many aspects dynamic as the user moved through the website.","Hosting the Site with AzureOnce we built the entire website, we tried to host it with Azure Student. However, the site's loading time is prolonged, causing the request to time out.Using the Google Maps APIWe planned to use the Google Maps Search API to show local restaurants, housing, and shopping near Virginia Tech; however, we ran into permissions problems because of the need for location services. Instead, we embedded a Google Maps that we had created, where we pinned all the information for the user to access.","Sleek and Efficient User Interface with dynamic components and use of react hooks. 
We are proud of our sleek and dynamic user interface, which we configured using React hooks. The setup of our website allows students to quickly find the resources they need whenever they need it.","Designing a Dynamic User InterfaceWe started with primarily static elements on our website, but one of the main things we learned was how to make some aspects dynamic to make them more visually appealing.Using APIs to Populate a WebsiteTo carry out what we envisioned for this website, we learned how to access adn embed APIs such as Spoonacular, a recipe database, to populate the recipes section of our wellness tab.","AI Chatbot to Navigate & Personalize UniHubWe plan to create a questionnaire upon Sign-In to create personalized recommendations for resources, restaurants, shopping, etc., based on the user's liking. Additionally, we will implement an AI chatbot that will help users navigate through the website and answer any questions.Accessibility to Universities other than Virginia TechUpon Sign-In, users will be able to choose which institution they attend and, therefore, receive tailored recommendations based on their location.Student Contribution PageStudents can sign up to contribute their personal experiences at their university, including places they enjoy, such as shopping, studying, or eating. This will enable a sense of community within the students of each university.","api, bootstrap, css, html, node.js, react, spoonacular",https://github.com/mcarr2k/UniHelper
VTHacks 12,Cache It!,https://devpost.com/software/cache-it-61x7pc,False,I collect and sell trading cards and was interested in the idea of cataloguing my items quickly and easily,"The Catch It! application scans an item and then finds its inherent value. The user can then store it in their ""Cache"" to track their total and individual item value.",The project was developed in Python utilizing the Google Vision and eBay Developer API,The project mostly came into problems dealing with identifying the items. It is much better than it started but has a lot of work to be done.,"I am proud of this project because it is nothing like I have worked on in the past. Coming from an Electrical Engineering background, this project taught me a lot about what I am capable of and how much more I can learn.",My main takeaways from this project is my basic learning of Python GUI construction and the use of AI and image processing from various APIs.,"Pursing this application further, I will work towards improving the identification of objects scanned and maybe even implement a machine learning aspect to build onto its own structure to recognize more items for the future.","ebay-api, google-cloud-vision-api, google-custom-search, opencv, pillow, python, tkinter",
VTHacks 12,TradeMaster,https://devpost.com/software/trademaster-6xdm12,False,"We were inspired by financial literacy throughout America and wanted to propose a game that would allow people to learn more about stocks in a simple and effective way, with feedback received through LLMs.","We generate real-time data for companies and generate graphs for them, allowing people to play a 24/7 stock market game that will always adapt to the market, while also taking a unique approach to developing the algorithm to use elements of probability to change the price.","We built this project using React for the frontend, while using Flask in the backend to connect everything in the frontend. We also used MongoDB Atlas to store our data generated from stocks and retrieve that data to generate graphs. Python was used to generate the mathematical functions, while HuggingFace was used in order to build a LLM chatbot.","We ran into difficulties with combining the frontend and the backend for integration testing. We also had issues deploying our LLM approach, with the end condition for displaying the summary not being reached. The frontend played a large role in this, as trying to add a case for LLMs made it so that the other buttons would not work.","We are proud that we were able to get a significant portion of this concept done in such a short time, simulating stock prices in a way that hasn't really been done before.","We learned a lot about the world of stock pricing along with various stochastic processes.
We also gained significant experience in the frontend area with React and Flask serving as our guide to completing this project. We also learned to implement a mathematical algorithm to simulate stock prices, which is what we did in this case.",We would like to make sure that the LLM front works properly along with getting the timings for the market accurate. We may also make it so that advice can be asked during the game and a multiplayer concept is implemented in the game.,"api, flask, huggingface, llm, mongodb, python, react",https://github.com/v-gatne/StockScanner
VTHacks 12,forecastai.biz,https://devpost.com/software/forecastai-biz,False,"Forecast AI is a stellar system which distributes crucial campus info to Hokies in an instant. This information can range from weather alerts, inclement weather, or emergency notices. We decided to take the weather aspect of this app and use AI to increase efficiency and scale it outside the university. Weather is recorded frequently leaving substantial data sets for many areas. This would allow an AI model to make meaningful predictions and suggest garments suitable for the given weather. All in all, we wanted to create a project that plays into the time saving nature of AI, while promoting safety among the general public.",A user can access this website to input personal data such as height and weight to identify vulnerabilities one may naturally have to certain weather. Contact info will be entered alongside leading to personalized real-time weather alerts to determine the best course of action.,"Our group of the best engineers and leaders gathered together at VTHacks 2024 and created something amazing! Utilizing JavaScript, Python, Heroku, Azure OpenAI, ChatGPT, Dall-E, FastAPI, Express.js, and others to deliver a fantastic app that is deployed for public testing.","There was an incredible amount of challenges which included the time, data, resources, and computing. The cutting-edge AI tooling would have been expensive to build locally but luckily Microsoft Azure and Cloudforce assisted us in providing their generative artificial intelligence solutions to us.","We were able to build a complete automation that incorporated not only predictive analytics with weather, but also image and text generation with cutting edge GPT technologies.","Throughout this project, we gained valuable insights into integrating various technologies and managing complex systems. One key lesson was the importance of interdisciplinary collaboration, as combining different skill sets from AI and machine learning to web development and user experience design was essential for the project’s success. We also learned about scalability and performance, recognizing the critical role of cloud platforms like Azure in handling large data sets and maintaining efficiency. The experience underscored the importance of user-centric design, as tailoring predictions and alerts to individual needs proved to be central to the project's effectiveness. Additionally, managing resources efficiently and leveraging external support, such as that provided by Microsoft Azure and Cloudforce, highlighted the value of strategic planning.","Looking ahead, we plan to expand our data sources to enhance prediction accuracy and coverage across more regions. We aim to refine our algorithms to offer even more personalized recommendations by better understanding individual vulnerabilities. Gathering and incorporating user feedback will be crucial for improving the app’s interface and functionality. We also intend to explore partnerships and collaborations to extend the app's reach and impact. Furthermore, we are excited to develop advanced features, such as integrating wearable technology for real-time health monitoring and automated garment suggestions based on live weather conditions, to provide even greater value to our users.","azure, chatgpt, dall-e, heroku, javascript, python","https://forecastai.biz/, http://20.84.71.87/docs, https://github.com/hammad93/vthacks24, https://github.com/HarshKhilawala/forecastai, https://forecastai-6441483f19f5.herokuapp.com/, https://gist.github.com/hammad93/329c1238ed67233a80fe79b83d190f44"
VTHacks 12,Busy or Not?,https://devpost.com/software/busy-or-not,False,"The idea for the website came from countless frustrating trips to stores only to find long lines and overcrowded spaces. Wanting to make everyday errands more efficient and stress-free, we set out to create a tool that helps people avoid the hassle and save time by knowing when businesses are busy before they leave home.","Busy or Not? allows the user to search for a desired location, day, and time in order to see how busy the location is. It generates both the day and week busyness forecasts and provides multiple outputs such as busyness scale, peak and non-peak hours, store hours, and a detailed heat map for the week.","The website was built using Vite, Vue3, TypeScript, Tailwind, and DaisyUI for the best possible user experience. We used the Google Places API to implement a high-quality location search engine and the BestTime API to analyze the important traffic data needed for the UI of the website. We used ApexCharts to visualize the week forecast in a heat map. Finally, we hosted our website on Netlify.","Visualizing the data into a heat map
Addressing difficulties with the UI/UX portion of the project",We are extremely proud to have achieved our primary goal of creating a fully functional website that lets users know the traffic levels of any specific location they’re interested in.,We learned how to integrate APIs and visualize data to create a functional and useful product for a wide range of users.,"Responsive design for mobile devices
More ways to visualize the data
Various UI/UX improvements
Adding the previous location search history","apexcharts, besttimeapi, daisyui, datavisualization, google-places, html, javascript, netlify, tailwind, typescript, vite, vue3","https://busyornot.netlify.app/, https://github.com/mariamsinger/busy-or-not/"
VTHacks 12,GeoNest,https://devpost.com/software/geonest,False,"Honestly, with global warming and climate change making the weather that much more catastrophic, we believed that a home shouldn't be affected. One's home is their place of refuge, where they can go to rest, to feel safe, but more often than not we aren't always prepared when disaster strikes. Therefore, we came up with a website which will allow future homeowners to know the environment in which they are buying a house, so they can prepare well for any disaster which could strike.","It essentially allows for houses which are to be sold to be showcased on a map, displaying alongside it the likely natural disasters which could occur, with tips. Additionally, if the sellers have uploaded their house as a bird's eye view to google maps, it will appear there for a more immersive first person experience.","We built it using a combination of react, html, css, flask, python and google maps api.","We ran into several challenges along the way: finding out how to create a map using google maps api with custom markers, finding a way to bridge the gap between react front end and python backend to use a weather api for the natural disasters, finding a way to properly customize the front end in new styles such as pop up text boxes and drop down search bars and finally putting it all together to make it one cohesive project.","To be quite frank, even though it may seem cliché or redundant, we are proud of everything we have done. This is because the whole project in itself has been such a massive challenge, as people who didn't normally work with these interfaces all together before. We all had a basic knowledge of python, git, html, react, css and so on but to use it to this extent and then manage to combine them, which costed sleep and mental sanity (albeit worth it), really makes us proud of what we have done. This of course doesn't mean there can't be improvements, but there definitely has been progress.","We learned how to mesh together different apis and interfaces, how to use different libraries within react, and the finer details of typescript, learned little tid bits of css which we didn't know before such as text drop downs and learned how annoying it is to combine python and react (seriously I don't know why we did that).","Well we are hoping to eventually make it even more immersive and user friendly than it is. We had some ideas on the back burner which were different from the conventional home sale/rental site for user interaction, but decided to save it due to time constraints. Aside from that however, I don't think we will really be coming back again for any other improvements unless the need arises as we continue to code, as it always will.","css3, fema, flask, google-maps, html5, python, react, typescript",https://github.com/vibz2/GeoNest
VTHacks 12,Choo Choo,https://devpost.com/software/chugga-chugga,False,"Playing board games with friends and family are a great way to get away from the chaos of our technology driven world. Mexican Train is a popular domino game that we both have played before. Recognizing the need to optimize scoring efficiency, we sought to utilize technology to innovate a solution.",Choo Choo is a mobile app that optimizes scoring efficiency utilizing a computer vision model with game and player management support.,"Swift is used for user interface development given its minimalist yet highly functional capabilities as seen in the Apple operating systems. Roboflow, YOLOv2, and Apple's CreateML allows for efficient creation of a dataset along with training functionality crucial in developing a model.",Dynamic rendering across views using Swift and developing robust detection models. These challenges were due to limited experience with the programming language and time constraint to develop an extensive dataset respectively.,"At the end of VT Hacks, we have a functioning mobile app.",We learned how to develop a mobile app using Swift and the importance of having a significantly large dataset for model training.,Bug fixes and deployment.,"createml, roboflow, swift, yolov2",https://github.com/andrewbviola/chugga-chugga
VTHacks 12,RepRight,https://devpost.com/software/repright,False,We wanted to start working out but were afraid of injuring ourselves. We aimed to ensure that we could complete workouts correctly without needing a gym buddy for guidance.,RepRightprovides on-demand posture correction using the power of computer vision AI. It analyzes your form during workouts and offers real-time feedback to ensure you exercise safely and effectively.,"We used Python as the main programming language. The frontend was developed using React, JavaScript, and CSS, while the backend was built with Flask. For computer vision, we integrated Mediapipe and OpenCV. Node.js was also used for additional backend functionalities. The project was deployed using Heroku.",Ensuring that the system could accurately determine whether the user's form was correct posed a significant challenge.,"We successfully perfected the accuracy of the form-checking system and created a simple, user-friendly web app that anyone can use instantly.","We learned a lot, as all team members were new to both computer vision and deploying web projects.","We plan to add exercise schedule plans, introduce more workout types, improve accuracy with more data, and enhance the UI for an even better user experience.","css, flask, heroku, javascript, mediapipe, node.js, opencv, python, react",https://github.com/RahemHossain/RepRight
VTHacks 12,Pantry Pilot,https://devpost.com/software/pantry-pilot,False,"    Inspiration
Our team has always been passionate about machine learning, so we were thrilled to take on the challenge of building a project that incorporated multiple aspects of AI. This hackathon gave us the perfect opportunity to bring our skills together in an exciting, innovative way. What it does
Our mobile app allows users to take a picture of their fridge, pantry, or food items, and from there, it does the heavy lifting. The image is analyzed using a computer vision model that detects the ingredients. These ingredients are cross-referenced with a large recipe dataset of over 5,000 recipes. Based on the user’s dietary restrictions and preferences, the app then generates a recipe, complete with healthy ingredient substitutions. This helps users, particularly those with dietary needs or restrictions, find easy meals they can prepare with what they have. How we built it
The front end is built with Swift and SwiftUI to provide an intuitive user interface. The app interfaces with a Flask-based backend hosted in the cloud. When an image is uploaded, the backend sends it to a custom-trained Azure Computer Vision model that identifies the ingredients present. The backend then searches for relevant recipes by comparing the detected ingredients to a large recipe dataset. Using retrieval-augmented generation (RAG), the ingredients, missing items, and recipe context are provided to an LLM (ChatGPT), which formulates a healthy recipe based on the user’s dietary restrictions. This entire process is powered by API calls and JSON data exchanges. Challenges we ran into
We encountered challenges like handling GitHub merge conflicts, training the computer vision model with accurate labeled data, dealing with oversized image files in API requests, and managing the complexities of setting up a seamless AI pipeline. Additionally, we faced the challenge of continuing development after losing a team member with limited time left. Accomplishments that we're proud of
We’re proud of successfully setting up the entire pipeline ourselves without relying on external frameworks like LangChain. We integrated the Spoonacular API to embed recipe data and used it to power our LLM model with context for RAG-based ingredient substitutions. Learning Swift in a short period and applying it to complete a project at our first hackathon was a significant achievement for the team. What we learned
We gained valuable experience working with Swift, Azure Cognitive Services, and setting up an end-to-end AI pipeline. The project also taught us how to manage complex system integrations and API handling. What's next for Pantry Pilot
We plan to continue refining and training our computer vision model for greater accuracy and implement additional features to cater to a wider and more diverse audience. We aim to make the app more inclusive and versatile for users with various dietary preferences and restrictions.",,,,,,,"azure, cv, flask, python, spoonacular, swiftui","https://github.com/vthacks2024/Pantry-Pilot-Frontend, https://github.com/vthacks2024/Pantry-Pilot-Backend"
VTHacks 12,Hearth,https://devpost.com/software/hearth-y52bpx,False,"The inspiration for Hearth came from two big issues: skyrocketing housing costs and the need for accessible services like nannies, tutors, or companions for the elderly. We thought, why not connect the two? Hearth is our modern, community-driven solution that matches homeowners with people seeking affordable housing in exchange for valuable services—making homes and hearts a little fuller, one match at a time!","Homeowners can create profiles detailing their available living spaces and the assistance they need, such as nannying, tutoring, elderly companionship, or housekeeping. They also specify the home’s location, and other important information, including amenities and the duration of stay. On the other side, service providers—such as students, caregivers, or workers—can sign up by creating profiles that highlight their skills, services offered, preferred locations, and availability. Hearth solves two challenges at once - providing affordable housing and fulfilling essential service needs.","We used React to create a dynamic and interactive user interface. We also utilized TailwindCSS for our styling needs, making it easier to design a clean and responsive layout. We also utilized Next.js for server-side rendering and routing. However, we also used HTML and CSS formatting. In our attempts to integrate AI, we mainly used python and looked into possible models/APIs like those from Hugging Face and OpenAI. Despite our efforts, these AI systems didn’t function as expected within our project. As a result, we were unable to achieve the AI functionality we had aimed for.","We ran into a lot of time constraints while creating our project, especially due to the fact that many of us had to learn all of the technologies we were working with for the first time! One of our biggest challenges was diving into front-end development. None of us had much experience with React, so merging our basic HTML and CSS experience with this new framework was a bit of a learning curve. We spent a lot of time figuring out how to adapt our previous web design skills to React’s component-based system. Another one of the biggest challenges we faced was integrating AI into our project, and, to be honest, we didn’t quite get it working the way we wanted. We wanted to build an AI-driven matching system to pair service providers with homeowners based on preferences, but it turned out to be a lot harder than expected. We spent a lot longer than planned fine-tuning the algorithms, trying to make sure they were accurate and useful, but it didn’t come together in time.","We all learned React, TailwindCSS, and Next.js, which we all had little to no experience with. Several of us used GitHub to collaborate, which most of us have not done before. In addition, we were able to create some beautiful mockups in Figma! We’re proud of the final, polished look of the website. Finally, we gained knowledge about the OpenAI API and utilizing and finetuning GPT models in our code. Although our AI matching system didn’t ultimately work, we made a lot of progress in understanding and implementing AI systems and API integration.","We learned that it’s important to plan out small, manageable steps, and figure out what everyone’s best at to have them work on it. In addition, we found that it’s important to get the functionality working before we code the front end of a website.","In the future, we aim to fully implement Hearth's smart AI matching system. While we made significant progress during this hackathon, we weren't able to get the system fully working. However, our goal was to have the AI pair homeowners with service providers by analyzing factors like location, skills, availability, and preferences. Once the system is working, it will improve the user experience by making seamless and accurate matches based on personalized data, ensuring both homeowners and service providers are satisfied with their pairing. We’re also excited to flesh out our database system by furthering the Explore Page’s ability to pull from the housing database that the List A Hearth page writes to! In addition, we’d love to implement a background check program with a third party such as Checkr to ensure that this program was safe and secure.","css, javascript, next.js, react, tailwind, tailwindcss, typescript",https://github.com/bluescreenjay/hearth/tree/main
VTHacks 12,Aurelia,https://devpost.com/software/aurelia-io73hl,False,"As black students, we are constantly exposed to emerging technologies. Sometimes, though, we ask ourselves, ""Why can't technology see me?"" The most frequent and frustrating example of this is failing to activate touchless faucets in bathrooms. It is especially frustrating watching others with lighter skin use these systems with ease. We wanted to develop a solution that paves the way for future tech that sees all skin tones.","Aurelia uses transfer learning and machine learning in JavaScript. We trained a neural network with datasets of over 650 images to precisely recognize diverse facial features in different lighting.  When users activate their camera, Aurelia will indicate if their face was recognized and whether the lighting was suitable, allowing them to change lighting if needed. It will display a number between 0 and 1, with 0 being low confidence and 1 being high confidence. As users scroll through the page, we added information about colorism in facial recognition technologies so users are aware of its dangers. We also added applications and privacy tabs to increase user awareness.","Our dataset training was based on aconvolutional neural network (CNN)using TensorFlow. We created a custom dataset using our own images. Then, we implemented it onto our website usingHTMLfor backend andJavaScriptandCSSfor frontend.","CSS and JavaScript: We had trouble with integrating the CNN model we used into our application using JavaScript, as most of our team's experience was with ReactJS, which uses functions such as useState for event listeners with different syntax as well. Defining our data: Choosing our dataset also proved to be challenging as we had to define what good lighting and bad lighting was. We decided front lighting was better than back lighting. We took various photos at different times throughout the day to have the best available dataset to tailor our model to our precise project. Model: We ran into some issues with our model detecting the discrepancies in lighting conditions. To solve this, we considered more variables that could change the results the model generated such as different backgrounds, multiple people in frame, and having the subject only partially in frame. We then simulated those situations when implementing our dataset. This resulted in more accurate readings for a broader range of facial features and lighting conditions.",First Hackathon: This hackathon was our first hackathon and we're super proud that we were able to finish from our problem formulation to solution implementation in 24 hours! We're also very proud that we were able to gear our hack towards an issue that will make a meaningful impact when implemented because colorism is an issue we deal with daily!,"CNN's: Each of us gained familiarity with CNNs, how they work, and real world applications. Github Collaboration: We gained a cohesive understanding of Github with commands such as push and pull to work on the project. Lighting: Different shots in photography and what is not susceptible to picking up on dark skinned features. Keyboard Commands: We learned new commands like CTRL + SHIFT + ALT + WIN + L (shoutout LinkedIN), ALT + SHIFT + Up/Down (to duplicate on VSCode), and CTRL + D (to select multiple instances)!","Commercial Applications in Professional Photography and Videography Services: Our tool can be used in professional environments to ensureaccurate lighting conditions for ALL skin tonesand inclusive media representation. Additionally, our technology could be implemented in online meeting platforms to help users present themselves better under different lighting situations during virtual calls. Applications in the Classroom: Aurelia ensures thatevery student is on a level playing fieldthrough allowing the chosen device to pick up each student's features accurately. Some common applications for this in the classroom include increasing connection in virtual learning environments via higher face detection and proctored test applications such as LockDown Browser.","css, html, javascript",https://github.com/nssim516/VTHacksJAN
VTHacks 12,Smart Parking Availability Tracker,https://devpost.com/software/smart-parking-availability-tracker,False,"Website Visit QR code Inspiration: 
Students often struggle to find parking spots, wasting valuable time driving around and ultimately arriving late to class. This frustration inspired us to create a solution that helps students locate available parking quickly and efficiently, saving time and reducing stress. What it does: 
The project allows users to scan QR codes at parking spots to check in. This data is saved in a database, and the system tracks the availability of parking spaces. Users can view which spots are available through a dynamic interface. How we built it: 
We developed the front end using HTML, CSS, and JavaScript for a smooth user experience. The back end was built with Flask, which manages the real-time parking data and connects with a database to store and fetch parking availability information. We generated QR codes for parking spots, which users scan to check in. Challenges we ran into: 
We initially planned to integrate real-time parking availability directly into the main page for users to see instantly, but we couldn't fully integrate the database with the front-end display due to technical constraints. As a workaround, we created a random simulation to demonstrate how many parking spots are available. Accomplishments that we're proud of: 
We successfully created a functional system where users can check in via QR codes and store data in a database. Despite challenges, we built a fully working API to track real-time parking availability, and managed to simulate the data display on the main interface. What we learned: 
We deepened our understanding of full-stack web development, including building APIs with Flask and creating responsive web designs using HTML, CSS, and JavaScript. We also learned valuable lessons in troubleshooting database integration. What's next for Smart Parking Availability Tracker: 
The next step is to integrate the database fully into the main page for seamless real-time availability display. We also plan to enhance the UI/UX design to improve user interaction and add more features for better functionality.",,,,,,,"css, custom-api-(flask), flask, git/github, html, javascript, python, qr-code-generator, sqlite","https://subhanshrestha.github.io/new-file/, https://github.com/Subhanshrestha/new-file"
VTHacks 12,Canary,https://devpost.com/software/canary-61wdxj,False,"Logo Extension Content Warning Drawing inspiration from the canary in a coal mine, Canary is designed to detect early signs of potentially distressing or triggering content in digital spaces. Canary is a Chrome browser extension that proactively takes action to block the content and provides a detailed score, empowering users to control the information they are exposed to online. Our project leverages toxic-bert, a 109-million-parameter large language model (LLM) specifically trained to detect and classify offensive content online. We have further fine-tuned the model to effectively parse website text and assign aCanary Scorethat reflects the content's level of sensitive material. In addition to this intelligent evaluation system, users can create a custom keyword list and any occurrence of these keywords will be flagged during Canary's evaluation of a website. By detecting and blocking harmful material before it reaches the user, Canary promotes mental well-being and empowers people to navigate digital spaces with confidence. This not only aids in protecting mental health but also advances Diversity, Equity, and Inclusion (DEI) by fostering a more inclusive and respectful internet environment for everyone.",,,,,,,"ai, css, defang, flask, html5, hugging-face, javascript, llm, python",https://github.com/alegrekev/Canary
VTHacks 12,Signiverse,https://devpost.com/software/signiverse-kubj3g,False,Our inspiration are deaf people or people with limited hearing who have trouble communicating online.,Our website intends to convert hand signs to the English alphabet in real time through video and AI/ML.,Through Python backend and HTML frontend,Effectively connecting frontend to backend,Learning how to use HTML and VSCode & how we completed our first hackathon,"Team-building, time management, commitment, VSCode, HTML, Python",To finish this product and share it with higher authorities to implement at a larger scale.,"html, python",https://github.com/aryap12vt/signiverse
VTHacks 12,HokiePool,https://devpost.com/software/hokiepool,False,"We kept on seeing students on the VT class of 202x stories, as well as fellow hackers at this hackathon, post on social media asking for rides and carpools to and from Virginia Tech. We were inspired to create this website to allow students to search for rides to and from long distances and carpool with one another.","Our website allows Hokies to both find and post rides. Students are able to post if they are either looking for a ride or willing to provide a ride, and other students can look on the website and see what works for them.","For our website we used React, JavaScript, HTML, and CSS for the front end and Python and MongoDB for the backend.",We struggled at first to connect the front end to the back end. We kept on getting invalid POST requests and had to figure out CORS (Cross-origin resource sharing).,We are proud of connecting the front end to the back end and getting our POST and GET requests to work and update the website live.,We learned that teamwork makes the dream work. (We also learned how to make an api and connect the front end to the back end).,We want to continue to develop the website as a group to bring it to campuses nationwide!,"css, fastapi, html, javascript, mongodb, python, react",https://github.com/rbgary0/vthacks12
VTHacks 12,Bites,https://devpost.com/software/bites-1mj09l,False,"People are busy. Not everyone has time to stay updated with the latest news in their interests, whether it be sports, tech, politics, or more. That’s why entertainment has shifted majorly towards short snippets of media, such as tik tok, youtube shorts, instagram reels, etc. However, the information from those sources can’t be particularly trusted, as at the end of the day, they’re forms of social media. We don’t have time to catch up on real news in the same way that we can watch these shorts of entertainment online.","When users first sign up for Bites, they are prompted to select from a wide array of interests, ranging from technology, health, and science to entertainment, sports, and more niche topics. Once their interests are set, Bites curates a personalized feed in the form of short highlights, ensuring that users stay informed about the latest developments in the areas they care about most, while also delivering accurate and well-sourced information. In addition to the curated news feed, Bites takes personalization a step further by using the selected interests to generate a custom daily podcast for each user. This podcast is a short, digestible format designed to fit into users' busy schedules, providing them with a quick yet comprehensive overview of the latest headlines and trends in their selected topics.","We developed the front-end and back-end of Bites simultaneously to satisfy the time constraint. For the front-end, we started by planning the UI in Figma. Although we continued to tweak the design throughout the project, we began translating the ideas into code after we had a rough draft. The framework we chose to use is Flutter, for its performance and variety of features. For the backend, we coded in python, and utilized serpapi to scrape resources from the internet, and OpenAI’s gpt 4 in order to detect truthful information and generate the podcasts. To train each model to detect truthful information, we did 3 shot testing, providing it with three examples with explanations as to why a given article excerpt could be deemed accurate or inaccurate. After these models were trained, the final decision on whether a source was accurate or not was made based on the decision of two LLM’s debating with each other, until an agreement was made. Finally, we used the distinguished truthful key points to generate the podcast.","One of the main challenges we ran into was ensuring that the information provided through Bites was both informative and trustworthy while also keeping it as engaging as popular social media platforms like Tiktok, Instagram, and Youtube. In order to do this, we had to strike the right balance between delivering short, digestible content while also maintaining accuracy and credibility. We developed a self-consistency reasoning algorithm which utilizes multiple LLMs to cross-verify results and ensure consistency of the content. Another issue we ran into was curating personalized news feeds (podcasts).","We are proud of creating a highly personalized experience that tailors news feeds and custom daily podcasts based on each user’s preferences. By leveraging advanced machine learning algorithms and real-time data processing, we’ve built a dynamic system that adapts as the user's interests change. Additionally, we successfully combined the need for high-quality, credible information with the digestible, short-form style popularized by social media.","Throughout the development of Bites, we gained valuable experience in several key areas. First, we deepened our understanding of Flutter, using it to build a responsive mobile application. This framework allowed us to efficiently design a user-friendly interface that meets the needs of users. We gained hands-on experience in LLM models. This was crucial for not only implementing the accuracy checking, but also tailoring the podcasts towards the users interests.","Next, Bites aims to utilize a MongoDB database to scale the application for multiple users, enabling a more dynamic and responsive experience. By open-sourcing this database, the platform can provide researchers with instructions for usage, fostering collaboration and innovation. Additionally, Bites will develop a modernized version of Reddit by employing an algorithm that tackles Reddit's current issues—such as inaccurate moderation and poor content—ensuring higher-quality and more reliable information is presented to users.","figma, flask, flutter, javascript, json, openai, python, requests, serpapi",https://github.com/abhi-sharma0809/VTHacks/tree/main
VTHacks 12,TinDnDr,https://devpost.com/software/lil-python-website,False,"We were inspired by last year's hackathon winner: LifeLens. We wanted to make something similar in the sense that users would input data in a fun way, we would save it, and then output something for them.","TinDnDr matches users with others with whom to play D&D. Users are first prompted to sign in, answer a few questions, and then be matched with other players with whom they could form a party.","Our frontend was built using React, and our backend was built using flask and sql.","We ran into many challenges for this project, as we had to learn about the tools we were using from the beginning.",We are proud that we have working parts to our project.,"We learned a lot during this project, as we learned everything we needed to build it during the development of it.","Should we continue this project, we could add a dnd class selector, a full character sheet generator, and a party former that will automatically match you with other users.","flask, node.js, python, react, sqlalchemy, yarn",https://github.com/witheringcrown/VT-Hacks-12
VTHacks 12,FliporFlop,https://devpost.com/software/fliporflop,False,Flipping houses is becoming increasingly popular. We noticed there is a lack of free and accessible tools to help individuals assess whether a property is truly worth the investment. We wanted to create a platform that simplifies the decision-making process for house flippers.,"Flip or Flop takes the guessing out of real estate flipping by analyzing key factors like property condition, market trends, renovation costs, location, quality of neighborhood and schools. It provides a clear recommendation and rating (1-5) on whether a property is worth buying to flip or if it's better to walk away. By entering a city or zip code you will receive a detailed evaluation, users can make informed investment decisions quickly and confidently.","We built Flip or Flop using a combination of React for the front end and a robust backend API built using Spring boot to handle data processing. The property evaluation algorithm is powered by data analysis techniques, including integrating market data from real estate APIs, renovation cost estimates, and predictive analytics. We also used Recharts for dynamic data visualizations, helping users understand price trends and projections.","One of our biggest challenges was web scraping reliable and up-to-date real estate data. Scraping data, such as listings and property records, dealing with inconsistent data formats, and ensuring that we were complying with web scraping policies.","We're proud of creating functional platform that provides real value to real estate investors. Our biggest accomplishment is developing an algorithm that effectively combines data from multiple sources to give actionable advice. We're also proud of the clean and user-friendly interface, making complex real estate decisions approachable for users.","Throughout this project, we learned a great deal about real estate market dynamics and the complexities involved in predicting investment returns. We also deepened our knowledge of full-stack development and working with datasets. We also gained insights into creating user-centric designs, emphasizing a balance between simplicity and detail in the UI/UX","Next, we plan to enhance Flip or Flop by adding more personalized features, such as allowing users to input their own renovation budgets for more tailored estimates. We also want to integrate more real estate data sources for even greater estimates. We also want to integrate more real estate data sources for even greater accuracy. Expanding to include rental property evaluation as an additional tool in another set, as well as adding features that suggest ideal contractors and financing options for users. Ultimately, our goal is to become the go-to platform for real estate investors at all experience levels.","css, html, java, javascript, python, react, springboot","https://github.com/RaunakC24/FlipOrFlop, https://github.com/RaunakC24/FlipOrFlop"
VTHacks 12,Smiles & Stats GPT,https://devpost.com/software/smiles-stats-gpt,False,,,"Sources & Tools used: WorldHappiness.report, ChatGPT Plus 4.0, Talking.Network for Voice Generation (Emma AI)",,,,,chatgpt,"https://chatgpt.com/g/g-d02JKVLEx-smiles-stats, https://github.com/AAA41107/Smiles-and-Stats/tree/main, https://youtu.be/loFKrfc3Q9M"
VTHacks 12,Studymon,https://devpost.com/software/studymon,False,"In recent years, there has been an uptick in 'gamifying' the learning and working process. With websites and apps such as Kahoot, Plant Focus, and Class Dojo, they have been inspiring users to learn and complete tasks. As students that sometimes struggle to find the motivation to complete work, we decided that it would be great if we could create a solution that motivated us to complete tasks.","Studymon is a website that allows for teachers to create classes and add students to classes. The teachers will be able to manage students and create and delete assignments for the students to do. Upon adding a student to the class, the student will receive a 'starter' pokemon, which they can raise and level up by completing tasks. A student will be able to manage their pokemon and choose which one they want to level up in the given moment. Once an assignment is completed by the student, they will receive experience (XP) that will in turn, help them level up their pokemon once they reach a certain threshold. By not completing the assignments, they will in turn lose that XP.","Front-end: HTML, CSS, JS
Back-end: Python","We had several challenges as we worked on our project. Firstly, a number of teammates left throughout the beginning and middle of the hackathon, leading us in need of more teammates. We also had quite a bit of trouble coming up with our initial idea and then starting to set it into motion, which potentially could have put us behind our peers. In terms of technical issues, some of us had not coded in a while and were thus, a bit rusty with the process and we struggled with the concept of Github and using it as a team to collaborate. API requests were a struggle to get to work and we ultimately were unable to get them to work. We also struggled to connect the front-end to the back-end portion.","We're incredibly proud of our ability to persevere as a team and as individuals. We learned many new skills and overcame several challenges in the creation of this website and project as a whole. Technically, we're incredibly proud of the website and front-end navigation. Despite the inability to connect the back-end, we still managed to include some features and usability.","We learned many things during our 48 hours of coding together. To begin with, we learned how to brainstorm and communicate ideas as a team. We attended several sessions in which we learned things such as how to create AI applications using Microsoft Azure and how to run our own hackathon. We learned how to implement Github into our work process and collaborate as a team using Github. We learned about APIs and how to potentially get the front-end and back-end to work with several different services and softwares available. Lastly, we learned how fun of an experience a hackathon could be.","If we were allotted more time or were more capable in our skills, we would like to implement several features. To begin with, we would like to connect the front-end to the back-end so that the front can receive the data from the back. With this, the front could dynamically create elements using the information and data from the back. We would also like to connect to the Pokemon API to gather our data and sprites for the website. If we were to make this an actual website, we would connect a database and SQL server that would store important user information and login details. Another option would be to connect to the Canvas LMS API to allow teachers to connect their classes and students to Studymon without needing to do too much extra work.","css, flask, github, html, javascript, json, python, vscode",https://github.com/Mena-GX/VTHACKS12
VTHacks 12,Habit Streaks,https://devpost.com/software/habit-streaks,False,Inspired by trying to improve my life through habits. A habit tracker felt like the perfect thing to help keep me accountable.,Tacks habits and how long you continue to do said habit.,"Built in React using JS, TS, Html and CSS.","This was my first project ever using React so I had to learn it from scratch. Furthermore, my knowledge of JS and Html is heavily limited, so I also needed to learn a lot about both of them.","I'm really proud of how much I learned in this project. Before this project, I had 0 experience in React, and came out of it learning so much about front end development.","I learned all about React, JS, Html, CSS and front end development.","Potentially added more views, like a monthly or a yearly to help track habits for even longer.","css, html, javascript, react, ts, typescript",https://github.com/HoldxnGit/Habit-Tracker
VTHacks 12,LifeStack,https://devpost.com/software/lifestack-u7l3nw,False,"We came across a problem for new grads especially. When comparing job offers or possible job offers, it can be difficult to figure out which location or which job you should take. So we decided to create an app that would compare the various costs of lifestyles if an individual were to take one job over the other. The idea grew to include the ability to do research and include all the what-if costs in our lives.","The app takes a location and a job or salary. If a job is inputted rather than a salary, the app will lookup the median income of that job in the location entered and use that. Then a user can add any additional costs, such as pets or medical conditions. This process is repeated a second time, where a user can enter separate data. After this is done, the app takes all the costs and calculates how much left over spending money you'll have once all your costs have been taken into consideration. By taking into consideration the cost of different lifestyles and disabilities, we draw awareness to differences in financial equity. Namely, that two people with the same salary might end up with different amounts of take-home income, depending on their circumstances or lifestyle. This prompts the user to consider the financial perspective of others.","The app is built on React for the frontend and Express.js on the backend. The Express.js is used as our mini API layer to fetch data from the database. For styling on the components Tailwind was used. For the database, we used CockroachDB. The data was found and imported from various sources on the web. Some of the major data sources include:",,,"Express.js was a new backend library for most of us, so it took a little bit to time to get going with it. Most of us also learned how to code some Typescript, a language that while only slightly different from Javascript, can take some getting used to.",There are some UI flickering bugs within LifeStack that can be ironed out in order to level-up the user experience ever so slightly. Additional conditions and data is also something that has tons of room for improvement. The possibilities with unlimited data could be endless. Some ideas for additional data could be:,"cockroachdb, express.js, javascript, postgresql, react, tailwind",https://github.com/didcreetsadgoku500/LifeStack
VTHacks 12,Emerge,https://devpost.com/software/emerge-gt0xi1,False,"The idea for Emerge stemmed from a desire to improve the efficiency and effectiveness of emergency response systems. Witnessing the potential of AI in many domains, we were inspired to build a system that could leverage the power of these new advancements to streamline emergency response.","Emerge takes user reports of emergency incidents and uses AI to quickly analyze these reports, evaluate their urgency, and assigns them an appropriate priority value. Emergency responders can use this information to best deploy their resources where they are needed most. Emerge also provides actionable recommendations for emergency personnel, including safety measures, suggested procedures, and necessary equipment.","Emerge was primarily built using Django, as it provided a high-level python web framework with rapid development. The AI functionality was achieved with the OpenAI Api.",Our greatest challenge was ensuring the accuracy of the priority values. It took a lot of fine tuning to develop a system prompt that lead to dependable and predictable values.,,,,"django, html, javascript, openai, python",https://github.com/aquileselbaum/Emerge
VTHacks 12,InHabit,https://devpost.com/software/inhabit-4eyoz6,False,"CoStar mentioned Real Estate and AI, we wondered how we could integrate both and came up with the idea of making a chatbot that helps you with finding a suitable area to live in based on your conditions, wants, and needs.","INHABIT is an AI-powered platform that combines intelligent chatbot technology with interactive mapping to guide you through the complex process of choosing a neighborhood. Whether you're a student looking for a vibrant community, a family searching for the best schools, or a professional seeking the perfect work-life balance, INHABIT personalizes your search. Our advanced AI analyzes your specific needs—budget, lifestyle preferences, and must-have amenities—and cross-references them with comprehensive neighborhood data. We provide insights on everything from local culture and safety to nearby parks and public transportation. But we don't just give you data; we help you interpret it. Our interactive map feature allows you to visualize and explore potential areas, while our chatbot offers conversational, context-aware guidance every step of the way. With INHABIT, you're not just finding a place to live—you're discovering a community that fits your life. We turn the overwhelming task of relocating into an exciting journey of discovery. Navigate Your Next Neighborhood with INHABIT, and turn your ideal living situation from a dream into a GPS coordinate.",We built it mostly using HTML/CSS and JavaScript for front end and mostly Python for the backend with a lot of libraries.,Integrating the AI into the program was somewhat challenging and took us quite a bit to figure out. We were able to debug and fix most of our other errors relating to formatting errors and UI issues.,"We successfully implemented an AI chatbot and made it work flawlessly.
Integrated a navigational user-friendly map with Mapbox API.","We learned how to integrate front-end and back-end using tools such as Node.js and Python.
We also gained experience using AI chatbot APIs in a real-world scenario.","Further development of features that would make the UX more appealing to the user such as but not limited to automated POI searches, weather data, crime rate data, demographic data, and more.","css, html, javascript, python",https://github.com/varunbudati/InHabit.
VTHacks 12,Lil Munster,https://devpost.com/software/lil-munster,False,"The idea for Lil Munster was born from a desire to support students and professionals in balancing productivity with mental well-being. With increasing workloads and stress, having a digital companion to check in on users, offer mental health breaks, and assist with assignments can be a game-changer. The concept was inspired by the need for personalized digital tools that promote self-care while helping users stay on track.","Lil Munster is a Chrome extension designed to interact with users throughout the day. It features a virtual character that checks in, asking about the user's well-being and encouraging mental health breaks. The character also provides AI-powered homework help, breaking down complex problems and offering tailored solutions to keep users on track with their studies or tasks. It helps manage both productivity and wellness in an engaging and personalized way.","The extension was built using HTML, CSS, and JavaScript for the user interface and front-end functionalities. For the AI-powered assignment help, we integrated Azure OpenAI's language model via an API, allowing the character to assist with a wide range of homework queries. The character animation and interactions were crafted with simple JavaScript logic, making the experience intuitive and responsive.","One of the biggest challenges was creating a seamless balance between the mental health check-ins and the homework help feature as well as combining the game-like aspect with the AI-powered chat box. It also was difficult to animate the ""munster"" as we had to use compatible Sprite sheets that allowed us to great gifs.  It was also difficult to ensure the character could intelligently switch between these two roles while still being user-friendly. Additionally, making sure the AI accurately understood and responded to a variety of academic questions as well as asked mental health checks at an appropriate time","We’re particularly proud of developing a character that not only provides practical support but also fosters a sense of companionship for users. The integration of AI-powered homework assistance was a major milestone, making the extension both fun and functional. We've also received positive feedback on the extension’s interface, which is both user-friendly and engaging.","Throughout the development process, we learned how to better integrate user experience with functional AI solutions. We also deepened our understanding of how digital tools can contribute to mental well-being, reinforcing the importance of empathetic design in tech products.","The next steps include expanding Lil Munster’s capabilities to offer more personalized study schedules and project management assistance. We also plan to integrate more advanced AI features for assignment help, making the tool applicable to a wider range of academic and professional needs. Ultimately, we aim to evolve Lil Munster into a full-fledged productivity and wellness companion that can be customized by users according to their preferences.","azure, css, html, javascript, openai",https://github.com/AlanGonzalez10123/LilMunster
VTHacks 12,ledgerio,https://devpost.com/software/legerio,False,"dashboard to view all your groups add others to your groups by generating unique invite links pay or request to be paid by others to keep track of expenses group view. detailed transactions, overall balance, easy one tap to ""pony up"" or pay entire dues to one person The idea for Ledger.io came from the constant struggle my roommates and I faced trying to track shared utilities and expenses. We needed an easy, organized way to manage who owes what and avoid any confusion.
Ledger.io simplifies group expense management. Users can create or join groups, log expenses, track who owes whom, and settle up—all in one app. It takes the hassle out of managing shared finances.
We built Ledger.io using Next.js for the frontend, Shadcn UI for a sleek interface, Supabase as the backend for database management, and TypeScript for scalability and type safety. We focused on making the user experience intuitive and efficient.
One of the biggest challenges was implementing the group invitation system to ensure users could easily join existing groups. Integrating Supabase smoothly and managing state for multiple users was also a hurdle.
We’re proud of creating a functional, user-friendly app in a short amount of time. Successfully implementing group management and dynamic expense tracking features feels like a big win.
We learned a lot about building full-stack applications quickly, managing real-time updates with Supabase, and collaborating effectively to deliver a polished product under time constraints.
Next, we plan to add features like automatic reminders, payment integrations, and more detailed analytics for expenses. We also want to improve scalability and make the app available for a wider audience!",,,,,,,"css, next.js, postgresql, react, supabase, tailwind, typescript",https://github.com/hessnt30/ledgerio.git
VTHacks 12,Sign-Connect,https://devpost.com/software/nexas,False,To not let deaf people feel Isolated and moreover to not let them feel different among people.,"Sign Language Detection and Word Translation.
Sentence Formation from Detected Words
Speech Output for Detected Signs",,"It was our first time hackathon & we all were new to ML. 
At first, our accuracy was just 0.2 then we reached to 0.4, 0.8 and finally we could make it to 100%.
Dealt with Kernel Crash.
camera not accessible.
Mac and opencv  compatibility issue.","100% accuracy
words to speech Implementation
First time implementation of ML model and being able to complete the project with fulfilling the higher expectation features.","ML models
Experience of being a first time hackathon
2 sleepless night
learnt American sign language","Implement it into hardware  (at public places, easy-to-carry hardware)","flow, lstm, machine-learning, mediapipe, opencv, python, tensor",https://github.com/Nayaab704/Sign-Connect
VTHacks 12,How's My House,https://devpost.com/software/how-s-my-house,False,"Our inspiration was homes.com, Rate my Professor, and Open AI workshop,","Our project provides uses with information about housing around universities, and it does this by using an AI powered chat bot that provides all the needed information to the recipient.","We built this project through react on the frontend and flask on the backend. Also, by using Open AI's API.","The biggest challenge we ran into was mostly time because we had planned to have so much more on our website, but realized that in such a short amount of time it would be unrealistic. Also, we had some other technical issues, like downloading issues, merging branches in GitHub, using the Open AI API, and setting up flask.","We are proud that our project works successfully and that the AI chat bot is running smoothly. Also, we are proud that we accomplished so much in a short amount of time and had to learn many new skills along the way.","Team work, time management, and how to use/configure AI APIs.","What's next for How's My House is continuing in our authentication process which we could not fully complete, building more of our pages that would contain more detailed information about each place, adding rating based of users experiences, and containing to fine tune the AI API.","css, flask, github, html, javascript, openai, python, react",https://github.com/HanzDaoang/vthacks
VTHacks 12,Haloe,https://devpost.com/software/haloe,False,"No one wants to walk home alone, especially at night. That’s why, for this hackathon, our team sought out solutions to the uneasiness and fear one might feel when navigating the streets of a dangerous city. We wanted to take the knowledge and insight that a local of a city might have and give it to any user of our app to help them make informed decisions on how to get home safely.","Haloe is an app that protects its users by finding the safest walkable path from their location to their desired destination. It does this by gathering tens of thousands of crime data points for each city and employing an AI model to determine the most secure route home. Along the way, our AI companion, ‘Loe,’ can talk to the user at their request to help them feel less alone and to ease their worries. Overall, Haloe aims to reduce the stress of those walking alone and helps the time pass faster.","We used React as our front-end, allowing us to get our UI up and running fast so we could focus on the functionality of the app. In order to find the safest route home, we needed crime data. We pulled ~40,000 lines of data from the city of Chicago in order to figure out which streets were safe and which ones were not so safe. Then, using these scores, we could map any route in the city using a maps API, our data, and AI to find the best route. To build ‘Loe,’ we used ChatGPT to feed in the prompts that the user selected. Then, we fed the stringified response into a TTS API to get the final audio.","There were a lot of challenges in building this app, the most prevalent of which was resolving merge conflicts. Working with four people all at once on a project is difficult, and when our editing of files overlapped, it often created many issues that took away precious time. Additionally, using the data to update the route was difficult, and we ended up having to implement our own server with Node JS to fix that issue.","We finished a logically complete app in a span of 48 hours
Our pathing algorithm successfully avoids dangerous areas of Chicago, as verified by locals we know
Our app has a simple, nice looking, and intuitive UI that has minimal button presses after initial registration","Mostly, we learned how to work together as individuals with different skill sets and personalities. We all had never worked together on a hackathon before, and half of us had never even completed a full project for a hackathon before. Also, we are composed of one senior, one junior, and two sophomores, all of various programming levels, so for the more inexperienced of us, we learned how to hit the ground running in a real project and keep our heads above water. And for the more experienced, we learned how to take more of a mentorship role and be patient when mistakes were made.","We did not have access to premium GPT or TTS models in order to make ‘Loe’ feel more real. So naturally, we want to implement the newest technologies in both of these areas (and maybe some nice animations) in order to hammer home the idea of ‘Loe’ being a real companion. With a better model, the stories and jokes will be better. We also want to make individual directions for the user, as opposed to just a path on a map. This will make navigation even easier. Finally, we want to expand this to more cities than just Chicago. Doing this would require massive computing and tens of millions of points of data, but doing this would make the app usable for everyone, and ease the burdens of people all around the world.","microsoft-ai, node.js, open-ai-api, react-native",https://github.com/rcnolan03/haloe
VTHacks 12,hokiesWayHome,https://devpost.com/software/hokieswayhome,False,"Every Hokie runs into this problem at least once in their academic career: where am I going to live next year? popular real estate applications like Apartments.com, Zillow, and more are very powerful, yet can be very overwhelming to a young first time renter. Our mission is to simplify the process and facilitate an easier transition to off-campus living.","Hokies Way Home is a website that lists the popular apartment complexes in the Blacksburg area. It allows users to explore the features of a listing, such as amenities, distance to the nearest grocery store, the number of bedrooms, how close the nearest Blacksburg Transit stop, and etc. For example by clicking the name of Foxridge you'll immediately find that the average listing costs $600 a month, contains two bedrooms, and that a parking pass is not required.","We used Python, HTML, CSS, Flask, Git, Github and MongoDB to create the core of our programming.","We were ambitious for our first hackathon as a group. Where we were not familiar with any of the libraries and most of the languages used. While not necessarily a bad thing, we were not able to fully implement the scope of our project with in the allotted time. A majority of the time was spent researching and testing the necessary tools. That being said, our project is functional and accomplishes the fundamental purpose.","We are proud that we were able to implement dynamic URL routing to adapt to and automatically display an unlimited amount of housing options into our website. By clicking any option, users are lead to a page which dynamically changes to contain the corresponding details. Moreover, we are proud that we were able to learn and execute a Hokies Way Home. Our team had little to no experience using Flask, HTML, CSS, Python, Git, Github, and MongoDB, so we consider the knowledge gained an accomplishment in and of it itself. Furthermore, connecting the front end to our back end proved to be a rewarding endeavor. Watching our website come together proved to be exciting.","Collaborating through Git as a team allowed us to gain experience with team-based programming. We also learned how to delegate tasks, coordinate meetings, set objectives, break down our project into a workable outline, debug live, use git and liveshare functionality, and articulate ideas in a timely manner.","We plan to integrate Google Maps within our project to display housing option locations from the main page. We would also like to add features to enhance the user-experience, such as bookmarks for locations the user would like to save and refer back to.","css, flask, git, github, html, mongodb, python",https://github.com/dtahero/hokiesWayHome/tree/main
VTHacks 12,Nutricampus,https://devpost.com/software/nutricampus,False,"We were inspired to build NutriCampus because of the nutritional challenges we face as students on a college campus. College life is busy, and students often find themselves choosing convenience over nutrition, leading to unhealthy eating habits and a lack of energy.",NutriCampus solves this by aggregating menu information from all campus dining facilities and organizing it based on nutritional content. Then it assesses the users ideal macronutrient ranges and gives them different dining hall options that fit those ranges.,We built the website and algorithms using react js. We got the data using Michigan Dining's open API to get real dining hall meals and locations.,We struggled getting the data from Michigan's API because it had a lot of limitations and restrictions. We also had a hard time figuring out how to work as a team because we all had different coding backgrounds and 3/4 group members knew JavaScript but didn't know React so we had to learn a bit of it.,We were very proud of our ability to find and clean data because the data we got wasn't concise or uniform so we had to re-organize it. We were also proud of our ability to learn React and compromise as a team to each find a role. Everyone worked to their personal strengths and that made us an effective team.,We learned how to work together and communicate effectively as a team throughout the planning phase and the development. We also learned how to use react for those of us that didn't know and learned glitch which we used to collaborate on the project. Most importantly we learned how fun hackathons can be because for most of the group this was our first real hackathon and we all enjoyed it a lot.,The next steps for NutriCampus are to integrate AI to make the recommendations a little bit more personalized and allow users to have more options other than just sliders. We also need to get the product out to Michigan students and get real reviews about the effectiveness of our product.,"css, excel, glitch, html5, postman, react, vscode",https://github.com/Ddundee/nutricampus.git
VTHacks 12,cabinIt,https://devpost.com/software/cabnit,False,"Ever have bad roommates? We have. Finding the right roommate is an important decision that follows you for months, and often times it's a difficult task. We wanted to take the guesswork out and make finding the perfect roommate easier.","By having you answer a questionnaire, cabnIt stores your answers and finds your unique matches by cross-referencing our database for other users who have answered similarly to you","We built it using Figma, React, HTML, CSS, Mongo DB, Express.js and node.js. User information is stored to an associated account which is placed in our databases.",Trying to implement our UX/UI design with the front end.,"Making a funciontional database that stores user account info and using that info to find users who are suitable for each other, and also the chat feature.","We learned how to incorporate UI/UX to our front-end, and how to use Mongo DB to store user data and manipulate that data.","We plan on incorporating a feature that allows you to browse homes with someone you have matched with, and also include chat moderation.","css3, figma, html5, mongodb, node.js, react, vercel","https://github.com/CodeXTL/tinder-for-housing, http://www.cabinit.co"
VTHacks 12,Dr.HowAI,https://devpost.com/software/dr-howai,False,"Three months ago, Vyas was preparing for wrist surgery. When he asked the doctor for a detailed explanation, the response was unclear and difficult to understand. This left Vyas with many unanswered questions. After searching for more information online, he found that the resources lacked the specificity he needed. Even chatbots like ChatGPT didn’t help—some even provided made-up sources. This experience motivated us to create a platform that makes medical information more accessible, breaking down complex yet essential concepts for the average person.","We’ve built a Question-Answer platform where users can ask Dr. HowAI their medical questions. Dr. HowAI consults a curated list of trusted sources—including publications, medical textbooks, and journals—using Multi-Modal Retrieval Augmented Generation (RAG) to generate accurate answers. Users can also hover over groups of sentences in the response to see which source contributed most to that portion. for the utmost transparency, Additionally, users can “like” or “dislike” specific sentence groups, helping Dr. HowAI adjust its future responses by weighting the sources accordingly. Dr.HowAI is also honest - if it is asked about a concept that is not within its sources, it will honestly say that it does not know the answer.","The website was built using React, leveraging its strengths in component reusability, performance, and ease of integration with external APIs. Tailwind CSS was used for efficient styling. The frontend communicates with a Flask API, which acts as a middleman between the user and the AI via HTTP requests. To answer user queries, we implemented Multi-Modal Retrieval Augmented Generation (RAG) alongside a vector database. First, we used CLIP, a visual-language model, to encode our resources—textbooks, articles, and images—into the Chroma vector database. When a user asks a question, it is also encoded by the CLIP model, and we query the encoding in the vector database to find relevant answers. Instead of traditional cosine-similarity matching, we implemented a custom algorithm inspired by Reinforcement Learning. Specifically, we use a variation of the Upper Confidence Bound (UCB) algorithm to determine the weight each source should contribute to the response. The UCB algorithm encourages exploration by prioritizing sources that haven’t been used much, while gradually favoring those that receive positive feedback from users. We then multiply the UCB weighting by the cosine similarity scores to select the best text chunks for the model to use. Finally, we use Claude 3.5 Sonnet to generate intelligent responses based on the selected chunks. If the highest weighting score is below a threshold, the Claude model will be prompted to say that it does not know the answer to a query,.","One of the biggest challenges was integrating our UCB weighting system with the RAG pipeline, as most libraries abstract many of these processes. This required us to gain an in-depth understanding of embedding spaces and transformer self-attention mechanisms in a short period of time. This way, we could integrate our custom vector-querying system with the ChromaDB pipeline. On the front end, designing a user-friendly interface was no easy task. We went through four different iterations of the Question-Answer system, struggling to decide how users should interact with the responses and rate each source.","We are really proud of how we integrated many complex components into a cohesive solution. Writing the custom RAG code was particularly rewarding, and the overall user experience of the website turned out to be smooth and intuitive.","We gained a lot of knowledge in front-end design, particularly with React and Tailwind. On the technical side, we learned about machine learning techniques and tools such as PyTorch, ChromaDB, and multi-modal analysis. We also developed a deeper understanding of the medical field and how to make complex information accessible.","Looking ahead, we aim to integrate more medical sources and potentially crowdsource resources from trusted medical professionals. We also plan to launch the platform online to gather more user feedback, which will help train and improve our weighting algorithm. Dr. HowAI’s mission is to make medical knowledge more transparent, and we are excited to continue that journey beyond this Hackathon.","chroma, machine-learning, nextjs, python, pytorch, react, tailwind",https://github.com/Vainateya/VTHax2024
VTHacks 12,ResumAId,https://devpost.com/software/resumaid,False,"We wanted to help students everywhere, and especially here at VT, as applicants prepare for interviews. Many students have trouble studying for interviews or answering questions thoughtfully, Resumaid will be there to fix that. Alongside this, Resumaid can help filter over tens of thousands of applicants that may apply.",Takes in a applicant's resume and job role description to generate questions to ask either behavioral or technical interview questions in order to help applicants practice and in order for companies to help filter people through a hiring process.,"We used Python, Streamlit, JavaScript, React, Node.js, and Firebase in order to implement a good looking UI/UX and have good Generative AI implementation for questioning.",,,,,"javascript, llm, openai, python, streamlit, venv",https://github.com/vyampally/Resum-AId
VTHacks 12,Recipeasy,https://devpost.com/software/recipeasy-0zonfa,False,"Our inspiration came from solving the problem of trying to share a noodle recipe with a friend, only to have constantly rewrite and correct the recipe to make it understandable.","It is a social media platform based on creating and sharing recipes that you love. In the main feed you will be recommended recipes based on how many of the ingredients you have, and hearted content. Each post has comments, a star rating and a difficulty rating, tags, location, and a story behind the recipe. You get to update the page with what ingredients you currently have and also save your favorite recipes. In each recipe post you can save the ingredients as a checklist to your phone, there is TTS for the blind to follow along, there are dietary restrictions or food allergies or intolerances, that you can enable to see alternative ingredients, links to teach you how to do each step, timer to help you follow along, convert from metric to us. There would be multilingual support, as users could submit in their native languages. Users could create tags or categories that highlight underrepresented cultures and their traditional recipes. For the visually impaired, there would be color contrast mode and dyslexia friendly font options. For any recipes with videos there would be automatic captions and visual aids for those with hearing impairments or cognitive disabilities. To include different cultures, there would be a tag to filter recipes by religious or cultural dietary practices (e.g., Halal, Kosher, Jain, etc.). We would also feature and promote recipes from marginalized or underrepresented communities. There would be diverse chefs from different gender identities, ages, and backgrounds. Allow filtering for recipes based on age group (e.g., kid-friendly, senior-friendly) to be more accessible. Interactive recipes with AI for accessible cooking.",We used HTML and CSS for the frontend and for any functionally we used JavaScript. We couldn't really get most of the ideas working.,Execution and implementation. We had to spend too much time learning and ended up being demotivated by the short deadline.,We all really like the idea and hope it succeeds in the future.,We learned that we should learn the basics of web development as preparation since we spent so much time learning it.,Most likely continued implementations until it is somewhat working properly.,"css3, html5, javascript",https://github.com/MandMcoding/Recipie_VTHacks12
VTHacks 12,Hokie Productivity Insight,https://devpost.com/software/hokie-productivity-insigh,False,"In today's digital age, social media has become a massive part of our lives. While it offers numerous benefits, it can also lead to a loss in productivity. Inspired by the growing concern over digital distraction and its impact on productivity, our team set out to create a tool that can quantify productivity loss based on social media usage. We wanted to provide users with insights into how their social media habits might be affecting their work and overall productivity.","Our Productivity Loss Prediction Web Application helps users understand the impact of their social media usage on productivity. By collecting data through a survey -- which measures daily social media minutes, platform used, addiction level, and reasons for usage -- the app uses a logistic regression model to predict potential productivity loss. Users receive a message directly after submitting the survey informing them of their results.","The logistic regression model was built in Python using sci-kit learn and pandas for data processing. The actual dataset was taken from Kaggle(https://www.kaggle.com/datasets/zeesolver/dark-web?resource=download)  and contained various features relating to social media usage and productivity. Flask, a python library, was employed to link the model to the front-end survey. HTML and CSS were used to design the survey and results page.","We attempted to use GitHub as we knew that people use it for collaborative projects. However, we ran into some issues and decided to stop using it. We were not very experienced in logistic regression. We struggled with the data processing aspect and making sure the dimensions of the dataframe were correct. Transmitting the survey results to the model and back to the front-end was also somewhat difficult and took us some time to figure out.","We managed to achieve an 87.5% accuracy on the training dataset after tinkering with what features to use and how to process the data(one hot encoding, standardization). We were also proud of linking the back-end model to the front-end webpage and achieve a ""full-stack"" application, as none of us had done that on our own before.","We learned how to preprocess data, transfer information between the front-end and back-end with Flask, and work on a programming project in a team.","We would like to improve the model, finetuning it to achieve a higher accuracy. We would also like to spice up the webpages and offer more insight on the final results page. This insight would relate to more personalized information about the user's survey results and tips to increase productivity.","css, flask, github, html, machine-learning, numpy, pandas, python, sklearn, visual-studio",https://github.com/sunakk8/VTHacks12
VTHacks 12,Ballot Pulse,https://devpost.com/software/election-data-inspector,False,"I am a political junkie, and now that it is election season I cannot stop listening to the news and hawking the latest polls. Past election behavior can give a strong indicator to the zeitgeist of the nation and what to expect in the future. I love looking at this information, but I often struggled to quickly find this election data. I'd have to search ""2008 election results Virginia"", and navigate 3 pages to find what I was looking for. This application fixes that nuisance.","Ballot Pulseis a one-stop-shop for all presidential election and election-related data visualizations going back to the 2000 election. You can view results in all kinds of formats such as viewing state-wide results in solid color (like a generic electoral college map). You can enablegradient-modeto gain insight into how close a state or county was. You can view differentials between parties from election to election. And most uniquely, you can compare demographic data such as voter turnout, population, education, and income to how people voted that year. Ballot Pulse gives you insight into past elections like no other election reporting platform.",,,,,"With just a day and a half to build this, I wasn't able to shove every feature I wanted to in. I want to add visualizations for other kinds of data such as race or more unexpected measures such as gasoline prices. I also want to add a feature for users to be able to zoom in on particular states.","census.gov, d3.js, python, react.js, typescript","https://lukedigiovanna.github.io/ballot-pulse/, https://github.com/lukedigiovanna/ballot-pulse"
VTHacks 12,BiteFight,https://devpost.com/software/thunder-dough,False,BiteFight was inspired by the desire to make meal planning more engaging and interactive. We noticed that choosing what to eat can be overwhelming with so many cuisine options available. Our goal was to simplify this process by creating a fun and interactive app that helps users explore different cuisines and discover new recipes based on their preferences.,"BiteFight allows users to battle different cuisine types against each other, such as Italian versus French. Once a user selects a winning cuisine, the app generates recipes based on the chosen cuisine and additional filters like calories and macros. It then presents users with detailed recipe ingredients and step-by-step instructions, making meal planning both easy and enjoyable.","We developed BiteFight using a combination of technologies:
JavaScript & React: For building the user interface and providing a dynamic and responsive experience.
Python & Flask: For handling the backend logic, including recipe generation and processing user inputs.
ChatGPT 3.5 Turbo: To generate and suggest recipes based on selected cuisines and user-defined filters.","User Interface Design: Creating an engaging and intuitive UI that balances functionality with a pleasant user experience. We wanted to make the process of using our app enjoyable and straightforward, so we used motion controls.
Ensuring Accuracy: Making sure that the recipe suggestions met user expectations in terms of dietary preferences and nutritional goals.","Interactive Cuisine Battles: Successfully implemented an engaging feature that allows users to choose their preferred cuisine.
User Experience: Designed a user-friendly interface that simplifies meal planning and recipe discovery.
Interaction Methods: Created a novel way of interacting with webapps and applied it to our problem.
Authentication: We used authentication to provide a scaffold that will allow us to provide user data across devices in the future.","AI Integration: Gained experience in integrating AI to generate user specific information.
Full-Stack Development: Enhanced skills in full-stack development, including frontend and backend integration and data management.
User-Centric Design: Learned the importance of balancing functionality with user experience to create an engaging app.
Team Collaboration: Improved collaboration and communication skills while working on the project with a team.","Enhanced AI Features: Plan to refine and expand the AI algorithms to include more diverse cuisines and personalized recipe recommendations.
User Feedback Integration: Gather and incorporate user feedback to continuously improve the app's functionality and user experience.
Additional Filters: Implement more advanced filtering options to cater to a wider range of dietary preferences and restrictions.","flask, javascript, mongodb, propelauth, python, react","https://bitefight.mbrenn.net/, https://github.com/jwdcs/vthacks2024"
VTHacks 12,Pure Skin AI,https://devpost.com/software/pure-skin-ai-f8l2ji,False,"From the patient perspective, skin health access has been a barrier across many user groups for various reasons: skin health awareness, skincare eudctaion, or easy connection to professionals. From the clinic perspective, often understaffed small businesses also need automation to speed up their workflow.",,,,,,,"airtable, css, html5, javascript, react, teachable-machine, voiceflow",https://github.com/KarstenCole/pure-skin-ai
VTHacks 12,Market Sprout,https://devpost.com/software/market-sprout-y32hkw,False,We wanted to address the intersectionality between accessible technology and healthy foods.,Market Sprout generates personalized meal plans based on financial limitations.,We built Market Sprout based on OpenAI API and a React interface.,We had trouble connecting to the API services and getting results we'd like from the AI.,We are proud of being able to complete our first product in such a short time frame.,We learned a lot about connecting to APIs and using AI.,We would love to expand on the framework we've built for Market Sprout and harnessing the power of AI to make healthy food accessible for everyone.,"chatgpt, node.js, react","https://marketsprout.co/, https://github.com/joelxgalarza/market-sprout.git"
VTHacks 12,AIRE,https://devpost.com/software/aire,False,"Our parents are retiring soon, and in a few decades, so will we. It is important to plan for the future, and we came up with this idea.","This product takes in a tax form that anyone fills out, and a person can ask a question to the model regarding a retirement plan. Then, the AI gives an output that helps the person understand what they should do in order to retire.","We utilized React for the front end and JavaScript for the back end. Furthermore, we utilized Gemini API that incorporates our code along with the user's input.","The main challenges we ran into were the accuracy of Gemini API along with the implementation of Flask. Initially we used Python for the backend and Flask for the frontend, but we couldn't connect the two after finishing the code. Furthermore, the most accurate AI with a free API was Gemini, but it isn't as accurate when compared to other AI models like ChatGPT and Perplexity AI.",We are incredibly proud that our project compiled with no errors even though the accuracy isn't there.,We learned many things throughout this process including utilizing Gemini AI API and implementing data engineering.,"The next steps for AIRE is to utilize ChatGPT's API to improve the accuracy for our prompt, and improving the prompt engineering that was developed in the back end. Furthermore, we can include much more specific outputs when connecting the user's question and our parsing information.","gemini, javascript, react",https://github.com/ishammansoor/RetirementAI
VTHacks 12,Gobbler Guantlet,https://devpost.com/software/gobbler-guantlet,False,"Many sports websites track the stats of players and teams throughout the season. These stats are often interesting to look at. So I thought why not do the same for a tournament that I play every semester with my fellow Hokies? Every semester we play a League of Legends tournament with current students and alumni. I wanted to create a website that would track various interesting stats to look at when it comes to a tournament such as most kills, deaths, assists, and more.",This website will keep track of player stats as the tournament goes on. We update real-time and give the top 5 players in each category.,We used Python for the backend to get all the data from the Riot Games API. Then I sorted the data so that the front end could use it. And we imported all of the needed information into our MongoDB database. Then we used express to set a server to have API end points for the frontend so that the front end can use data. The front end was created using react node.js etc.,One challenge we went through was parsing through the Riot Games API. The API returns extremely large json's which is hard to read and parse through. We also had trouble trying to set up the MongoDB database with multiple users.,Creating a full stack project where we worked as a team. While everyone had their own tasks we still helped each other with things that we were more experienced in. This led us all to have a great time working on a project together.,We learned how to set up a server for MongoDB and use RESTful API's to gather data and put it up on the front end.,"Gobbler Gauntlet is a project that doesn't end here. It has a future we plan on hosting the project and expanding its capabilities to show team stats, match histories, and more. These features will create a much more interesting application for the community to view","express.js, javascript, mongodb, node.js, pymongo, python, react, restful-web-services-unspsc",https://github.com/benha04/hack
VTHacks 12,PantryPlanner,https://devpost.com/software/pantryplanner,False,"As college students, we find being resourceful with our time, money and effort is of utmost importance. Specifically, regarding food, we want to make sure none of our groceries go to waste, and in a pinch we can find things to make with the groceries we already have. 
Additionally, food waste is a massive global problem, contributing to 8-10% of global emissions. We wanted our project to address this issue by helping people find creative ways to use ingredients they already have, reducing waste, and making a positive impact on both a personal and environmental level.","Pantry Planner takes an input of ingredients, and provides up to 10 recipes you can make containing those ingredients. It provides nutrition facts and an ingredient list for the recipe, as well as a link to the more detailed source recipe with instructions. Additionally, Pantry Planner can accommodate to various dietary restrictions and allergies including but not limited to: dairy, gluten, soy, egg, fish and nut allergies, as well as kosher, keto-friendly, halal, vegetarian, high-protein, low-carb diets and more.","Pantry Planner was primarily built using Python and the Edamam API for the backend, and implemented using HTML and CSS for the frontend, with Flask to integrate the two. Specifically, we called the Edamam API endpoint to provide the best match recipes, and the data regarding the recipes before displaying it on the webpage.","We ran into challenges both big and small while building Pantry Planner, but one stood out. Data management was difficult at scale because the API response we were receiving was so large that our terminal was truncating the response. As a result, we found it difficult to extract the data necessary to us as it was difficult to view the hierarchy of the json response. We worked through this by first using the json library to format our data with indentations properly, then tracing the hierarchy to successfully filter our data.","We are most proud of exceeding our expectations regarding the scope of the project. When we began planning the project, we did not anticipate including any nutritional information regarding the recipes, nor did we expect to accommodate for dietary restrictions. Being able to include these in our final product makes us proud because it is greatly expanding the reach of Pantry Planner to the approximately 17% of adults who have dietary restrictions.
Additionally, we are incredibly proud of the work we were able to accomplish on the frontend. Before this weekend, none of us had any experience in frontend development, so we put in a lot of research regarding components of good web design and UIs, and explored dozens of websites to observe everything from text placement and size to fonts to color combinations to button features and more. As a result, we were able to construct a polished webpage as a final result.
Lastly, we are proud of the growth potential of our product. Cooking and grocery shopping are universally 
performed activities, and everyone deals with food waste and lack of creativity regarding meals at some point. By solving a key issue people all over the country and world face every day, Pantry Planner has the ability to positively impact millions. Additionally, 12% of the US population lives below the poverty line, and often they have little control over what food they can make with the groceries they have available at that time. As a result, being able to provide meals that can be readily made with the food they have on hand can be revolutionary for putting food on the table for them and their families.","Working on this project expanded our skillset and taught us valuable lessons in problem-solving and researching. Due to Edamam API currently switching from v1 of their API to v2, we needed to problem-solve regarding issues connecting to the API, as well as inferring the syntax for some of the capabilities that were not yet documented in v2. Additionally, we reinforced our skill of researching and gathering information to create the best possible product by doing extensive research regarding user interfaces and frontend development. Developing these skills helped us work together to create the best possible product.","We hope to expand Pantry Planner's capabilities on a technological front by adding more features to make it more personalizable. Some examples include adding a meal prep/cook time feature which allows you to see upfront how much time making a recipe will take. A couple other ways we hope to expand the capabilities of Pantry Planner is to connect the application to a third party product like Plaid, which can provide an itemized list of what a customer bought on a grocery run. This will allow Edamam API to routinely suggest meal ideas based on what it knows the user has in store at the moment. Along a similar vein, we hope to add a camera feature using YOLOv8 where you can scan a counter full of groceries and it stores these values in a database that Edamam API can pull from to generate recipes.
We would like to also make Pantry Planner available as an iOS and Android app so it is more accessible for those without laptops and home internet connections, as well as added convenience for the camera features and for working in a kitchen. We hope to eventually see Pantry Planner become a widely used app in households to make cooking easier, less wasteful and more accessible for all.","css, edamamapi, flask, html, python",https://github.com/aasthasharm/pantryplanner
VTHacks 12,WeatherWatch,https://devpost.com/software/weatherwatch,False,,,,,,,"We were inspired by the Japan 2011 tsunami and we realized that so many lives could have been saved if people were properly warned and so we wanted to create a weather app that predicts disasters by using past data and weird occurrences from the weather to predict when and where disaster may strike. We used machine learning to predict information that we fetched from the APIS for the upcoming weather. We used Flask as our framework for our backend development language while we used python for the rest of our backend development. Some libraries we used included the sklearn, requests, and pandas. The front-end development was made with HTML and CSS. Some challenges we faced was with the import statements because while we had flask and pandas and requests imported into our computer using Linux, it still not would recognize it in our system so we had to restart several times and redownload Flask and requests. Some accomplishments is that we're proud of is using machine learning  for the first time in our college careers and making it work. Some things we learned was how to use pandas and HTML and CSS while making it work. Perhaps next for WeatherWatch, we will focus on making it an SMS notification system for the people in the relevant area.","css, flask, html, linux, machine-learning, pandas, python",https://github.com/pratheekt72/vthacks24_.git
VTHacks 12,Ticket Racer,https://devpost.com/software/ticket-racer,False,"Inspired by our favorite racing games, we made Ticket Racer to help us collect tickets!",It is a browser-based game where the user can drive around and collect tickets in special locations.,"We used a prebuilt game engine and redesigned significant portions, including custom textures, models, and designs.",Limited experience with 3D modeling and switching projects midway through the competition.,"As many of us were first time hackers, we're proud that we were able to make a submission!","We learned to use many new tools across the worlds of 3d modeling (Blender), web development (Node, Flask), version control (Git), and more!","Adding additional features! Our goal is to make a fully-fledged racing game moving forward, with perhaps just a hint of something more...","blender, css, git, github, html, javascript, typescript, vscode",https://github.com/cmmcirvin/ticket_racer
VTHacks 12,HokieHood,https://devpost.com/software/hokiehood,False,"A desire to help fellow Hokies find affordable, convenient housing. Our goal was to combat food insecurity, financial instability, and transportation limitation.","Our website application allows Virginia Tech students to see housing all across Blacksburg and compare housing options based on their own interests. Any student can determine the importance of food proximity, safety, entertainment, and distance to campus by giving weights to each category. Students can then see scores for any given location to better judge whether or not it is a good fit for them.","We set-up a website using Microsoft Power Pages. We then spent some time researching different API resources, such as Zillow and Apartments.com to see if we could get local housing information directly into our website. With the limited time and access we have, we decided to get data via google map's API as it had information regarding location, reviews, and distance to other interesting points. We then tried to include this information in our website in an easily digestible manner.","Integrating JavaScript with Power Pages was difficult, but we eventually worked around it with jQuery instead. Unfortunately our team didn't have as much time available this weekend to dedicate as we would have liked, but we still feel that we learned a lot. Some of us didn't have a ton of exposure to JS and other web technologies before this point, so troubleshooting dysfunctional code was a challenge!",,"We learned about different technologies we can use in the future for building our website, and gained experience for choosing our provider next time. We gained a stronger understanding of JavaScript, HTML, and Google's JavaScript Maps API. We also know more about the limitations and abilities of the technologies we worked with, and how we can better leverage them in the future.","Given more time, we would further investigate the capabilities of the Google Map's API to further expand the overview of each rental property/agency in the Blacksburg area. In addition, we would reach out to companies like Apartments.com and Zillow to get API access to their public housing data. We would also implement a feature to do side-by-side comparisons of 2-3 locations.","css, github, google, google-maps, html, javascript, jquery, microsoft-copilot, power-pages","https://HokieHood.powerappsportals.com, https://github.com/vjonesb/HokieHood"
VTHacks 12,Hackathon Matcher,https://devpost.com/software/hackathon-matcher,False,"I found it frustrating to repeatedly explain my skills and face ""sorry, we're full"" responses while searching for a hackathon team.",,,,,,,"django, flask, python, sqlite","http://127.0.0.1:5000, https://github.com/yuchuehw/hackathon_matcher/tree/main/src"
VTHacks 12,UniRide,https://devpost.com/software/uniride-ain71m,False,"MongoDB was used to handle form submissions Our team of four first-time hackers was formed with a dual purpose: to have fun, and to learn. On the first night of VTHacks, as we began to become familiar with each other and discussed ideas, we established our collective mentality about the hackathon; we wanted something that would challenge us and require new knowledge, but most importantly, something that each of us would enjoy working on. We decided that we would address public transportation in Blacksburg, an issue that interested all of us. We began by investigating the official Blacksburg Transit website, and identifying useful features relating to public transportation that we could implement of our own accord. Van suggested a feedback form for passengers to fill out, and a myriad of other ideas followed. Not have any website or app development experience between the four of us, we began with Android Studio. Each of us were completely new to website and app development, let alone in a timed, competitive environment. This is what brought our team together, but also what troubled us. We laid out a plan to use React for frontend, and Flask and MongoDB for the backend. Flask was especially valuable, because Python was our strongest language. However, React, and learning JavaScript as a whole, proved to be a bit unreasonable for a 24 hour time period. In the end, we laid an extremely strong foundation, but we spent so much time learning that there was little left to code and bugfix. In the end, we took no issue with this. We accomplished our original goal: we learned about each other, about software, and about coding. While the pressure was frustrating, the overarching experience was wonderful. For a relatively inexperienced team, we areproudof what we have done.",,,,,,,"axios, expo.io, flask, java, javascript, mapquest-nominatim-search, mongodb, overpass-openstreetmap, pymongo, python, react-native",https://github.com/aritro4231/transit.git
VTHacks 12,Looking for Study Group,https://devpost.com/software/looking-for-study-group,False,We wanted to connect students and empower the community by making it simple and easy to form study groups.,You can see and join study groups your peers have made as well as make study groups of your own.,We built it with React Native which is a framework that allows you to develop mobile apps with React and Firebase which is a cloud based database that can store students' data securely.,We ran into challenges with React Native and integrating a database.,We are proud that we made a React Native app with an intuitive UI and a fully functioning data store.,We learned a lot about React Native plus Firebase and how it can be leveraged to make cool applications quickly.,We want to implement more features such as searching for specific groups based on keywords or tags and improve security.,"firebase, javascript, react, react-native",https://github.com/dannychung23/LFSG
VTHacks 12,Rate My Exam,https://devpost.com/software/event-planner-zncxla,False,"The idea for ""Rate My Exam"" emerged from a widespread need for detailed insights into exams from students who have previously taken them. Similar to how ""Rate My Professor"" helps students understand professors' teaching styles, ""Rate My Exam"" aims to provide a platform where students can share and access information about past exams from people who have taken them. This inspiration came from our own experiences struggling to find reliable information about what to expect from exams, leading to the creation of a centralized resource for exam reviews and feedback.",,,,,,,"css, html, javascript, python, sql","http://github.com/adityagarwal23/VTHacks12, https://tinyurl.com/RateMyExam"
VTHacks 12,Meal Gauge,https://devpost.com/software/meal-gauge-pazmix,False,"-The Grubhub service provides useful insight into how crowded dining halls are at any given time -But it doesn’t give the full picture -We use CV on an embedded system to count the number of people in line, in-person, to give an accurate prediction of how long it will be until you get food -By combining the two sources, students can make better choices about what to eat -Built to be low cost and low maintenance, runs entirely on a Raspi","Uses a tensorflow based person detection model running on a Raspberry Pi to determine how many people are in line, and serve that data, along with other useful metrics, to a flask-based website also running on the Pi.",,"-The raspberry pi has much lower RAM and CPU compute power relative to a modern computer -Trying to run the CV, Website, and analysis code all at once is a major struggle -We fixed this by changing our pre-processing pipeline and reducing served framerate","We managed to have both the web server and computation all on-device, including the camera itself. This makes the project low-cost and highly replicable for different situations",,,"cv2, flask, grubhub, python, raspberry-pi, solidworks, tensorflow",https://github.com/blee03/Meal-Gauge
VTHacks 12,Homeview,https://devpost.com/software/homeview,False,"We are a group of close friends who are very passionate about machine learning and its application in real problem spaces. Marketplaces like homes.com and apartments.com have helped millions of consumers to make wise financial and lifestyle decisions by finding a home that is right for them. In addition, Generative Artificial Intelligence (AI) has demonstrated itself to be a powerful technology when used to the benefit of it's users. For our hack,Homeview, we decided to explore the possibilities of how Generative AI could further the goal of bringing equitable housing to low-income communities.","Current real estate marketplace experiences are defined by search queries parameterized by a wide set of criteria. These systems operate well, and could be made even better by integrating Generative AI to assist a variety of users.Large Language Models (LLMs)may be used as a way of increasing accessibility to the platform, saving users' time, and delivering highly personalized suggestions for each user, all through an intuitive chat interface.","In the span of 36 hours, we brainstormed, designed, and implemented a guided real estate marketplace experience as Homeview, a full stack web application. We leverage several real estate datasets with information on house sizes, prices, and amenities, in order to cover the widest range of information a mindful user could possibly want. We are running a custom Python web server on a virtual private server (VPS) hosted in the cloud. We designed an intuitive user interface on our front end with custom styling and graphics in order to make our tool as easy to use as possible. On the backend, we use Open AI's GPT-4o model API, which serves as the core of our user personalization system.  In order to give this AI knowledge of real estate in order to benefit the user, we used a machine learning technique called Retrieval-Augmented Generation (RAG), In order to enable the model to most effectively retrieve real estate data. RAG works by first translating the raw data from dataset formats into vector embeddings, which are a compressed and symbolic form of data storage that the model can understand. We store these vector embeddings in FAISS, Facebook's vector database designed for machine learning applications. Our model can then readily retrieve any particular real estate data with efficiency to provide information to the user. We may sound like a broken record here, but this technology is incredible! It issoexciting to be able to apply higher-level mathematics to problems in the real world.","We wanted to implementMixtral 7B, which is notable for outperforming Meta's Llama 1 and Llama 2 on many benchmarks. Due to computing limitations, we weren't able to query responses fast enough, and instead pivot to using Open AI's GTP-4o model API. We also initially struggled to find the right balance between maximizing the data available to our model and the amount of time we had. There are plenty of datasets describing quantifiable aspects of real estate, but we weren't really satisfied with just getting the numbers. A house might have a great price and the perfect number of bedrooms, but there could be other critical information on, say, a homes.com posting, that a numerical dataset wouldn't account for. We ultimately settled on using an open source solution to aggregate results from many different real estate marketplaces to create a more accurate consensus. We worried that we did not have time to implement such a complex mechanism, but we were able to finish it just in time before the hackathon ended.",,,"We are so proud of what we accomplished in 36 hours, and we also hope to continue working on Homeview to see the impact that Generative AI could have on the real estate industry. We see many potential advancements helping consumers make wise financial decisions and find homes that they fall in love with. Some ideas we want to continue to explore are:","ai, almalinux, artificial-intelligence, bash, caddy, gai, generativeai, javascript, linux, llms, machine-learning, mistral, ml, node.js, propelauth, python, tmux, ubuntu, vim, wsl","https://homeview.studio/, https://github.com/F4llow/Homeview"
VTHacks 12,Apartmate,https://devpost.com/software/apartmate-ix51s9,False,"Moving to college has made me realize how difficult it can be to connect to people, especially when you have to move into a new dorm with someone you've never met. Apartmate tries to change this by allowing you to meet a roommate that you share similar interest with before you move in. Think of it like tinder and Zillow had a baby.",The user enters in there information and the website finds apartments and roommates near them.,I used mongoDB to store the users information and connect them with other users in the data base. I also used googles Geolocation API to find nearby apartments around thee users price range. I also employed Node.js Javascript html and css to build this project. I also hosted the website using heroku but ran into some problems with googles API when trying to employ it. I also tried to implement auth0 but again while trying to employ I ran into many difficulties.,"Getting the website deployed was a very big challenge as many of the APIs gave me trouble. It took a lot of trouble shooting to get the layout correct along with getting the right APIs setup. However, these challenges did allow me to learn a lot and get very uncomfortable.",I'm very proud to get the recommendations and database to work and I'm very glad that I managed to put something out all by myself in such a short amount of time.,Teams are very helpful and having one would've saved me a lot of stress. I also learned to prioritize things that are more important and will be more beneficial in the long run. Not to mention the amount of code I learned. This project SIGNIFICANTLY made be a better programmer.,I would love to make the site more professional and deploy it one day. After working on it for so long I became very passionate about this project and i'm excited to see where it can go.,"css, google-geocoding, heroku, html, javascript, mongodb, node.js","https://github.com/skytip1/istg, https://obscure-spire-98057-6f0dbcf5d4dc.herokuapp.com/, https://github.com/skytip1/istg, https://github.com/skytip1/apartmate"
VTHacks 12,SeiyuuSync,https://devpost.com/software/seiyuusync,False,"When I start a new anime, I'm always curious as to what voice actors have also voiced shows I've already seen. There were a couple of online sites that provided voice actor info, but none of them had the ability to filter the info to just what I've watched. Therefore I decided to use this hackathon as an opportunity to create a basic prototype of my idea.","SeiyuuSync is a desktop application that allows me to look up animes by name search. I can add this anime to my personalMyAnimeListaccount as well as add the voice actor information to the database. I can also compare the selected anime series with all other animes in my personal database, which returns a list of every common voice actor along with their show and character info.",The application is a C# WinForms desktop app developed in Visual Studio. It uses a MongoDB database to cache information about animes and voice actors that I've already watched. It also leverages theMyAnimeListAPI andAniListAPI for anime and voice actor information.,,"As a 2-person team, we weren't sure we could finish this project, especially since we weren't planning to grind at max effort the whole time. However, we finished a working implementation well before the deadline. Even though it's not perfect, it's still a solid prototype and we're both very proud of actually finishing it.",,"As per usual with hackathons, the codebase is an absolute mess. I plan to go back and clean up the code to improve readability and stability. Although the current prototype is functional, there's a lot of UI that can be prettier as well as functional aspects that can be smoother. There's also room for new features, such as:","c#, mongodb, myanimelist, visual-studio",https://github.com/tikkikkit21/SeiyuuSync/
VTHacks 12,SmartCalendar,https://devpost.com/software/smartcalendar-qioyad,False,I was stressed my freshman year of college and I wanted an easier way to manage my schedule.,It helps you plan your upcoming events by analyzing your patterns and free time to help you make smart decisions to stay on top of your responsibilities.,"We used Python and lots of research, as well as Google's API for Google Calendar and Perplexity's API for their advanced LLM to analyze your Google calendar events along with a user description to produce a logical place for your event.","APIs are difficult. We had to do a lot of research to understand the errors we were having with our API calls. Additionally, using an LLM with non-standard input is very difficult and takes extensive prompting, failure, and reprompting to get proper output.","We never knew before this how to use APIs. We never knew how LLMs could be used in a project. This is our first hackathon and we have several API calls, LLM integration, and overall made a working product.","We learned how to use APIs, and smart AI use, how to effectively collaborate with others under time restrictions, how to use an LLM for performing a job, and much more.",We have lots of plans we didn't have time to implement. We'd like to allow users to see a series of several suggestions for possible places to put an event. We'd like the calendar to be able to suggest moving user events around if possible. We'd also like to be able to read other calendars and plans from sites like Canvas to have it plan when you can best do your work. We'd like to have it analyze your previous events and predict how long they would take you.,"github, google, google-calendar, json, perplexity, python, vscode",https://github.com/evang27/SmartCalendar
VTHacks 12,Real Estate Analyzer,https://devpost.com/software/real-estate-analyzer-07jwv2,False,"Our project was inspired by the movieThe Big Short, which explores the 2007-2008 financial crisis. This film highlighted the importance of accurate financial forecasting and analysis, motivating us to create a tool that predicts real estate prices based on various inputs.",,,,,,,"azure, git, github, placesapi, react, rest, sql",https://github.com/lakshya46jain/Real-Estate-Analyzer
VTHacks 12,InclusiveBot,https://devpost.com/software/fhrhjr,False,,,,,,,,"discord, github, node.js, perspectiveapi",https://github.com/miles1744/VT-HACKS-CIT-APP
VTHacks 12,Organic Food FInder,https://devpost.com/software/organic-food-finder,False,"I am into health and neuroscience. 
US has an ongoing epidemic of metabolic diseases.
There is research saying processed foods are stripped of nutrients
organic foods have, altering the metabolism",AI powered search of organic foods based on user's pereference,,Getting APIs,It's working,Web app deployment,Include Map interface.,"beautiful-soup, openai, python",https://github.com/danpartigiano/VTHAX2024
VTHacks 12,SquatMap,https://devpost.com/software/squatmap,False," Our project, ""SquatDown,"" has goal to assist people by giving real-time info on foreclosure bids. This help users find properties with low current bid amounts, showing lower chance for squatting risk. The idea started because many homes are left empty from foreclosures, while at the same time, more people need a place to stay. We wanted to close this gap by making a tool where squatters can find good properties nearby, changing vacant houses into shelters for those who need them. During the progress of this project, we learned many things about how complicated real estate and foreclosure markets can be. We also got practical experience with using mapping APIs, managing live data, and understanding legal and ethical issues related to squatting and property foreclosures. We built ""SquatDown"" using a full stack of modern technologies: Frontend: React together with TypeScript and Tailwind CSS, which help create a responsive and lively user interface. Backend: Node.js to handle API requests and data processing. Map Integration: Use Google Maps API for offering an interactive map. It will show properties and help users locate points of interest in different areas. At first, we wanted to use the Property Radar API for getting detailed information on foreclosures, like how many bids are placed on such properties. But sadly, we faced an issue because our application for using the API got delayed. As a workaround, we used mock data to demonstrate the project's concept and functionality. One big problem we faced was waiting for the approval of Property Radar API. Because it took time, we had to change our plan fast and use fake data instead to show how the API works. Also, adding the map to show live updates based on how users interact was not easy. Even with these hard parts, we managed to make a working model that shows what our idea can do. When we get access to Property Radar API, our main aim is use live data completely to show real-time information about foreclosure properties and their bid status. This will be useful for people searching homes and might help lower the number of empty houses in communities.",,,,,,,"css, google-maps, html, javascript, node.js, react, rest, tailwind, typescript",https://github.com/harshalarakala/squatdown
VTHacks 12,OriGaming,https://devpost.com/software/origaming,False,"Since high school, we have had an interest in Origami. And as CS majors (and ONE Engineering major), we thought we'd try to bring this detailed and intricate paper art form into the digital world.","0rigAmIng is a ""paper-folding simulator"" that tries to mimic the behavior of paper folding in real life.",We built 0rigAmIng fully in Godot Engine 3D.,"Where do we even begin? To start, none of us had any experience in Godot 3D, and one team member was completely new to Godot/Game Development (and hackathons at all!). Every new feature we implemented came with its challenges (for example, we found out that everything in the game engine needed to be a triangle, and origami models can have non-triangular shapes), since we needed to really sit down and think about how paper would fold in real life.","Our first big strides in the project were when we were able to successfully ""triangulate"" a polygon so that they could be folded in the game. Then, we struck out again, getting a working fold routine that would take a line to fold along and apply a fold to the ""paper."" Aside from actual code, we're also proud of our organization. Since this was a pretty rare time that we actually worked together on a coding project, we had to keep our code really organized. Using a feature branch/develop branch scheme, we worked on different features in parallel, creating ""contracts"" between the code that we were working on, and then slowly merging and testing each others' code into our own.","Well, we learned that hackathons are crazy! But the time constraint really gets you thinking hard. Regardless of the outcome, this hackathon has given us more training in teamwork and",,godot,https://github.com/vaaar/OriGaming
VTHacks 12,Ask Hokie,https://devpost.com/software/hokie-plus-plus,False,"Ask HOKIE Problem Statement Technical Framework Database Flowchart Impact Testimonials  The inspiration for developing a chatbot for Virginia Tech came from the desire to enhance the student experience by providing a readily accessible, intelligent assistant. With increasing demands for real-time information and support, we aimed to create a tool that would simplify and streamline access to campus resources, answer frequently asked questions, and offer personalized assistance to students, faculty, and visitors. The Virginia Tech chatbot serves as a virtual assistant designed to:
•⁠  ⁠Provide instant answers to common questions related to admissions, campus facilities, academic programs, and events.
•⁠  ⁠Assist with navigation around campus and offer information about departmental services.
•⁠  ⁠Facilitate student engagement by offering details about extracurricular activities, clubs, and support services.
•⁠  ⁠Provided responses for emergency response in case of natural calamities and criminal activities. Our platform uses Python libraries (openai, pymongo, fastapi, sentence transformers, pydantic, pytorch, requests, and beautifulsoup), MongoDB Atlas, GPT-4-Turbo, PropelAuth, and React. We think the biggest challenge was data collection as Virginia Tech's website has a lot of information, and collecting and processing takes a lot of time. We also faced challenges in integrating multiple modules together. Even after the above-mentioned challenges, we were successfully able to accomplish most of our goals. The biggest accomplishment for us is that we worked with LLMs for the first time and were able to create a good chatbot that is able to answer most questions. We were able to learn a lot of skills over this hackathon. Interacting with other hackers and industry experts we were able to gain a lot knowledge. We attended various workshops and incorporated the methods taught. We learned about LLMs and how RAG can be used to refine the LLM results. We aim to include other VT domains like offcampus.vt.edu and cs.vt.edu. These would help us to make the chatbot more concise and integrated.",,,,,,,"css3, gpt, llm, mlh, mongodb, propelauth, react",https://github.com/mahatokunal/askHokie
VTHacks 12,GrocerAi,https://devpost.com/software/grocerai,False,,,,,,,,"azure, css, html, javascript, langchain, node.js, python, react","https://www.lesstimeshoppingandmoretimeto.study, https://github.com/DaanishTheGreat/GrocerAi"
VTHacks 12,internot.blog,https://devpost.com/software/internot-blog,False,"Inspired by the concept of the 'dead internet theory', and the rising prevalence of LLM powered bots on social media platforms, we set out to create a blog showing what an AI dominated future of the internet would look like.","Internot.blog is a blog website where every post is generated by an LLM, based off of a randomly selected topic. The blog also features AI generated ""ads"" that are created through the use of the image generation AI technique stable diffusion.","We set up and used a locally hosted instance of Meta's Llama3.1-8b model for text generation, and a locally hosted instance of stable diffusion for the image generation. Both models were run in Docker containers for easy spin-up and spin-down.
We wrote a prompt generation and output capture system in Go that utilizes a (somewhat) curated list of subjects and actions to be used in blog post generation.
We then use GitHub actions and the static site generator Hugo to build and deploy the blog from our static blog post files (written in markdown).","Generating images that don't look like complete garbage took a lot of tuning (and oversight).
Getting the LLM to correctly generate post front-matter was also really difficult even with tuning.","The system is able to operate entirely autonomously (by running the generation system as a cronjob), without any human interaction.
We learned a lot about how model prompting and fine tuning works (particularly in regards to image selection)",AI is really good at making up stories about events that didn't happen,We will let it run and generate new content for the forseeable future...,"css3, go, html5, hugo, llama3.1, rust, stable-diffusion","https://internot.blog/, https://github.com/internot-blog"
